{"2024-07-08T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.06192v1","updated":"2024-07-08T17:59:57Z","published":"2024-07-08T17:59:57Z","title":"Multi-Object Hallucination in Vision-Language Models","summary":"  Large vision language models (LVLMs) often suffer from object hallucination,\nproducing objects not present in the given images. While current benchmarks for\nobject hallucination primarily concentrate on the presence of a single object\nclass rather than individual entities, this work systematically investigates\nmulti-object hallucination, examining how models misperceive (e.g., invent\nnonexistent objects or become distracted) when tasked with focusing on multiple\nobjects simultaneously. We introduce Recognition-based Object Probing\nEvaluation (ROPE), an automated evaluation protocol that considers the\ndistribution of object classes within a single image during testing and uses\nvisual referring prompts to eliminate ambiguity. With comprehensive empirical\nstudies and analysis of potential factors leading to multi-object\nhallucination, we found that (1) LVLMs suffer more hallucinations when focusing\non multiple objects compared to a single object. (2) The tested object class\ndistribution affects hallucination behaviors, indicating that LVLMs may follow\nshortcuts and spurious correlations.(3) Hallucinatory behaviors are influenced\nby data-specific factors, salience and frequency, and model intrinsic\nbehaviors. We hope to enable LVLMs to recognize and reason about multiple\nobjects that often occur in realistic visual scenes, provide insights, and\nquantify our progress towards mitigating the issues.\n","authors":["Xuweiyi Chen","Ziqiao Ma","Xuejun Zhang","Sihan Xu","Shengyi Qian","Jianing Yang","David F. Fouhey","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2407.06192v1.pdf","comment":"Accepted to ALVR @ ACL 2024 | Project page:\n  https://multi-object-hallucination.github.io/"},{"id":"http://arxiv.org/abs/2405.14734v2","updated":"2024-07-08T17:55:24Z","published":"2024-05-23T16:01:46Z","title":"SimPO: Simple Preference Optimization with a Reference-Free Reward","summary":"  Direct Preference Optimization (DPO) is a widely used offline preference\noptimization algorithm that reparameterizes reward functions in reinforcement\nlearning from human feedback (RLHF) to enhance simplicity and training\nstability. In this work, we propose SimPO, a simpler yet more effective\napproach. The effectiveness of SimPO is attributed to a key design: using the\naverage log probability of a sequence as the implicit reward. This reward\nformulation better aligns with model generation and eliminates the need for a\nreference model, making it more compute and memory efficient. Additionally, we\nintroduce a target reward margin to the Bradley-Terry objective to encourage a\nlarger margin between the winning and losing responses, further enhancing the\nalgorithm's performance. We compare SimPO to DPO and its latest variants across\nvarious state-of-the-art training setups, including both base and\ninstruction-tuned models like Mistral and Llama3. We evaluated on extensive\ninstruction-following benchmarks, including AlpacaEval 2, MT-Bench, and the\nrecent challenging Arena-Hard benchmark. Our results demonstrate that SimPO\nconsistently and significantly outperforms existing approaches without\nsubstantially increasing response length. Specifically, SimPO outperforms DPO\nby up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our\ntop-performing model, built on Llama3-8B-Instruct, achieves a remarkable 53.7\nlength-controlled win rate on AlpacaEval 2 -- surpassing Claude 3 Opus on the\nleaderboard, and a 36.5 win rate on Arena-Hard -- making it the strongest 8B\nopen-source model.\n","authors":["Yu Meng","Mengzhou Xia","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14734v2.pdf","comment":"Code: https://github.com/princeton-nlp/SimPO. v2 updates: additional\n  baselines (RRHF, SLiC-HF, CPO); a new setting Llama3-Instruct-v0.2 (Appendix\n  G); more analyses (Section 4.4 & Appendix H)"},{"id":"http://arxiv.org/abs/2407.06177v1","updated":"2024-07-08T17:50:00Z","published":"2024-07-08T17:50:00Z","title":"Vision-Language Models under Cultural and Inclusive Considerations","summary":"  Large vision-language models (VLMs) can assist visually impaired people by\ndescribing images from their daily lives. Current evaluation datasets may not\nreflect diverse cultural user backgrounds or the situational context of this\nuse case. To address this problem, we create a survey to determine caption\npreferences and propose a culture-centric evaluation benchmark by filtering\nVizWiz, an existing dataset with images taken by people who are blind. We then\nevaluate several VLMs, investigating their reliability as visual assistants in\na culturally diverse setting. While our results for state-of-the-art models are\npromising, we identify challenges such as hallucination and misalignment of\nautomatic evaluation metrics with human judgment. We make our survey, data,\ncode, and model outputs publicly available.\n","authors":["Antonia Karamolegkou","Phillip Rust","Yong Cao","Ruixiang Cui","Anders SÃ¸gaard","Daniel Hershcovich"],"pdf_url":"https://arxiv.org/pdf/2407.06177v1.pdf","comment":"HuCLLM @ ACL 2024"},{"id":"http://arxiv.org/abs/2407.06172v1","updated":"2024-07-08T17:48:42Z","published":"2024-07-08T17:48:42Z","title":"On Speeding Up Language Model Evaluation","summary":"  Large language models (LLMs) currently dominate the field of natural language\nprocessing (NLP), representing the state-of-the-art across a diverse array of\ntasks. Developing a model of this nature, from training to inference, requires\nmaking numerous decisions which define a combinatorial search problem. For\nexample, selecting the optimal pre-trained LLM, prompt, or hyperparameters to\nattain the best performance for a task often requires evaluating multiple\ncandidates on an entire test set. This exhaustive evaluation can be\ntime-consuming and costly, as both inference and metric computation with LLMs\nare resource-intensive. In this paper, we address the challenge of identifying\nthe best method within a limited budget for evaluating methods on test\nexamples. By leveraging the well-studied multi-armed bandit framework, which\nsequentially selects the next method-example pair to evaluate, our approach,\ncombining multi-armed bandit algorithms with low-rank factorization,\nsignificantly reduces the required resources. Experiments show that our\nalgorithms can identify the top-performing method using only 5-15\\% of the\ntypically needed resources, resulting in an 85-95\\% reduction in cost.\n","authors":["Jin Peng Zhou","Christian K. Belardi","Ruihan Wu","Travis Zhang","Carla P. Gomes","Wen Sun","Kilian Q. Weinberger"],"pdf_url":"https://arxiv.org/pdf/2407.06172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04313v3","updated":"2024-07-08T17:42:41Z","published":"2024-06-06T17:57:04Z","title":"Improving Alignment and Robustness with Circuit Breakers","summary":"  AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that interrupts the models as they respond with harmful outputs\nwith \"circuit breakers.\" Existing techniques aimed at improving alignment, such\nas refusal training, are often bypassed. Techniques such as adversarial\ntraining try to plug these holes by countering specific attacks. As an\nalternative to refusal training and adversarial training, circuit-breaking\ndirectly controls the representations that are responsible for harmful outputs\nin the first place. Our technique can be applied to both text-only and\nmultimodal language models to prevent the generation of harmful outputs without\nsacrificing utility -- even in the presence of powerful unseen attacks.\nNotably, while adversarial robustness in standalone image recognition remains\nan open challenge, circuit breakers allow the larger multimodal system to\nreliably withstand image \"hijacks\" that aim to produce harmful content.\nFinally, we extend our approach to AI agents, demonstrating considerable\nreductions in the rate of harmful actions when they are under attack. Our\napproach represents a significant step forward in the development of reliable\nsafeguards to harmful behavior and adversarial attacks.\n","authors":["Andy Zou","Long Phan","Justin Wang","Derek Duenas","Maxwell Lin","Maksym Andriushchenko","Rowan Wang","Zico Kolter","Matt Fredrikson","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2406.04313v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15233v2","updated":"2024-07-08T17:34:02Z","published":"2023-05-24T15:14:49Z","title":"Cross-lingual QA: A Key to Unlocking In-context Cross-lingual\n  Performance","summary":"  Multilingual large language models (MLLMs) have demonstrated significant\ncross-lingual capabilities through in-context learning. Existing approaches\ntypically construct monolingual few-shot examples, either in the source or\ntarget language. However, translating entire in-context examples into the\ntarget language might compromise contextual integrity and be costly in the case\nof long-context passages. To address this, we introduce Cross-lingual QA, a\ncross-lingual prompting method that translates only the question and answer\nparts, thus reducing translation costs. Experiments on four typologically\ndiverse multilingual benchmarks show that Cross-lingual QA prompting\neffectively stimulates models to elicit their cross-lingual knowledge,\noutperforming prior monolingual few-shot prompting approaches. Furthermore, we\nshow that prompting open-source MLLMs with cross-lingual few-shot examples\nenhances performance as the model scale increases.\n","authors":["Sunkyoung Kim","Dayeon Ki","Yireun Kim","Jinsik Lee"],"pdf_url":"https://arxiv.org/pdf/2305.15233v2.pdf","comment":"Accepted to ICML 2024 Workshop on In-Context Learning"},{"id":"http://arxiv.org/abs/2407.06153v1","updated":"2024-07-08T17:27:17Z","published":"2024-07-08T17:27:17Z","title":"What's Wrong with Your Code Generated by Large Language Models? An\n  Extensive Study","summary":"  The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems.\n","authors":["Shihan Dou","Haoxiang Jia","Shenxi Wu","Huiyuan Zheng","Weikang Zhou","Muling Wu","Mingxu Chai","Jessica Fan","Caishuang Huang","Yunbo Tao","Yan Liu","Enyu Zhou","Ming Zhang","Yuhao Zhou","Yueming Wu","Rui Zheng","Ming Wen","Rongxiang Weng","Jingang Wang","Xunliang Cai","Tao Gui","Xipeng Qiu","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2407.06153v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.06146v1","updated":"2024-07-08T17:19:59Z","published":"2024-07-08T17:19:59Z","title":"Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling\n  Tasks","summary":"  We present and evaluate a method called grammar masking, which is used to\nguide large language models (LLMs) toward producing syntactically correct\nmodels for a given context-free grammar. Prompt engineering methods such as\nfew-shot learning or priming can be used to improve the chances of an LLM\nproducing correct syntax, but the more complex the grammar, the more\ntime-consuming and less promising these methods become. Previous work is\nfocused primarily on the usage of either language model training or prompt\nengineering. In this work, a method is presented that restricts the output to a\ngiven grammar using constrained decoding to ensure the output adheres to a\nvalid syntax. We use several DSLs built with MontiCore and task multiple LLMs\nto produce models with and without constrained decoding. A corresponding parser\nis used to confirm the syntactic correctness of each model. We show that\ngrammar masking can dramatically improve the modeling capabilities of several\nLLMs, reducing the need for well-refined prompting while increasing the chance\nof producing correct models.\n","authors":["Lukas Netz","Jan Reimar","Bernhard Rumpe"],"pdf_url":"https://arxiv.org/pdf/2407.06146v1.pdf","comment":"Preprint to be published in the MODELS Workshop \"MDE Intelligence\""},{"id":"http://arxiv.org/abs/2407.06135v1","updated":"2024-07-08T17:08:02Z","published":"2024-07-08T17:08:02Z","title":"ANOLE: An Open, Autoregressive, Native Large Multimodal Models for\n  Interleaved Image-Text Generation","summary":"  Previous open-source large multimodal models (LMMs) have faced several\nlimitations: (1) they often lack native integration, requiring adapters to\nalign visual representations with pre-trained large language models (LLMs); (2)\nmany are restricted to single-modal generation; (3) while some support\nmultimodal generation, they rely on separate diffusion models for visual\nmodeling and generation. To mitigate these limitations, we present Anole, an\nopen, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build Anole from Meta AI's Chameleon, adopting an innovative\nfine-tuning strategy that is both data-efficient and parameter-efficient. Anole\ndemonstrates high-quality, coherent multimodal generation capabilities. We have\nopen-sourced our model, training framework, and instruction tuning data.\n","authors":["Ethan Chern","Jiadi Su","Yan Ma","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.06135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06112v1","updated":"2024-07-08T16:48:48Z","published":"2024-07-08T16:48:48Z","title":"Enhancing Language Model Rationality with Bi-Directional Deliberation\n  Reasoning","summary":"  This paper introduces BI-Directional DEliberation Reasoning (BIDDER), a novel\nreasoning approach to enhance the decision rationality of language models.\nTraditional reasoning methods typically rely on historical information and\nemploy uni-directional (left-to-right) reasoning strategy. This lack of\nbi-directional deliberation reasoning results in limited awareness of potential\nfuture outcomes and insufficient integration of historical context, leading to\nsuboptimal decisions. BIDDER addresses this gap by incorporating principles of\nrational decision-making, specifically managing uncertainty and predicting\nexpected utility. Our approach involves three key processes: Inferring hidden\nstates to represent uncertain information in the decision-making process from\nhistorical data; Using these hidden states to predict future potential states\nand potential outcomes; Integrating historical information (past contexts) and\nlong-term outcomes (future contexts) to inform reasoning. By leveraging\nbi-directional reasoning, BIDDER ensures thorough exploration of both past and\nfuture contexts, leading to more informed and rational decisions. We tested\nBIDDER's effectiveness in two well-defined scenarios: Poker (Limit Texas\nHold'em) and Negotiation. Our experiments demonstrate that BIDDER significantly\nimproves the decision-making capabilities of LLMs and LLM agents.\n","authors":["Yadong Zhang","Shaoguang Mao","Wenshan Wu","Yan Xia","Tao Ge","Man Lan","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.06112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05540v2","updated":"2024-07-08T16:39:35Z","published":"2024-06-08T18:11:30Z","title":"A Fine-tuning Dataset and Benchmark for Large Language Models for\n  Protein Understanding","summary":"  The parallels between protein sequences and natural language in their\nsequential structures have inspired the application of large language models\n(LLMs) to protein understanding. Despite the success of LLMs in NLP, their\neffectiveness in comprehending protein sequences remains an open question,\nlargely due to the absence of datasets linking protein sequences to descriptive\ntext. Researchers have then attempted to adapt LLMs for protein understanding\nby integrating a protein sequence encoder with a pre-trained LLM. However, this\nadaptation raises a fundamental question: \"Can LLMs, originally designed for\nNLP, effectively comprehend protein sequences as a form of language?\" Current\ndatasets fall short in addressing this question due to the lack of a direct\ncorrelation between protein sequences and corresponding text descriptions,\nlimiting the ability to train and evaluate LLMs for protein understanding\neffectively. To bridge this gap, we introduce ProteinLMDataset, a dataset\nspecifically designed for further self-supervised pretraining and supervised\nfine-tuning (SFT) of LLMs to enhance their capability for protein sequence\ncomprehension. Specifically, ProteinLMDataset includes 17.46 billion tokens for\npretraining and 893,000 instructions for SFT. Additionally, we present\nProteinLMBench, the first benchmark dataset consisting of 944 manually verified\nmultiple-choice questions for assessing the protein understanding capabilities\nof LLMs. ProteinLMBench incorporates protein-related details and sequences in\nmultiple languages, establishing a new standard for evaluating LLMs' abilities\nin protein comprehension. The large language model InternLM2-7B, pretrained and\nfine-tuned on the ProteinLMDataset, outperforms GPT-4 on ProteinLMBench,\nachieving the highest accuracy score.\n","authors":["Yiqing Shen","Zan Chen","Michail Mamalakis","Luhan He","Haiyang Xia","Tianbin Li","Yanzhou Su","Junjun He","Yu Guang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.05540v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06098v1","updated":"2024-07-08T16:38:31Z","published":"2024-07-08T16:38:31Z","title":"Epistemological Bias As a Means for the Automated Detection of\n  Injustices in Text","summary":"  Injustice occurs when someone experiences unfair treatment or their rights\nare violated and is often due to the presence of implicit biases and prejudice\nsuch as stereotypes. The automated identification of injustice in text has\nreceived little attention, due in part to the fact that underlying implicit\nbiases or stereotypes are rarely explicitly stated and that instances often\noccur unconsciously due to the pervasive nature of prejudice in society. Here,\nwe describe a novel framework that combines the use of a fine-tuned BERT-based\nbias detection model, two stereotype detection models, and a lexicon-based\napproach to show that epistemological biases (i.e., words, which presupposes,\nentails, asserts, hedges, or boosts text to erode or assert a person's capacity\nas a knower) can assist with the automatic detection of injustice in text. The\nnews media has many instances of injustice (i.e. discriminatory narratives),\nthus it is our use case here. We conduct and discuss an empirical qualitative\nresearch study which shows how the framework can be applied to detect\ninjustices, even at higher volumes of data.\n","authors":["Kenya Andrews","Lamogha Chiazor"],"pdf_url":"https://arxiv.org/pdf/2407.06098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06089v1","updated":"2024-07-08T16:29:08Z","published":"2024-07-08T16:29:08Z","title":"Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in\n  the Era of Large Language Models","summary":"  The remarkable success of Large Language Models (LLMs) has ushered natural\nlanguage processing (NLP) research into a new era. Despite their diverse\ncapabilities, LLMs trained on different corpora exhibit varying strengths and\nweaknesses, leading to challenges in maximizing their overall efficiency and\nversatility. To address these challenges, recent studies have explored\ncollaborative strategies for LLMs. This paper provides a comprehensive overview\nof this emerging research area, highlighting the motivation behind such\ncollaborations. Specifically, we categorize collaborative strategies into three\nprimary approaches: Merging, Ensemble, and Cooperation. Merging involves\nintegrating multiple LLMs in the parameter space. Ensemble combines the outputs\nof various LLMs. Cooperation} leverages different LLMs to allow full play to\ntheir diverse capabilities for specific tasks. We provide in-depth\nintroductions to these methods from different perspectives and discuss their\npotential applications. Additionally, we outline future research directions,\nhoping this work will catalyze further studies on LLM collaborations and paving\nthe way for advanced NLP applications.\n","authors":["Jinliang Lu","Ziliang Pang","Min Xiao","Yaochen Zhu","Rui Xia","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.06089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17987v2","updated":"2024-07-08T16:16:20Z","published":"2024-06-26T00:00:45Z","title":"Multi-step Inference over Unstructured Data","summary":"  The advent of Large Language Models (LLMs) and Generative AI has\nrevolutionized natural language applications across various domains. However,\nhigh-stakes decision-making tasks in fields such as medical, legal and finance\nrequire a level of precision, comprehensiveness, and logical consistency that\npure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to\ndeliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI\nplatform to tackle these problems. The platform integrates fine-tuned LLMs for\nknowledge extraction and alignment with a robust symbolic reasoning engine for\nlogical inference, planning and interactive constraint solving. We describe\nCora, a Collaborative Research Assistant built on this platform, that is\ndesigned to perform complex research and discovery tasks in high-stakes\ndomains. This paper discusses the multi-step inference challenges inherent in\nsuch domains, critiques the limitations of existing LLM-based methods, and\ndemonstrates how Cora's neuro-symbolic approach effectively addresses these\nissues. We provide an overview of the system architecture, key algorithms for\nknowledge extraction and formal reasoning, and present preliminary evaluation\nresults that highlight Cora's superior performance compared to well-known LLM\nand RAG baselines.\n","authors":["Aditya Kalyanpur","Kailash Saravanakumar","Victor Barres","CJ McFate","Lori Moon","Nati Seifu","Maksim Eremeev","Jose Barrera","Eric Brown","David Ferrucci"],"pdf_url":"https://arxiv.org/pdf/2406.17987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06071v1","updated":"2024-07-08T16:13:42Z","published":"2024-07-08T16:13:42Z","title":"From Loops to Oops: Fallback Behaviors of Language Models Under\n  Uncertainty","summary":"  Large language models (LLMs) often exhibit undesirable behaviors, such as\nhallucinations and sequence repetitions. We propose to view these behaviors as\nfallbacks that models exhibit under uncertainty, and investigate the connection\nbetween them. We categorize fallback behaviors -- sequence repetitions,\ndegenerate text, and hallucinations -- and extensively analyze them in models\nfrom the same family that differ by the amount of pretraining tokens, parameter\ncount, or the inclusion of instruction-following training. Our experiments\nreveal a clear and consistent ordering of fallback behaviors, across all these\naxes: the more advanced an LLM is (i.e., trained on more tokens, has more\nparameters, or instruction-tuned), its fallback behavior shifts from sequence\nrepetitions, to degenerate text, and then to hallucinations. Moreover, the same\nordering is observed throughout a single generation, even for the\nbest-performing models; as uncertainty increases, models shift from generating\nhallucinations to producing degenerate text and then sequence repetitions.\nLastly, we demonstrate that while common decoding techniques, such as random\nsampling, might alleviate some unwanted behaviors like sequence repetitions,\nthey increase harder-to-detect hallucinations.\n","authors":["Maor Ivgi","Ori Yoran","Jonathan Berant","Mor Geva"],"pdf_url":"https://arxiv.org/pdf/2407.06071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00557v3","updated":"2024-07-08T16:02:18Z","published":"2024-05-01T15:06:05Z","title":"Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and\n  Expert Mixtures in Self-Alignment","summary":"  As the capabilities of large language models (LLMs) have expanded\ndramatically, aligning these models with human values presents a significant\nchallenge. Traditional alignment strategies rely heavily on human intervention,\nsuch as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human\nFeedback (RLHF), or on the self-alignment capacities of LLMs, which usually\nrequire a strong LLM's emergent ability to improve its original bad answer. To\naddress these challenges, we propose a novel self-alignment method that\nutilizes a Chain of Thought (CoT) approach, termed AlignCoT. This method\nencompasses stages of Question Analysis, Answer Guidance, and Safe Answer\nproduction. It is designed to enable LLMs to generate high-quality, safe\nresponses throughout various stages of their development. Furthermore, we\nintroduce the Mixture of insighTful Experts (MoTE) architecture, which applies\nmixture of experts to enhance each component of the AlignCoT process, markedly\nincreasing alignment efficiency. The MoTE approach not only outperforms\nexisting methods in aligning LLMs with human values but also highlights the\nbenefits of using self-generated data, revealing the dual benefits of improved\nalignment and training efficiency.\n","authors":["Zhili Liu","Yunhao Gou","Kai Chen","Lanqing Hong","Jiahui Gao","Fei Mi","Yu Zhang","Zhenguo Li","Xin Jiang","Qun Liu","James T. Kwok"],"pdf_url":"https://arxiv.org/pdf/2405.00557v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06057v1","updated":"2024-07-08T15:59:44Z","published":"2024-07-08T15:59:44Z","title":"Variational Best-of-N Alignment","summary":"  Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on a controlled\ngeneration task suggest that while variational BoN is not as effective as BoN\nin aligning language models, it is close to BoN performance as vBoN appears\nmore often on the Pareto frontier of reward and KL divergence compared to\nmodels trained with KL-constrained RL objective.\n","authors":["Afra Amini","Tim Vieira","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2407.06057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06048v1","updated":"2024-07-08T15:51:37Z","published":"2024-07-08T15:51:37Z","title":"Vision-Braille: An End-to-End Tool for Chinese Braille Image-to-Text\n  Translation","summary":"  Visually impaired people are a large group who can only use braille for\nreading and writing. However, the lack of special educational resources is the\nbottleneck for educating them. Educational equity is a reflection of the level\nof social civilization, cultural equality, and individual dignity. Facilitating\nand improving lifelong learning channels for the visually impaired is of great\nsignificance. Their written braille homework or exam papers cannot be\nunderstood by sighted teachers, because of the lack of a highly accurate\nbraille translation system, especially in Chinese which has tone marks. braille\nwriters often omit tone marks to save space, leading to confusion when braille\nwith the same consonants and vowels is translated into Chinese. Previous\nalgorithms were insufficient in extracting contextual information, resulting in\nlow accuracy of braille translations into Chinese. This project informatively\nfine-tuned the mT5 model with an Encoder-decoder architecture for braille to\nChinese character conversion. This research created a training set of braille\nand corresponding Chinese text from the Leipzig Corpora. This project\nsignificantly reduced the confusion in braille, achieving $62.4$ and $62.3$\nBLEU scores in the validation and test sets, with a curriculum learning\nfine-tuning method. By incorporating the braille recognition algorithm, this\nproject is the first publicly available braille translation system and can\nbenefit lots of visually impaired students and families who are preparing for\nthe Chinese College Test and help to propel their college dreams in the future.\nThere is a demo on our homepage\\footnote{\\url{https://vision-braille.com/}}.\n","authors":["Alan Wu","Ye Yuan","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.06048v1.pdf","comment":"This paper is submitted to NeurIPS 2024 High School Project Track"},{"id":"http://arxiv.org/abs/2407.06041v1","updated":"2024-07-08T15:37:51Z","published":"2024-07-08T15:37:51Z","title":"MST5 -- Multilingual Question Answering over Knowledge Graphs","summary":"  Knowledge Graph Question Answering (KGQA) simplifies querying vast amounts of\nknowledge stored in a graph-based model using natural language. However, the\nresearch has largely concentrated on English, putting non-English speakers at a\ndisadvantage. Meanwhile, existing multilingual KGQA systems face challenges in\nachieving performance comparable to English systems, highlighting the\ndifficulty of generating SPARQL queries from diverse languages. In this\nresearch, we propose a simplified approach to enhance multilingual KGQA systems\nby incorporating linguistic context and entity information directly into the\nprocessing pipeline of a language model. Unlike existing methods that rely on\nseparate encoders for integrating auxiliary information, our strategy leverages\na single, pretrained multilingual transformer-based language model to manage\nboth the primary input and the auxiliary data. Our methodology significantly\nimproves the language model's ability to accurately convert a natural language\nquery into a relevant SPARQL query. It demonstrates promising results on the\nmost recent QALD datasets, namely QALD-9-Plus and QALD-10. Furthermore, we\nintroduce and evaluate our approach on Chinese and Japanese, thereby expanding\nthe language diversity of the existing datasets.\n","authors":["Nikit Srivastava","Mengshi Ma","Daniel Vollmers","Hamada Zahera","Diego Moussallem","Axel-Cyrille Ngonga Ngomo"],"pdf_url":"https://arxiv.org/pdf/2407.06041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06027v1","updated":"2024-07-08T15:25:33Z","published":"2024-07-08T15:25:33Z","title":"PAS: Data-Efficient Plug-and-Play Prompt Augmentation System","summary":"  In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering.\n","authors":["Miao Zheng","Hao Liang","Fan Yang","Haoze Sun","Tianpeng Li","Lingchu Xiong","Yan Zhang","Yozhen Wu","Kun Li","Yanjun Sheng","Mingan Lin","Tao Zhang","Guosheng Dong","Yujing Qiao","Kun Fang","Weipeng Chen","Bin Cui","Wentao Zhang","Zenan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.06027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06023v1","updated":"2024-07-08T15:17:46Z","published":"2024-07-08T15:17:46Z","title":"Distilling System 2 into System 1","summary":"  Large language models (LLMs) can spend extra compute during inference to\ngenerate intermediate thoughts, which helps to produce better final responses.\nSince Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have\nbeen proposed such as Rephrase and Respond (Deng et al., 2023a), System 2\nAttention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,\n2023). In this work we investigate self-supervised methods to ``compile''\n(distill) higher quality outputs from System 2 techniques back into LLM\ngenerations without intermediate reasoning token sequences, as this reasoning\nhas been distilled into System 1. We show that several such techniques can be\nsuccessfully distilled, resulting in improved results compared to the original\nSystem 1 performance, and with less inference cost than System 2. We posit that\nsuch System 2 distillation will be an important feature of future continually\nlearning AI systems, enabling them to focus System 2 capabilities on the\nreasoning tasks that they cannot yet do well.\n","authors":["Ping Yu","Jing Xu","Jason Weston","Ilia Kulikov"],"pdf_url":"https://arxiv.org/pdf/2407.06023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06011v1","updated":"2024-07-08T15:04:21Z","published":"2024-07-08T15:04:21Z","title":"Igea: a Decoder-Only Language Model for Biomedical Text Generation in\n  Italian","summary":"  The development of domain-specific language models has significantly advanced\nnatural language processing applications in various specialized fields,\nparticularly in biomedicine. However, the focus has largely been on\nEnglish-language models, leaving a gap for less-resourced languages such as\nItalian. This paper introduces Igea, the first decoder-only language model\ndesigned explicitly for biomedical text generation in Italian. Built on the\nMinerva model and continually pretrained on a diverse corpus of Italian medical\ntexts, Igea is available in three model sizes: 350 million, 1 billion, and 3\nbillion parameters. The models aim to balance computational efficiency and\nperformance, addressing the challenges of managing the peculiarities of medical\nterminology in Italian. We evaluate Igea using a mix of in-domain biomedical\ncorpora and general-purpose benchmarks, highlighting its efficacy and retention\nof general knowledge even after the domain-specific training. This paper\ndiscusses the model's development and evaluation, providing a foundation for\nfuture advancements in Italian biomedical NLP.\n","authors":["Tommaso Mario Buonocore","Simone Rancati","Enea Parimbelli"],"pdf_url":"https://arxiv.org/pdf/2407.06011v1.pdf","comment":"6 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2407.06004v1","updated":"2024-07-08T14:58:29Z","published":"2024-07-08T14:58:29Z","title":"Perceptions to Beliefs: Exploring Precursory Inferences for Theory of\n  Mind in Large Language Models","summary":"  While humans naturally develop theory of mind (ToM), the capability to\nunderstand other people's mental states and beliefs, state-of-the-art large\nlanguage models (LLMs) underperform on simple ToM benchmarks. We posit that we\ncan extend our understanding of LLMs' ToM abilities by evaluating key human ToM\nprecursors -- perception inference and perception-to-belief inference -- in\nLLMs. We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate\nthese precursory inferences for ToM in LLMs by annotating characters'\nperceptions on ToMi and FANToM, respectively. Our evaluation of eight\nstate-of-the-art LLMs reveals that the models generally perform well in\nperception inference while exhibiting limited capability in\nperception-to-belief inference (e.g., lack of inhibitory control). Based on\nthese results, we present PercepToM, a novel ToM method leveraging LLMs' strong\nperception inference capability while supplementing their limited\nperception-to-belief inference. Experimental results demonstrate that PercepToM\nsignificantly enhances LLM's performance, especially in false belief scenarios.\n","authors":["Chani Jung","Dongkwan Kim","Jiho Jin","Jiseon Kim","Yeon Seonwoo","Yejin Choi","Alice Oh","Hyunwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.06004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04472v2","updated":"2024-07-08T14:50:49Z","published":"2024-07-05T12:42:31Z","title":"EventChat: Implementation and user-centric evaluation of a large\n  language model-driven conversational recommender system for exploring leisure\n  events in an SME context","summary":"  Large language models (LLMs) present an enormous evolution in the strategic\npotential of conversational recommender systems (CRS). Yet to date, research\nhas predominantly focused upon technical frameworks to implement LLM-driven\nCRS, rather than end-user evaluations or strategic implications for firms,\nparticularly from the perspective of a small to medium enterprises (SME) that\nmakeup the bedrock of the global economy. In the current paper, we detail the\ndesign of an LLM-driven CRS in an SME setting, and its subsequent performance\nin the field using both objective system metrics and subjective user\nevaluations. While doing so, we additionally outline a short-form revised\nResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly\nevolving field. Our results reveal good system performance from a user\nexperience perspective (85.5% recommendation accuracy) but underscore latency,\ncost, and quality issues challenging business viability. Notably, with a median\ncost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and\nresponse time emerge as crucial areas for achieving a more user-friendly and\neconomically viable LLM-driven CRS for SME settings. One major driver of these\ncosts is the use of an advanced LLM as a ranker within the retrieval-augmented\ngeneration (RAG) technique. Our results additionally indicate that relying\nsolely on approaches such as Prompt-based learning with ChatGPT as the\nunderlying LLM makes it challenging to achieve satisfying quality in a\nproduction environment. Strategic considerations for SMEs deploying an\nLLM-driven CRS are outlined, particularly considering trade-offs in the current\ntechnical landscape.\n","authors":["Hannes Kunstmann","Joseph Ollier","Joel Persson","Florian von Wangenheim"],"pdf_url":"https://arxiv.org/pdf/2407.04472v2.pdf","comment":"27 pages, 3 tables, 5 figures, pre-print manuscript, updated version\n  of manuscript due to typo (previous version, Figure 5 was incorrectly named\n  Figure 6)"},{"id":"http://arxiv.org/abs/2309.11508v2","updated":"2024-07-08T14:28:41Z","published":"2023-09-09T22:25:56Z","title":"Towards LLM-based Autograding for Short Textual Answers","summary":"  Grading exams is an important, labor-intensive, subjective, repetitive, and\nfrequently challenging task. The feasibility of autograding textual responses\nhas greatly increased thanks to the availability of large language models\n(LLMs) such as ChatGPT and the substantial influx of data brought about by\ndigitalization. However, entrusting AI models with decision-making roles raises\nethical considerations, mainly stemming from potential biases and issues\nrelated to generating false information. Thus, in this manuscript, we provide\nan evaluation of a large language model for the purpose of autograding, while\nalso highlighting how LLMs can support educators in validating their grading\nprocedures. Our evaluation is targeted towards automatic short textual answers\ngrading (ASAG), spanning various languages and examinations from two distinct\ncourses. Our findings suggest that while \"out-of-the-box\" LLMs provide a\nvaluable tool to provide a complementary perspective, their readiness for\nindependent automated grading remains a work in progress, necessitating human\noversight.\n","authors":["Johannes Schneider","Bernd Schenk","Christina Niklaus"],"pdf_url":"https://arxiv.org/pdf/2309.11508v2.pdf","comment":"Proceedings of the 16th International Conference on Computer\n  Supported Education (CSEDU 2024)"},{"id":"http://arxiv.org/abs/2406.18682v2","updated":"2024-07-08T14:26:16Z","published":"2024-06-26T18:39:08Z","title":"The Multilingual Alignment Prism: Aligning Global and Local Preferences\n  to Reduce Harm","summary":"  A key concern with the concept of \"alignment\" is the implicit question of\n\"alignment to what?\". AI systems are increasingly used across the world, yet\nsafety alignment is often focused on homogeneous monolingual settings.\nAdditionally, preference training and safety measures often overfit to harms\ncommon in Western-centric datasets. Here, we explore the viability of different\nalignment approaches when balancing dual objectives: addressing and optimizing\nfor a non-homogeneous set of languages and cultural preferences while\nminimizing both global and local harms. We collect the first set of human\nannotated red-teaming prompts in different languages distinguishing between\nglobal and local harm, which serve as a laboratory for understanding the\nreliability of alignment techniques when faced with preference distributions\nthat are non-stationary across geographies and languages. While this setting is\nseldom covered by the literature to date, which primarily centers on English\nharm mitigation, it captures real-world interactions with AI systems around the\nworld. We establish a new precedent for state-of-the-art alignment techniques\nacross 6 languages with minimal degradation in general performance. Our work\nprovides important insights into cross-lingual transfer and novel optimization\napproaches to safeguard AI systems designed to serve global populations.\n","authors":[" Aakanksha","Arash Ahmadian","Beyza Ermis","Seraphina Goldfarb-Tarrant","Julia Kreutzer","Marzieh Fadaee","Sara Hooker"],"pdf_url":"https://arxiv.org/pdf/2406.18682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05975v1","updated":"2024-07-08T14:18:28Z","published":"2024-07-08T14:18:28Z","title":"LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation\n  Capabilities Beyond 100 Languages","summary":"  Large Language Models~(LLMs) demonstrate remarkable translation capabilities\nin high-resource language tasks, yet their performance in low-resource\nlanguages is hindered by insufficient multilingual data during pre-training. To\naddress this, we dedicate 35,000 A100-SXM4-80GB GPU hours in conducting\nextensive multilingual continual pre-training on the LLaMA series models,\nenabling translation support across more than 100 languages. Through a\ncomprehensive analysis of training strategies, such as vocabulary expansion and\ndata augmentation, we develop LLaMAX. Remarkably, without sacrificing its\ngeneralization ability, LLaMAX achieves significantly higher translation\nperformance compared to existing open-source LLMs~(by more than 10 spBLEU\npoints) and performs on-par with specialized translation model~(M2M-100-12B) on\nthe Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve\nas a robust multilingual foundation model. The\ncode~\\footnote{\\url{https://github.com/CONE-MT/LLaMAX/.}} and\nmodels~\\footnote{\\url{https://huggingface.co/LLaMAX/.}} are publicly available.\n","authors":["Yinquan Lu","Wenhao Zhu","Lei Li","Yu Qiao","Fei Yuan"],"pdf_url":"https://arxiv.org/pdf/2407.05975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05965v1","updated":"2024-07-08T14:04:58Z","published":"2024-07-08T14:04:58Z","title":"T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models","summary":"  The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset using LLMs and jailbreaking prompt attacks. Based on our evaluation\nresults, we draw several important findings, including: 1) no single model\nexcels in all aspects, with different models showing various strengths; 2) the\ncorrelation between GPT-4 assessments and manual reviews is generally high; 3)\nthere is a trade-off between the usability and safety of text-to-video\ngenerative models. This indicates that as the field of video generation rapidly\nadvances, safety risks are set to surge, highlighting the urgency of\nprioritizing video safety. We hope that T2VSafetyBench can provide insights for\nbetter understanding the safety of video generation in the era of generative\nAI.\n","authors":["Yibo Miao","Yifan Zhu","Yinpeng Dong","Lijia Yu","Jun Zhu","Xiao-Shan Gao"],"pdf_url":"https://arxiv.org/pdf/2407.05965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06775v2","updated":"2024-07-08T14:01:20Z","published":"2023-12-12T20:54:51Z","title":"Large language models in healthcare and medical domain: A review","summary":"  The deployment of large language models (LLMs) within the healthcare sector\nhas sparked both enthusiasm and apprehension. These models exhibit the\nremarkable capability to provide proficient responses to free-text queries,\ndemonstrating a nuanced understanding of professional medical knowledge. This\ncomprehensive survey delves into the functionalities of existing LLMs designed\nfor healthcare applications, elucidating the trajectory of their development,\nstarting from traditional Pretrained Language Models (PLMs) to the present\nstate of LLMs in healthcare sector. First, we explore the potential of LLMs to\namplify the efficiency and effectiveness of diverse healthcare applications,\nparticularly focusing on clinical language understanding tasks. These tasks\nencompass a wide spectrum, ranging from named entity recognition and relation\nextraction to natural language inference, multi-modal medical applications,\ndocument classification, and question-answering. Additionally, we conduct an\nextensive comparison of the most recent state-of-the-art LLMs in the healthcare\ndomain, while also assessing the utilization of various open-source LLMs and\nhighlighting their significance in healthcare applications. Furthermore, we\npresent the essential performance metrics employed to evaluate LLMs in the\nbiomedical domain, shedding light on their effectiveness and limitations.\nFinally, we summarize the prominent challenges and constraints faced by large\nlanguage models in the healthcare sector, offering a holistic perspective on\ntheir potential benefits and shortcomings. This review provides a comprehensive\nexploration of the current landscape of LLMs in healthcare, addressing their\nrole in transforming medical applications and the areas that warrant further\nresearch and development.\n","authors":["Zabir Al Nazi","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2401.06775v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10246v2","updated":"2024-07-08T13:44:56Z","published":"2023-07-17T06:54:36Z","title":"Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding\n  (Survey)","summary":"  Can we obtain insights about the brain using AI models? How is the\ninformation in deep learning models related to brain recordings? Can we improve\nAI models with the help of brain recordings? Such questions can be tackled by\nstudying brain recordings like functional magnetic resonance imaging (fMRI). As\na first step, the neuroscience community has contributed several large\ncognitive neuroscience datasets related to passive reading/listening/viewing of\nconcept words, narratives, pictures, and movies. Encoding and decoding models\nusing these datasets have also been proposed in the past two decades. These\nmodels serve as additional tools for basic cognitive science and neuroscience\nresearch. Encoding models aim at generating fMRI brain representations given a\nstimulus automatically. They have several practical applications in evaluating\nand diagnosing neurological conditions and thus may also help design therapies\nfor brain damage. Decoding models solve the inverse problem of reconstructing\nthe stimuli given the fMRI. They are useful for designing brain-machine or\nbrain-computer interfaces. Inspired by the effectiveness of deep learning\nmodels for natural language processing, computer vision, and speech, several\nneural encoding and decoding models have been recently proposed. In this\nsurvey, we will first discuss popular representations of language, vision and\nspeech stimuli, and present a summary of neuroscience datasets. Further, we\nwill review popular deep learning based encoding and decoding architectures and\nnote their benefits and limitations. Finally, we will conclude with a summary\nand discussion about future trends. Given the large amount of recently\npublished work in the computational cognitive neuroscience (CCN) community, we\nbelieve that this survey enables an entry point for DNN researchers to\ndiversify into CCN research.\n","authors":["Subba Reddy Oota","Zijiao Chen","Manish Gupta","Raju S. Bapi","Gael Jobard","Frederic Alexandre","Xavier Hinaut"],"pdf_url":"https://arxiv.org/pdf/2307.10246v2.pdf","comment":"47 pages, 23 figures"},{"id":"http://arxiv.org/abs/2406.15193v4","updated":"2024-07-08T13:34:01Z","published":"2024-06-21T14:35:16Z","title":"Reward Steering with Evolutionary Heuristics for Decoding-time Alignment","summary":"  The widespread applicability and increasing omnipresence of LLMs have\ninstigated a need to align LLM responses to user and stakeholder preferences.\nMany preference optimization approaches have been proposed that fine-tune LLM\nparameters to achieve good alignment. However, such parameter tuning is known\nto interfere with model performance on many tasks. Moreover, keeping up with\nshifting user preferences is tricky in such a situation. Decoding-time\nalignment with reward model guidance solves these issues at the cost of\nincreased inference time. However, most of such methods fail to strike the\nright balance between exploration and exploitation of reward -- often due to\nthe conflated formulation of these two aspects - to give well-aligned\nresponses. To remedy this we decouple these two aspects and implement them in\nan evolutionary fashion: exploration is enforced by decoding from mutated\ninstructions and exploitation is represented as the periodic replacement of\npoorly-rewarded generations with well-rewarded ones. Empirical evidences\nindicate that this strategy outperforms many preference optimization and\ndecode-time alignment approaches on two widely accepted alignment benchmarks\nAlpacaEval 2 and MT-Bench. Our implementation will be available at:\nhttps://darwin-alignment.github.io.\n","authors":["Chia-Yu Hung","Navonil Majumder","Ambuj Mehrish","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2406.15193v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05925v1","updated":"2024-07-08T13:32:14Z","published":"2024-07-08T13:32:14Z","title":"Towards Optimizing and Evaluating a Retrieval Augmented QA Chatbot using\n  LLMs with Human in the Loop","summary":"  Large Language Models have found application in various mundane and\nrepetitive tasks including Human Resource (HR) support. We worked with the\ndomain experts of SAP SE to develop an HR support chatbot as an efficient and\neffective tool for addressing employee inquiries. We inserted a\nhuman-in-the-loop in various parts of the development cycles such as dataset\ncollection, prompt optimization, and evaluation of generated output. By\nenhancing the LLM-driven chatbot's response quality and exploring alternative\nretrieval methods, we have created an efficient, scalable, and flexible tool\nfor HR professionals to address employee inquiries effectively. Our experiments\nand evaluation conclude that GPT-4 outperforms other models and can overcome\ninconsistencies in data through internal reasoning capabilities. Additionally,\nthrough expert analysis, we infer that reference-free evaluation metrics such\nas G-Eval and Prometheus demonstrate reliability closely aligned with that of\nhuman evaluation.\n","authors":["Anum Afzal","Alexander Kowsik","Rajna Fani","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2407.05925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02623v2","updated":"2024-07-08T13:09:39Z","published":"2024-07-02T19:27:00Z","title":"Uplifting Lower-Income Data: Strategies for Socioeconomic Perspective\n  Shifts in Vision-Language Models","summary":"  Unequal representation across cultures and socioeconomic groups in AI is a\nsignificant and challenging problem, often leading to uneven model performance.\nAs a step toward addressing this issue, we formulate translated non-English,\ngeographic, and socioeconomic integrated prompts and evaluate their impact on\nVL model performance for data from different countries and income groups. Our\nfindings show that geographic and socioeconomic integrated prompts improve VL\nperformance on lower-income data and favor the retrieval of topic appearances\ncommonly found in data from low-income households. From our analyses, we\nidentify and highlight contexts where these strategies yield the most\nimprovements. Our model analysis code is publicly available at\nhttps://github.com/Anniejoan/Uplifting-Lower-income-data .\n","authors":["Joan Nwatu","Oana Ignat","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2407.02623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05890v1","updated":"2024-07-08T12:52:46Z","published":"2024-07-08T12:52:46Z","title":"Affordances-Oriented Planning using Foundation Models for Continuous\n  Vision-Language Navigation","summary":"  LLM-based agents have demonstrated impressive zero-shot performance in the\nvision-language navigation (VLN) task. However, these zero-shot methods focus\nonly on solving high-level task planning by selecting nodes in predefined\nnavigation graphs for movements, overlooking low-level control in realistic\nnavigation scenarios. To bridge this gap, we propose AO-Planner, a novel\naffordances-oriented planning framework for continuous VLN task. Our AO-Planner\nintegrates various foundation models to achieve affordances-oriented motion\nplanning and action decision-making, both performed in a zero-shot manner.\nSpecifically, we employ a visual affordances prompting (VAP) approach, where\nvisible ground is segmented utilizing SAM to provide navigational affordances,\nbased on which the LLM selects potential next waypoints and generates low-level\npath planning towards selected waypoints. We further introduce a high-level\nagent, PathAgent, to identify the most probable pixel-based path and convert it\ninto 3D coordinates to fulfill low-level motion. Experimental results on the\nchallenging R2R-CE benchmark demonstrate that AO-Planner achieves\nstate-of-the-art zero-shot performance (5.5% improvement in SPL). Our method\nestablishes an effective connection between LLM and 3D world to circumvent the\ndifficulty of directly predicting world coordinates, presenting novel prospects\nfor employing foundation models in low-level motion control.\n","authors":["Jiaqi Chen","Bingqian Lin","Xinmin Liu","Xiaodan Liang","Kwan-Yee K. Wong"],"pdf_url":"https://arxiv.org/pdf/2407.05890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05887v1","updated":"2024-07-08T12:47:03Z","published":"2024-07-08T12:47:03Z","title":"Generation and De-Identification of Indian Clinical Discharge Summaries\n  using LLMs","summary":"  The consequences of a healthcare data breach can be devastating for the\npatients, providers, and payers. The average financial impact of a data breach\nin recent months has been estimated to be close to USD 10 million. This is\nespecially significant for healthcare organizations in India that are managing\nrapid digitization while still establishing data governance procedures that\nalign with the letter and spirit of the law. Computer-based systems for\nde-identification of personal information are vulnerable to data drift, often\nrendering them ineffective in cross-institution settings. Therefore, a rigorous\nassessment of existing de-identification against local health datasets is\nimperative to support the safe adoption of digital health initiatives in India.\nUsing a small set of de-identified patient discharge summaries provided by an\nIndian healthcare institution, in this paper, we report the nominal performance\nof de-identification algorithms (based on language models) trained on publicly\navailable non-Indian datasets, pointing towards a lack of cross-institutional\ngeneralization. Similarly, experimentation with off-the-shelf de-identification\nsystems reveals potential risks associated with the approach. To overcome data\nscarcity, we explore generating synthetic clinical reports (using publicly\navailable and Indian summaries) by performing in-context learning over Large\nLanguage Models (LLMs). Our experiments demonstrate the use of generated\nreports as an effective strategy for creating high-performing de-identification\nsystems with good generalization capabilities.\n","authors":["Sanjeet Singh","Shreya Gupta","Niralee Gupta","Naimish Sharma","Lokesh Srivastava","Vibhu Agarwal","Ashutosh Modi"],"pdf_url":"https://arxiv.org/pdf/2407.05887v1.pdf","comment":"Accepted at BioNLP Workshop at ACL 2024; 21 pages (9 pages main\n  content)"},{"id":"http://arxiv.org/abs/2407.05868v1","updated":"2024-07-08T12:31:03Z","published":"2024-07-08T12:31:03Z","title":"KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge\n  Graph-based False Premise Questions","summary":"  Recent studies have demonstrated that large language models (LLMs) are\nsusceptible to being misled by false premise questions (FPQs), leading to\nerrors in factual knowledge, know as factuality hallucination. Existing\nbenchmarks that assess this vulnerability primarily rely on manual\nconstruction, resulting in limited scale and lack of scalability. In this work,\nwe introduce an automated, scalable pipeline to create FPQs based on knowledge\ngraphs (KGs). The first step is modifying true triplets extracted from KGs to\ncreate false premises. Subsequently, utilizing the state-of-the-art\ncapabilities of GPTs, we generate semantically rich FPQs. Based on the proposed\nmethod, we present a comprehensive benchmark, the Knowledge Graph-based False\nPremise Questions (KG-FPQ), which contains approximately 178k FPQs across three\nknowledge domains, at six levels of confusability, and in two task formats.\nUsing KG-FPQ, we conduct extensive evaluations on several representative LLMs\nand provide valuable insights. The KG-FPQ dataset and code are available\nat~https://github.com/yanxuzhu/KG-FPQ.\n","authors":["Yanxu Zhu","Jinlin Xiao","Yuhang Wang","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2407.05868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08454v2","updated":"2024-07-08T11:50:19Z","published":"2024-05-14T09:20:59Z","title":"Alignment Helps Make the Most of Multimodal Data","summary":"  When studying political communication, combining the information from text,\naudio, and video signals promises to reflect the richness of human\ncommunication more comprehensively than confining it to individual modalities\nalone. However, its heterogeneity, connectedness, and interaction are\nchallenging to address when modeling such multimodal data. We argue that\naligning the respective modalities can be an essential step in entirely using\nthe potential of multimodal data because it informs the model with human\nunderstanding. Taking care of the data-generating process of multimodal data,\nour framework proposes four principles to organize alignment and, thus, address\nthe challenges of multimodal data. We illustrate the utility of these\nprinciples by analyzing how German MPs address members of the far-right AfD in\ntheir speeches and predicting the tone of video advertising in the context of\nthe 2020 US presidential race. Our paper offers important insights to all keen\nto analyze multimodal data effectively.\n","authors":["Christian Arnold","Andreas KÃ¼pfer"],"pdf_url":"https://arxiv.org/pdf/2405.08454v2.pdf","comment":"Working Paper"},{"id":"http://arxiv.org/abs/2407.05841v1","updated":"2024-07-08T11:38:49Z","published":"2024-07-08T11:38:49Z","title":"An Empirical Comparison of Vocabulary Expansion and Initialization\n  Approaches for Language Models","summary":"  Language Models (LMs) excel in natural language processing tasks for English\nbut show reduced performance in most other languages. This problem is commonly\ntackled by continually pre-training and fine-tuning these models for said\nlanguages. A significant issue in this process is the limited vocabulary\ncoverage in the original model's tokenizer, leading to inadequate\nrepresentation of new languages and necessitating an expansion of the\ntokenizer. The initialization of the embeddings corresponding to new vocabulary\nitems presents a further challenge. Current strategies require cross-lingual\nembeddings and lack a solid theoretical foundation as well as comparisons with\nstrong baselines. In this paper, we first establish theoretically that\ninitializing within the convex hull of existing embeddings is a good\ninitialization, followed by a novel but simple approach, Constrained Word2Vec\n(CW2V), which does not require cross-lingual embeddings. Our study evaluates\ndifferent initialization methods for expanding RoBERTa and LLaMA 2 across four\nlanguages and five tasks. The results show that CW2V performs equally well or\neven better than more advanced techniques. Additionally, simpler approaches\nlike multivariate initialization perform on par with these advanced methods\nindicating that efficient large-scale multilingual continued pretraining can be\nachieved even with simpler initialization methods.\n","authors":["Nandini Mundra","Aditya Nanda Kishore","Raj Dabre","Ratish Puduppully","Anoop Kunchukuttan","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2407.05841v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2405.10040v2","updated":"2024-07-08T11:20:42Z","published":"2024-05-16T12:22:41Z","title":"SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation","summary":"  It is often desirable to distill the capabilities of large language models\n(LLMs) into smaller student models due to compute and memory constraints. One\nway to do this for classification tasks is via dataset synthesis, which can be\naccomplished by generating examples of each label from the LLM. Prior\napproaches to synthesis use few-shot prompting, which relies on the LLM's\nparametric knowledge to generate usable examples. However, this leads to issues\nof repetition, bias towards popular entities, and stylistic differences from\nhuman text. In this work, we propose Synthesize by Retrieval and Refinement\n(SynthesizRR), which uses retrieval augmentation to introduce variety into the\ndataset synthesis process: as retrieved passages vary, the LLM is seeded with\ndifferent content to generate its examples. We empirically study the synthesis\nof six datasets, covering topic classification, sentiment analysis, tone\ndetection, and humor, requiring complex synthesis strategies. We find that\nSynthesizRR greatly improves lexical and semantic diversity, similarity to\nhuman-written text, and distillation performance, when compared to 32-shot\nprompting and four prior approaches. We release our extensive codebase at\nhttps://github.com/amazon-science/synthesizrr\n","authors":["Abhishek Divekar","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2405.10040v2.pdf","comment":"Code available at https://github.com/amazon-science/synthesizrr"},{"id":"http://arxiv.org/abs/2209.14272v3","updated":"2024-07-08T10:50:56Z","published":"2022-09-28T17:36:47Z","title":"Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and\n  First Results","summary":"  Humor is a substantial element of human social behavior, affect, and\ncognition. Its automatic understanding can facilitate a more naturalistic\nhuman-AI interaction. Current methods of humor detection have been exclusively\nbased on staged data, making them inadequate for \"real-world\" applications. We\ncontribute to addressing this deficiency by introducing the novel\nPassau-Spontaneous Football Coach Humor (Passau-SFCH) dataset, comprising about\n11 hours of recordings. The Passau-SFCH dataset is annotated for the presence\nof humor and its dimensions (sentiment and direction) as proposed in Martin's\nHumor Style Questionnaire. We conduct a series of experiments employing\npretrained Transformers, convolutional neural networks, and expert-designed\nfeatures. The performance of each modality (text, audio, video) for spontaneous\nhumor recognition is analyzed and their complementarity is investigated. Our\nfindings suggest that for the automatic analysis of humor and its sentiment,\nfacial expressions are most promising, while humor direction can be best\nmodeled via text-based features. Further, we experiment with different\nmultimodal approaches to humor recognition, including decision-level fusion and\nMulT, a multimodal Transformer approach. In this context, we propose a novel\nmultimodal architecture that yields the best overall results. Finally, we make\nour code publicly available at https://www.github.com/lc0197/passau-sfch. The\nPassau-SFCH dataset is available upon request.\n","authors":["Lukas Christ","Shahin Amiriparian","Alexander Kathan","Niklas MÃ¼ller","Andreas KÃ¶nig","BjÃ¶rn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2209.14272v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible (Major Revision)"},{"id":"http://arxiv.org/abs/2407.05786v1","updated":"2024-07-08T09:49:03Z","published":"2024-07-08T09:49:03Z","title":"Large Language Models for Judicial Entity Extraction: A Comparative\n  Study","summary":"  Domain-specific Entity Recognition holds significant importance in legal\ncontexts, serving as a fundamental task that supports various applications such\nas question-answering systems, text summarization, machine translation,\nsentiment analysis, and information retrieval specifically within case law\ndocuments. Recent advancements have highlighted the efficacy of Large Language\nModels in natural language processing tasks, demonstrating their capability to\naccurately detect and classify domain-specific facts (entities) from\nspecialized texts like clinical and financial documents. This research\ninvestigates the application of Large Language Models in identifying\ndomain-specific entities (e.g., courts, petitioner, judge, lawyer, respondents,\nFIR nos.) within case law documents, with a specific focus on their aptitude\nfor handling domain-specific language complexity and contextual variations. The\nstudy evaluates the performance of state-of-the-art Large Language Model\narchitectures, including Large Language Model Meta AI 3, Mistral, and Gemma, in\nthe context of extracting judicial facts tailored to Indian judicial texts.\nMistral and Gemma emerged as the top-performing models, showcasing balanced\nprecision and recall crucial for accurate entity identification. These findings\nconfirm the value of Large Language Models in judicial documents and\ndemonstrate how they can facilitate and quicken scientific research by\nproducing precise, organised data outputs that are appropriate for in-depth\nexamination.\n","authors":["Atin Sakkeer Hussain","Anu Thomas"],"pdf_url":"https://arxiv.org/pdf/2407.05786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03805v2","updated":"2024-07-08T09:42:20Z","published":"2024-07-04T10:28:48Z","title":"Cognitive Modeling with Scaffolded LLMs: A Case Study of Referential\n  Expression Generation","summary":"  To what extent can LLMs be used as part of a cognitive model of language\ngeneration? In this paper, we approach this question by exploring a\nneuro-symbolic implementation of an algorithmic cognitive model of referential\nexpression generation by Dale & Reiter (1995). The symbolic task analysis\nimplements the generation as an iterative procedure that scaffolds symbolic and\ngpt-3.5-turbo-based modules. We compare this implementation to an ablated model\nand a one-shot LLM-only baseline on the A3DS dataset (Tsvilodub & Franke,\n2023). We find that our hybrid approach is cognitively plausible and performs\nwell in complex contexts, while allowing for more open-ended modeling of\nlanguage generation in a larger domain.\n","authors":["Polina Tsvilodub","Michael Franke","Fausto Carcassi"],"pdf_url":"https://arxiv.org/pdf/2407.03805v2.pdf","comment":"11 pages, 3 figures, 2 algorithms, to appear at the ICML 2024\n  workshop on Large Language Models and Cognition"},{"id":"http://arxiv.org/abs/2407.05778v1","updated":"2024-07-08T09:37:27Z","published":"2024-07-08T09:37:27Z","title":"When is the consistent prediction likely to be a correct prediction?","summary":"  Self-consistency (Wang et al., 2023) suggests that the most consistent answer\nobtained through large language models (LLMs) is more likely to be correct. In\nthis paper, we challenge this argument and propose a nuanced correction. Our\nobservations indicate that consistent answers derived through more computation\ni.e. longer reasoning texts, rather than simply the most consistent answer\nacross all outputs, are more likely to be correct. This is predominantly\nbecause we demonstrate that LLMs can autonomously produce chain-of-thought\n(CoT) style reasoning with no custom prompts merely while generating longer\nresponses, which lead to consistent predictions that are more accurate. In the\nzero-shot setting, by sampling Mixtral-8x7B model multiple times and\nconsidering longer responses, we achieve 86% of its self-consistency\nperformance obtained through zero-shot CoT prompting on the GSM8K and\nMultiArith datasets. Finally, we demonstrate that the probability of LLMs\ngenerating a longer response is quite low, highlighting the need for decoding\nstrategies conditioned on output length.\n","authors":["Alex Nguyen","Dheeraj Mekala","Chengyu Dong","Jingbo Shang"],"pdf_url":"https://arxiv.org/pdf/2407.05778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05750v1","updated":"2024-07-08T09:03:12Z","published":"2024-07-08T09:03:12Z","title":"Large Language Models Understand Layouts","summary":"  Large language models (LLMs) demonstrate extraordinary abilities in a wide\nrange of natural language processing (NLP) tasks. In this paper, we show that,\nbeyond text understanding capability, LLMs are capable of processing text\nlayouts that are denoted by spatial markers. They are able to answer questions\nthat require explicit spatial perceiving and reasoning, while a drastic\nperformance drop is observed when the spatial markers from the original data\nare excluded. We perform a series of experiments with the GPT-3.5, Baichuan2,\nLlama2 and ChatGLM3 models on various types of layout-sensitive datasets for\nfurther analysis. The experimental results reveal that the layout understanding\nability of LLMs is mainly introduced by the coding data for pretraining, which\nis further enhanced at the instruction-tuning stage. In addition, layout\nunderstanding can be enhanced by integrating low-cost, auto-generated data\napproached by a novel text game. Finally, we show that layout understanding\nability is beneficial for building efficient visual question-answering (VQA)\nsystems.\n","authors":["Weiming Li","Manni Duan","Dong An","Yan Shao"],"pdf_url":"https://arxiv.org/pdf/2407.05750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10251v2","updated":"2024-07-08T08:52:56Z","published":"2024-06-10T08:23:52Z","title":"The Impact of Quantization on Retrieval-Augmented Generation: An\n  Analysis of Small LLMs","summary":"  Post-training quantization reduces the computational demand of Large Language\nModels (LLMs) but can weaken some of their capabilities. Since LLM abilities\nemerge with scale, smaller LLMs are more sensitive to quantization. In this\npaper, we explore how quantization affects smaller LLMs' ability to perform\nretrieval-augmented generation (RAG), specifically in longer contexts. We chose\npersonalization for evaluation because it is a challenging domain to perform\nusing RAG as it requires long-context reasoning over multiple documents. We\ncompare the original FP16 and the quantized INT4 performance of multiple 7B and\n8B LLMs on two tasks while progressively increasing the number of retrieved\ndocuments to test how quantized models fare against longer contexts. To better\nunderstand the effect of retrieval, we evaluate three retrieval models in our\nexperiments. Our findings reveal that if a 7B LLM performs the task well,\nquantization does not impair its performance and long-context reasoning\ncapabilities. We conclude that it is possible to utilize RAG with quantized\nsmaller LLMs.\n","authors":["Mert Yazan","Suzan Verberne","Frederik Situmeang"],"pdf_url":"https://arxiv.org/pdf/2406.10251v2.pdf","comment":"Accepted to the IR-RAG Workshop at SIGIR 2024"},{"id":"http://arxiv.org/abs/2407.05740v1","updated":"2024-07-08T08:46:50Z","published":"2024-07-08T08:46:50Z","title":"Do Multilingual Large Language Models Mitigate Stereotype Bias?","summary":"  While preliminary findings indicate that multilingual LLMs exhibit reduced\nbias compared to monolingual ones, a comprehensive understanding of the effect\nof multilingual training on bias mitigation, is lacking. This study addresses\nthis gap by systematically training six LLMs of identical size (2.6B\nparameters) and architecture: five monolingual models (English, German, French,\nItalian, and Spanish) and one multilingual model trained on an equal\ndistribution of data across these languages, all using publicly available data.\nTo ensure robust evaluation, standard bias benchmarks were automatically\ntranslated into the five target languages and verified for both translation\nquality and bias preservation by human annotators. Our results consistently\ndemonstrate that multilingual training effectively mitigates bias. Moreover, we\nobserve that multilingual models achieve not only lower bias but also superior\nprediction accuracy when compared to monolingual models with the same amount of\ntraining data, model architecture, and size.\n","authors":["Shangrui Nie","Michael Fromm","Charles Welch","Rebekka GÃ¶rge","Akbar Karimi","Joan Plepi","Nazia Afsan Mowmita","Nicolas Flores-Herr","Mehdi Ali","Lucie Flek"],"pdf_url":"https://arxiv.org/pdf/2407.05740v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.06461v4","updated":"2024-07-08T08:45:55Z","published":"2024-01-12T09:15:20Z","title":"Between Lines of Code: Unraveling the Distinct Patterns of Machine and\n  Human Programmers","summary":"  Large language models have catalyzed an unprecedented wave in code\ngeneration. While achieving significant advances, they blur the distinctions\nbetween machine- and human-authored source code, causing integrity and\nauthenticity issues of software artifacts. Previous methods such as DetectGPT\nhave proven effective in discerning machine-generated texts, but they do not\nidentify and harness the unique patterns of machine-generated code. Thus, its\napplicability falters when applied to code. In this paper, we carefully study\nthe specific patterns that characterize machine- and human-authored code.\nThrough a rigorous analysis of code attributes such as lexical diversity,\nconciseness, and naturalness, we expose unique patterns inherent to each\nsource. We particularly notice that the syntactic segmentation of code is a\ncritical factor in identifying its provenance. Based on our findings, we\npropose DetectCodeGPT, a novel method for detecting machine-generated code,\nwhich improves DetectGPT by capturing the distinct stylized patterns of code.\nDiverging from conventional techniques that depend on external LLMs for\nperturbations, DetectCodeGPT perturbs the code corpus by strategically\ninserting spaces and newlines, ensuring both efficacy and efficiency.\nExperiment results show that our approach significantly outperforms\nstate-of-the-art techniques in detecting machine-generated code.\n","authors":["Yuling Shi","Hongyu Zhang","Chengcheng Wan","Xiaodong Gu"],"pdf_url":"https://arxiv.org/pdf/2401.06461v4.pdf","comment":"Accepted by the 47th International Conference on Software Engineering\n  (ICSE 2025). Code available at https://github.com/YerbaPage/DetectCodeGPT"},{"id":"http://arxiv.org/abs/2407.05734v1","updated":"2024-07-08T08:38:43Z","published":"2024-07-08T08:38:43Z","title":"Empirical Study of Symmetrical Reasoning in Conversational Chatbots","summary":"  This work explores the capability of conversational chatbots powered by large\nlanguage models (LLMs), to understand and characterize predicate symmetry, a\ncognitive linguistic function traditionally believed to be an inherent human\ntrait. Leveraging in-context learning (ICL), a paradigm shift enabling chatbots\nto learn new tasks from prompts without re-training, we assess the symmetrical\nreasoning of five chatbots: ChatGPT 4, Huggingface chat AI, Microsoft's Copilot\nAI, LLaMA through Perplexity, and Gemini Advanced. Using the Symmetry Inference\nSentence (SIS) dataset by Tanchip et al. (2020), we compare chatbot responses\nagainst human evaluations to gauge their understanding of predicate symmetry.\nExperiment results reveal varied performance among chatbots, with some\napproaching human-like reasoning capabilities. Gemini, for example, reaches a\ncorrelation of 0.85 with human scores, while providing a sounding justification\nfor each symmetry evaluation. This study underscores the potential and\nlimitations of LLMs in mirroring complex cognitive processes as symmetrical\nreasoning.\n","authors":["Daniela N. Rim","Heeyoul Choi"],"pdf_url":"https://arxiv.org/pdf/2407.05734v1.pdf","comment":"Accepted in Future Technology Conference (FTC) 2024"},{"id":"http://arxiv.org/abs/2406.14024v3","updated":"2024-07-08T08:37:33Z","published":"2024-06-20T06:42:27Z","title":"LLM Critics Help Catch Bugs in Mathematics: Towards a Better\n  Mathematical Verifier with Natural Language Feedback","summary":"  Mathematical verfier achieves success in mathematical reasoning tasks by\nvalidating the correctness of solutions. However, existing verifiers are\ntrained with binary classification labels, which are not informative enough for\nthe model to accurately assess the solutions. To mitigate the aforementioned\ninsufficiency of binary labels, we introduce step-wise natural language\nfeedbacks as rationale labels (i.e., the correctness of the current step and\nthe explanations). In this paper, we propose \\textbf{Math-Minos}, a natural\nlanguage feedback enhanced verifier by constructing automatically-generated\ntraining data and a two-stage training paradigm for effective training and\nefficient inference. Our experiments reveal that a small set (30k) of natural\nlanguage feedbacks can significantly boost the performance of the verifier by\nthe accuracy of 1.6\\% (86.6\\% $\\rightarrow$ 88.2\\%) on GSM8K and 0.8\\% (37.8\\%\n$\\rightarrow$ 38.6\\%) on MATH. We have released our code and data for further\nexploration.\n","authors":["Bofei Gao","Zefan Cai","Runxin Xu","Peiyi Wang","Ce Zheng","Runji Lin","Keming Lu","Dayiheng Liu","Chang Zhou","Wen Xiao","Junjie Hu","Tianyu Liu","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2406.14024v3.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2407.05733v1","updated":"2024-07-08T08:37:00Z","published":"2024-07-08T08:37:00Z","title":"Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative\n  Judgment Approach Based on Rater Cognition","summary":"  Large Language Models (LLMs) have shown promise in Automated Essay Scoring\n(AES), but their zero-shot and few-shot performance often falls short compared\nto state-of-the-art models and human raters. However, fine-tuning LLMs for each\nspecific task is impractical due to the variety of essay prompts and rubrics\nused in real-world educational contexts. This study proposes a novel approach\ncombining LLMs and Comparative Judgment (CJ) for AES, using zero-shot prompting\nto choose between two essays. We demonstrate that a CJ method surpasses\ntraditional rubric-based scoring in essay scoring using LLMs.\n","authors":["Seungju Kim","Meounggun Jo"],"pdf_url":"https://arxiv.org/pdf/2407.05733v1.pdf","comment":"16 pages, 3 figures, Learning @ Scale 2024"},{"id":"http://arxiv.org/abs/2407.05721v1","updated":"2024-07-08T08:25:56Z","published":"2024-07-08T08:25:56Z","title":"PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation","summary":"  Mental health has attracted substantial attention in recent years and LLM can\nbe an effective technology for alleviating this problem owing to its capability\nin text understanding and dialogue. However, existing research in this domain\noften suffers from limitations, such as training on datasets lacking crucial\nprior knowledge and evidence, and the absence of comprehensive evaluation\nmethods. In this paper, we propose a specialized psychological large language\nmodel (LLM), named PsycoLLM, trained on a proposed high-quality psychological\ndataset, including single-turn QA, multi-turn dialogues enriched with prior\nknowledge and knowledge-based QA. Additionally, to compare the performance of\nPsycoLLM with other LLMs, we develop a comprehensive psychological benchmark\nbased on authoritative psychological counseling examinations in China, which\nincludes assessments of professional ethics, theoretical proficiency, and case\nanalysis. The experimental results on the benchmark illustrates the\neffectiveness of PsycoLLM, which demonstrates superior performance compared to\nother LLMs.\n","authors":["Jinpeng Hu","Tengteng Dong","Hui Ma","Peng Zou","Xiao Sun","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05721v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2407.05718v1","updated":"2024-07-08T08:23:11Z","published":"2024-07-08T08:23:11Z","title":"A Factuality and Diversity Reconciled Decoding Method for\n  Knowledge-Grounded Dialogue Generation","summary":"  Grounding external knowledge can enhance the factuality of responses in\ndialogue generation. However, excessive emphasis on it might result in the lack\nof engaging and diverse expressions. Through the introduction of randomness in\nsampling, current approaches can increase the diversity. Nevertheless, such\nsampling method could undermine the factuality in dialogue generation. In this\nstudy, to discover a solution for advancing creativity without relying on\nquestionable randomness and to subtly reconcile the factuality and diversity\nwithin the source-grounded paradigm, a novel method named DoGe is proposed.\nDoGe can dynamically alternate between the utilization of internal parameter\nknowledge and external source knowledge based on the model's factual\nconfidence. Extensive experiments on three widely-used datasets show that DoGe\ncan not only enhance response diversity but also maintain factuality, and it\nsignificantly surpasses other various decoding strategy baselines.\n","authors":["Chenxu Yang","Zheng Lin","Chong Tian","Liang Pang","Lanrui Wang","Zhengyang Tong","Qirong Ho","Yanan Cao","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05700v1","updated":"2024-07-08T08:00:05Z","published":"2024-07-08T08:00:05Z","title":"InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with\n  Inverse-Instruct","summary":"  Recent advancements in open-source code large language models (LLMs) have\ndemonstrated remarkable coding abilities by fine-tuning on the data generated\nfrom powerful closed-source LLMs such as GPT-3.5 and GPT-4 for instruction\ntuning. This paper explores how to further improve an instruction-tuned code\nLLM by generating data from itself rather than querying closed-source LLMs. Our\nkey observation is the misalignment between the translation of formal and\ninformal languages: translating formal language (i.e., code) to informal\nlanguage (i.e., natural language) is more straightforward than the reverse.\nBased on this observation, we propose INVERSE-INSTRUCT, which summarizes\ninstructions from code snippets instead of the reverse. Specifically, given an\ninstruction tuning corpus for code and the resulting instruction-tuned code\nLLM, we ask the code LLM to generate additional high-quality instructions for\nthe original corpus through code summarization and self-evaluation. Then, we\nfine-tune the base LLM on the combination of the original corpus and the\nself-generated one, which yields a stronger instruction-tuned LLM. We present a\nseries of code LLMs named InverseCoder, which surpasses the performance of the\noriginal code LLMs on a wide range of benchmarks, including Python text-to-code\ngeneration, multilingual coding, and data-science code generation.\n","authors":["Yutong Wu","Di Huang","Wenxuan Shi","Wei Wang","Lingzhe Gao","Shihao Liu","Ziyuan Nan","Kaizhao Yuan","Rui Zhang","Xishan Zhang","Zidong Du","Qi Guo","Yewen Pu","Dawei Yin","Xing Hu","Yunji Chen"],"pdf_url":"https://arxiv.org/pdf/2407.05700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12313v3","updated":"2024-07-08T07:58:34Z","published":"2023-02-23T20:18:52Z","title":"Testing AI on language comprehension tasks reveals insensitivity to\n  underlying meaning","summary":"  Large Language Models (LLMs) are recruited in applications that span from\nclinical assistance and legal support to question answering and education.\nTheir success in specialized tasks has led to the claim that they possess\nhuman-like linguistic capabilities related to compositional understanding and\nreasoning. Yet, reverse-engineering is bound by Moravec's Paradox, according to\nwhich easy skills are hard. We systematically assess 7 state-of-the-art models\non a novel benchmark. Models answered a series of comprehension questions, each\nprompted multiple times in two settings, permitting one-word or open-length\nreplies. Each question targets a short text featuring high-frequency linguistic\nconstructions. To establish a baseline for achieving human-like performance, we\ntested 400 humans on the same prompts. Based on a dataset of n=26,680\ndatapoints, we discovered that LLMs perform at chance accuracy and waver\nconsiderably in their answers. Quantitatively, the tested models are\noutperformed by humans, and qualitatively their answers showcase distinctly\nnon-human errors in language understanding. We interpret this evidence as\nsuggesting that, despite their usefulness in various tasks, current AI models\nfall short of understanding language in a way that matches humans, and we argue\nthat this may be due to their lack of a compositional operator for regulating\ngrammatical and semantic information.\n","authors":["Vittoria Dentella","Elliot Murphy","Gary Marcus","Evelina Leivada"],"pdf_url":"https://arxiv.org/pdf/2302.12313v3.pdf","comment":"18 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2104.08540v3","updated":"2024-07-08T07:56:49Z","published":"2021-04-17T13:34:45Z","title":"DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages","summary":"  Word meaning is notoriously difficult to capture, both synchronically and\ndiachronically. In this paper, we describe the creation of the largest resource\nof graded contextualized, diachronic word meaning annotation in four different\nlanguages, based on 100,000 human semantic proximity judgments. We thoroughly\ndescribe the multi-round incremental annotation process, the choice for a\nclustering algorithm to group usages into senses, and possible - diachronic and\nsynchronic - uses for this dataset.\n","authors":["Dominik Schlechtweg","Nina Tahmasebi","Simon Hengchen","Haim Dubossarsky","Barbara McGillivray"],"pdf_url":"https://arxiv.org/pdf/2104.08540v3.pdf","comment":"Dominik Schlechtweg, Nina Tahmasebi, Simon Hengchen, Haim\n  Dubossarsky, and Barbara McGillivray. 2021. DWUG: A large Resource of\n  Diachronic Word Usage Graphs in Four Languages. In Proceedings of the 2021\n  Conference on Empirical Methods in Natural Language Processing, pages\n  7079--7091, Online and Punta Cana, Dominican Republic. Association for\n  Computational Linguistics"},{"id":"http://arxiv.org/abs/2407.05694v1","updated":"2024-07-08T07:53:06Z","published":"2024-07-08T07:53:06Z","title":"On the Limitations of Compute Thresholds as a Governance Strategy","summary":"  At face value, this essay is about understanding a fairly esoteric governance\ntool called compute thresholds. However, in order to grapple with whether these\nthresholds will achieve anything, we must first understand how they came to be.\nThis requires engaging with a decades-old debate at the heart of computer\nscience progress, namely, is bigger always better? Hence, this essay may be of\ninterest not only to policymakers and the wider public but also to computer\nscientists interested in understanding the role of compute in unlocking\nbreakthroughs. Does a certain inflection point of compute result in changes to\nthe risk profile of a model? This discussion is increasingly urgent given the\nwide adoption of governance approaches that suggest greater compute equates\nwith higher propensity for harm. Several leading frontier AI companies have\nreleased responsible scaling policies. Both the White House Executive Orders on\nAI Safety (EO) and the EU AI Act encode the use of FLOP or floating-point\noperations as a way to identify more powerful systems. What is striking about\nthe choice of compute thresholds to-date is that no models currently deployed\nin the wild fulfill the current criteria set by the EO. This implies that the\nemphasis is often not on auditing the risks and harms incurred by currently\ndeployed models - but rather is based upon the belief that future levels of\ncompute will introduce unforeseen new risks. A key conclusion of this essay is\nthat compute thresholds as currently implemented are shortsighted and likely to\nfail to mitigate risk. Governance that is overly reliant on compute fails to\nunderstand that the relationship between compute and risk is highly uncertain\nand rapidly changing. It also overestimates our ability to predict what\nabilities emerge at different scales. This essay ends with recommendations for\na better way forward.\n","authors":["Sara Hooker"],"pdf_url":"https://arxiv.org/pdf/2407.05694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10118v3","updated":"2024-07-08T07:49:40Z","published":"2024-06-14T15:23:39Z","title":"SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for\n  Southeast Asian Languages","summary":"  Southeast Asia (SEA) is a region rich in linguistic diversity and cultural\nvariety, with over 1,300 indigenous languages and a population of 671 million\npeople. However, prevailing AI models suffer from a significant lack of\nrepresentation of texts, images, and audio datasets from SEA, compromising the\nquality of AI models for SEA languages. Evaluating models for SEA languages is\nchallenging due to the scarcity of high-quality datasets, compounded by the\ndominance of English training data, raising concerns about potential cultural\nmisrepresentation. To address these challenges, we introduce SEACrowd, a\ncollaborative initiative that consolidates a comprehensive resource hub that\nfills the resource gap by providing standardized corpora in nearly 1,000 SEA\nlanguages across three modalities. Through our SEACrowd benchmarks, we assess\nthe quality of AI models on 36 indigenous languages across 13 tasks, offering\nvaluable insights into the current AI landscape in SEA. Furthermore, we propose\nstrategies to facilitate greater AI advancements, maximizing potential utility\nand resource equity for the future of AI in SEA.\n","authors":["Holy Lovenia","Rahmad Mahendra","Salsabil Maulana Akbar","Lester James V. Miranda","Jennifer Santoso","Elyanah Aco","Akhdan Fadhilah","Jonibek Mansurov","Joseph Marvin Imperial","Onno P. Kampman","Joel Ruben Antony Moniz","Muhammad Ravi Shulthan Habibi","Frederikus Hudi","Railey Montalan","Ryan Ignatius","Joanito Agili Lopo","William Nixon","BÃ¶rje F. Karlsson","James Jaya","Ryandito Diandaru","Yuze Gao","Patrick Amadeus","Bin Wang","Jan Christian Blaise Cruz","Chenxi Whitehouse","Ivan Halim Parmonangan","Maria Khelli","Wenyu Zhang","Lucky Susanto","Reynard Adha Ryanda","Sonny Lazuardi Hermawan","Dan John Velasco","Muhammad Dehan Al Kautsar","Willy Fitra Hendria","Yasmin Moslem","Noah Flynn","Muhammad Farid Adilazuarda","Haochen Li","Johanes Lee","R. Damanhuri","Shuo Sun","Muhammad Reza Qorib","Amirbek Djanibekov","Wei Qi Leong","Quyet V. Do","Niklas Muennighoff","Tanrada Pansuwan","Ilham Firdausi Putra","Yan Xu","Ngee Chia Tai","Ayu Purwarianti","Sebastian Ruder","William Tjhi","Peerat Limkonchotiwat","Alham Fikri Aji","Sedrick Keh","Genta Indra Winata","Ruochen Zhang","Fajri Koto","Zheng-Xin Yong","Samuel Cahyawijaya"],"pdf_url":"https://arxiv.org/pdf/2406.10118v3.pdf","comment":"https://github.com/SEACrowd"},{"id":"http://arxiv.org/abs/2407.05690v1","updated":"2024-07-08T07:45:38Z","published":"2024-07-08T07:45:38Z","title":"Pruning Large Language Models to Intra-module Low-rank Architecture with\n  Transitional Activations","summary":"  Structured pruning fundamentally reduces computational and memory overheads\nof large language models (LLMs) and offers a feasible solution for end-side LLM\ndeployment. Structurally pruned models remain dense and high-precision, highly\ncompatible with further tuning and compression. However, as the coarse-grained\nstructured pruning poses large damage to the highly interconnected model,\nachieving a high compression ratio for scaled-up LLMs remains a challenge. In\nthis paper, we introduce a task-agnostic structured pruning approach coupled\nwith a compact Transformer architecture design. The proposed approach, named\nTransAct, reduces transitional activations inside multi-head attention (MHA)\nand multi-layer perceptron (MLP) modules, while preserving the inter-module\nactivations that are sensitive to perturbations. Hence, the LLM is pruned into\nan intra-module low-rank architecture, significantly reducing weights, KV Cache\nand attention computation. TransAct is implemented on the LLaMA model and\nevaluated on downstream benchmarks. Results verify the optimality of our\napproach at high compression with respect to both efficiency and performance.\nFurther, ablation studies reveal the strength of activation-guided iterative\npruning and provide experimental analysis on the redundancy of MHA and MLP\nmodules.\n","authors":["Bowen Shen","Zheng Lin","Daren Zha","Wei Liu","Jian Luan","Bin Wang","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05690v1.pdf","comment":"Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2406.15485v3","updated":"2024-07-08T07:41:50Z","published":"2024-06-17T11:00:04Z","title":"SegHist: A General Segmentation-based Framework for Chinese Historical\n  Document Text Line Detection","summary":"  Text line detection is a key task in historical document analysis facing many\nchallenges of arbitrary-shaped text lines, dense texts, and text lines with\nhigh aspect ratios, etc. In this paper, we propose a general framework for\nhistorical document text detection (SegHist), enabling existing\nsegmentation-based text detection methods to effectively address the\nchallenges, especially text lines with high aspect ratios. Integrating the\nSegHist framework with the commonly used method DB++, we develop DB-SegHist.\nThis approach achieves SOTA on the CHDAC, MTHv2, and competitive results on\nHDRC datasets, with a significant improvement of 1.19% on the most challenging\nCHDAC dataset which features more text lines with high aspect ratios. Moreover,\nour method attains SOTA on rotated MTHv2 and rotated HDRC, demonstrating its\nrotational robustness. The code is available at\nhttps://github.com/LumionHXJ/SegHist.\n","authors":["Xingjian Hu","Baole Wei","Liangcai Gao","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2406.15485v3.pdf","comment":"Accepted by ICDAR2024 (poster)"},{"id":"http://arxiv.org/abs/2405.13049v3","updated":"2024-07-08T07:32:28Z","published":"2024-05-19T09:59:00Z","title":"SemEval-2024 Task 3: Multimodal Emotion Cause Analysis in Conversations","summary":"  The ability to understand emotions is an essential component of human-like\nartificial intelligence, as emotions greatly influence human cognition,\ndecision making, and social interactions. In addition to emotion recognition in\nconversations, the task of identifying the potential causes behind an\nindividual's emotional state in conversations, is of great importance in many\napplication scenarios. We organize SemEval-2024 Task 3, named Multimodal\nEmotion Cause Analysis in Conversations, which aims at extracting all pairs of\nemotions and their corresponding causes from conversations. Under different\nmodality settings, it consists of two subtasks: Textual Emotion-Cause Pair\nExtraction in Conversations (TECPE) and Multimodal Emotion-Cause Pair\nExtraction in Conversations (MECPE). The shared task has attracted 143\nregistrations and 216 successful submissions. In this paper, we introduce the\ntask, dataset and evaluation settings, summarize the systems of the top teams,\nand discuss the findings of the participants.\n","authors":["Fanfan Wang","Heqing Ma","Jianfei Yu","Rui Xia","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2405.13049v3.pdf","comment":"Accepted to the 18th International Workshop on Semantic Evaluation\n  (SemEval-2024). 12 pages, 3 figures, 4 Tables"},{"id":"http://arxiv.org/abs/2407.05682v1","updated":"2024-07-08T07:32:26Z","published":"2024-07-08T07:32:26Z","title":"Retrieved In-Context Principles from Previous Mistakes","summary":"  In-context learning (ICL) has been instrumental in adapting Large Language\nModels (LLMs) to downstream tasks using correct input-output examples. Recent\nadvances have attempted to improve model performance through principles derived\nfrom mistakes, yet these approaches suffer from lack of customization and\ninadequate error coverage. To address these limitations, we propose Retrieved\nIn-Context Principles (RICP), a novel teacher-student framework. In RICP, the\nteacher model analyzes mistakes from the student model to generate reasons and\ninsights for preventing similar mistakes. These mistakes are clustered based on\ntheir underlying reasons for developing task-level principles, enhancing the\nerror coverage of principles. During inference, the most relevant mistakes for\neach question are retrieved to create question-level principles, improving the\ncustomization of the provided guidance. RICP is orthogonal to existing\nprompting methods and does not require intervention from the teacher model\nduring inference. Experimental results across seven reasoning benchmarks reveal\nthat RICP effectively enhances performance when applied to various prompting\nstrategies.\n","authors":["Hao Sun","Yong Jiang","Bo Wang","Yingyan Hou","Yan Zhang","Pengjun Xie","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2407.05682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05674v1","updated":"2024-07-08T07:17:40Z","published":"2024-07-08T07:17:40Z","title":"LLM-Based Open-Domain Integrated Task and Knowledge Assistants with\n  Programmable Policies","summary":"  Programming LLM-based knowledge and task assistants that faithfully conform\nto developer-provided policies is challenging. These agents must retrieve and\nprovide consistent, accurate, and relevant information to address user's\nqueries and needs. Yet such agents generate unfounded responses\n(\"hallucinate\"). Traditional dialogue trees can only handle a limited number of\nconversation flows, making them inherently brittle. To this end, we present\nKITA - a programmable framework for creating task-oriented conversational\nagents that are designed to handle complex user interactions. Unlike LLMs, KITA\nprovides reliable grounded responses, with controllable agent policies through\nits expressive specification, KITA Worksheet. In contrast to dialog trees, it\nis resilient to diverse user queries, helpful with knowledge sources, and\noffers ease of programming policies through its declarative paradigm. Through a\nreal-user study involving 62 participants, we show that KITA beats the GPT-4\nwith function calling baseline by 26.1, 22.5, and 52.4 points on execution\naccuracy, dialogue act accuracy, and goal completion rate, respectively. We\nalso release 22 real-user conversations with KITA manually corrected to ensure\naccuracy.\n","authors":["Harshit Joshi","Shicheng Liu","James Chen","Robert Weigle","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2407.05674v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2407.05656v1","updated":"2024-07-08T06:29:46Z","published":"2024-07-08T06:29:46Z","title":"Multi-label Learning with Random Circular Vectors","summary":"  The extreme multi-label classification~(XMC) task involves learning a\nclassifier that can predict from a large label set the most relevant subset of\nlabels for a data instance. While deep neural networks~(DNNs) have demonstrated\nremarkable success in XMC problems, the task is still challenging because it\nmust deal with a large number of output labels, which make the DNN training\ncomputationally expensive. This paper addresses the issue by exploring the use\nof random circular vectors, where each vector component is represented as a\ncomplex amplitude. In our framework, we can develop an output layer and loss\nfunction of DNNs for XMC by representing the final output layer as a fully\nconnected layer that directly predicts a low-dimensional circular vector\nencoding a set of labels for a data instance. We conducted experiments on\nsynthetic datasets to verify that circular vectors have better label encoding\ncapacity and retrieval ability than normal real-valued vectors. Then, we\nconducted experiments on actual XMC datasets and found that these appealing\nproperties of circular vectors contribute to significant improvements in task\nperformance compared with a previous model using random real-valued vectors,\nwhile reducing the size of the output layers by up to 99%.\n","authors":["Ken Nishida","Kojiro Machi","Kazuma Onishi","Katsuhiko Hayashi","Hidetaka Kamigaito"],"pdf_url":"https://arxiv.org/pdf/2407.05656v1.pdf","comment":"11 pages, 6 figures, 3 tables; accepted to workshop RepL4NLP held in\n  conjunction with ACL 2024"},{"id":"http://arxiv.org/abs/2407.05627v1","updated":"2024-07-08T05:42:29Z","published":"2024-07-08T05:42:29Z","title":"New Directions in Text Classification Research: Maximizing The\n  Performance of Sentiment Classification from Limited Data","summary":"  The stakeholders' needs in sentiment analysis for various issues, whether\npositive or negative, are speed and accuracy. One new challenge in sentiment\nanalysis tasks is the limited training data, which often leads to suboptimal\nmachine learning models and poor performance on test data. This paper discusses\nthe problem of text classification based on limited training data (300 to 600\nsamples) into three classes: positive, negative, and neutral. A benchmark\ndataset is provided for training and testing data on the issue of Kaesang\nPangarep's appointment as Chairman of PSI. External data for aggregation and\naugmentation purposes are provided, consisting of two datasets: the topic of\nCovid Vaccination sentiment and an open topic. The official score used is the\nF1-score, which balances precision and recall among the three classes,\npositive, negative, and neutral. A baseline score is provided as a reference\nfor researchers for unoptimized classification methods. The optimized score is\nprovided as a reference for the target score to be achieved by any proposed\nmethod. Both scoring (baseline and optimized) use the SVM method, which is\nwidely reported as the state-of-the-art in conventional machine learning\nmethods. The F1-scores achieved by the baseline and optimized methods are\n40.83% and 51.28%, respectively.\n","authors":["Surya Agustian","Muhammad Irfan Syah","Nurul Fatiara","Rahmad Abdillah"],"pdf_url":"https://arxiv.org/pdf/2407.05627v1.pdf","comment":"9 pages, in Indonesian language. intro to a shared task in sentiment\n  classification"},{"id":"http://arxiv.org/abs/2407.05609v1","updated":"2024-07-08T04:52:49Z","published":"2024-07-08T04:52:49Z","title":"Open-world Multi-label Text Classification with Extremely Weak\n  Supervision","summary":"  We study open-world multi-label text classification under extremely weak\nsupervision (XWS), where the user only provides a brief description for\nclassification objectives without any labels or ground-truth label space.\nSimilar single-label XWS settings have been explored recently, however, these\nmethods cannot be easily adapted for multi-label. We observe that (1) most\ndocuments have a dominant class covering the majority of content and (2)\nlong-tail labels would appear in some documents as a dominant class. Therefore,\nwe first utilize the user description to prompt a large language model (LLM)\nfor dominant keyphrases of a subset of raw documents, and then construct a\n(initial) label space via clustering. We further apply a zero-shot multi-label\nclassifier to locate the documents with small top predicted scores, so we can\nrevisit their dominant keyphrases for more long-tail labels. We iterate this\nprocess to discover a comprehensive label space and construct a multi-label\nclassifier as a novel method, X-MLClass. X-MLClass exhibits a remarkable\nincrease in ground-truth label space coverage on various datasets, for example,\na 40% improvement on the AAPD dataset over topic modeling and keyword\nextraction methods. Moreover, X-MLClass achieves the best end-to-end\nmulti-label classification accuracy.\n","authors":["Xintong Li","Jinya Jiang","Ria Dharmani","Jayanth Srinivasa","Gaowen Liu","Jingbo Shang"],"pdf_url":"https://arxiv.org/pdf/2407.05609v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.00994v2","updated":"2024-07-08T04:52:23Z","published":"2024-07-01T06:11:30Z","title":"LLM Uncertainty Quantification through Directional Entailment Graph and\n  Claim Level Response Augmentation","summary":"  The Large language models (LLMs) have showcased superior capabilities in\nsophisticated tasks across various domains, stemming from basic question-answer\n(QA), they are nowadays used as decision assistants or explainers for\nunfamiliar content. However, they are not always correct due to the data\nsparsity in specific domain corpus, or the model's hallucination problems.\nGiven this, how much should we trust the responses from LLMs? This paper\npresents a novel way to evaluate the uncertainty that captures the directional\ninstability, by constructing a directional graph from entailment probabilities,\nand we innovatively conduct Random Walk Laplacian given the asymmetric property\nof a constructed directed graph, then the uncertainty is aggregated by the\nderived eigenvalues from the Laplacian process. We also provide a way to\nincorporate the existing work's semantics uncertainty with our proposed layer.\nBesides, this paper identifies the vagueness issues in the raw response set and\nproposes an augmentation approach to mitigate such a problem, we conducted\nextensive empirical experiments and demonstrated the superiority of our\nproposed solutions.\n","authors":["Longchao Da","Tiejin Chen","Lu Cheng","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2407.00994v2.pdf","comment":"11 pages main content, 5 pages appendix"},{"id":"http://arxiv.org/abs/2407.05608v1","updated":"2024-07-08T04:48:43Z","published":"2024-07-08T04:48:43Z","title":"A Benchmark for Multi-speaker Anonymization","summary":"  Privacy-preserving voice protection approaches primarily suppress\nprivacy-related information derived from paralinguistic attributes while\npreserving the linguistic content. Existing solutions focus on single-speaker\nscenarios. However, they lack practicality for real-world applications, i.e.,\nmulti-speaker scenarios. In this paper, we present an initial attempt to\nprovide a multi-speaker anonymization benchmark by defining the task and\nevaluation protocol, proposing benchmarking solutions, and discussing the\nprivacy leakage of overlapping conversations. Specifically, ideal multi-speaker\nanonymization should preserve the number of speakers and the turn-taking\nstructure of the conversation, ensuring accurate context conveyance while\nmaintaining privacy. To achieve that, a cascaded system uses speaker\ndiarization to aggregate the speech of each speaker and speaker anonymization\nto conceal speaker privacy and preserve speech content. Additionally, we\npropose two conversation-level speaker vector anonymization methods to improve\nthe utility further. Both methods aim to make the original and corresponding\npseudo-speaker identities of each speaker unlinkable while preserving or even\nimproving the distinguishability among pseudo-speakers in a conversation. The\nfirst method minimizes the differential similarity across speaker pairs in the\noriginal and anonymized conversations to maintain original speaker\nrelationships in the anonymized version. The other method minimizes the\naggregated similarity across anonymized speakers to achieve better\ndifferentiation between speakers. Experiments conducted on both non-overlap\nsimulated and real-world datasets demonstrate the effectiveness of the\nmulti-speaker anonymization system with the proposed speaker anonymizers.\nAdditionally, we analyzed overlapping speech regarding privacy leakage and\nprovide potential solutions.\n","authors":["Xiaoxiao Miao","Ruijie Tao","Chang Zeng","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05599v1","updated":"2024-07-08T04:21:58Z","published":"2024-07-08T04:21:58Z","title":"Generative Debunking of Climate Misinformation","summary":"  Misinformation about climate change causes numerous negative impacts,\nnecessitating corrective responses. Psychological research has offered various\nstrategies for reducing the influence of climate misinformation, such as the\nfact-myth-fallacy-fact-structure. However, practically implementing corrective\ninterventions at scale represents a challenge. Automatic detection and\ncorrection of misinformation offers a solution to the misinformation problem.\nThis study documents the development of large language models that accept as\ninput a climate myth and produce a debunking that adheres to the\nfact-myth-fallacy-fact (``truth sandwich'') structure, by incorporating\ncontrarian claim classification and fallacy detection into an LLM prompting\nframework. We combine open (Mixtral, Palm2) and proprietary (GPT-4) LLMs with\nprompting strategies of varying complexity. Experiments reveal promising\nperformance of GPT-4 and Mixtral if combined with structured prompts. We\nidentify specific challenges of debunking generation and human evaluation, and\nmap out avenues for future work. We release a dataset of high-quality\ntruth-sandwich debunkings, source code and a demo of the debunking system.\n","authors":["Francisco Zanartu","Yulia Otmakhova","John Cook","Lea Frermann"],"pdf_url":"https://arxiv.org/pdf/2407.05599v1.pdf","comment":"Accepter to ClimateNLP 2024 workshop at ACL 2024"},{"id":"http://arxiv.org/abs/2407.05591v1","updated":"2024-07-08T04:08:35Z","published":"2024-07-08T04:08:35Z","title":"On the Power of Convolution Augmented Transformer","summary":"  The transformer architecture has catalyzed revolutionary advances in language\nmodeling. However, recent architectural recipes, such as state-space models,\nhave bridged the performance gap. Motivated by this, we examine the benefits of\nConvolution-Augmented Transformer (CAT) for recall, copying, and length\ngeneralization tasks. CAT incorporates convolutional filters in the K/Q/V\nembeddings of an attention layer. Through CAT, we show that the locality of the\nconvolution synergizes with the global view of the attention. Unlike comparable\narchitectures, such as Mamba or transformer, CAT can provably solve the\nassociative recall (AR) and copying tasks using a single layer while also\nenjoying guaranteed length generalization. We also establish computational\ntradeoffs between convolution and attention by characterizing how convolution\ncan mitigate the need for full attention by summarizing the context window and\ncreating salient summary tokens to attend. Evaluations on real datasets\ncorroborate our findings and demonstrate that CAT and its variations indeed\nenhance the language modeling performance.\n","authors":["Mingchen Li","Xuechen Zhang","Yixiao Huang","Samet Oymak"],"pdf_url":"https://arxiv.org/pdf/2407.05591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06138v2","updated":"2024-07-08T03:33:52Z","published":"2024-04-09T09:04:30Z","title":"Cendol: Open Instruction-tuned Generative Large Language Models for\n  Indonesian Languages","summary":"  Large language models (LLMs) show remarkable human-like capability in various\ndomains and languages. However, a notable quality gap arises in low-resource\nlanguages, e.g., Indonesian indigenous languages, rendering them ineffective\nand inefficient in such linguistic contexts. To bridge this quality gap, we\nintroduce Cendol, a collection of Indonesian LLMs encompassing both\ndecoder-only and encoder-decoder architectures across a range of model sizes.\nWe highlight Cendol's effectiveness across a diverse array of tasks, attaining\n20% improvement, and demonstrate its capability to generalize to unseen tasks\nand indigenous languages of Indonesia. Furthermore, Cendol models showcase\nimproved human favorability despite their limitations in capturing indigenous\nknowledge and cultural values in Indonesia. In addition, we discuss the\nshortcomings of parameter-efficient tunings, such as LoRA, for language\nadaptation. Alternatively, we propose the usage of vocabulary adaptation to\nenhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that\nsafety in pre-training in one language such as English is transferable to\nlow-resource languages, such as Indonesian, even without RLHF and safety\nfine-tuning.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Fajri Koto","Rifki Afina Putri","Emmanuel Dave","Jhonson Lee","Nuur Shadieq","Wawan Cenggoro","Salsabil Maulana Akbar","Muhammad Ihza Mahendra","Dea Annisayanti Putri","Bryan Wilie","Genta Indra Winata","Alham Fikri Aji","Ayu Purwarianti","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2404.06138v2.pdf","comment":"Cendol models are released under Apache 2.0 license and will be made\n  publicly available soon"},{"id":"http://arxiv.org/abs/2310.01991v2","updated":"2024-07-08T03:33:43Z","published":"2023-10-03T12:03:06Z","title":"Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward\n  Reasoning in Math Word Problems","summary":"  While forward reasoning (i.e., find the answer given the question) has been\nexplored extensively in recent literature, backward reasoning is relatively\nunexplored. We examine the backward reasoning capabilities of LLMs on Math Word\nProblems (MWPs): given a mathematical question and its answer, with some\ndetails omitted from the question, can LLMs effectively retrieve the missing\ninformation? On modifying three benchmark datasets for this task, to evaluate\nthis task: GSM8k, SVAMP, and MultiArith, we find a significant drop in the\naccuracy of models on this task compared to forward reasoning across SOTA LLMs\n(GPT4, GPT3.5, PaLM-2, and LLaMa). Motivated by the fact backward reasoning can\nbe seen as the ''inverse'' of forward reasoning, we propose variations of three\ndifferent forward reasoning strategies to improve performance. Rephrase\nreformulates the given problem into a forward reasoning problem, PAL-Tools\ncombines the idea of Program-Aided LLMs to produce a set of equations that can\nbe solved by an external solver, and Check your Work exploits the availability\nof natural verifier of high accuracy in the forward direction, interleaving\nsolving and verification steps. Finally, realizing that each of our base\nmethods correctly solves a different set of problems, we propose a novel\nBayesian formulation for creating an ensemble over the base methods to further\nboost the accuracy. Extensive experimentation demonstrates successive\nimprovement in the performance of LLMs on the backward reasoning task, using\nour strategies, with our ensemble-based method resulting in significant\nperformance gains compared to the SOTA forward reasoning strategies we adapt.\n","authors":["Aniruddha Deb","Neeva Oza","Sarthak Singla","Dinesh Khandelwal","Dinesh Garg","Parag Singla"],"pdf_url":"https://arxiv.org/pdf/2310.01991v2.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.03627v2","updated":"2024-07-08T03:13:12Z","published":"2024-07-04T04:30:04Z","title":"DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation","summary":"  Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems.\n","authors":["Taeho Hwang","Soyeong Jeong","Sukmin Cho","SeungYoon Han","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2407.03627v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06692v3","updated":"2024-07-08T02:52:05Z","published":"2024-01-12T16:56:54Z","title":"An Experimental Design Framework for Label-Efficient Supervised\n  Finetuning of Large Language Models","summary":"  Supervised finetuning (SFT) on instruction datasets has played a crucial role\nin achieving the remarkable zero-shot generalization capabilities observed in\nmodern large language models (LLMs). However, the annotation efforts required\nto produce high quality responses for instructions are becoming prohibitively\nexpensive, especially as the number of tasks spanned by instruction datasets\ncontinues to increase. Active learning is effective in identifying useful\nsubsets of samples to annotate from an unlabeled pool, but its high\ncomputational cost remains a barrier to its widespread applicability in the\ncontext of LLMs. To mitigate the annotation cost of SFT and circumvent the\ncomputational bottlenecks of active learning, we propose using experimental\ndesign. Experimental design techniques select the most informative samples to\nlabel, and typically maximize some notion of uncertainty and/or diversity. In\nour work, we implement a framework that evaluates several existing and novel\nexperimental design techniques and find that these methods consistently yield\nsignificant gains in label efficiency with little computational overhead. On\ngenerative tasks, our methods achieve the same generalization performance with\nonly $50\\%$ of annotation cost required by random sampling.\n","authors":["Gantavya Bhatt","Yifang Chen","Arnav M. Das","Jifan Zhang","Sang T. Truong","Stephen Mussmann","Yinglun Zhu","Jeffrey Bilmes","Simon S. Du","Kevin Jamieson","Jordan T. Ash","Robert D. Nowak"],"pdf_url":"https://arxiv.org/pdf/2401.06692v3.pdf","comment":"Accepted to Findings of the Association for Computational\n  Linguistics: ACL 2024"},{"id":"http://arxiv.org/abs/2407.05563v1","updated":"2024-07-08T02:39:33Z","published":"2024-07-08T02:39:33Z","title":"LLMBox: A Comprehensive Library for Large Language Models","summary":"  To facilitate the research on large language models (LLMs), this paper\npresents a comprehensive and unified library, LLMBox, to ease the development,\nuse, and evaluation of LLMs. This library is featured with three main merits:\n(1) a unified data interface that supports the flexible implementation of\nvarious training strategies, (2) a comprehensive evaluation that covers\nextensive tasks, datasets, and models, and (3) more practical consideration,\nespecially on user-friendliness and efficiency. With our library, users can\neasily reproduce existing methods, train new models, and conduct comprehensive\nperformance comparisons. To rigorously test LLMBox, we conduct extensive\nexperiments in a diverse coverage of evaluation settings, and experimental\nresults demonstrate the effectiveness and efficiency of our library in\nsupporting various implementations related to LLMs. The detailed introduction\nand usage guidance can be found at https://github.com/RUCAIBox/LLMBox.\n","authors":["Tianyi Tang","Yiwen Hu","Bingqian Li","Wenyang Luo","Zijing Qin","Haoxiang Sun","Jiapeng Wang","Shiyi Xu","Xiaoxue Cheng","Geyang Guo","Han Peng","Bowen Zheng","Yiru Tang","Yingqian Min","Yushuo Chen","Jie Chen","Yuanqian Zhao","Luran Ding","Yuhao Wang","Zican Dong","Chunxuan Xia","Junyi Li","Kun Zhou","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2407.05563v1.pdf","comment":"Accepted by ACL 2024 Demo"},{"id":"http://arxiv.org/abs/2406.15786v2","updated":"2024-07-08T00:28:52Z","published":"2024-06-22T08:41:48Z","title":"What Matters in Transformers? Not All Attention is Needed","summary":"  Scaling Transformer-based large language models (LLMs) has demonstrated\npromising performance across various tasks. However, this scaling also\nintroduces redundant structures, posing challenges for real-world deployment.\nDespite some recognition of redundancy in LLMs, the variability of redundancy\nacross different structures, such as MLP and Attention layers, is\nunder-explored. In this work, we investigate the varying redundancy across\ndifferent modules within Transformers, including Blocks, MLP, and Attention\nlayers, using a similarity-based metric. This metric operates on the premise\nthat redundant structures produce outputs highly similar to their inputs.\nSurprisingly, while attention layers are essential for transformers and\ndistinguish them from other mainstream architectures, we found that a large\nproportion of attention layers exhibit excessively high similarity and can be\nsafely pruned without degrading performance, leading to reduced memory and\ncomputation costs. Additionally, we further propose a method that jointly drops\nAttention and MLP layers, achieving improved performance and dropping ratios.\nExtensive experiments demonstrate the effectiveness of our methods, e.g.,\nLlama-3-70B maintains comparable performance even after pruning half of the\nattention layers. Our findings provide valuable insights for future network\narchitecture design. The code will be released at:\n\\url{https://github.com/Shwai-He/LLM-Drop}.\n","authors":["Shwai He","Guoheng Sun","Zheyu Shen","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2406.15786v2.pdf","comment":"15 pages, 13 figures, 6 tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.06192v1","updated":"2024-07-08T17:59:57Z","published":"2024-07-08T17:59:57Z","title":"Multi-Object Hallucination in Vision-Language Models","summary":"  Large vision language models (LVLMs) often suffer from object hallucination,\nproducing objects not present in the given images. While current benchmarks for\nobject hallucination primarily concentrate on the presence of a single object\nclass rather than individual entities, this work systematically investigates\nmulti-object hallucination, examining how models misperceive (e.g., invent\nnonexistent objects or become distracted) when tasked with focusing on multiple\nobjects simultaneously. We introduce Recognition-based Object Probing\nEvaluation (ROPE), an automated evaluation protocol that considers the\ndistribution of object classes within a single image during testing and uses\nvisual referring prompts to eliminate ambiguity. With comprehensive empirical\nstudies and analysis of potential factors leading to multi-object\nhallucination, we found that (1) LVLMs suffer more hallucinations when focusing\non multiple objects compared to a single object. (2) The tested object class\ndistribution affects hallucination behaviors, indicating that LVLMs may follow\nshortcuts and spurious correlations.(3) Hallucinatory behaviors are influenced\nby data-specific factors, salience and frequency, and model intrinsic\nbehaviors. We hope to enable LVLMs to recognize and reason about multiple\nobjects that often occur in realistic visual scenes, provide insights, and\nquantify our progress towards mitigating the issues.\n","authors":["Xuweiyi Chen","Ziqiao Ma","Xuejun Zhang","Sihan Xu","Shengyi Qian","Jianing Yang","David F. Fouhey","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2407.06192v1.pdf","comment":"Accepted to ALVR @ ACL 2024 | Project page:\n  https://multi-object-hallucination.github.io/"},{"id":"http://arxiv.org/abs/2407.06191v1","updated":"2024-07-08T17:59:55Z","published":"2024-07-08T17:59:55Z","title":"Tailor3D: Customized 3D Assets Editing and Generation with Dual-Side\n  Images","summary":"  Recent advances in 3D AIGC have shown promise in directly creating 3D objects\nfrom text and images, offering significant cost savings in animation and\nproduct design. However, detailed edit and customization of 3D assets remains a\nlong-standing challenge. Specifically, 3D Generation methods lack the ability\nto follow finely detailed instructions as precisely as their 2D image creation\ncounterparts. Imagine you can get a toy through 3D AIGC but with undesired\naccessories and dressing. To tackle this challenge, we propose a novel pipeline\ncalled Tailor3D, which swiftly creates customized 3D assets from editable\ndual-side images. We aim to emulate a tailor's ability to locally change\nobjects or perform overall style transfer. Unlike creating 3D assets from\nmultiple views, using dual-side images eliminates conflicts on overlapping\nareas that occur when editing individual views. Specifically, it begins by\nediting the front view, then generates the back view of the object through\nmulti-view diffusion. Afterward, it proceeds to edit the back views. Finally, a\nDual-sided LRM is proposed to seamlessly stitch together the front and back 3D\nfeatures, akin to a tailor sewing together the front and back of a garment. The\nDual-sided LRM rectifies imperfect consistencies between the front and back\nviews, enhancing editing capabilities and reducing memory burdens while\nseamlessly integrating them into a unified 3D representation with the LoRA\nTriplane Transformer. Experimental results demonstrate Tailor3D's effectiveness\nacross various 3D generation and editing tasks, including 3D generative fill\nand style transfer. It provides a user-friendly, efficient solution for editing\n3D assets, with each editing step taking only seconds to complete.\n","authors":["Zhangyang Qi","Yunhan Yang","Mengchen Zhang","Long Xing","Xiaoyang Wu","Tong Wu","Dahua Lin","Xihui Liu","Jiaqi Wang","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.06191v1.pdf","comment":"Project Page: https://tailor3d-2024.github.io/"},{"id":"http://arxiv.org/abs/2407.06190v1","updated":"2024-07-08T17:59:54Z","published":"2024-07-08T17:59:54Z","title":"4D Contrastive Superflows are Dense 3D Representation Learners","summary":"  In the realm of autonomous driving, accurate 3D perception is the foundation.\nHowever, developing such models relies on extensive human annotations -- a\nprocess that is both costly and labor-intensive. To address this challenge from\na data representation learning perspective, we introduce SuperFlow, a novel\nframework designed to harness consecutive LiDAR-camera pairs for establishing\nspatiotemporal pretraining objectives. SuperFlow stands out by integrating two\nkey designs: 1) a dense-to-sparse consistency regularization, which promotes\ninsensitivity to point cloud density variations during feature learning, and 2)\na flow-based contrastive learning module, carefully crafted to extract\nmeaningful temporal cues from readily available sensor calibrations. To further\nboost learning efficiency, we incorporate a plug-and-play view consistency\nmodule that enhances the alignment of the knowledge distilled from camera\nviews. Extensive comparative and ablation studies across 11 heterogeneous LiDAR\ndatasets validate our effectiveness and superiority. Additionally, we observe\nseveral interesting emerging properties by scaling up the 2D and 3D backbones\nduring pretraining, shedding light on the future research of 3D foundation\nmodels for LiDAR-based perception.\n","authors":["Xiang Xu","Lingdong Kong","Hui Shuai","Wenwei Zhang","Liang Pan","Kai Chen","Ziwei Liu","Qingshan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.06190v1.pdf","comment":"ECCV 2024; 36 pages, 11 figures, 11 tables; Code at\n  https://github.com/Xiangxu-0103/SuperFlow"},{"id":"http://arxiv.org/abs/2407.06189v1","updated":"2024-07-08T17:59:42Z","published":"2024-07-08T17:59:42Z","title":"Video-STaR: Self-Training Enables Video Instruction Tuning with Any\n  Supervision","summary":"  The performance of Large Vision Language Models (LVLMs) is dependent on the\nsize and quality of their training datasets. Existing video instruction tuning\ndatasets lack diversity as they are derived by prompting large language models\nwith video captions to generate question-answer pairs, and are therefore mostly\ndescriptive. Meanwhile, many labeled video datasets with diverse labels and\nsupervision exist - however, we find that their integration into LVLMs is\nnon-trivial. Herein, we present Video Self-Training with augmented Reasoning\n(Video-STaR), the first video self-training approach. Video-STaR allows the\nutilization of any labeled video dataset for video instruction tuning. In\nVideo-STaR, an LVLM cycles between instruction generation and finetuning, which\nwe show (I) improves general video understanding and (II) adapts LVLMs to novel\ndownstream tasks with existing supervision. During generation, an LVLM is\nprompted to propose an answer. The answers are then filtered only to those that\ncontain the original video labels, and the LVLM is then re-trained on the\ngenerated dataset. By only training on generated answers that contain the\ncorrect video labels, Video-STaR utilizes these existing video labels as weak\nsupervision for video instruction tuning. Our results demonstrate that\nVideo-STaR-enhanced LVLMs exhibit improved performance in (I) general video QA,\nwhere TempCompass performance improved by 10%, and (II) on downstream tasks,\nwhere Video-STaR improved Kinetics700-QA accuracy by 20% and action quality\nassessment on FineDiving by 15%.\n","authors":["Orr Zohar","Xiaohan Wang","Yonatan Bitton","Idan Szpektor","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2407.06189v1.pdf","comment":"Project page: https://orrzohar.github.io/projects/video-star/"},{"id":"http://arxiv.org/abs/2407.06188v1","updated":"2024-07-08T17:59:36Z","published":"2024-07-08T17:59:36Z","title":"CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation","summary":"  Crowd Motion Generation is essential in entertainment industries such as\nanimation and games as well as in strategic fields like urban simulation and\nplanning. This new task requires an intricate integration of control and\ngeneration to realistically synthesize crowd dynamics under specific spatial\nand semantic constraints, whose challenges are yet to be fully explored. On the\none hand, existing human motion generation models typically focus on individual\nbehaviors, neglecting the complexities of collective behaviors. On the other\nhand, recent methods for multi-person motion generation depend heavily on\npre-defined scenarios and are limited to a fixed, small number of inter-person\ninteractions, thus hampering their practicality. To overcome these challenges,\nwe introduce CrowdMoGen, a zero-shot text-driven framework that harnesses the\npower of Large Language Model (LLM) to incorporate the collective intelligence\ninto the motion generation framework as guidance, thereby enabling\ngeneralizable planning and generation of crowd motions without paired training\ndata. Our framework consists of two key components: 1) Crowd Scene Planner that\nlearns to coordinate motions and dynamics according to specific scene contexts\nor introduced perturbations, and 2) Collective Motion Generator that\nefficiently synthesizes the required collective motions based on the holistic\nplans. Extensive quantitative and qualitative experiments have validated the\neffectiveness of our framework, which not only fills a critical gap by\nproviding scalable and generalizable solutions for Crowd Motion Generation task\nbut also achieves high levels of realism and flexibility.\n","authors":["Xinying Guo","Mingyuan Zhang","Haozhe Xie","Chenyang Gu","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.06188v1.pdf","comment":"Project page: https://gxyes.github.io/projects/CrowdMoGen.html"},{"id":"http://arxiv.org/abs/2407.06187v1","updated":"2024-07-08T17:59:02Z","published":"2024-07-08T17:59:02Z","title":"JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized\n  Text-to-Image Generation","summary":"  Personalized text-to-image generation models enable users to create images\nthat depict their individual possessions in diverse scenes, finding\napplications in various domains. To achieve the personalization capability,\nexisting methods rely on finetuning a text-to-image foundation model on a\nuser's custom dataset, which can be non-trivial for general users,\nresource-intensive, and time-consuming. Despite attempts to develop\nfinetuning-free methods, their generation quality is much lower compared to\ntheir finetuning counterparts. In this paper, we propose Joint-Image Diffusion\n(\\jedi), an effective technique for learning a finetuning-free personalization\nmodel. Our key idea is to learn the joint distribution of multiple related\ntext-image pairs that share a common subject. To facilitate learning, we\npropose a scalable synthetic dataset generation technique. Once trained, our\nmodel enables fast and easy personalization at test time by simply using\nreference images as input during the sampling process. Our approach does not\nrequire any expensive optimization process or additional modules and can\nfaithfully preserve the identity represented by any number of reference images.\nExperimental results show that our model achieves state-of-the-art generation\nquality, both quantitatively and qualitatively, significantly outperforming\nboth the prior finetuning-based and finetuning-free personalization baselines.\n","authors":["Yu Zeng","Vishal M. Patel","Haochen Wang","Xun Huang","Ting-Chun Wang","Ming-Yu Liu","Yogesh Balaji"],"pdf_url":"https://arxiv.org/pdf/2407.06187v1.pdf","comment":"CVPR 24"},{"id":"http://arxiv.org/abs/2407.06178v1","updated":"2024-07-08T17:52:23Z","published":"2024-07-08T17:52:23Z","title":"Transfer Learning with Self-Supervised Vision Transformers for Snake\n  Identification","summary":"  We present our approach for the SnakeCLEF 2024 competition to predict snake\nspecies from images. We explore and use Meta's DINOv2 vision transformer model\nfor feature extraction to tackle species' high variability and visual\nsimilarity in a dataset of 182,261 images. We perform exploratory analysis on\nembeddings to understand their structure, and train a linear classifier on the\nembeddings to predict species. Despite achieving a score of 39.69, our results\nshow promise for DINOv2 embeddings in snake identification. All code for this\nproject is available at https://github.com/dsgt-kaggle-clef/snakeclef-2024.\n","authors":["Anthony Miyaguchi","Murilo Gustineli","Austin Fischer","Ryan Lundqvist"],"pdf_url":"https://arxiv.org/pdf/2407.06178v1.pdf","comment":"Paper submitted to CLEF 2024 CEUR-WS"},{"id":"http://arxiv.org/abs/2407.06177v1","updated":"2024-07-08T17:50:00Z","published":"2024-07-08T17:50:00Z","title":"Vision-Language Models under Cultural and Inclusive Considerations","summary":"  Large vision-language models (VLMs) can assist visually impaired people by\ndescribing images from their daily lives. Current evaluation datasets may not\nreflect diverse cultural user backgrounds or the situational context of this\nuse case. To address this problem, we create a survey to determine caption\npreferences and propose a culture-centric evaluation benchmark by filtering\nVizWiz, an existing dataset with images taken by people who are blind. We then\nevaluate several VLMs, investigating their reliability as visual assistants in\na culturally diverse setting. While our results for state-of-the-art models are\npromising, we identify challenges such as hallucination and misalignment of\nautomatic evaluation metrics with human judgment. We make our survey, data,\ncode, and model outputs publicly available.\n","authors":["Antonia Karamolegkou","Phillip Rust","Yong Cao","Ruixiang Cui","Anders SÃ¸gaard","Daniel Hershcovich"],"pdf_url":"https://arxiv.org/pdf/2407.06177v1.pdf","comment":"HuCLLM @ ACL 2024"},{"id":"http://arxiv.org/abs/2407.06174v1","updated":"2024-07-08T17:49:41Z","published":"2024-07-08T17:49:41Z","title":"The Tug-of-War Between Deepfake Generation and Detection","summary":"  Multimodal generative models are rapidly evolving, leading to a surge in the\ngeneration of realistic video and audio that offers exciting possibilities but\nalso serious risks. Deepfake videos, which can convincingly impersonate\nindividuals, have particularly garnered attention due to their potential misuse\nin spreading misinformation and creating fraudulent content. This survey paper\nexamines the dual landscape of deepfake video generation and detection,\nemphasizing the need for effective countermeasures against potential abuses. We\nprovide a comprehensive overview of current deepfake generation techniques,\nincluding face swapping, reenactment, and audio-driven animation, which\nleverage cutting-edge technologies like generative adversarial networks and\ndiffusion models to produce highly realistic fake videos. Additionally, we\nanalyze various detection approaches designed to differentiate authentic from\naltered videos, from detecting visual artifacts to deploying advanced\nalgorithms that pinpoint inconsistencies across video and audio signals.\n  The effectiveness of these detection methods heavily relies on the diversity\nand quality of datasets used for training and evaluation. We discuss the\nevolution of deepfake datasets, highlighting the importance of robust, diverse,\nand frequently updated collections to enhance the detection accuracy and\ngeneralizability. As deepfakes become increasingly indistinguishable from\nauthentic content, developing advanced detection techniques that can keep pace\nwith generation technologies is crucial. We advocate for a proactive approach\nin the \"tug-of-war\" between deepfake creators and detectors, emphasizing the\nneed for continuous research collaboration, standardization of evaluation\nmetrics, and the creation of comprehensive benchmarks.\n","authors":["Hannah Lee","Changyeon Lee","Kevin Farhat","Lin Qiu","Steve Geluso","Aerin Kim","Oren Etzioni"],"pdf_url":"https://arxiv.org/pdf/2407.06174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06169v1","updated":"2024-07-08T17:48:39Z","published":"2024-07-08T17:48:39Z","title":"Potential Based Diffusion Motion Planning","summary":"  Effective motion planning in high dimensional spaces is a long-standing open\nproblem in robotics. One class of traditional motion planning algorithms\ncorresponds to potential-based motion planning. An advantage of potential based\nmotion planning is composability -- different motion constraints can be easily\ncombined by adding corresponding potentials. However, constructing motion paths\nfrom potentials requires solving a global optimization across configuration\nspace potential landscape, which is often prone to local minima. We propose a\nnew approach towards learning potential based motion planning, where we train a\nneural network to capture and learn an easily optimizable potentials over\nmotion planning trajectories. We illustrate the effectiveness of such approach,\nsignificantly outperforming both classical and recent learned motion planning\napproaches and avoiding issues with local minima. We further illustrate its\ninherent composability, enabling us to generalize to a multitude of different\nmotion constraints.\n","authors":["Yunhao Luo","Chen Sun","Joshua B. Tenenbaum","Yilun Du"],"pdf_url":"https://arxiv.org/pdf/2407.06169v1.pdf","comment":"ICML 2024. Project page and code at\n  https://energy-based-model.github.io/potential-motion-plan/"},{"id":"http://arxiv.org/abs/2407.06168v1","updated":"2024-07-08T17:47:45Z","published":"2024-07-08T17:47:45Z","title":"TARGO: Benchmarking Target-driven Object Grasping under Occlusions","summary":"  Recent advances in predicting 6D grasp poses from a single depth image have\nled to promising performance in robotic grasping. However, previous grasping\nmodels face challenges in cluttered environments where nearby objects impact\nthe target object's grasp. In this paper, we first establish a new benchmark\ndataset for TARget-driven Grasping under Occlusions, named TARGO. We make the\nfollowing contributions: 1) We are the first to study the occlusion level of\ngrasping. 2) We set up an evaluation benchmark consisting of large-scale\nsynthetic data and part of real-world data, and we evaluated five grasp models\nand found that even the current SOTA model suffers when the occlusion level\nincreases, leaving grasping under occlusion still a challenge. 3) We also\ngenerate a large-scale training dataset via a scalable pipeline, which can be\nused to boost the performance of grasping under occlusion and generalized to\nthe real world. 4) We further propose a transformer-based grasping model\ninvolving a shape completion module, termed TARGO-Net, which performs most\nrobustly as occlusion increases. Our benchmark dataset can be found at\nhttps://TARGO-benchmark.github.io/.\n","authors":["Yan Xia","Ran Ding","Ziyuan Qin","Guanqi Zhan","Kaichen Zhou","Long Yang","Hao Dong","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2407.06168v1.pdf","comment":"19 pages, 17 figures"},{"id":"http://arxiv.org/abs/2407.06167v1","updated":"2024-07-08T17:45:40Z","published":"2024-07-08T17:45:40Z","title":"DÎµpS: Delayed Îµ-Shrinking for Faster Once-For-All\n  Training","summary":"  CNNs are increasingly deployed across different hardware, dynamic\nenvironments, and low-power embedded devices. This has led to the design and\ntraining of CNN architectures with the goal of maximizing accuracy subject to\nsuch variable deployment constraints. As the number of deployment scenarios\ngrows, there is a need to find scalable solutions to design and train\nspecialized CNNs. Once-for-all training has emerged as a scalable approach that\njointly co-trains many models (subnets) at once with a constant training cost\nand finds specialized CNNs later. The scalability is achieved by training the\nfull model and simultaneously reducing it to smaller subnets that share model\nweights (weight-shared shrinking). However, existing once-for-all training\napproaches incur huge training costs reaching 1200 GPU hours. We argue this is\nbecause they either start the process of shrinking the full model too early or\ntoo late. Hence, we propose Delayed $\\epsilon$-Shrinking (D$\\epsilon$pS) that\nstarts the process of shrinking the full model when it is partially trained\n(~50%) which leads to training cost improvement and better in-place knowledge\ndistillation to smaller models. The proposed approach also consists of novel\nheuristics that dynamically adjust subnet learning rates incrementally (E),\nleading to improved weight-shared knowledge distillation from larger to smaller\nsubnets as well. As a result, DEpS outperforms state-of-the-art once-for-all\ntraining techniques across different datasets including CIFAR10/100,\nImageNet-100, and ImageNet-1k on accuracy and cost. It achieves 1.83% higher\nImageNet-1k top1 accuracy or the same accuracy with 1.3x reduction in FLOPs and\n2.5x drop in training cost (GPU*hrs)\n","authors":["Aditya Annavajjala","Alind Khare","Animesh Agrawal","Igor Fedorov","Hugo Latapie","Myungjin Lee","Alexey Tumanov"],"pdf_url":"https://arxiv.org/pdf/2407.06167v1.pdf","comment":"Accepted to the 18th European Conference on Computer Vision (ECCV\n  2024)"},{"id":"http://arxiv.org/abs/2406.04313v3","updated":"2024-07-08T17:42:41Z","published":"2024-06-06T17:57:04Z","title":"Improving Alignment and Robustness with Circuit Breakers","summary":"  AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that interrupts the models as they respond with harmful outputs\nwith \"circuit breakers.\" Existing techniques aimed at improving alignment, such\nas refusal training, are often bypassed. Techniques such as adversarial\ntraining try to plug these holes by countering specific attacks. As an\nalternative to refusal training and adversarial training, circuit-breaking\ndirectly controls the representations that are responsible for harmful outputs\nin the first place. Our technique can be applied to both text-only and\nmultimodal language models to prevent the generation of harmful outputs without\nsacrificing utility -- even in the presence of powerful unseen attacks.\nNotably, while adversarial robustness in standalone image recognition remains\nan open challenge, circuit breakers allow the larger multimodal system to\nreliably withstand image \"hijacks\" that aim to produce harmful content.\nFinally, we extend our approach to AI agents, demonstrating considerable\nreductions in the rate of harmful actions when they are under attack. Our\napproach represents a significant step forward in the development of reliable\nsafeguards to harmful behavior and adversarial attacks.\n","authors":["Andy Zou","Long Phan","Justin Wang","Derek Duenas","Maxwell Lin","Maksym Andriushchenko","Rowan Wang","Zico Kolter","Matt Fredrikson","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2406.04313v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17119v2","updated":"2024-07-08T17:23:22Z","published":"2024-06-24T20:13:23Z","title":"Accelerating Phase Field Simulations Through a Hybrid Adaptive Fourier\n  Neural Operator with U-Net Backbone","summary":"  Prolonged contact between a corrosive liquid and metal alloys can cause\nprogressive dealloying. For such liquid-metal dealloying (LMD) process, phase\nfield models have been developed. However, the governing equations often\ninvolve coupled non-linear partial differential equations (PDE), which are\nchallenging to solve numerically. In particular, stiffness in the PDEs requires\nan extremely small time steps (e.g. $10^{-12}$ or smaller). This computational\nbottleneck is especially problematic when running LMD simulation until a late\ntime horizon is required. This motivates the development of surrogate models\ncapable of leaping forward in time, by skipping several consecutive time steps\nat-once. In this paper, we propose U-Shaped Adaptive Fourier Neural Operators\n(U-AFNO), a machine learning (ML) model inspired by recent advances in neural\noperator learning. U-AFNO employs U-Nets for extracting and reconstructing\nlocal features within the physical fields, and passes the latent space through\na vision transformer (ViT) implemented in the Fourier space (AFNO). We use\nU-AFNOs to learn the dynamics mapping the field at a current time step into a\nlater time step. We also identify global quantities of interest (QoI)\ndescribing the corrosion process (e.g. the deformation of the liquid-metal\ninterface) and show that our proposed U-AFNO model is able to accurately\npredict the field dynamics, in-spite of the chaotic nature of LMD. Our model\nreproduces the key micro-structure statistics and QoIs with a level of accuracy\non-par with the high-fidelity numerical solver. We also investigate the\nopportunity of using hybrid simulations, in which we alternate forward leap in\ntime using the U-AFNO with high-fidelity time stepping. We demonstrate that\nwhile advantageous for some surrogate model design choices, our proposed U-AFNO\nmodel in fully auto-regressive settings consistently outperforms hybrid\nschemes.\n","authors":["Christophe Bonneville","Nathan Bieberdorf","Arun Hegde","Mark Asta","Habib N. Najm","Laurent Capolungo","Cosmin Safta"],"pdf_url":"https://arxiv.org/pdf/2406.17119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06150v1","updated":"2024-07-08T17:22:27Z","published":"2024-07-08T17:22:27Z","title":"PanDORA: Casual HDR Radiance Acquisition for Indoor Scenes","summary":"  Most novel view synthesis methods such as NeRF are unable to capture the true\nhigh dynamic range (HDR) radiance of scenes since they are typically trained on\nphotos captured with standard low dynamic range (LDR) cameras. While the\ntraditional exposure bracketing approach which captures several images at\ndifferent exposures has recently been adapted to the multi-view case, we find\nsuch methods to fall short of capturing the full dynamic range of indoor\nscenes, which includes very bright light sources. In this paper, we present\nPanDORA: a PANoramic Dual-Observer Radiance Acquisition system for the casual\ncapture of indoor scenes in high dynamic range. Our proposed system comprises\ntwo 360{\\deg} cameras rigidly attached to a portable tripod. The cameras\nsimultaneously acquire two 360{\\deg} videos: one at a regular exposure and the\nother at a very fast exposure, allowing a user to simply wave the apparatus\ncasually around the scene in a matter of minutes. The resulting images are fed\nto a NeRF-based algorithm that reconstructs the scene's full high dynamic\nrange. Compared to HDR baselines from previous work, our approach reconstructs\nthe full HDR radiance of indoor scenes without sacrificing the visual quality\nwhile retaining the ease of capture from recent NeRF-like approaches.\n","authors":["Mohammad Reza Karimi Dastjerdi","FrÃ©dÃ©ric Fortier-Chouinard","Yannick Hold-Geoffroy","Marc HÃ©bert","Claude Demers","Nima Kalantari","Jean-FranÃ§ois Lalonde"],"pdf_url":"https://arxiv.org/pdf/2407.06150v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.06136v1","updated":"2024-07-08T17:09:39Z","published":"2024-07-08T17:09:39Z","title":"Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for\n  Few-Shot Class-Incremental Learning","summary":"  Few-shot class-incremental learning (FSCIL) confronts the challenge of\nintegrating new classes into a model with minimal training samples while\npreserving the knowledge of previously learned classes. Traditional methods\nwidely adopt static adaptation relying on a fixed parameter space to learn from\ndata that arrive sequentially, prone to overfitting to the current session.\nExisting dynamic strategies require the expansion of the parameter space\ncontinually, leading to increased complexity. To address these challenges, we\nintegrate the recently proposed selective state space model (SSM) into FSCIL.\nConcretely, we propose a dual selective SSM projector that dynamically adjusts\nthe projection parameters based on the intermediate features for dynamic\nadaptation. The dual design enables the model to maintain the robust features\nof base classes, while adaptively learning distinctive feature shifts for novel\nclasses. Additionally, we develop a class-sensitive selective scan mechanism to\nguide dynamic adaptation. It minimizes the disruption to base-class\nrepresentations caused by training on novel data, and meanwhile, forces the\nselective scan to perform in distinct patterns between base and novel classes.\nExperiments on miniImageNet, CUB-200, and CIFAR-100 demonstrate that our\nframework outperforms the existing state-of-the-art methods. The code is\navailable at https://github.com/xiaojieli0903/Mamba-FSCIL.\n","authors":["Xiaojie Li","Yibo Yang","Jianlong Wu","Bernard Ghanem","Liqiang Nie","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.06136v1.pdf","comment":"Code: https://github.com/xiaojieli0903/Mamba-FSCIL"},{"id":"http://arxiv.org/abs/2407.06135v1","updated":"2024-07-08T17:08:02Z","published":"2024-07-08T17:08:02Z","title":"ANOLE: An Open, Autoregressive, Native Large Multimodal Models for\n  Interleaved Image-Text Generation","summary":"  Previous open-source large multimodal models (LMMs) have faced several\nlimitations: (1) they often lack native integration, requiring adapters to\nalign visual representations with pre-trained large language models (LLMs); (2)\nmany are restricted to single-modal generation; (3) while some support\nmultimodal generation, they rely on separate diffusion models for visual\nmodeling and generation. To mitigate these limitations, we present Anole, an\nopen, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build Anole from Meta AI's Chameleon, adopting an innovative\nfine-tuning strategy that is both data-efficient and parameter-efficient. Anole\ndemonstrates high-quality, coherent multimodal generation capabilities. We have\nopen-sourced our model, training framework, and instruction tuning data.\n","authors":["Ethan Chern","Jiadi Su","Yan Ma","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.06135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06124v1","updated":"2024-07-08T17:00:28Z","published":"2024-07-08T17:00:28Z","title":"Structured Generations: Using Hierarchical Clusters to guide Diffusion\n  Models","summary":"  This paper introduces Diffuse-TreeVAE, a deep generative model that\nintegrates hierarchical clustering into the framework of Denoising Diffusion\nProbabilistic Models (DDPMs). The proposed approach generates new images by\nsampling from a root embedding of a learned latent tree VAE-based structure, it\nthen propagates through hierarchical paths, and utilizes a second-stage DDPM to\nrefine and generate distinct, high-quality images for each data cluster. The\nresult is a model that not only improves image clarity but also ensures that\nthe generated samples are representative of their respective clusters,\naddressing the limitations of previous VAE-based methods and advancing the\nstate of clustering-based generative modeling.\n","authors":["Jorge da Silva Goncalves","Laura Manduchi","Moritz Vandenhirtz","Julia Vogt"],"pdf_url":"https://arxiv.org/pdf/2407.06124v1.pdf","comment":"8 pages, 7 figures, Structured Probabilistic Inference & Generative\n  Modeling workshop of ICML 2024"},{"id":"http://arxiv.org/abs/2311.14775v2","updated":"2024-07-08T16:59:16Z","published":"2023-11-24T15:07:29Z","title":"VSViG: Real-time Video-based Seizure Detection via Skeleton-based\n  Spatiotemporal ViG","summary":"  An accurate and efficient epileptic seizure onset detection can significantly\nbenefit patients. Traditional diagnostic methods, primarily relying on\nelectroencephalograms (EEGs), often result in cumbersome and non-portable\nsolutions, making continuous patient monitoring challenging. The video-based\nseizure detection system is expected to free patients from the constraints of\nscalp or implanted EEG devices and enable remote monitoring in residential\nsettings. Previous video-based methods neither enable all-day monitoring nor\nprovide short detection latency due to insufficient resources and ineffective\npatient action recognition techniques. Additionally, skeleton-based action\nrecognition approaches remain limitations in identifying subtle seizure-related\nactions. To address these challenges, we propose a novel Video-based Seizure\ndetection model via a skeleton-based spatiotemporal Vision Graph neural network\n(VSViG) for its efficient, accurate and timely purpose in real-time scenarios.\nOur experimental results indicate VSViG outperforms previous state-of-the-art\naction recognition models on our collected patients' video data with higher\naccuracy (5.9% error), lower FLOPs (0.4G), and smaller model size (1.4M).\nFurthermore, by integrating a decision-making rule that combines output\nprobabilities and an accumulative function, we achieve a 5.1 s detection\nlatency after EEG onset, a 13.1 s detection advance before clinical onset, and\na zero false detection rate. The project homepage is available at:\nhttps://github.com/xuyankun/VSViG/\n","authors":["Yankun Xu","Junzhe Wang","Yun-Hsuan Chen","Jie Yang","Wenjie Ming","Shuang Wang","Mohamad Sawan"],"pdf_url":"https://arxiv.org/pdf/2311.14775v2.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.06113v1","updated":"2024-07-08T16:49:01Z","published":"2024-07-08T16:49:01Z","title":"C2C: Component-to-Composition Learning for Zero-Shot Compositional\n  Action Recognition","summary":"  Compositional actions consist of dynamic (verbs) and static (objects)\nconcepts. Humans can easily recognize unseen compositions using the learned\nconcepts. For machines, solving such a problem requires a model to recognize\nunseen actions composed of previously observed verbs and objects, thus\nrequiring, so-called, compositional generalization ability. To facilitate this\nresearch, we propose a novel Zero-Shot Compositional Action Recognition\n(ZS-CAR) task. For evaluating the task, we construct a new benchmark,\nSomething-composition (Sth-com), based on the widely used Something-Something\nV2 dataset. We also propose a novel Component-to-Composition (C2C) learning\nmethod to solve the new ZS-CAR task. C2C includes an independent component\nlearning module and a composition inference module. Last, we devise an enhanced\ntraining strategy to address the challenges of component variation between seen\nand unseen compositions and to handle the subtle balance between learning seen\nand unseen actions. The experimental results demonstrate that the proposed\nframework significantly surpasses the existing compositional generalization\nmethods and sets a new state-of-the-art. The new Sth-com benchmark and code are\navailable at https://github.com/RongchangLi/ZSCAR_C2C.\n","authors":["Rongchang Li","Zhenhua Feng","Tianyang Xu","Linze Li","Xiao-Jun Wu","Muhammad Awais","Sara Atito","Josef Kittler"],"pdf_url":"https://arxiv.org/pdf/2407.06113v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.06110v1","updated":"2024-07-08T16:47:19Z","published":"2024-07-08T16:47:19Z","title":"FGA: Fourier-Guided Attention Network for Crowd Count Estimation","summary":"  Crowd counting is gaining societal relevance, particularly in domains of\nUrban Planning, Crowd Management, and Public Safety. This paper introduces\nFourier-guided attention (FGA), a novel attention mechanism for crowd count\nestimation designed to address the inefficient full-scale global pattern\ncapture in existing works on convolution-based attention networks. FGA\nefficiently captures multi-scale information, including full-scale global\npatterns, by utilizing Fast-Fourier Transformations (FFT) along with spatial\nattention for global features and convolutions with channel-wise attention for\nsemi-global and local features. The architecture of FGA involves a dual-path\napproach: (1) a path for processing full-scale global features through FFT,\nallowing for efficient extraction of information in the frequency domain, and\n(2) a path for processing remaining feature maps for semi-global and local\nfeatures using traditional convolutions and channel-wise attention. This\ndual-path architecture enables FGA to seamlessly integrate frequency and\nspatial information, enhancing its ability to capture diverse crowd patterns.\nWe apply FGA in the last layers of two popular crowd-counting works, CSRNet and\nCANNet, to evaluate the module's performance on benchmark datasets such as\nShanghaiTech-A, ShanghaiTech-B, UCF-CC-50, and JHU++ crowd. The experiments\ndemonstrate a notable improvement across all datasets based on\nMean-Squared-Error (MSE) and Mean-Absolute-Error (MAE) metrics, showing\ncomparable performance to recent state-of-the-art methods. Additionally, we\nillustrate the interpretability using qualitative analysis, leveraging Grad-CAM\nheatmaps, to show the effectiveness of FGA in capturing crowd patterns.\n","authors":["Yashwardhan Chaudhuri","Ankit Kumar","Arun Balaji Buduru","Adel Alshamrani"],"pdf_url":"https://arxiv.org/pdf/2407.06110v1.pdf","comment":"Accepted to IJCNN'24"},{"id":"http://arxiv.org/abs/2407.06109v1","updated":"2024-07-08T16:46:47Z","published":"2024-07-08T16:46:47Z","title":"PerlDiff: Controllable Street View Synthesis Using Perspective-Layout\n  Diffusion Models","summary":"  Controllable generation is considered a potentially vital approach to address\nthe challenge of annotating 3D data, and the precision of such controllable\ngeneration becomes particularly imperative in the context of data production\nfor autonomous driving. Existing methods focus on the integration of diverse\ngenerative information into controlling inputs, utilizing frameworks such as\nGLIGEN or ControlNet, to produce commendable outcomes in controllable\ngeneration. However, such approaches intrinsically restrict generation\nperformance to the learning capacities of predefined network architectures. In\nthis paper, we explore the integration of controlling information and introduce\nPerlDiff (Perspective-Layout Diffusion Models), a method for effective street\nview image generation that fully leverages perspective 3D geometric\ninformation. Our PerlDiff employs 3D geometric priors to guide the generation\nof street view images with precise object-level control within the network\nlearning process, resulting in a more robust and controllable output. Moreover,\nit demonstrates superior controllability compared to alternative layout control\nmethods. Empirical results justify that our PerlDiff markedly enhances the\nprecision of generation on the NuScenes and KITTI datasets. Our codes and\nmodels are publicly available at https://github.com/LabShuHangGU/PerlDiff.\n","authors":["Jinhua Zhang","Hualian Sheng","Sijia Cai","Bing Deng","Qiao Liang","Wen Li","Ying Fu","Jieping Ye","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2407.06109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06096v1","updated":"2024-07-08T16:36:55Z","published":"2024-07-08T16:36:55Z","title":"Muzzle-Based Cattle Identification System Using Artificial Intelligence\n  (AI)","summary":"  Absence of tamper-proof cattle identification technology was a significant\nproblem preventing insurance companies from providing livestock insurance. This\nlack of technology had devastating financial consequences for marginal farmers\nas they did not have the opportunity to claim compensation for any unexpected\nevents such as the accidental death of cattle in Bangladesh. Using machine\nlearning and deep learning algorithms, we have solved the bottleneck of cattle\nidentification by developing and introducing a muzzle-based cattle\nidentification system. The uniqueness of cattle muzzles has been scientifically\nestablished, which resembles human fingerprints. This is the fundamental\npremise that prompted us to develop a cattle identification system that\nextracts the uniqueness of cattle muzzles. For this purpose, we collected\n32,374 images from 826 cattle. Contrast-limited adaptive histogram equalization\n(CLAHE) with sharpening filters was applied in the preprocessing steps to\nremove noise from images. We used the YOLO algorithm for cattle muzzle\ndetection in the image and the FaceNet architecture to learn unified embeddings\nfrom muzzle images using squared $L_2$ distances. Our system performs with an\naccuracy of $96.489\\%$, $F_1$ score of $97.334\\%$, and a true positive rate\n(tpr) of $87.993\\%$ at a remarkably low false positive rate (fpr) of $0.098\\%$.\nThis reliable and efficient system for identifying cattle can significantly\nadvance livestock insurance and precision farming.\n","authors":["Hasan Zohirul Islam","Safayet Khan","Sanjib Kumar Paul","Sheikh Imtiaz Rahi","Fahim Hossain Sifat","Md. Mahadi Hasan Sany","Md. Shahjahan Ali Sarker","Tareq Anam","Ismail Hossain Polas"],"pdf_url":"https://arxiv.org/pdf/2407.06096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06095v1","updated":"2024-07-08T16:36:12Z","published":"2024-07-08T16:36:12Z","title":"Accelerating Diffusion for SAR-to-Optical Image Translation via\n  Adversarial Consistency Distillation","summary":"  Synthetic Aperture Radar (SAR) provides all-weather, high-resolution imaging\ncapabilities, but its unique imaging mechanism often requires expert\ninterpretation, limiting its widespread applicability. Translating SAR images\ninto more easily recognizable optical images using diffusion models helps\naddress this challenge. However, diffusion models suffer from high latency due\nto numerous iterative inferences, while Generative Adversarial Networks (GANs)\ncan achieve image translation with just a single iteration but often at the\ncost of image quality. To overcome these issues, we propose a new training\nframework for SAR-to-optical image translation that combines the strengths of\nboth approaches. Our method employs consistency distillation to reduce\niterative inference steps and integrates adversarial learning to ensure image\nclarity and minimize color shifts. Additionally, our approach allows for a\ntrade-off between quality and speed, providing flexibility based on application\nrequirements. We conducted experiments on SEN12 and GF3 datasets, performing\nquantitative evaluations using Peak Signal-to-Noise Ratio (PSNR), Structural\nSimilarity Index (SSIM), and Frechet Inception Distance (FID), as well as\ncalculating the inference latency. The results demonstrate that our approach\nsignificantly improves inference speed by 131 times while maintaining the\nvisual quality of the generated images, thus offering a robust and efficient\nsolution for SAR-to-optical image translation.\n","authors":["Xinyu Bai","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2407.06095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06092v1","updated":"2024-07-08T16:31:49Z","published":"2024-07-08T16:31:49Z","title":"Assessing Cardiomegaly in Dogs Using a Simple CNN Model","summary":"  This paper introduces DogHeart, a dataset comprising 1400 training, 200\nvalidation, and 400 test images categorized as small, normal, and large based\non VHS score. A custom CNN model is developed, featuring a straightforward\narchitecture with 4 convolutional layers and 4 fully connected layers. Despite\nthe absence of data augmentation, the model achieves a 72\\% accuracy in\nclassifying cardiomegaly severity. The study contributes to automated\nassessment of cardiac conditions in dogs, highlighting the potential for early\ndetection and intervention in veterinary care.\n","authors":["Nikhil Deekonda"],"pdf_url":"https://arxiv.org/pdf/2407.06092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06084v1","updated":"2024-07-08T16:26:52Z","published":"2024-07-08T16:26:52Z","title":"3D Vision and Language Pretraining with Large-Scale Synthetic Data","summary":"  3D Vision-Language Pre-training (3D-VLP) aims to provide a pre-train model\nwhich can bridge 3D scenes with natural language, which is an important\ntechnique for embodied intelligence. However, current 3D-VLP datasets are\nhindered by limited scene-level diversity and insufficient fine-grained\nannotations (only 1.2K scenes and 280K textual annotations in ScanScribe),\nprimarily due to the labor-intensive of collecting and annotating 3D scenes. To\novercome these obstacles, we construct SynVL3D, a comprehensive synthetic\nscene-text corpus with 10K indoor scenes and 1M descriptions at object, view,\nand room levels, which has the advantages of diverse scene data, rich textual\ndescriptions, multi-grained 3D-text associations, and low collection cost.\nUtilizing the rich annotations in SynVL3D, we pre-train a simple and unified\nTransformer for aligning 3D and language with multi-grained pretraining tasks.\nMoreover, we propose a synthetic-to-real domain adaptation in downstream task\nfine-tuning process to address the domain shift. Through extensive experiments,\nwe verify the effectiveness of our model design by achieving state-of-the-art\nperformance on downstream tasks including visual grounding, dense captioning,\nand question answering.\n","authors":["Dejie Yang","Zhu Xu","Wentao Mo","Qingchao Chen","Siyuan Huang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.06084v1.pdf","comment":"accepted by IJCAI2024"},{"id":"http://arxiv.org/abs/2407.06079v1","updated":"2024-07-08T16:25:34Z","published":"2024-07-08T16:25:34Z","title":"Layered Diffusion Model for One-Shot High Resolution Text-to-Image\n  Synthesis","summary":"  We present a one-shot text-to-image diffusion model that can generate\nhigh-resolution images from natural language descriptions. Our model employs a\nlayered U-Net architecture that simultaneously synthesizes images at multiple\nresolution scales. We show that this method outperforms the baseline of\nsynthesizing images only at the target resolution, while reducing the\ncomputational cost per step. We demonstrate that higher resolution synthesis\ncan be achieved by layering convolutions at additional resolution scales, in\ncontrast to other methods which require additional models for super-resolution\nsynthesis.\n","authors":["Emaad Khwaja","Abdullah Rashwan","Ting Chen","Oliver Wang","Suraj Kothawade","Yeqing Li"],"pdf_url":"https://arxiv.org/pdf/2407.06079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06077v1","updated":"2024-07-08T16:25:01Z","published":"2024-07-08T16:25:01Z","title":"Object-Oriented Material Classification and 3D Clustering for Improved\n  Semantic Perception and Mapping in Mobile Robots","summary":"  Classification of different object surface material types can play a\nsignificant role in the decision-making algorithms for mobile robots and\nautonomous vehicles. RGB-based scene-level semantic segmentation has been\nwell-addressed in the literature. However, improving material recognition using\nthe depth modality and its integration with SLAM algorithms for 3D semantic\nmapping could unlock new potential benefits in the robotics perception\npipeline. To this end, we propose a complementarity-aware deep learning\napproach for RGB-D-based material classification built on top of an\nobject-oriented pipeline. The approach further integrates the ORB-SLAM2 method\nfor 3D scene mapping with multiscale clustering of the detected material\nsemantics in the point cloud map generated by the visual SLAM algorithm.\nExtensive experimental results with existing public datasets and newly\ncontributed real-world robot datasets demonstrate a significant improvement in\nmaterial classification and 3D clustering accuracy compared to state-of-the-art\napproaches for 3D semantic scene mapping.\n","authors":["Siva Krishna Ravipati","Ehsan Latif","Ramviyas Parasuraman","Suchendra M. Bhandarkar"],"pdf_url":"https://arxiv.org/pdf/2407.06077v1.pdf","comment":"Accepted to IROS 2024"},{"id":"http://arxiv.org/abs/2407.06076v1","updated":"2024-07-08T16:21:53Z","published":"2024-07-08T16:21:53Z","title":"Understanding Visual Feature Reliance through the Lens of Complexity","summary":"  Recent studies suggest that deep learning models inductive bias towards\nfavoring simpler features may be one of the sources of shortcut learning. Yet,\nthere has been limited focus on understanding the complexity of the myriad\nfeatures that models learn. In this work, we introduce a new metric for\nquantifying feature complexity, based on $\\mathscr{V}$-information and\ncapturing whether a feature requires complex computational transformations to\nbe extracted. Using this $\\mathscr{V}$-information metric, we analyze the\ncomplexities of 10,000 features, represented as directions in the penultimate\nlayer, that were extracted from a standard ImageNet-trained vision model. Our\nstudy addresses four key questions: First, we ask what features look like as a\nfunction of complexity and find a spectrum of simple to complex features\npresent within the model. Second, we ask when features are learned during\ntraining. We find that simpler features dominate early in training, and more\ncomplex features emerge gradually. Third, we investigate where within the\nnetwork simple and complex features flow, and find that simpler features tend\nto bypass the visual hierarchy via residual connections. Fourth, we explore the\nconnection between features complexity and their importance in driving the\nnetworks decision. We find that complex features tend to be less important.\nSurprisingly, important features become accessible at earlier layers during\ntraining, like a sedimentation process, allowing the model to build upon these\nfoundational elements.\n","authors":["Thomas Fel","Louis Bethune","Andrew Kyle Lampinen","Thomas Serre","Katherine Hermann"],"pdf_url":"https://arxiv.org/pdf/2407.06076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.09345v2","updated":"2024-07-08T16:12:08Z","published":"2022-08-19T13:54:15Z","title":"IPNET:Influential Prototypical Networks for Few Shot Learning","summary":"  Prototypical network (PN) is a simple yet effective few shot learning\nstrategy. It is a metric-based meta-learning technique where classification is\nperformed by computing Euclidean distances to prototypical representations of\neach class. Conventional PN attributes equal importance to all samples and\ngenerates prototypes by simply averaging the support sample embeddings\nbelonging to each class. In this work, we propose a novel version of PN that\nattributes weights to support samples corresponding to their influence on the\nsupport sample distribution. Influence weights of samples are calculated based\non maximum mean discrepancy (MMD) between the mean embeddings of sample\ndistributions including and excluding the sample. Further, the influence factor\nof a sample is measured using MMD based on the shift in the distribution in the\nabsence of that sample.\n","authors":["Ranjana Roy Chowdhury","Deepti R. Bathula"],"pdf_url":"https://arxiv.org/pdf/2208.09345v2.pdf","comment":"I had uploaded this file thinking that this will be my latest version\n  of my experiments but it did not work out and so I want to remove this\n  version"},{"id":"http://arxiv.org/abs/2407.06064v1","updated":"2024-07-08T16:05:56Z","published":"2024-07-08T16:05:56Z","title":"Pan-denoising: Guided Hyperspectral Image Denoising via Weighted\n  Represent Coefficient Total Variation","summary":"  This paper introduces a novel paradigm for hyperspectral image (HSI)\ndenoising, which is termed \\textit{pan-denoising}. In a given scene,\npanchromatic (PAN) images capture similar structures and textures to HSIs but\nwith less noise. This enables the utilization of PAN images to guide the HSI\ndenoising process. Consequently, pan-denoising, which incorporates an\nadditional prior, has the potential to uncover underlying structures and\ndetails beyond the internal information modeling of traditional HSI denoising\nmethods. However, the proper modeling of this additional prior poses a\nsignificant challenge. To alleviate this issue, the paper proposes a novel\nregularization term, Panchromatic Weighted Representation Coefficient Total\nVariation (PWRCTV). It employs the gradient maps of PAN images to automatically\nassign different weights of TV regularization for each pixel, resulting in\nlarger weights for smooth areas and smaller weights for edges. This\nregularization forms the basis of a pan-denoising model, which is solved using\nthe Alternating Direction Method of Multipliers. Extensive experiments on\nsynthetic and real-world datasets demonstrate that PWRCTV outperforms several\nstate-of-the-art methods in terms of metrics and visual quality. Furthermore,\nan HSI classification experiment confirms that PWRCTV, as a preprocessing\nmethod, can enhance the performance of downstream classification tasks. The\ncode and data are available at https://github.com/shuangxu96/PWRCTV.\n","authors":["Shuang Xu","Qiao Ke","Jiangjun Peng","Xiangyong Cao","Zixiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.06064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06059v1","updated":"2024-07-08T16:01:00Z","published":"2024-07-08T16:01:00Z","title":"LaFAM: Unsupervised Feature Attribution with Label-free Activation Maps","summary":"  Convolutional Neural Networks (CNNs) are known for their ability to learn\nhierarchical structures, naturally developing detectors for objects, and\nsemantic concepts within their deeper layers. Activation maps (AMs) reveal\nthese saliency regions, which are crucial for many Explainable AI (XAI)\nmethods. However, the direct exploitation of raw AMs in CNNs for feature\nattribution remains underexplored in literature. This work revises Class\nActivation Map (CAM) methods by introducing the Label-free Activation Map\n(LaFAM), a streamlined approach utilizing raw AMs for feature attribution\nwithout reliance on labels. LaFAM presents an efficient alternative to\nconventional CAM methods, demonstrating particular effectiveness in saliency\nmap generation for self-supervised learning while maintaining applicability in\nsupervised learning scenarios.\n","authors":["Aray Karjauv","Sahin Albayrak"],"pdf_url":"https://arxiv.org/pdf/2407.06059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13705v2","updated":"2024-07-08T15:51:29Z","published":"2024-06-19T16:58:28Z","title":"EndoUIC: Promptable Diffusion Transformer for Unified Illumination\n  Correction in Capsule Endoscopy","summary":"  Wireless Capsule Endoscopy (WCE) is highly valued for its non-invasive and\npainless approach, though its effectiveness is compromised by uneven\nillumination from hardware constraints and complex internal dynamics, leading\nto overexposed or underexposed images. While researchers have discussed the\nchallenges of low-light enhancement in WCE, the issue of correcting for\ndifferent exposure levels remains underexplored. To tackle this, we introduce\nEndoUIC, a WCE unified illumination correction solution using an end-to-end\npromptable diffusion transformer (DiT) model. In our work, the illumination\nprompt module shall navigate the model to adapt to different exposure levels\nand perform targeted image enhancement, in which the Adaptive Prompt\nIntegration (API) and Global Prompt Scanner (GPS) modules shall further boost\nthe concurrent representation learning between the prompt parameters and\nfeatures. Besides, the U-shaped restoration DiT model shall capture the\nlong-range dependencies and contextual information for unified illumination\nrestoration. Moreover, we present a novel Capsule-endoscopy Exposure Correction\n(CEC) dataset, including ground-truth and corrupted image pairs annotated by\nexpert photographers. Extensive experiments against a variety of\nstate-of-the-art (SOTA) methods on four datasets showcase the effectiveness of\nour proposed method and components in WCE illumination restoration, and the\nadditional downstream experiments further demonstrate its utility for clinical\ndiagnosis and surgical assistance.\n","authors":["Long Bai","Tong Chen","Qiaozhi Tan","Wan Jun Nah","Yanheng Li","Zhicheng He","Sishen Yuan","Zhen Chen","Jinlin Wu","Mobarakol Islam","Zhen Li","Hongbin Liu","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2406.13705v2.pdf","comment":"To appear in MICCAI 2024. Code and dataset availability:\n  https://github.com/longbai1006/EndoUIC"},{"id":"http://arxiv.org/abs/2405.12218v2","updated":"2024-07-08T15:47:58Z","published":"2024-05-20T17:59:30Z","title":"MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from\n  Multi-View Stereo","summary":"  We present MVSGaussian, a new generalizable 3D Gaussian representation\napproach derived from Multi-View Stereo (MVS) that can efficiently reconstruct\nunseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware\nGaussian representations and decode them into Gaussian parameters. 2) To\nfurther enhance performance, we propose a hybrid Gaussian rendering that\nintegrates an efficient volume rendering design for novel view synthesis. 3) To\nsupport fast fine-tuning for specific scenes, we introduce a multi-view\ngeometric consistent aggregation strategy to effectively aggregate the point\nclouds generated by the generalizable model, serving as the initialization for\nper-scene optimization. Compared with previous generalizable NeRF-based\nmethods, which typically require minutes of fine-tuning and seconds of\nrendering per image, MVSGaussian achieves real-time rendering with better\nsynthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian\nachieves better view synthesis with less training computational cost. Extensive\nexperiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples\ndatasets validate that MVSGaussian attains state-of-the-art performance with\nconvincing generalizability, real-time rendering speed, and fast per-scene\noptimization.\n","authors":["Tianqi Liu","Guangcong Wang","Shoukang Hu","Liao Shen","Xinyi Ye","Yuhang Zang","Zhiguo Cao","Wei Li","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2405.12218v2.pdf","comment":"ECCV2024, Project page: https://mvsgaussian.github.io/"},{"id":"http://arxiv.org/abs/2212.04745v5","updated":"2024-07-08T15:45:16Z","published":"2022-12-09T09:45:43Z","title":"SLAM for Visually Impaired People: a Survey","summary":"  In recent decades, several assistive technologies have been developed to\nimprove the ability of blind and visually impaired individuals to navigate\nindependently and safely. At the same time, simultaneous localization and\nmapping (SLAM) techniques have become sufficiently robust and efficient to be\nadopted in developing these assistive technologies. We present the first\nsystematic literature review of 54 recent studies on SLAM-based solutions for\nblind and visually impaired people, focusing on literature published from 2017\nonward. This review explores various localization and mapping techniques\nemployed in this context. We systematically identified and categorized diverse\nSLAM approaches and analyzed their localization and mapping techniques, sensor\ntypes, computing resources, and machine-learning methods. We discuss the\nadvantages and limitations of these techniques for blind and visually impaired\nnavigation. Moreover, we examine the major challenges described across studies,\nincluding practical considerations that affect usability and adoption. Our\nanalysis also evaluates the effectiveness of these SLAM-based solutions in\nreal-world scenarios and user satisfaction, providing insights into their\npractical impact on BVI mobility. The insights derived from this review\nidentify critical gaps and opportunities for future research activities,\nparticularly in addressing the challenges presented by dynamic and complex\nenvironments. We explain how SLAM technology offers the potential to improve\nthe ability of visually impaired individuals to navigate effectively. Finally,\nwe present future opportunities and challenges in this domain.\n","authors":["Marziyeh Bamdad","Davide Scaramuzza","Alireza Darvishy"],"pdf_url":"https://arxiv.org/pdf/2212.04745v5.pdf","comment":"46 pages, 38 tables, 6 figures"},{"id":"http://arxiv.org/abs/2407.06045v1","updated":"2024-07-08T15:42:02Z","published":"2024-07-08T15:42:02Z","title":"OpenCIL: Benchmarking Out-of-Distribution Detection in Class-Incremental\n  Learning","summary":"  Class incremental learning (CIL) aims to learn a model that can not only\nincrementally accommodate new classes, but also maintain the learned knowledge\nof old classes. Out-of-distribution (OOD) detection in CIL is to retain this\nincremental learning ability, while being able to reject unknown samples that\nare drawn from different distributions of the learned classes. This capability\nis crucial to the safety of deploying CIL models in open worlds. However,\ndespite remarkable advancements in the respective CIL and OOD detection, there\nlacks a systematic and large-scale benchmark to assess the capability of\nadvanced CIL models in detecting OOD samples. To fill this gap, in this study\nwe design a comprehensive empirical study to establish such a benchmark, named\n$\\textbf{OpenCIL}$. To this end, we propose two principled frameworks for\nenabling four representative CIL models with 15 diverse OOD detection methods,\nresulting in 60 baseline models for OOD detection in CIL. The empirical\nevaluation is performed on two popular CIL datasets with six commonly-used OOD\ndatasets. One key observation we find through our comprehensive evaluation is\nthat the CIL models can be severely biased towards the OOD samples and newly\nadded classes when they are exposed to open environments. Motivated by this, we\nfurther propose a new baseline for OOD detection in CIL, namely Bi-directional\nEnergy Regularization ($\\textbf{BER}$), which is specially designed to mitigate\nthese two biases in different CIL models by having energy regularization on\nboth old and new classes. Its superior performance is justified in our\nexperiments. All codes and datasets are open-source at\n$https://github.com/mala-lab/OpenCIL$.\n","authors":["Wenjun Miao","Guansong Pang","Trong-Tung Nguyen","Ruohang Fang","Jin Zheng","Xiao Bai"],"pdf_url":"https://arxiv.org/pdf/2407.06045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06043v1","updated":"2024-07-08T15:40:28Z","published":"2024-07-08T15:40:28Z","title":"Test-time adaptation for geospatial point cloud semantic segmentation\n  with distinct domain shifts","summary":"  Domain adaptation (DA) techniques help deep learning models generalize across\ndata shifts for point cloud semantic segmentation (PCSS). Test-time adaptation\n(TTA) allows direct adaptation of a pre-trained model to unlabeled data during\ninference stage without access to source data or additional training, avoiding\nprivacy issues and large computational resources. We address TTA for geospatial\nPCSS by introducing three domain shift paradigms: photogrammetric to airborne\nLiDAR, airborne to mobile LiDAR, and synthetic to mobile laser scanning. We\npropose a TTA method that progressively updates batch normalization (BN)\nstatistics with each testing batch. Additionally, a self-supervised learning\nmodule optimizes learnable BN affine parameters. Information maximization and\nreliability-constrained pseudo-labeling improve prediction confidence and\nsupply supervisory signals. Experimental results show our method improves\nclassification accuracy by up to 20\\% mIoU, outperforming other methods. For\nphotogrammetric (SensatUrban) to airborne (Hessigheim 3D) adaptation at the\ninference stage, our method achieves 59.46\\% mIoU and 85.97\\% OA without\nretraining or fine-turning.\n","authors":["Puzuo Wang","Wei Yao","Jie Shao","Zhiyi He"],"pdf_url":"https://arxiv.org/pdf/2407.06043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16269v3","updated":"2024-07-08T15:32:52Z","published":"2023-05-25T17:25:14Z","title":"UDPM: Upsampling Diffusion Probabilistic Models","summary":"  Denoising Diffusion Probabilistic Models (DDPM) have recently gained\nsignificant attention. DDPMs compose a Markovian process that begins in the\ndata domain and gradually adds noise until reaching pure white noise. DDPMs\ngenerate high-quality samples from complex data distributions by defining an\ninverse process and training a deep neural network to learn this mapping.\nHowever, these models are inefficient because they require many diffusion steps\nto produce aesthetically pleasing samples. Additionally, unlike generative\nadversarial networks (GANs), the latent space of diffusion models is less\ninterpretable. In this work, we propose to generalize the denoising diffusion\nprocess into an Upsampling Diffusion Probabilistic Model (UDPM). In the forward\nprocess, we reduce the latent variable dimension through downsampling, followed\nby the traditional noise perturbation. As a result, the reverse process\ngradually denoises and upsamples the latent variable to produce a sample from\nthe data distribution. We formalize the Markovian diffusion processes of UDPM\nand demonstrate its generation capabilities on the popular FFHQ, AFHQv2, and\nCIFAR10 datasets. UDPM generates images with as few as three network\nevaluations, whose overall computational cost is less than a single DDPM or EDM\nstep, while achieving an FID score of 6.86. This surpasses current\nstate-of-the-art efficient diffusion models that use a single denoising step\nfor sampling. Additionally, UDPM offers an interpretable and interpolable\nlatent space, which gives it an advantage over traditional DDPMs. Our code is\navailable online: \\url{https://github.com/shadyabh/UDPM/}\n","authors":["Shady Abu-Hussein","Raja Giryes"],"pdf_url":"https://arxiv.org/pdf/2305.16269v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08526v2","updated":"2024-07-08T15:32:09Z","published":"2024-04-12T15:15:39Z","title":"Masked Image Modeling as a Framework for Self-Supervised Learning across\n  Eye Movements","summary":"  To make sense of their surroundings, intelligent systems must transform\ncomplex sensory inputs to structured codes that are reduced to task-relevant\ninformation such as object category. Biological agents achieve this in a\nlargely autonomous manner, presumably via self-supervised learning. Whereas\nprevious attempts to model the underlying mechanisms were largely\ndiscriminative in nature, there is ample evidence that the brain employs a\ngenerative model of the world. Here, we propose that eye movements, in\ncombination with the focused nature of primate vision, constitute a generative,\nself-supervised task of predicting and revealing visual information. We\nconstruct a proof-of-principle model starting from the framework of masked\nimage modeling (MIM), a common approach in deep representation learning. To do\nso, we analyze how core components of MIM such as masking technique and data\naugmentation influence the formation of category-specific representations. This\nallows us not only to better understand the principles behind MIM, but to then\nreassemble a MIM more in line with the focused nature of biological perception.\nWe find that MIM disentangles neurons in latent space without explicit\nregularization, a property that has been suggested to structure visual\nrepresentations in primates. Together with previous findings of invariance\nlearning, this highlights an interesting connection of MIM to latent\nregularization approaches for self-supervised learning. The source code is\navailable under https://github.com/RobinWeiler/FocusMIM\n","authors":["Robin Weiler","Matthias Brucklacher","Cyriel M. A. Pennartz","Sander M. BohtÃ©"],"pdf_url":"https://arxiv.org/pdf/2404.08526v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06277v2","updated":"2024-07-08T15:29:27Z","published":"2024-04-09T13:01:26Z","title":"Learning Embeddings with Centroid Triplet Loss for Object Identification\n  in Robotic Grasping","summary":"  Foundation models are a strong trend in deep learning and computer vision.\nThese models serve as a base for applications as they require minor or no\nfurther fine-tuning by developers to integrate into their applications.\nFoundation models for zero-shot object segmentation such as Segment Anything\n(SAM) output segmentation masks from images without any further object\ninformation. When they are followed in a pipeline by an object identification\nmodel, they can perform object detection without training. Here, we focus on\ntraining such an object identification model. A crucial practical aspect for an\nobject identification model is to be flexible in input size. As object\nidentification is an image retrieval problem, a suitable method should handle\nmulti-query multi-gallery situations without constraining the number of input\nimages (e.g. by having fixed-size aggregation layers). The key solution to\ntrain such a model is the centroid triplet loss (CTL), which aggregates image\nfeatures to their centroids. CTL yields high accuracy, avoids misleading\ntraining signals and keeps the model input size flexible. In our experiments,\nwe establish a new state of the art on the ArmBench object identification task,\nwhich shows general applicability of our model. We furthermore demonstrate an\nintegrated unseen object detection pipeline on the challenging HOPE dataset,\nwhich requires fine-grained detection. There, our pipeline matches and\nsurpasses related methods which have been trained on dataset-specific data.\n","authors":["Anas Gouda","Max Schwarz","Christopher Reining","Sven Behnke","Alice Kirchheim"],"pdf_url":"https://arxiv.org/pdf/2404.06277v2.pdf","comment":"Accepted to CASE 2024"},{"id":"http://arxiv.org/abs/2407.06018v1","updated":"2024-07-08T15:08:41Z","published":"2024-07-08T15:08:41Z","title":"Leveraging Transformers for Weakly Supervised Object Localization in\n  Unconstrained Videos","summary":"  Weakly-Supervised Video Object Localization (WSVOL) involves localizing an\nobject in videos using only video-level labels, also referred to as tags.\nState-of-the-art WSVOL methods like Temporal CAM (TCAM) rely on class\nactivation mapping (CAM) and typically require a pre-trained CNN classifier.\nHowever, their localization accuracy is affected by their tendency to minimize\nthe mutual information between different instances of a class and exploit\ntemporal information during training for downstream tasks, e.g., detection and\ntracking. In the absence of bounding box annotation, it is challenging to\nexploit precise information about objects from temporal cues because the model\nstruggles to locate objects over time. To address these issues, a novel method\ncalled transformer based CAM for videos (TrCAM-V), is proposed for WSVOL. It\nconsists of a DeiT backbone with two heads for classification and localization.\nThe classification head is trained using standard classification loss (CL),\nwhile the localization head is trained using pseudo-labels that are extracted\nusing a pre-trained CLIP model. From these pseudo-labels, the high and low\nactivation values are considered to be foreground and background regions,\nrespectively. Our TrCAM-V method allows training a localization network by\nsampling pseudo-pixels on the fly from these regions. Additionally, a\nconditional random field (CRF) loss is employed to align the object boundaries\nwith the foreground map. During inference, the model can process individual\nframes for real-time localization applications. Extensive experiments on\nchallenging YouTube-Objects unconstrained video datasets show that our TrCAM-V\nmethod achieves new state-of-the-art performance in terms of classification and\nlocalization accuracy.\n","authors":["Shakeeb Murtaza","Marco Pedersoli","Aydin Sarraf","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2407.06018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06016v1","updated":"2024-07-08T15:07:09Z","published":"2024-07-08T15:07:09Z","title":"RHRSegNet: Relighting High-Resolution Night-Time Semantic Segmentation","summary":"  Night time semantic segmentation is a crucial task in computer vision,\nfocusing on accurately classifying and segmenting objects in low-light\nconditions. Unlike daytime techniques, which often perform worse in nighttime\nscenes, it is essential for autonomous driving due to insufficient lighting,\nlow illumination, dynamic lighting, shadow effects, and reduced contrast. We\npropose RHRSegNet, implementing a relighting model over a High-Resolution\nNetwork for semantic segmentation. RHRSegNet implements residual convolutional\nfeature learning to handle complex lighting conditions. Our model then feeds\nthe lightened scene feature maps into a high-resolution network for scene\nsegmentation. The network consists of a convolutional producing feature maps\nwith varying resolutions, achieving different levels of resolution through\ndown-sampling and up-sampling. Large nighttime datasets are used for training\nand evaluation, such as NightCity, City-Scape, and Dark-Zurich datasets. Our\nproposed model increases the HRnet segmentation performance by 5% in low-light\nor nighttime images.\n","authors":["Sarah Elmahdy","Rodaina Hebishy","Ali Hamdi"],"pdf_url":"https://arxiv.org/pdf/2407.06016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06005v1","updated":"2024-07-08T14:59:10Z","published":"2024-07-08T14:59:10Z","title":"Advancing Automated Deception Detection: A Multimodal Approach to\n  Feature Extraction and Analysis","summary":"  With the exponential increase in video content, the need for accurate\ndeception detection in human-centric video analysis has become paramount. This\nresearch focuses on the extraction and combination of various features to\nenhance the accuracy of deception detection models. By systematically\nextracting features from visual, audio, and text data, and experimenting with\ndifferent combinations, we developed a robust model that achieved an impressive\n99% accuracy. Our methodology emphasizes the significance of feature\nengineering in deception detection, providing a clear and interpretable\nframework. We trained various machine learning models, including LSTM, BiLSTM,\nand pre-trained CNNs, using both single and multi-modal approaches. The results\ndemonstrated that combining multiple modalities significantly enhances\ndetection performance compared to single modality training. This study\nhighlights the potential of strategic feature extraction and combination in\ndeveloping reliable and transparent automated deception detection systems in\nvideo analysis, paving the way for more advanced and accurate detection\nmethodologies in future research.\n","authors":["Mohamed Bahaa","Mena Hany","Ehab E. Zakaria"],"pdf_url":"https://arxiv.org/pdf/2407.06005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06001v1","updated":"2024-07-08T14:53:07Z","published":"2024-07-08T14:53:07Z","title":"Pseudo-triplet Guided Few-shot Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) is a challenging task that aims to retrieve\nthe target image based on a multimodal query, i.e., a reference image and its\ncorresponding modification text. While previous supervised or zero-shot\nlearning paradigms all fail to strike a good trade-off between time-consuming\nannotation cost and retrieval performance, recent researchers introduced the\ntask of few-shot CIR (FS-CIR) and proposed a textual inversion-based network\nbased on pretrained CLIP model to realize it. Despite its promising\nperformance, the approach suffers from two key limitations: insufficient\nmultimodal query composition training and indiscriminative training triplet\nselection. To address these two limitations, in this work, we propose a novel\ntwo-stage pseudo triplet guided few-shot CIR scheme, dubbed PTG-FSCIR. In the\nfirst stage, we employ a masked training strategy and advanced image caption\ngenerator to construct pseudo triplets from pure image data to enable the model\nto acquire primary knowledge related to multimodal query composition. In the\nsecond stage, based on active learning, we design a pseudo modification\ntext-based query-target distance metric to evaluate the challenging score for\neach unlabeled sample. Meanwhile, we propose a robust top range-based random\nsampling strategy according to the 3-$\\sigma$ rule in statistics, to sample the\nchallenging samples for fine-tuning the pretrained model. Notably, our scheme\nis plug-and-play and compatible with any existing supervised CIR models. We\ntested our scheme across three backbones on three public datasets (i.e.,\nFashionIQ, CIRR, and Birds-to-Words), achieving maximum improvements of 26.4%,\n25.5% and 21.6% respectively, demonstrating our scheme's effectiveness.\n","authors":["Bohan Hou","Haoqiang Lin","Haokun Wen","Meng Liu","Xuemeng Song"],"pdf_url":"https://arxiv.org/pdf/2407.06001v1.pdf","comment":"15 pages, 5 figures,"},{"id":"http://arxiv.org/abs/2407.06000v1","updated":"2024-07-08T14:52:03Z","published":"2024-07-08T14:52:03Z","title":"Bounding Boxes and Probabilistic Graphical Models: Video Anomaly\n  Detection Simplified","summary":"  In this study, we formulate the task of Video Anomaly Detection as a\nprobabilistic analysis of object bounding boxes. We hypothesize that the\nrepresentation of objects via their bounding boxes only, can be sufficient to\nsuccessfully identify anomalous events in a scene. The implied value of this\napproach is increased object anonymization, faster model training and fewer\ncomputational resources. This can particularly benefit applications within\nvideo surveillance running on edge devices such as cameras. We design our model\nbased on human reasoning which lends itself to explaining model output in\nhuman-understandable terms. Meanwhile, the slowest model trains within less\nthan 7 seconds on a 11th Generation Intel Core i9 Processor. While our approach\nconstitutes a drastic reduction of problem feature space in comparison with\nprior art, we show that this does not result in a reduction in performance: the\nresults we report are highly competitive on the benchmark datasets CUHK Avenue\nand ShanghaiTech, and significantly exceed on the latest State-of-the-Art\nresults on StreetScene, which has so far proven to be the most challenging VAD\ndataset.\n","authors":["Mia Siemon","Thomas B. Moeslund","Barry Norton","Kamal Nasrollahi"],"pdf_url":"https://arxiv.org/pdf/2407.06000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04538v2","updated":"2024-07-08T14:44:06Z","published":"2024-07-05T14:24:37Z","title":"PDiscoFormer: Relaxing Part Discovery Constraints with Vision\n  Transformers","summary":"  Computer vision methods that explicitly detect object parts and reason on\nthem are a step towards inherently interpretable models. Existing approaches\nthat perform part discovery driven by a fine-grained classification task make\nvery restrictive assumptions on the geometric properties of the discovered\nparts; they should be small and compact. Although this prior is useful in some\ncases, in this paper we show that pre-trained transformer-based vision models,\nsuch as self-supervised DINOv2 ViT, enable the relaxation of these constraints.\nIn particular, we find that a total variation (TV) prior, which allows for\nmultiple connected components of any size, substantially outperforms previous\nwork. We test our approach on three fine-grained classification benchmarks:\nCUB, PartImageNet and Oxford Flowers, and compare our results to previously\npublished methods as well as a re-implementation of the state-of-the-art method\nPDiscoNet with a transformer-based backbone. We consistently obtain substantial\nimprovements across the board, both on part discovery metrics and the\ndownstream classification task, showing that the strong inductive biases in\nself-supervised ViT models require to rethink the geometric priors that can be\nused for unsupervised part discovery.\n","authors":["Ananthu Aniraj","Cassio F. Dantas","Dino Ienco","Diego Marcos"],"pdf_url":"https://arxiv.org/pdf/2407.04538v2.pdf","comment":"Accepted as a main conference paper at the European Conference of\n  Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2407.05993v1","updated":"2024-07-08T14:41:53Z","published":"2024-07-08T14:41:53Z","title":"Self-Prior Guided Mamba-UNet Networks for Medical Image Super-Resolution","summary":"  In this paper, we propose a self-prior guided Mamba-UNet network\n(SMamba-UNet) for medical image super-resolution. Existing methods are\nprimarily based on convolutional neural networks (CNNs) or Transformers.\nCNNs-based methods fail to capture long-range dependencies, while\nTransformer-based approaches face heavy calculation challenges due to their\nquadratic computational complexity. Recently, State Space Models (SSMs)\nespecially Mamba have emerged, capable of modeling long-range dependencies with\nlinear computational complexity. Inspired by Mamba, our approach aims to learn\nthe self-prior multi-scale contextual features under Mamba-UNet networks, which\nmay help to super-resolve low-resolution medical images in an efficient way.\nSpecifically, we obtain self-priors by perturbing the brightness inpainting of\nthe input image during network training, which can learn detailed texture and\nbrightness information that is beneficial for super-resolution. Furthermore, we\ncombine Mamba with Unet network to mine global features at different levels. We\nalso design an improved 2D-Selective-Scan (ISS2D) module to divide image\nfeatures into different directional sequences to learn long-range dependencies\nin multiple directions, and adaptively fuse sequence information to enhance\nsuper-resolved feature representation. Both qualitative and quantitative\nexperimental results demonstrate that our approach outperforms current\nstate-of-the-art methods on two public medical datasets: the IXI and fastMRI.\n","authors":["Zexin Ji","Beiji Zou","Xiaoyan Kui","Pierre Vera","Su Ruan"],"pdf_url":"https://arxiv.org/pdf/2407.05993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09327v3","updated":"2024-07-08T14:30:42Z","published":"2024-06-13T17:06:15Z","title":"Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN\n  Pipeline applied on PSMA PET/CT Scans","summary":"  Assessing tumor response to systemic therapies is one of the main\napplications of PET/CT. Routinely, only a small subset of index lesions out of\nmultiple lesions is analyzed. However, this operator dependent selection may\nbias the results due to possible significant inter-metastatic heterogeneity of\nresponse to therapy. Automated, AI based approaches for lesion tracking hold\npromise in enabling the analysis of many more lesions and thus providing a\nbetter assessment of tumor response. This work introduces a Siamese CNN\napproach for lesion tracking between PET/CT scans. Our approach is applied on\nthe laborious task of tracking a high number of bone lesions in full-body\nbaseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cycles\nof [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancer\npatients. Data preparation includes lesion segmentation and affine\nregistration. Our algorithm extracts suitable lesion patches and forwards them\ninto a Siamese CNN trained to classify the lesion patch pairs as corresponding\nor non-corresponding lesions. Experiments have been performed with different\ninput patch types and a Siamese network in 2D and 3D. The CNN model\nsuccessfully learned to classify lesion assignments, reaching a lesion tracking\naccuracy of 83 % in its best configuration with an AUC = 0.91. For remaining\nlesions the pipeline accomplished a re-identification rate of 89 %. We proved\nthat a CNN may facilitate the tracking of multiple lesions in PSMA PET/CT\nscans. Future clinical studies are necessary if this improves the prediction of\nthe outcome of therapies.\n","authors":["Stefan P. Hein","Manuel Schultheiss","Andrei Gafita","Raphael Zaum","Farid Yagubbayli","Robert Tauber","Isabel Rauscher","Matthias Eiber","Franz Pfeiffer","Wolfgang A. Weber"],"pdf_url":"https://arxiv.org/pdf/2406.09327v3.pdf","comment":"25 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.05986v1","updated":"2024-07-08T14:26:30Z","published":"2024-07-08T14:26:30Z","title":"KidSat: satellite imagery to map childhood poverty dataset and benchmark","summary":"  Satellite imagery has emerged as an important tool to analyse demographic,\nhealth, and development indicators. While various deep learning models have\nbeen built for these tasks, each is specific to a particular problem, with few\nstandard benchmarks available. We propose a new dataset pairing satellite\nimagery and high-quality survey data on child poverty to benchmark satellite\nfeature representations. Our dataset consists of 33,608 images, each 10 km\n$\\times$ 10 km, from 19 countries in Eastern and Southern Africa in the time\nperiod 1997-2022. As defined by UNICEF, multidimensional child poverty covers\nsix dimensions and it can be calculated from the face-to-face Demographic and\nHealth Surveys (DHS) Program . As part of the benchmark, we test spatial as\nwell as temporal generalization, by testing on unseen locations, and on data\nafter the training years. Using our dataset we benchmark multiple models, from\nlow-level satellite imagery models such as MOSAIKS , to deep learning\nfoundation models, which include both generic vision models such as\nSelf-Distillation with no Labels (DINOv2) models and specific satellite imagery\nmodels such as SatMAE. We provide open source code for building the satellite\ndataset, obtaining ground truth data from DHS and running various models\nassessed in our work.\n","authors":["Makkunda Sharma","Fan Yang","Duy-Nhat Vo","Esra Suel","Swapnil Mishra","Samir Bhatt","Oliver Fiala","William Rudgard","Seth Flaxman"],"pdf_url":"https://arxiv.org/pdf/2407.05986v1.pdf","comment":"15 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.05983v1","updated":"2024-07-08T14:25:46Z","published":"2024-07-08T14:25:46Z","title":"Towards A Comprehensive Visual Saliency Explanation Framework for\n  AI-based Face Recognition Systems","summary":"  Over recent years, deep convolutional neural networks have significantly\nadvanced the field of face recognition techniques for both verification and\nidentification purposes. Despite the impressive accuracy, these neural networks\nare often criticized for lacking explainability. There is a growing demand for\nunderstanding the decision-making process of AI-based face recognition systems.\nSome studies have investigated the use of visual saliency maps as explanations,\nbut they have predominantly focused on the specific face verification case. The\ndiscussion on more general face recognition scenarios and the corresponding\nevaluation methodology for these explanations have long been absent in current\nresearch. Therefore, this manuscript conceives a comprehensive explanation\nframework for face recognition tasks. Firstly, an exhaustive definition of\nvisual saliency map-based explanations for AI-based face recognition systems is\nprovided, taking into account the two most common recognition situations\nindividually, i.e., face verification and identification. Secondly, a new\nmodel-agnostic explanation method named CorrRISE is proposed to produce\nsaliency maps, which reveal both the similar and dissimilar regions between any\ngiven face images. Subsequently, the explanation framework conceives a new\nevaluation methodology that offers quantitative measurement and comparison of\nthe performance of general visual saliency explanation methods in face\nrecognition. Consequently, extensive experiments are carried out on multiple\nverification and identification scenarios. The results showcase that CorrRISE\ngenerates insightful saliency maps and demonstrates superior performance,\nparticularly in similarity maps in comparison with the state-of-the-art\nexplanation approaches.\n","authors":["Yuhang Lu","Zewei Xu","Touradj Ebrahimi"],"pdf_url":"https://arxiv.org/pdf/2407.05983v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.08546"},{"id":"http://arxiv.org/abs/2407.05982v1","updated":"2024-07-08T14:25:39Z","published":"2024-07-08T14:25:39Z","title":"MTL-Split: Multi-Task Learning for Edge Devices using Split Computing","summary":"  Split Computing (SC), where a Deep Neural Network (DNN) is intelligently\nsplit with a part of it deployed on an edge device and the rest on a remote\nserver is emerging as a promising approach. It allows the power of DNNs to be\nleveraged for latency-sensitive applications that do not allow the entire DNN\nto be deployed remotely, while not having sufficient computation bandwidth\navailable locally. In many such embedded systems scenarios, such as those in\nthe automotive domain, computational resource constraints also necessitate\nMulti-Task Learning (MTL), where the same DNN is used for multiple inference\ntasks instead of having dedicated DNNs for each task, which would need more\ncomputing bandwidth. However, how to partition such a multi-tasking DNN to be\ndeployed within a SC framework has not been sufficiently studied. This paper\nstudies this problem, and MTL-Split, our novel proposed architecture, shows\nencouraging results on both synthetic and real-world data. The source code is\navailable at https://github.com/intelligolabs/MTL-Split.\n","authors":["Luigi Capogrosso","Enrico Fraccaroli","Samarjit Chakraborty","Franco Fummi","Marco Cristani"],"pdf_url":"https://arxiv.org/pdf/2407.05982v1.pdf","comment":"Accepted at the 61st Design Automation Conference (DAC 2024)"},{"id":"http://arxiv.org/abs/2312.04521v2","updated":"2024-07-08T14:24:22Z","published":"2023-12-07T18:41:21Z","title":"Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping","summary":"  The paper explores the industrial multimodal Anomaly Detection (AD) task,\nwhich exploits point clouds and RGB images to localize anomalies. We introduce\na novel light and fast framework that learns to map features from one modality\nto the other on nominal samples. At test time, anomalies are detected by\npinpointing inconsistencies between observed and mapped features. Extensive\nexperiments show that our approach achieves state-of-the-art detection and\nsegmentation performance in both the standard and few-shot settings on the\nMVTec 3D-AD dataset while achieving faster inference and occupying less memory\nthan previous multimodal AD methods. Moreover, we propose a layer-pruning\ntechnique to improve memory and time efficiency with a marginal sacrifice in\nperformance.\n","authors":["Alex Costanzino","Pierluigi Zama Ramirez","Giuseppe Lisanti","Luigi Di Stefano"],"pdf_url":"https://arxiv.org/pdf/2312.04521v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2407.05980v1","updated":"2024-07-08T14:22:46Z","published":"2024-07-08T14:22:46Z","title":"MMIS: Multimodal Dataset for Interior Scene Visual Generation and\n  Recognition","summary":"  We introduce MMIS, a novel dataset designed to advance MultiModal Interior\nScene generation and recognition. MMIS consists of nearly 160,000 images. Each\nimage within the dataset is accompanied by its corresponding textual\ndescription and an audio recording of that description, providing rich and\ndiverse sources of information for scene generation and recognition. MMIS\nencompasses a wide range of interior spaces, capturing various styles, layouts,\nand furnishings. To construct this dataset, we employed careful processes\ninvolving the collection of images, the generation of textual descriptions, and\ncorresponding speech annotations. The presented dataset contributes to research\nin multi-modal representation learning tasks such as image generation,\nretrieval, captioning, and classification.\n","authors":["Hozaifa Kassab","Ahmed Mahmoud","Mohamed Bahaa","Ammar Mohamed","Ali Hamdi"],"pdf_url":"https://arxiv.org/pdf/2407.05980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08345v2","updated":"2024-07-08T14:18:44Z","published":"2024-02-13T10:23:45Z","title":"Conditional Information Gain Trellis","summary":"  Conditional computing processes an input using only part of the neural\nnetwork's computational units. Learning to execute parts of a deep\nconvolutional network by routing individual samples has several advantages:\nReducing the computational burden is an obvious advantage. Furthermore, if\nsimilar classes are routed to the same path, that part of the network learns to\ndiscriminate between finer differences and better classification accuracies can\nbe attained with fewer parameters. Recently, several papers have exploited this\nidea to take a particular child of a node in a tree-shaped network or to skip\nparts of a network. In this work, we follow a Trellis-based approach for\ngenerating specific execution paths in a deep convolutional neural network. We\nhave designed routing mechanisms that use differentiable information gain-based\ncost functions to determine which subset of features in a convolutional layer\nwill be executed. We call our method Conditional Information Gain Trellis\n(CIGT). We show that our conditional execution mechanism achieves comparable or\nbetter model performance compared to unconditional baselines, using only a\nfraction of the computational resources.\n","authors":["Ufuk Can Bicici","Tuna Han Salih Meral","Lale Akarun"],"pdf_url":"https://arxiv.org/pdf/2402.08345v2.pdf","comment":"Accepted by Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2407.01168v2","updated":"2024-07-08T14:17:26Z","published":"2024-07-01T10:38:08Z","title":"Multi-View Black-Box Physical Attacks on Infrared Pedestrian Detectors\n  Using Adversarial Infrared Grid","summary":"  While extensive research exists on physical adversarial attacks within the\nvisible spectrum, studies on such techniques in the infrared spectrum are\nlimited. Infrared object detectors are vital in modern technological\napplications but are susceptible to adversarial attacks, posing significant\nsecurity threats. Previous studies using physical perturbations like light bulb\narrays and aerogels for white-box attacks, or hot and cold patches for\nblack-box attacks, have proven impractical or limited in multi-view support. To\naddress these issues, we propose the Adversarial Infrared Grid (AdvGrid), which\nmodels perturbations in a grid format and uses a genetic algorithm for\nblack-box optimization. These perturbations are cyclically applied to various\nparts of a pedestrian's clothing to facilitate multi-view black-box physical\nattacks on infrared pedestrian detectors. Extensive experiments validate\nAdvGrid's effectiveness, stealthiness, and robustness. The method achieves\nattack success rates of 80.00\\% in digital environments and 91.86\\% in physical\nenvironments, outperforming baseline methods. Additionally, the average attack\nsuccess rate exceeds 50\\% against mainstream detectors, demonstrating AdvGrid's\nrobustness. Our analyses include ablation studies, transfer attacks, and\nadversarial defenses, confirming the method's superiority.\n","authors":["Kalibinuer Tiliwalidi","Chengyin Hu","Weiwen Shi"],"pdf_url":"https://arxiv.org/pdf/2407.01168v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05973v1","updated":"2024-07-08T14:16:05Z","published":"2024-07-08T14:16:05Z","title":"Active Label Refinement for Robust Training of Imbalanced Medical Image\n  Classification Tasks in the Presence of High Label Noise","summary":"  The robustness of supervised deep learning-based medical image classification\nis significantly undermined by label noise. Although several methods have been\nproposed to enhance classification performance in the presence of noisy labels,\nthey face some challenges: 1) a struggle with class-imbalanced datasets,\nleading to the frequent overlooking of minority classes as noisy samples; 2) a\nsingular focus on maximizing performance using noisy datasets, without\nincorporating experts-in-the-loop for actively cleaning the noisy labels. To\nmitigate these challenges, we propose a two-phase approach that combines\nLearning with Noisy Labels (LNL) and active learning. This approach not only\nimproves the robustness of medical image classification in the presence of\nnoisy labels, but also iteratively improves the quality of the dataset by\nrelabeling the important incorrect labels, under a limited annotation budget.\nFurthermore, we introduce a novel Variance of Gradients approach in LNL phase,\nwhich complements the loss-based sample selection by also sampling\nunder-represented samples. Using two imbalanced noisy medical classification\ndatasets, we demonstrate that that our proposed technique is superior to its\npredecessors at handling class imbalance by not misidentifying clean samples\nfrom minority classes as mostly noisy samples.\n","authors":["Bidur Khanal","Tianhong Dai","Binod Bhattarai","Cristian Linte"],"pdf_url":"https://arxiv.org/pdf/2407.05973v1.pdf","comment":"Accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2405.15243v3","updated":"2024-07-08T14:13:17Z","published":"2024-05-24T06:10:23Z","title":"Less is More: Discovering Concise Network Explanations","summary":"  We introduce Discovering Conceptual Network Explanations (DCNE), a new\napproach for generating human-comprehensible visual explanations to enhance the\ninterpretability of deep neural image classifiers. Our method automatically\nfinds visual explanations that are critical for discriminating between classes.\nThis is achieved by simultaneously optimizing three criteria: the explanations\nshould be few, diverse, and human-interpretable. Our approach builds on the\nrecently introduced Concept Relevance Propagation (CRP) explainability method.\nWhile CRP is effective at describing individual neuronal activations, it\ngenerates too many concepts, which impacts human comprehension. Instead, DCNE\nselects the few most important explanations. We introduce a new evaluation\ndataset centered on the challenging task of classifying birds, enabling us to\ncompare the alignment of DCNE's explanations to those of human expert-defined\nones. Compared to existing eXplainable Artificial Intelligence (XAI) methods,\nDCNE has a desirable trade-off between conciseness and completeness when\nsummarizing network explanations. It produces 1/30 of CRP's explanations while\nonly resulting in a slight reduction in explanation quality. DCNE represents a\nstep forward in making neural network decisions accessible and interpretable to\nhumans, providing a valuable tool for both researchers and practitioners in XAI\nand model alignment.\n","authors":["Neehar Kondapaneni","Markus Marks","Oisin Mac Aodha","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2405.15243v3.pdf","comment":"9 pages, 5 figures; ICLR Re-Align Workshop 2024; Project Page:\n  https://www.vision.caltech.edu/dcne/ Github:\n  https://github.com/nkondapa/DiscoveringConciseNetworkExplanations"},{"id":"http://arxiv.org/abs/2312.00732v2","updated":"2024-07-08T14:11:51Z","published":"2023-12-01T17:09:31Z","title":"Gaussian Grouping: Segment and Edit Anything in 3D Scenes","summary":"  The recent Gaussian Splatting achieves high-quality and real-time novel-view\nsynthesis of the 3D scenes. However, it is solely concentrated on the\nappearance and geometry modeling, while lacking in fine-grained object-level\nscene understanding. To address this issue, we propose Gaussian Grouping, which\nextends Gaussian Splatting to jointly reconstruct and segment anything in\nopen-world 3D scenes. We augment each Gaussian with a compact Identity\nEncoding, allowing the Gaussians to be grouped according to their object\ninstance or stuff membership in the 3D scene. Instead of resorting to expensive\n3D labels, we supervise the Identity Encodings during the differentiable\nrendering by leveraging the 2D mask predictions by Segment Anything Model\n(SAM), along with introduced 3D spatial consistency regularization. Compared to\nthe implicit NeRF representation, we show that the discrete and grouped 3D\nGaussians can reconstruct, segment and edit anything in 3D with high visual\nquality, fine granularity and efficiency. Based on Gaussian Grouping, we\nfurther propose a local Gaussian Editing scheme, which shows efficacy in\nversatile scene editing applications, including 3D object removal, inpainting,\ncolorization, style transfer and scene recomposition. Our code and models are\nat https://github.com/lkeab/gaussian-grouping.\n","authors":["Mingqiao Ye","Martin Danelljan","Fisher Yu","Lei Ke"],"pdf_url":"https://arxiv.org/pdf/2312.00732v2.pdf","comment":"ECCV 2024. Gaussian Grouping extends Gaussian Splatting to\n  fine-grained open-world 3D scene understanding. Github:\n  https://github.com/lkeab/gaussian-grouping"},{"id":"http://arxiv.org/abs/2407.05969v1","updated":"2024-07-08T14:07:26Z","published":"2024-07-08T14:07:26Z","title":"Deform-Mamba Network for MRI Super-Resolution","summary":"  In this paper, we propose a new architecture, called Deform-Mamba, for MR\nimage super-resolution. Unlike conventional CNN or Transformer-based\nsuper-resolution approaches which encounter challenges related to the local\nrespective field or heavy computational cost, our approach aims to effectively\nexplore the local and global information of images. Specifically, we develop a\nDeform-Mamba encoder which is composed of two branches, modulated deform block\nand vision Mamba block. We also design a multi-view context module in the\nbottleneck layer to explore the multi-view contextual content. Thanks to the\nextracted features of the encoder, which include content-adaptive local and\nefficient global information, the vision Mamba decoder finally generates\nhigh-quality MR images. Moreover, we introduce a contrastive edge loss to\npromote the reconstruction of edge and contrast related content. Quantitative\nand qualitative experimental results indicate that our approach on IXI and\nfastMRI datasets achieves competitive performance.\n","authors":["Zexin Ji","Beiji Zou","Xiaoyan Kui","Pierre Vera","Su Ruan"],"pdf_url":"https://arxiv.org/pdf/2407.05969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03755v2","updated":"2024-07-08T14:05:29Z","published":"2024-07-04T09:07:25Z","title":"A Computer Vision Approach to Estimate the Localized Sea State","summary":"  This research presents a novel application of computer vision (CV) and deep\nlearning methods for real-time sea state recognition, aiming to contribute to\nimproving the operational safety and energy efficiency of seagoing vessels, key\nfactors in meeting the legislative carbon reduction targets. Our work focuses\non utilizing sea images in operational envelopes captured by a single\nstationary camera mounted on the ship bridge. The collected images are used to\ntrain a deep learning model to automatically recognize the state of the sea\nbased on the Beaufort scale. To recognize the sea state, we used 4\nstate-of-the-art deep neural networks with different characteristics that\nproved useful in various computer vision tasks: Resnet-101, NASNet,\nMobileNet_v2, and Transformer ViT-b32. Furthermore, we have defined a unique\nlarge-scale dataset, collected over a broad range of sea conditions from an\nocean-going vessel prepared for machine learning. We used the transfer learning\napproach to fine-tune the models on our dataset. The obtained results\ndemonstrate the potential for this approach to complement traditional methods,\nparticularly where in-situ measurements are unfeasible or interpolated weather\nbuoy data is insufficiently accurate. This study sets the groundwork for\nfurther development of sea state classification models to address recognized\ngaps in maritime research and enable safer and more efficient maritime\noperations.\n","authors":["Aleksandar Vorkapic","Miran Pobar","Marina Ivasic-Kos"],"pdf_url":"https://arxiv.org/pdf/2407.03755v2.pdf","comment":"Accepted for publication in Ocean Engineering"},{"id":"http://arxiv.org/abs/2407.05967v1","updated":"2024-07-08T14:05:27Z","published":"2024-07-08T14:05:27Z","title":"STMR: Spiral Transformer for Hand Mesh Reconstruction","summary":"  Recent advancements in both transformer-based methods and spiral neighbor\nsampling techniques have greatly enhanced hand mesh reconstruction.\nTransformers excel in capturing complex vertex relationships, and spiral\nneighbor sampling is vital for utilizing topological structures. This paper\ningeniously integrates spiral sampling into the Transformer architecture,\nenhancing its ability to leverage mesh topology for superior performance in\nhand mesh reconstruction, resulting in substantial accuracy boosts. STMR\nemploys a single image encoder for model efficiency. To augment its information\nextraction capability, we design the multi-scale pose feature extraction\n(MSPFE) module, which facilitates the extraction of rich pose features,\nultimately enhancing the model's performance. Moreover, the proposed predefined\npose-to-vertex lifting (PPVL) method improves vertex feature representation,\nfurther boosting reconstruction performance. Extensive experiments on the\nFreiHAND dataset demonstrate the state-of-the-art performance and unparalleled\ninference speed of STMR compared with similar backbone methods, showcasing its\nefficiency and effectiveness. The code is available at\nhttps://github.com/SmallXieGithub/STMR.\n","authors":["Huilong Xie","Wenwei Song","Wenxiong Kang","Yihong Lin"],"pdf_url":"https://arxiv.org/pdf/2407.05967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05965v1","updated":"2024-07-08T14:04:58Z","published":"2024-07-08T14:04:58Z","title":"T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models","summary":"  The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset using LLMs and jailbreaking prompt attacks. Based on our evaluation\nresults, we draw several important findings, including: 1) no single model\nexcels in all aspects, with different models showing various strengths; 2) the\ncorrelation between GPT-4 assessments and manual reviews is generally high; 3)\nthere is a trade-off between the usability and safety of text-to-video\ngenerative models. This indicates that as the field of video generation rapidly\nadvances, safety risks are set to surge, highlighting the urgency of\nprioritizing video safety. We hope that T2VSafetyBench can provide insights for\nbetter understanding the safety of video generation in the era of generative\nAI.\n","authors":["Yibo Miao","Yifan Zhu","Yinpeng Dong","Lijia Yu","Jun Zhu","Xiao-Shan Gao"],"pdf_url":"https://arxiv.org/pdf/2407.05965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10543v6","updated":"2024-07-08T13:45:34Z","published":"2023-11-17T14:10:55Z","title":"Unified theory for joint covariance properties under geometric image\n  transformations for spatio-temporal receptive fields according to the\n  generalized Gaussian derivative model for visual receptive fields","summary":"  The influence of natural image transformations on receptive field responses\nis crucial for modelling visual operations in computer vision and biological\nvision. In this regard, covariance properties with respect to geometric image\ntransformations in the earliest layers of the visual hierarchy are essential\nfor expressing robust image operations, and for formulating invariant visual\noperations at higher levels.\n  This paper defines and proves a set of joint covariance properties for\nspatio-temporal receptive fields in terms of spatio-temporal derivative\noperators applied to spatio-temporally smoothed image data under compositions\nof spatial scaling transformations, spatial affine transformations, Galilean\ntransformations and temporal scaling transformations. Specifically, the derived\nrelations show how the parameters of the receptive fields need to be\ntransformed, in order to match the output from spatio-temporal receptive fields\nunder composed spatio-temporal image transformations.\n  For this purpose, we also fundamentally extend the notion of scale-normalized\nderivatives to affine-normalized derivatives, that are computed based on\nspatial smoothing with affine Gaussian kernels, and analyze the covariance\nproperties of the resulting affine-normalized derivatives for the affine group\nas well as for important subgroups thereof.\n  We conclude with a geometric analysis, showing how the derived joint\ncovariance properties make it possible to relate or match spatio-temporal\nreceptive field responses, when observing, possibly moving, local surface\npatches from different views, under locally linearized perspective or\nprojective transformations, as well as when observing different instances of\nspatio-temporal events, that may occur either faster or slower between\ndifferent views of similar spatio-temporal events.\n","authors":["Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2311.10543v6.pdf","comment":"41 pages, 13 figures. Note: From version 4, this paper considers a\n  different form of joint composition of the geometric image transformations\n  than in the earlier versions"},{"id":"http://arxiv.org/abs/2307.10246v2","updated":"2024-07-08T13:44:56Z","published":"2023-07-17T06:54:36Z","title":"Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding\n  (Survey)","summary":"  Can we obtain insights about the brain using AI models? How is the\ninformation in deep learning models related to brain recordings? Can we improve\nAI models with the help of brain recordings? Such questions can be tackled by\nstudying brain recordings like functional magnetic resonance imaging (fMRI). As\na first step, the neuroscience community has contributed several large\ncognitive neuroscience datasets related to passive reading/listening/viewing of\nconcept words, narratives, pictures, and movies. Encoding and decoding models\nusing these datasets have also been proposed in the past two decades. These\nmodels serve as additional tools for basic cognitive science and neuroscience\nresearch. Encoding models aim at generating fMRI brain representations given a\nstimulus automatically. They have several practical applications in evaluating\nand diagnosing neurological conditions and thus may also help design therapies\nfor brain damage. Decoding models solve the inverse problem of reconstructing\nthe stimuli given the fMRI. They are useful for designing brain-machine or\nbrain-computer interfaces. Inspired by the effectiveness of deep learning\nmodels for natural language processing, computer vision, and speech, several\nneural encoding and decoding models have been recently proposed. In this\nsurvey, we will first discuss popular representations of language, vision and\nspeech stimuli, and present a summary of neuroscience datasets. Further, we\nwill review popular deep learning based encoding and decoding architectures and\nnote their benefits and limitations. Finally, we will conclude with a summary\nand discussion about future trends. Given the large amount of recently\npublished work in the computational cognitive neuroscience (CCN) community, we\nbelieve that this survey enables an entry point for DNN researchers to\ndiversify into CCN research.\n","authors":["Subba Reddy Oota","Zijiao Chen","Manish Gupta","Raju S. Bapi","Gael Jobard","Frederic Alexandre","Xavier Hinaut"],"pdf_url":"https://arxiv.org/pdf/2307.10246v2.pdf","comment":"47 pages, 23 figures"},{"id":"http://arxiv.org/abs/2109.08730v2","updated":"2024-07-08T13:42:17Z","published":"2021-09-17T19:23:31Z","title":"Unsupervised View-Invariant Human Posture Representation","summary":"  Most recent view-invariant action recognition and performance assessment\napproaches rely on a large amount of annotated 3D skeleton data to extract\nview-invariant features. However, acquiring 3D skeleton data can be cumbersome,\nif not impractical, in in-the-wild scenarios. To overcome this problem, we\npresent a novel unsupervised approach that learns to extract view-invariant 3D\nhuman pose representation from a 2D image without using 3D joint data. Our\nmodel is trained by exploiting the intrinsic view-invariant properties of human\npose between simultaneous frames from different viewpoints and their\nequivariant properties between augmented frames from the same viewpoint. We\nevaluate the learned view-invariant pose representations for two downstream\ntasks. We perform comparative experiments that show improvements on the\nstate-of-the-art unsupervised cross-view action classification accuracy on NTU\nRGB+D by a significant margin, on both RGB and depth images. We also show the\nefficiency of transferring the learned representations from NTU RGB+D to obtain\nthe first ever unsupervised cross-view and cross-subject rank correlation\nresults on the multi-view human movement quality dataset, QMAR, and marginally\nimprove on the-state-of-the-art supervised results for this dataset. We also\ncarry out ablation studies to examine the contributions of the different\ncomponents of our proposed network.\n","authors":["Faegheh Sardari","BjÃ¶rn Ommer","Majid Mirmehdi"],"pdf_url":"https://arxiv.org/pdf/2109.08730v2.pdf","comment":"Accpeted at BMVC 2021"},{"id":"http://arxiv.org/abs/2407.04604v2","updated":"2024-07-08T13:38:49Z","published":"2024-07-05T15:53:04Z","title":"PartCraft: Crafting Creative Objects by Parts","summary":"  This paper propels creative control in generative visual AI by allowing users\nto \"select\". Departing from traditional text or sketch-based methods, we for\nthe first time allow users to choose visual concepts by parts for their\ncreative endeavors. The outcome is fine-grained generation that precisely\ncaptures selected visual concepts, ensuring a holistically faithful and\nplausible result. To achieve this, we first parse objects into parts through\nunsupervised feature clustering. Then, we encode parts into text tokens and\nintroduce an entropy-based normalized attention loss that operates on them.\nThis loss design enables our model to learn generic prior topology knowledge\nabout object's part composition, and further generalize to novel part\ncompositions to ensure the generation looks holistically faithful. Lastly, we\nemploy a bottleneck encoder to project the part tokens. This not only enhances\nfidelity but also accelerates learning, by leveraging shared knowledge and\nfacilitating information exchange among instances. Visual results in the paper\nand supplementary material showcase the compelling power of PartCraft in\ncrafting highly customized, innovative creations, exemplified by the \"charming\"\nand creative birds. Code is released at https://github.com/kamwoh/partcraft.\n","authors":["Kam Woh Ng","Xiatian Zhu","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2407.04604v2.pdf","comment":"ECCV 2024. arXiv admin note: substantial text overlap with\n  arXiv:2311.15477"},{"id":"http://arxiv.org/abs/2407.04061v2","updated":"2024-07-08T13:37:41Z","published":"2024-07-04T17:06:16Z","title":"Detect Closer Surfaces that can be Seen: New Modeling and Evaluation in\n  Cross-domain 3D Object Detection","summary":"  The performance of domain adaptation technologies has not yet reached an\nideal level in the current 3D object detection field for autonomous driving,\nwhich is mainly due to significant differences in the size of vehicles, as well\nas the environments they operate in when applied across domains. These factors\ntogether hinder the effective transfer and application of knowledge learned\nfrom specific datasets. Since the existing evaluation metrics are initially\ndesigned for evaluation on a single domain by calculating the 2D or 3D overlap\nbetween the prediction and ground-truth bounding boxes, they often suffer from\nthe overfitting problem caused by the size differences among datasets. This\nraises a fundamental question related to the evaluation of the 3D object\ndetection models' cross-domain performance: Do we really need models to\nmaintain excellent performance in their original 3D bounding boxes after being\napplied across domains? From a practical application perspective, one of our\nmain focuses is actually on preventing collisions between vehicles and other\nobstacles, especially in cross-domain scenarios where correctly predicting the\nsize of vehicles is much more difficult. In other words, as long as a model can\naccurately identify the closest surfaces to the ego vehicle, it is sufficient\nto effectively avoid obstacles. In this paper, we propose two metrics to\nmeasure 3D object detection models' ability of detecting the closer surfaces to\nthe sensor on the ego vehicle, which can be used to evaluate their cross-domain\nperformance more comprehensively and reasonably. Furthermore, we propose a\nrefinement head, named EdgeHead, to guide models to focus more on the learnable\ncloser surfaces, which can greatly improve the cross-domain performance of\nexisting models not only under our new metrics, but even also under the\noriginal BEV/3D metrics.\n","authors":["Ruixiao Zhang","Yihong Wu","Juheon Lee","Adam Prugel-Bennett","Xiaohao Cai"],"pdf_url":"https://arxiv.org/pdf/2407.04061v2.pdf","comment":"Accepted by the 27th European Conference on Artificial Intelligence\n  (ECAI 2024)"},{"id":"http://arxiv.org/abs/2407.05924v1","updated":"2024-07-08T13:32:01Z","published":"2024-07-08T13:32:01Z","title":"Graph-Boosted Attentive Network for Semantic Body Parsing","summary":"  Human body parsing remains a challenging problem in natural scenes due to\nmulti-instance and inter-part semantic confusions as well as occlusions. This\npaper proposes a novel approach to decomposing multiple human bodies into\nsemantic part regions in unconstrained environments. Specifically we propose a\nconvolutional neural network (CNN) architecture which comprises of novel\nsemantic and contour attention mechanisms across feature hierarchy to resolve\nthe semantic ambiguities and boundary localization issues related to semantic\nbody parsing. We further propose to encode estimated pose as higher-level\ncontextual information which is combined with local semantic cues in a novel\ngraphical model in a principled manner. In this proposed model, the lower-level\nsemantic cues can be recursively updated by propagating higher-level contextual\ninformation from estimated pose and vice versa across the graph, so as to\nalleviate erroneous pose information and pixel level predictions. We further\npropose an optimization technique to efficiently derive the solutions. Our\nproposed method achieves the state-of-art results on the challenging Pascal\nPerson-Part dataset.\n","authors":["Tinghuai Wang","Huiling Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02647v2","updated":"2024-07-08T13:30:09Z","published":"2024-07-02T20:29:23Z","title":"Spectral Graph Reasoning Network for Hyperspectral Image Classification","summary":"  Convolutional neural networks (CNNs) have achieved remarkable performance in\nhyperspectral image (HSI) classification over the last few years. Despite the\nprogress that has been made, rich and informative spectral information of HSI\nhas been largely underutilized by existing methods which employ convolutional\nkernels with limited size of receptive field in the spectral domain. To address\nthis issue, we propose a spectral graph reasoning network (SGR) learning\nframework comprising two crucial modules: 1) a spectral decoupling module which\nunpacks and casts multiple spectral embeddings into a unified graph whose node\ncorresponds to an individual spectral feature channel in the embedding space;\nthe graph performs interpretable reasoning to aggregate and align spectral\ninformation to guide learning spectral-specific graph embeddings at multiple\ncontextual levels 2) a spectral ensembling module explores the interactions and\ninterdependencies across graph embedding hierarchy via a novel recurrent graph\npropagation mechanism. Experiments on two HSI datasets demonstrate that the\nproposed architecture can significantly improve the classification accuracy\ncompared with the existing methods with a sizable margin.\n","authors":["Huiling Wang"],"pdf_url":"https://arxiv.org/pdf/2407.02647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05921v1","updated":"2024-07-08T13:28:47Z","published":"2024-07-08T13:28:47Z","title":"TAPVid-3D: A Benchmark for Tracking Any Point in 3D","summary":"  We introduce a new benchmark, TAPVid-3D, for evaluating the task of\nlong-range Tracking Any Point in 3D (TAP-3D). While point tracking in two\ndimensions (TAP) has many benchmarks measuring performance on real-world\nvideos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To\nthis end, leveraging existing footage, we build a new benchmark for 3D point\ntracking featuring 4,000+ real-world videos, composed of three different data\nsources spanning a variety of object types, motion patterns, and indoor and\noutdoor environments. To measure performance on the TAP-3D task, we formulate a\ncollection of metrics that extend the Jaccard-based metric used in TAP to\nhandle the complexities of ambiguous depth scales across models, occlusions,\nand multi-track spatio-temporal smoothness. We manually verify a large sample\nof trajectories to ensure correct video annotations, and assess the current\nstate of the TAP-3D task by constructing competitive baselines using existing\ntracking models. We anticipate this benchmark will serve as a guidepost to\nimprove our ability to understand precise 3D motion and surface deformation\nfrom monocular video. Code for dataset download, generation, and model\nevaluation is available at https://tapvid3d.github.io\n","authors":["Skanda Koppula","Ignacio Rocco","Yi Yang","Joe Heyward","JoÃ£o Carreira","Andrew Zisserman","Gabriel Brostow","Carl Doersch"],"pdf_url":"https://arxiv.org/pdf/2407.05921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08551v4","updated":"2024-07-08T13:22:14Z","published":"2024-03-13T14:02:54Z","title":"GaussianImage: 1000 FPS Image Representation and Compression by 2D\n  Gaussian Splatting","summary":"  Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 2000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding. Code is available at\nhttps://github.com/Xinjie-Q/GaussianImage.\n","authors":["Xinjie Zhang","Xingtong Ge","Tongda Xu","Dailan He","Yan Wang","Hongwei Qin","Guo Lu","Jing Geng","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08551v4.pdf","comment":"Accepted by ECCV 2024. Project\n  Page:https://xingtongge.github.io/GaussianImage-page; Code:\n  https://github.com/Xinjie-Q/GaussianImage"},{"id":"http://arxiv.org/abs/2407.05916v1","updated":"2024-07-08T13:22:13Z","published":"2024-07-08T13:22:13Z","title":"Non-parametric Contextual Relationship Learning for Semantic Video\n  Object Segmentation","summary":"  We propose a novel approach for modeling semantic contextual relationships in\nvideos. This graph-based model enables the learning and propagation of\nhigher-level spatial-temporal contexts to facilitate the semantic labeling of\nlocal regions. We introduce an exemplar-based nonparametric view of contextual\ncues, where the inherent relationships implied by object hypotheses are encoded\non a similarity graph of regions. Contextual relationships learning and\npropagation are performed to estimate the pairwise contexts between all pairs\nof unlabeled local regions. Our algorithm integrates the learned contexts into\na Conditional Random Field (CRF) in the form of pairwise potentials and infers\nthe per-region semantic labels. We evaluate our approach on the challenging\nYouTube-Objects dataset which shows that the proposed contextual relationship\nmodel outperforms the state-of-the-art methods.\n","authors":["Tinghuai Wang","Huiling Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19091v2","updated":"2024-07-08T13:20:16Z","published":"2024-02-29T12:18:43Z","title":"Leveraging Representations from Intermediate Encoder-blocks for\n  Synthetic Image Detection","summary":"  The recently developed and publicly available synthetic image generation\nmethods and services make it possible to create extremely realistic imagery on\ndemand, raising great risks for the integrity and safety of online information.\nState-of-the-art Synthetic Image Detection (SID) research has led to strong\nevidence on the advantages of feature extraction from foundation models.\nHowever, such extracted features mostly encapsulate high-level visual semantics\ninstead of fine-grained details, which are more important for the SID task. On\nthe contrary, shallow layers encode low-level visual information. In this work,\nwe leverage the image representations extracted by intermediate Transformer\nblocks of CLIP's image-encoder via a lightweight network that maps them to a\nlearnable forgery-aware vector space capable of generalizing exceptionally\nwell. We also employ a trainable module to incorporate the importance of each\nTransformer block to the final prediction. Our method is compared against the\nstate-of-the-art by evaluating it on 20 test datasets and exhibits an average\n+10.6% absolute performance improvement. Notably, the best performing models\nrequire just a single epoch for training (~8 minutes). Code available at\nhttps://github.com/mever-team/rine.\n","authors":["Christos Koutlis","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2402.19091v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.04092v2","updated":"2024-07-08T13:19:01Z","published":"2024-07-04T17:59:26Z","title":"Looking for Tiny Defects via Forward-Backward Feature Transfer","summary":"  Motivated by efficiency requirements, most anomaly detection and segmentation\n(AD&S) methods focus on processing low-resolution images, e.g., $224\\times 224$\npixels, obtained by downsampling the original input images. In this setting,\ndownsampling is typically applied also to the provided ground-truth defect\nmasks. Yet, as numerous industrial applications demand identification of both\nlarge and tiny defects, the above-described protocol may fall short in\nproviding a realistic picture of the actual performance attainable by current\nmethods. Hence, in this work, we introduce a novel benchmark that evaluates\nmethods on the original, high-resolution image and ground-truth masks, focusing\non segmentation performance as a function of the size of anomalies. Our\nbenchmark includes a metric that captures robustness with respect to defect\nsize, i.e., the ability of a method to preserve good localization from large\nanomalies to tiny ones. Furthermore, we introduce an AD&S approach based on a\nnovel Teacher-Student paradigm which relies on two shallow MLPs (the Students)\nthat learn to transfer patch features across the layers of a frozen vision\ntransformer (the Teacher). By means of our benchmark, we evaluate our proposal\nand other recent AD&S methods on high-resolution inputs containing large and\ntiny defects. Our proposal features the highest robustness to defect size, runs\nat the fastest speed, yields state-of-the-art performance on the MVTec AD\ndataset and state-of-the-art segmentation performance on the VisA dataset.\n","authors":["Alex Costanzino","Pierluigi Zama Ramirez","Giuseppe Lisanti","Luigi Di Stefano"],"pdf_url":"https://arxiv.org/pdf/2407.04092v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05913v1","updated":"2024-07-08T13:18:49Z","published":"2024-07-08T13:18:49Z","title":"Submodular video object proposal selection for semantic object\n  segmentation","summary":"  Learning a data-driven spatio-temporal semantic representation of the objects\nis the key to coherent and consistent labelling in video. This paper proposes\nto achieve semantic video object segmentation by learning a data-driven\nrepresentation which captures the synergy of multiple instances from continuous\nframes. To prune the noisy detections, we exploit the rich information among\nmultiple instances and select the discriminative and representative subset.\nThis selection process is formulated as a facility location problem solved by\nmaximising a submodular function. Our method retrieves the longer term\ncontextual dependencies which underpins a robust semantic video object\nsegmentation algorithm. We present extensive experiments on a challenging\ndataset that demonstrate the superior performance of our approach compared with\nthe state-of-the-art methods.\n","authors":["Tinghuai Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05913v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:1606.02280"},{"id":"http://arxiv.org/abs/2407.05910v1","updated":"2024-07-08T13:15:11Z","published":"2024-07-08T13:15:11Z","title":"Enhancing Vision-Language Models with Scene Graphs for Traffic Accident\n  Understanding","summary":"  Recognizing a traffic accident is an essential part of any autonomous driving\nor road monitoring system. An accident can appear in a wide variety of forms,\nand understanding what type of accident is taking place may be useful to\nprevent it from reoccurring. The task of being able to classify a traffic scene\nas a specific type of accident is the focus of this work. We approach the\nproblem by likening a traffic scene to a graph, where objects such as cars can\nbe represented as nodes, and relative distances and directions between them as\nedges. This representation of an accident can be referred to as a scene graph,\nand is used as input for an accident classifier. Better results can be obtained\nwith a classifier that fuses the scene graph input with representations from\nvision and language. This work introduces a multi-stage, multimodal pipeline to\npre-process videos of traffic accidents, encode them as scene graphs, and align\nthis representation with vision and language modalities for accident\nclassification. When trained on 4 classes, our method achieves a balanced\naccuracy score of 57.77% on an (unbalanced) subset of the popular Detection of\nTraffic Anomaly (DoTA) benchmark, representing an increase of close to 5\npercentage points from the case where scene graph information is not taken into\naccount.\n","authors":["Aaron Lohner","Francesco Compagno","Jonathan Francis","Alessandro Oltramari"],"pdf_url":"https://arxiv.org/pdf/2407.05910v1.pdf","comment":"Accepted: 1st Workshop on Semantic Reasoning and Goal Understanding\n  in Robotics, at the Robotics Science and Systems Conference (SemRob @ RSS\n  2024)"},{"id":"http://arxiv.org/abs/2407.05909v1","updated":"2024-07-08T13:14:25Z","published":"2024-07-08T13:14:25Z","title":"Multi-clue Consistency Learning to Bridge Gaps Between General and\n  Oriented Object in Semi-supervised Detection","summary":"  While existing semi-supervised object detection (SSOD) methods perform well\nin general scenes, they encounter challenges in handling oriented objects in\naerial images. We experimentally find three gaps between general and oriented\nobject detection in semi-supervised learning: 1) Sampling inconsistency: the\ncommon center sampling is not suitable for oriented objects with larger aspect\nratios when selecting positive labels from labeled data. 2) Assignment\ninconsistency: balancing the precision and localization quality of oriented\npseudo-boxes poses greater challenges which introduces more noise when\nselecting positive labels from unlabeled data. 3) Confidence inconsistency:\nthere exists more mismatch between the predicted classification and\nlocalization qualities when considering oriented objects, affecting the\nselection of pseudo-labels. Therefore, we propose a Multi-clue Consistency\nLearning (MCL) framework to bridge gaps between general and oriented objects in\nsemi-supervised detection. Specifically, considering various shapes of rotated\nobjects, the Gaussian Center Assignment is specially designed to select the\npixel-level positive labels from labeled data. We then introduce the\nScale-aware Label Assignment to select pixel-level pseudo-labels instead of\nunreliable pseudo-boxes, which is a divide-and-rule strategy suited for objects\nwith various scales. The Consistent Confidence Soft Label is adopted to further\nboost the detector by maintaining the alignment of the predicted results.\nComprehensive experiments on DOTA-v1.5 and DOTA-v1.0 benchmarks demonstrate\nthat our proposed MCL can achieve state-of-the-art performance in the\nsemi-supervised oriented object detection task.\n","authors":["Chenxu Wang","Chunyan Xu","Ziqi Gu","Zhen Cui"],"pdf_url":"https://arxiv.org/pdf/2407.05909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02623v2","updated":"2024-07-08T13:09:39Z","published":"2024-07-02T19:27:00Z","title":"Uplifting Lower-Income Data: Strategies for Socioeconomic Perspective\n  Shifts in Vision-Language Models","summary":"  Unequal representation across cultures and socioeconomic groups in AI is a\nsignificant and challenging problem, often leading to uneven model performance.\nAs a step toward addressing this issue, we formulate translated non-English,\ngeographic, and socioeconomic integrated prompts and evaluate their impact on\nVL model performance for data from different countries and income groups. Our\nfindings show that geographic and socioeconomic integrated prompts improve VL\nperformance on lower-income data and favor the retrieval of topic appearances\ncommonly found in data from low-income households. From our analyses, we\nidentify and highlight contexts where these strategies yield the most\nimprovements. Our model analysis code is publicly available at\nhttps://github.com/Anniejoan/Uplifting-Lower-income-data .\n","authors":["Joan Nwatu","Oana Ignat","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2407.02623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05897v1","updated":"2024-07-08T13:04:40Z","published":"2024-07-08T13:04:40Z","title":"Deciphering the Role of Representation Disentanglement: Investigating\n  Compositional Generalization in CLIP Models","summary":"  CLIP models have recently shown to exhibit Out of Distribution (OoD)\ngeneralization capabilities. However, Compositional Out of Distribution (C-OoD)\ngeneralization, which is a crucial aspect of a model's ability to understand\nunseen compositions of known concepts, is relatively unexplored for the CLIP\nmodels. Our goal is to address this problem and identify the factors that\ncontribute to the C-OoD in CLIPs. We noted that previous studies regarding\ncompositional understanding of CLIPs frequently fail to ensure that test\nsamples are genuinely novel relative to the CLIP training data. To this end, we\ncarefully synthesized a large and diverse dataset in the single object setting,\ncomprising attributes for objects that are highly unlikely to be encountered in\nthe combined training datasets of various CLIP models. This dataset enables an\nauthentic evaluation of C-OoD generalization. Our observations reveal varying\nlevels of C-OoD generalization across different CLIP models. We propose that\nthe disentanglement of CLIP representations serves as a critical indicator in\nthis context. By utilizing our synthesized datasets and other existing\ndatasets, we assess various disentanglement metrics of text and image\nrepresentations. Our study reveals that the disentanglement of image and text\nrepresentations, particularly with respect to their compositional elements,\nplays a crucial role in improving the generalization of CLIP models in\nout-of-distribution settings. This finding suggests promising opportunities for\nadvancing out-of-distribution generalization in CLIPs.\n","authors":["Reza Abbasi","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2407.05897v1.pdf","comment":"Accepted for publication at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.05892v1","updated":"2024-07-08T12:59:28Z","published":"2024-07-08T12:59:28Z","title":"An efficient method to automate tooth identification and 3D bounding box\n  extraction from Cone Beam CT Images","summary":"  Accurate identification, localization, and segregation of teeth from Cone\nBeam Computed Tomography (CBCT) images are essential for analyzing dental\npathologies. Modeling an individual tooth can be challenging and intricate to\naccomplish, especially when fillings and other restorations introduce\nartifacts. This paper proposes a method for automatically detecting,\nidentifying, and extracting teeth from CBCT images. Our approach involves\ndividing the three-dimensional images into axial slices for image detection.\nTeeth are pinpointed and labeled using a single-stage object detector.\nSubsequently, bounding boxes are delineated and identified to create\nthree-dimensional representations of each tooth. The proposed solution has been\nsuccessfully integrated into the dental analysis tool Dentomo.\n","authors":["Ignacio Garrido Botella","Ignacio Arranz Ãgueda","Juan Carlos Armenteros Carmona","Oleg Vorontsov","Fernando BayÃ³n Robledo","AdriÃ¡n Alonso Barriuso"],"pdf_url":"https://arxiv.org/pdf/2407.05892v1.pdf","comment":"7 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2407.01295v2","updated":"2024-07-08T12:56:26Z","published":"2024-07-01T13:47:54Z","title":"Formal Verification of Object Detection","summary":"  Deep Neural Networks (DNNs) are ubiquitous in real-world applications, yet\nthey remain vulnerable to errors and adversarial attacks. This work tackles the\nchallenge of applying formal verification to ensure the safety of computer\nvision models, extending verification beyond image classification to object\ndetection. We propose a general formulation for certifying the robustness of\nobject detection models using formal verification and outline implementation\nstrategies compatible with state-of-the-art verification tools. Our approach\nenables the application of these tools, originally designed for verifying\nclassification models, to object detection. We define various attacks for\nobject detection, illustrating the diverse ways adversarial inputs can\ncompromise neural network outputs. Our experiments, conducted on several common\ndatasets and networks, reveal potential errors in object detection models,\nhighlighting system vulnerabilities and emphasizing the need for expanding\nformal verification to these new domains. This work paves the way for further\nresearch in integrating formal verification across a broader range of computer\nvision applications.\n","authors":["Avraham Raviv","Yizhak Y. Elboher","Michelle Aluf-Medina","Yael Leibovich Weiss","Omer Cohen","Roy Assa","Guy Katz","Hillel Kugler"],"pdf_url":"https://arxiv.org/pdf/2407.01295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05878v1","updated":"2024-07-08T12:42:10Z","published":"2024-07-08T12:42:10Z","title":"HiT-SR: Hierarchical Transformer for Efficient Image Super-Resolution","summary":"  Transformers have exhibited promising performance in computer vision tasks\nincluding image super-resolution (SR). However, popular transformer-based SR\nmethods often employ window self-attention with quadratic computational\ncomplexity to window sizes, resulting in fixed small windows with limited\nreceptive fields. In this paper, we present a general strategy to convert\ntransformer-based SR networks to hierarchical transformers (HiT-SR), boosting\nSR performance with multi-scale features while maintaining an efficient design.\nSpecifically, we first replace the commonly used fixed small windows with\nexpanding hierarchical windows to aggregate features at different scales and\nestablish long-range dependencies. Considering the intensive computation\nrequired for large windows, we further design a spatial-channel correlation\nmethod with linear complexity to window sizes, efficiently gathering spatial\nand channel information from hierarchical windows. Extensive experiments verify\nthe effectiveness and efficiency of our HiT-SR, and our improved versions of\nSwinIR-Light, SwinIR-NG, and SRFormer-Light yield state-of-the-art SR results\nwith fewer parameters, FLOPs, and faster speeds ($\\sim7\\times$).\n","authors":["Xiang Zhang","Yulun Zhang","Fisher Yu"],"pdf_url":"https://arxiv.org/pdf/2407.05878v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.02639v2","updated":"2024-07-08T12:42:02Z","published":"2024-07-02T20:11:09Z","title":"Holistically-Nested Structure-Aware Graph Neural Network for Road\n  Extraction","summary":"  Convolutional neural networks (CNN) have made significant advances in\ndetecting roads from satellite images. However, existing CNN approaches are\ngenerally repurposed semantic segmentation architectures and suffer from the\npoor delineation of long and curved regions. Lack of overall road topology and\nstructure information further deteriorates their performance on challenging\nremote sensing images. This paper presents a novel multi-task graph neural\nnetwork (GNN) which simultaneously detects both road regions and road borders;\nthe inter-play between these two tasks unlocks superior performance from two\nperspectives: (1) the hierarchically detected road borders enable the network\nto capture and encode holistic road structure to enhance road connectivity (2)\nidentifying the intrinsic correlation of semantic landcover regions mitigates\nthe difficulty in recognizing roads cluttered by regions with similar\nappearance. Experiments on challenging dataset demonstrate that the proposed\narchitecture can improve the road border delineation and road extraction\naccuracy compared with the existing methods.\n","authors":["Tinghuai Wang","Guangming Wang","Kuan Eeik Tan"],"pdf_url":"https://arxiv.org/pdf/2407.02639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05875v1","updated":"2024-07-08T12:33:54Z","published":"2024-07-08T12:33:54Z","title":"Minutes to Seconds: Speeded-up DDPM-based Image Inpainting with\n  Coarse-to-Fine Sampling","summary":"  For image inpainting, the existing Denoising Diffusion Probabilistic Model\n(DDPM) based method i.e. RePaint can produce high-quality images for any\ninpainting form. It utilizes a pre-trained DDPM as a prior and generates\ninpainting results by conditioning on the reverse diffusion process, namely\ndenoising process. However, this process is significantly time-consuming. In\nthis paper, we propose an efficient DDPM-based image inpainting method which\nincludes three speed-up strategies. First, we utilize a pre-trained\nLight-Weight Diffusion Model (LWDM) to reduce the number of parameters. Second,\nwe introduce a skip-step sampling scheme of Denoising Diffusion Implicit Models\n(DDIM) for the denoising process. Finally, we propose Coarse-to-Fine Sampling\n(CFS), which speeds up inference by reducing image resolution in the coarse\nstage and decreasing denoising timesteps in the refinement stage. We conduct\nextensive experiments on both faces and general-purpose image inpainting tasks,\nand our method achieves competitive performance with approximately 60 times\nspeedup.\n","authors":["Lintao Zhang","Xiangcheng Du","LeoWu TomyEnrique","Yiqun Wang","Yingbin Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2407.05875v1.pdf","comment":"The code is avaliable at: https://github.com/linghuyuhangyuan/M2S"},{"id":"http://arxiv.org/abs/2407.05862v1","updated":"2024-07-08T12:28:56Z","published":"2024-07-08T12:28:56Z","title":"Bringing Masked Autoencoders Explicit Contrastive Properties for Point\n  Cloud Self-Supervised Learning","summary":"  Contrastive learning (CL) for Vision Transformers (ViTs) in image domains has\nachieved performance comparable to CL for traditional convolutional backbones.\nHowever, in 3D point cloud pretraining with ViTs, masked autoencoder (MAE)\nmodeling remains dominant. This raises the question: Can we take the best of\nboth worlds? To answer this question, we first empirically validate that\nintegrating MAE-based point cloud pre-training with the standard contrastive\nlearning paradigm, even with meticulous design, can lead to a decrease in\nperformance. To address this limitation, we reintroduce CL into the MAE-based\npoint cloud pre-training paradigm by leveraging the inherent contrastive\nproperties of MAE. Specifically, rather than relying on extensive data\naugmentation as commonly used in the image domain, we randomly mask the input\ntokens twice to generate contrastive input pairs. Subsequently, a\nweight-sharing encoder and two identically structured decoders are utilized to\nperform masked token reconstruction. Additionally, we propose that for an input\ntoken masked by both masks simultaneously, the reconstructed features should be\nas similar as possible. This naturally establishes an explicit contrastive\nconstraint within the generative MAE-based pre-training paradigm, resulting in\nour proposed method, Point-CMAE. Consequently, Point-CMAE effectively enhances\nthe representation quality and transfer performance compared to its MAE\ncounterpart. Experimental evaluations across various downstream applications,\nincluding classification, part segmentation, and few-shot learning, demonstrate\nthe efficacy of our framework in surpassing state-of-the-art techniques under\nstandard ViTs and single-modal settings. The source code and trained models are\navailable at: https://github.com/Amazingren/Point-CMAE.\n","authors":["Bin Ren","Guofeng Mei","Danda Pani Paudel","Weijie Wang","Yawei Li","Mengyuan Liu","Rita Cucchiara","Luc Van Gool","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2407.05862v1.pdf","comment":"Bringing Masked Autoencoders Explicit Contrastive Properties for\n  Point Cloud Self-Supervised Learning"},{"id":"http://arxiv.org/abs/2310.06549v5","updated":"2024-07-08T12:05:50Z","published":"2023-10-10T11:51:12Z","title":"Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield\n  but Also a Catalyst for Model Inversion Attacks","summary":"  Label smoothing -- using softened labels instead of hard ones -- is a widely\nadopted regularization method for deep learning, showing diverse benefits such\nas enhanced generalization and calibration. Its implications for preserving\nmodel privacy, however, have remained unexplored. To fill this gap, we\ninvestigate the impact of label smoothing on model inversion attacks (MIAs),\nwhich aim to generate class-representative samples by exploiting the knowledge\nencoded in a classifier, thereby inferring sensitive information about its\ntraining data. Through extensive analyses, we uncover that traditional label\nsmoothing fosters MIAs, thereby increasing a model's privacy leakage. Even\nmore, we reveal that smoothing with negative factors counters this trend,\nimpeding the extraction of class-related information and leading to privacy\npreservation, beating state-of-the-art defenses. This establishes a practical\nand powerful novel way for enhancing model resilience against MIAs.\n","authors":["Lukas Struppek","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.06549v5.pdf","comment":"Published as a conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.10883v2","updated":"2024-07-08T12:03:14Z","published":"2024-03-16T10:32:24Z","title":"Improving Adversarial Transferability of Vision-Language Pre-training\n  Models through Collaborative Multimodal Interaction","summary":"  Despite the substantial advancements in Vision-Language Pre-training (VLP)\nmodels, their susceptibility to adversarial attacks poses a significant\nchallenge. Existing work rarely studies the transferability of attacks on VLP\nmodels, resulting in a substantial performance gap from white-box attacks. We\nobserve that prior work overlooks the interaction mechanisms between\nmodalities, which plays a crucial role in understanding the intricacies of VLP\nmodels. In response, we propose a novel attack, called Collaborative Multimodal\nInteraction Attack (CMI-Attack), leveraging modality interaction through\nembedding guidance and interaction enhancement. Specifically, attacking text at\nthe embedding level while preserving semantics, as well as utilizing\ninteraction image gradients to enhance constraints on perturbations of texts\nand images. Significantly, in the image-text retrieval task on Flickr30K\ndataset, CMI-Attack raises the transfer success rates from ALBEF to TCL,\n$\\text{CLIP}_{\\text{ViT}}$ and $\\text{CLIP}_{\\text{CNN}}$ by 8.11%-16.75% over\nstate-of-the-art methods. Moreover, CMI-Attack also demonstrates superior\nperformance in cross-task generalization scenarios. Our work addresses the\nunderexplored realm of transfer attacks on VLP models, shedding light on the\nimportance of modality interaction for enhanced adversarial robustness.\n","authors":["Jiyuan Fu","Zhaoyu Chen","Kaixun Jiang","Haijing Guo","Jiafeng Wang","Shuyong Gao","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.10883v2.pdf","comment":"This work won first place in CVPR 2024 Workshop Challenge: Black-box\n  Adversarial Attacks on Vision Foundation Models"},{"id":"http://arxiv.org/abs/2407.05848v1","updated":"2024-07-08T11:55:10Z","published":"2024-07-08T11:55:10Z","title":"Wavelet Convolutions for Large Receptive Fields","summary":"  In recent years, there have been attempts to increase the kernel size of\nConvolutional Neural Nets (CNNs) to mimic the global receptive field of Vision\nTransformers' (ViTs) self-attention blocks. That approach, however, quickly hit\nan upper bound and saturated way before achieving a global receptive field. In\nthis work, we demonstrate that by leveraging the Wavelet Transform (WT), it is,\nin fact, possible to obtain very large receptive fields without suffering from\nover-parameterization, e.g., for a $k \\times k$ receptive field, the number of\ntrainable parameters in the proposed method grows only logarithmically with\n$k$. The proposed layer, named WTConv, can be used as a drop-in replacement in\nexisting architectures, results in an effective multi-frequency response, and\nscales gracefully with the size of the receptive field. We demonstrate the\neffectiveness of the WTConv layer within ConvNeXt and MobileNetV2 architectures\nfor image classification, as well as backbones for downstream tasks, and show\nit yields additional properties such as robustness to image corruption and an\nincreased response to shapes over textures. Our code is available at\nhttps://github.com/BGU-CS-VIL/WTConv.\n","authors":["Shahaf E. Finder","Roy Amoyal","Eran Treister","Oren Freifeld"],"pdf_url":"https://arxiv.org/pdf/2407.05848v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.05844v1","updated":"2024-07-08T11:44:15Z","published":"2024-07-08T11:44:15Z","title":"Anatomy-guided Pathology Segmentation","summary":"  Pathological structures in medical images are typically deviations from the\nexpected anatomy of a patient. While clinicians consider this interplay between\nanatomy and pathology, recent deep learning algorithms specialize in\nrecognizing either one of the two, rarely considering the patient's body from\nsuch a joint perspective. In this paper, we develop a generalist segmentation\nmodel that combines anatomical and pathological information, aiming to enhance\nthe segmentation accuracy of pathological features. Our Anatomy-Pathology\nExchange (APEx) training utilizes a query-based segmentation transformer which\ndecodes a joint feature space into query-representations for human anatomy and\ninterleaves them via a mixing strategy into the pathology-decoder for\nanatomy-informed pathology predictions. In doing so, we are able to report the\nbest results across the board on FDG-PET-CT and Chest X-Ray pathology\nsegmentation tasks with a margin of up to 3.3% as compared to strong baseline\nmethods. Code and models will be publicly available at\ngithub.com/alexanderjaus/APEx.\n","authors":["Alexander Jaus","Constantin Seibold","Simon ReiÃ","Lukas Heine","Anton Schily","Moon Kim","Fin Hendrik Bahnsen","Ken Herrmann","Rainer Stiefelhagen","Jens Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2407.05844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05843v1","updated":"2024-07-08T11:41:32Z","published":"2024-07-08T11:41:32Z","title":"Evaluating the Fairness of Neural Collapse in Medical Image\n  Classification","summary":"  Deep learning has achieved impressive performance across various medical\nimaging tasks. However, its inherent bias against specific groups hinders its\nclinical applicability in equitable healthcare systems. A recently discovered\nphenomenon, Neural Collapse (NC), has shown potential in improving the\ngeneralization of state-of-the-art deep learning models. Nonetheless, its\nimplications on bias in medical imaging remain unexplored. Our study\ninvestigates deep learning fairness through the lens of NC. We analyze the\ntraining dynamics of models as they approach NC when training using biased\ndatasets, and examine the subsequent impact on test performance, specifically\nfocusing on label bias. We find that biased training initially results in\ndifferent NC configurations across subgroups, before converging to a final NC\nsolution by memorizing all data samples. Through extensive experiments on three\nmedical imaging datasets -- PAPILA, HAM10000, and CheXpert -- we find that in\nbiased settings, NC can lead to a significant drop in F1 score across all\nsubgroups. Our code is available at\nhttps://gitlab.com/radiology/neuro/neural-collapse-fairness\n","authors":["Kaouther Mouheb","Marawan Elbatel","Stefan Klein","Esther E. Bron"],"pdf_url":"https://arxiv.org/pdf/2407.05843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05842v1","updated":"2024-07-08T11:39:21Z","published":"2024-07-08T11:39:21Z","title":"3D Vessel Graph Generation Using Denoising Diffusion","summary":"  Blood vessel networks, represented as 3D graphs, help predict disease\nbiomarkers, simulate blood flow, and aid in synthetic image generation,\nrelevant in both clinical and pre-clinical settings. However, generating\nrealistic vessel graphs that correspond to an anatomy of interest is\nchallenging. Previous methods aimed at generating vessel trees mostly in an\nautoregressive style and could not be applied to vessel graphs with cycles such\nas capillaries or specific anatomical structures such as the Circle of Willis.\nAddressing this gap, we introduce the first application of \\textit{denoising\ndiffusion models} in 3D vessel graph generation. Our contributions include a\nnovel, two-stage generation method that sequentially denoises node coordinates\nand edges. We experiment with two real-world vessel datasets, consisting of\nmicroscopic capillaries and major cerebral vessels, and demonstrate the\ngeneralizability of our method for producing diverse, novel, and anatomically\nplausible vessel graphs.\n","authors":["Chinmay Prabhakar","Suprosanna Shit","Fabio Musio","Kaiyuan Yang","Tamaz Amiranashvili","Johannes C. Paetzold","Hongwei Bran Li","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2407.05842v1.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2309.00903v4","updated":"2024-07-08T11:07:55Z","published":"2023-09-02T10:46:05Z","title":"An explainable three dimension framework to uncover learning patterns: A\n  unified look in variable sulci recognition","summary":"  The significant features identified in a representative subset of the dataset\nduring the learning process of an artificial intelligence model are referred to\nas a 'global' explanation. Three-dimensional (3D) global explanations are\ncrucial in neuroimaging where a complex representational space demands more\nthan basic two-dimensional interpretations. Curently, studies in the literature\nlack accurate, low-complexity, and 3D global explanations in neuroimaging and\nbeyond. To fill this gap, we develop a novel explainable artificial\nintelligence (XAI) 3D-Framework that provides robust, faithful, and\nlow-complexity global explanations. We evaluated our framework on various 3D\ndeep learning networks trained, validated, and tested on a well-annotated\ncohort of 596 MRI images. The focus of detection was on the presence or absence\nof the paracingulate sulcus, a highly variable feature of brain topology\nassociated with symptoms of psychosis. Our proposed 3D-Framework outperformed\ntraditional XAI methods in terms of faithfulness for global explanations. As a\nresult, these explanations uncovered new patterns that not only enhance the\ncredibility and reliability of the training process but also reveal the broader\ndevelopmental landscape of the human cortex. Our XAI 3D-Framework proposes for\nthe first time, a way to utilize global explanations to discover the context in\nwhich detection of specific features are embedded, opening our understanding of\nnormative brain development and atypical trajectories that can lead to the\nemergence of mental illness.\n","authors":["Michail Mamalakis","Heloise de Vareilles","Atheer AI-Manea","Samantha C. Mitchell","Ingrid Arartz","Lynn Egeland Morch-Johnsen","Jane Garrison","Jon Simons","Pietro Lio","John Suckling","Graham Murray"],"pdf_url":"https://arxiv.org/pdf/2309.00903v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07051v2","updated":"2024-07-08T11:03:42Z","published":"2023-12-12T08:08:34Z","title":"Mask as Supervision: Leveraging Unified Mask Information for\n  Unsupervised 3D Pose Estimation","summary":"  Automatic estimation of 3D human pose from monocular RGB images is a\nchallenging and unsolved problem in computer vision. In a supervised manner,\napproaches heavily rely on laborious annotations and present hampered\ngeneralization ability due to the limited diversity of 3D pose datasets. To\naddress these challenges, we propose a unified framework that leverages mask as\nsupervision for unsupervised 3D pose estimation. With general unsupervised\nsegmentation algorithms, the proposed model employs skeleton and physique\nrepresentations that exploit accurate pose information from coarse to fine.\nCompared with previous unsupervised approaches, we organize the human skeleton\nin a fully unsupervised way which enables the processing of annotation-free\ndata and provides ready-to-use estimation results. Comprehensive experiments\ndemonstrate our state-of-the-art pose estimation performance on Human3.6M and\nMPI-INF-3DHP datasets. Further experiments on in-the-wild datasets also\nillustrate the capability to access more data to boost our model. Code will be\navailable at https://github.com/Charrrrrlie/Mask-as-Supervision.\n","authors":["Yuchen Yang","Yu Qiao","Xiao Sun"],"pdf_url":"https://arxiv.org/pdf/2312.07051v2.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2311.06322v3","updated":"2024-07-08T11:02:47Z","published":"2023-11-10T09:10:09Z","title":"Post-training Quantization for Text-to-Image Diffusion Models with\n  Progressive Calibration and Activation Relaxing","summary":"  High computational overhead is a troublesome problem for diffusion models.\nRecent studies have leveraged post-training quantization (PTQ) to compress\ndiffusion models. However, most of them only focus on unconditional models,\nleaving the quantization of widely-used pretrained text-to-image models, e.g.,\nStable Diffusion, largely unexplored. In this paper, we propose a novel\npost-training quantization method PCR (Progressive Calibration and Relaxing)\nfor text-to-image diffusion models, which consists of a progressive calibration\nstrategy that considers the accumulated quantization error across timesteps,\nand an activation relaxing strategy that improves the performance with\nnegligible cost. Additionally, we demonstrate the previous metrics for\ntext-to-image diffusion model quantization are not accurate due to the\ndistribution gap. To tackle the problem, we propose a novel QDiffBench\nbenchmark, which utilizes data in the same domain for more accurate evaluation.\nBesides, QDiffBench also considers the generalization performance of the\nquantized model outside the calibration dataset. Extensive experiments on\nStable Diffusion and Stable Diffusion XL demonstrate the superiority of our\nmethod and benchmark. Moreover, we are the first to achieve quantization for\nStable Diffusion XL while maintaining the performance.\n","authors":["Siao Tang","Xin Wang","Hong Chen","Chaoyu Guan","Zewen Wu","Yansong Tang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.06322v3.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2404.01964v2","updated":"2024-07-08T11:00:51Z","published":"2024-04-02T13:57:30Z","title":"CAM-Based Methods Can See through Walls","summary":"  CAM-based methods are widely-used post-hoc interpretability method that\nproduce a saliency map to explain the decision of an image classification\nmodel. The saliency map highlights the important areas of the image relevant to\nthe prediction. In this paper, we show that most of these methods can\nincorrectly attribute an important score to parts of the image that the model\ncannot see. We show that this phenomenon occurs both theoretically and\nexperimentally. On the theory side, we analyze the behavior of GradCAM on a\nsimple masked CNN model at initialization. Experimentally, we train a VGG-like\nmodel constrained to not use the lower part of the image and nevertheless\nobserve positive scores in the unseen part of the image. This behavior is\nevaluated quantitatively on two new datasets. We believe that this is\nproblematic, potentially leading to mis-interpretation of the model's behavior.\n","authors":["Magamed Taimeskhanov","Ronan Sicre","Damien Garreau"],"pdf_url":"https://arxiv.org/pdf/2404.01964v2.pdf","comment":"Accepted for publication at ECML 2024 (28 pages, 9 figures)"},{"id":"http://arxiv.org/abs/2407.05814v1","updated":"2024-07-08T10:51:03Z","published":"2024-07-08T10:51:03Z","title":"Cross-domain Few-shot In-context Learning for Enhancing Traffic Sign\n  Recognition","summary":"  Recent multimodal large language models (MLLM) such as GPT-4o and GPT-4v have\nshown great potential in autonomous driving. In this paper, we propose a\ncross-domain few-shot in-context learning method based on the MLLM for\nenhancing traffic sign recognition (TSR). We first construct a traffic sign\ndetection network based on Vision Transformer Adapter and an extraction module\nto extract traffic signs from the original road images. To reduce the\ndependence on training data and improve the performance stability of\ncross-country TSR, we introduce a cross-domain few-shot in-context learning\nmethod based on the MLLM. To enhance MLLM's fine-grained recognition ability of\ntraffic signs, the proposed method generates corresponding description texts\nusing template traffic signs. These description texts contain key information\nabout the shape, color, and composition of traffic signs, which can stimulate\nthe ability of MLLM to perceive fine-grained traffic sign categories. By using\nthe description texts, our method reduces the cross-domain differences between\ntemplate and real traffic signs. Our approach requires only simple and uniform\ntextual indications, without the need for large-scale traffic sign images and\nlabels. We perform comprehensive evaluations on the German traffic sign\nrecognition benchmark dataset, the Belgium traffic sign dataset, and two\nreal-world datasets taken from Japan. The experimental results show that our\nmethod significantly enhances the TSR performance.\n","authors":["Yaozong Gan","Guang Li","Ren Togo","Keisuke Maeda","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2407.05814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.14272v3","updated":"2024-07-08T10:50:56Z","published":"2022-09-28T17:36:47Z","title":"Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and\n  First Results","summary":"  Humor is a substantial element of human social behavior, affect, and\ncognition. Its automatic understanding can facilitate a more naturalistic\nhuman-AI interaction. Current methods of humor detection have been exclusively\nbased on staged data, making them inadequate for \"real-world\" applications. We\ncontribute to addressing this deficiency by introducing the novel\nPassau-Spontaneous Football Coach Humor (Passau-SFCH) dataset, comprising about\n11 hours of recordings. The Passau-SFCH dataset is annotated for the presence\nof humor and its dimensions (sentiment and direction) as proposed in Martin's\nHumor Style Questionnaire. We conduct a series of experiments employing\npretrained Transformers, convolutional neural networks, and expert-designed\nfeatures. The performance of each modality (text, audio, video) for spontaneous\nhumor recognition is analyzed and their complementarity is investigated. Our\nfindings suggest that for the automatic analysis of humor and its sentiment,\nfacial expressions are most promising, while humor direction can be best\nmodeled via text-based features. Further, we experiment with different\nmultimodal approaches to humor recognition, including decision-level fusion and\nMulT, a multimodal Transformer approach. In this context, we propose a novel\nmultimodal architecture that yields the best overall results. Finally, we make\nour code publicly available at https://www.github.com/lc0197/passau-sfch. The\nPassau-SFCH dataset is available upon request.\n","authors":["Lukas Christ","Shahin Amiriparian","Alexander Kathan","Niklas MÃ¼ller","Andreas KÃ¶nig","BjÃ¶rn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2209.14272v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible (Major Revision)"},{"id":"http://arxiv.org/abs/2407.05811v1","updated":"2024-07-08T10:45:30Z","published":"2024-07-08T10:45:30Z","title":"MapsTP: HD Map Images Based Multimodal Trajectory Prediction for\n  Automated Vehicles","summary":"  Predicting ego vehicle trajectories remains a critical challenge, especially\nin urban and dense areas due to the unpredictable behaviours of other vehicles\nand pedestrians. Multimodal trajectory prediction enhances decision-making by\nconsidering multiple possible future trajectories based on diverse sources of\nenvironmental data. In this approach, we leverage ResNet-50 to extract image\nfeatures from high-definition map data and use IMU sensor data to calculate\nspeed, acceleration, and yaw rate. A temporal probabilistic network is employed\nto compute potential trajectories, selecting the most accurate and highly\nprobable trajectory paths. This method integrates HD map data to improve the\nrobustness and reliability of trajectory predictions for autonomous vehicles.\n","authors":["Sushil Sharma","Arindam Das","Ganesh Sistu","Mark Halton","CiarÃ¡n Eising"],"pdf_url":"https://arxiv.org/pdf/2407.05811v1.pdf","comment":"Accepted for publication at th 26th Irish Machine Vision and Image\n  Processing Conference, 2024"},{"id":"http://arxiv.org/abs/2407.03835v2","updated":"2024-07-08T10:40:53Z","published":"2024-07-04T11:04:29Z","title":"7th ABAW Competition: Multi-Task Learning and Compound Expression\n  Recognition","summary":"  This paper describes the 7th Affective Behavior Analysis in-the-wild (ABAW)\nCompetition, which is part of the respective Workshop held in conjunction with\nECCV 2024. The 7th ABAW Competition addresses novel challenges in understanding\nhuman expressions and behaviors, crucial for the development of human-centered\ntechnologies. The Competition comprises of two sub-challenges: i) Multi-Task\nLearning (the goal is to learn at the same time, in a multi-task learning\nsetting, to estimate two continuous affect dimensions, valence and arousal, to\nrecognise between the mutually exclusive classes of the 7 basic expressions and\n'other'), and to detect 12 Action Units); and ii) Compound Expression\nRecognition (the target is to recognise between the 7 mutually exclusive\ncompound expression classes). s-Aff-Wild2, which is a static version of the A/V\nAff-Wild2 database and contains annotations for valence-arousal, expressions\nand Action Units, is utilized for the purposes of the Multi-Task Learning\nChallenge; a part of C-EXPR-DB, which is an A/V in-the-wild database with\ncompound expression annotations, is utilized for the purposes of the Compound\nExpression Recognition Challenge. In this paper, we introduce the two\nchallenges, detailing their datasets and the protocols followed for each. We\nalso outline the evaluation metrics, and highlight the baseline systems and\ntheir results. Additional information about the competition can be found at\n\\url{https://affective-behavior-analysis-in-the-wild.github.io/7th}.\n","authors":["Dimitrios Kollias","Stefanos Zafeiriou","Irene Kotsia","Abhinav Dhall","Shreya Ghosh","Chunchang Shao","Guanyu Hu"],"pdf_url":"https://arxiv.org/pdf/2407.03835v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05800v1","updated":"2024-07-08T10:10:07Z","published":"2024-07-08T10:10:07Z","title":"FedMRL: Data Heterogeneity Aware Federated Multi-agent Deep\n  Reinforcement Learning for Medical Imaging","summary":"  Despite recent advancements in federated learning (FL) for medical image\ndiagnosis, addressing data heterogeneity among clients remains a significant\nchallenge for practical implementation. A primary hurdle in FL arises from the\nnon-IID nature of data samples across clients, which typically results in a\ndecline in the performance of the aggregated global model. In this study, we\nintroduce FedMRL, a novel federated multi-agent deep reinforcement learning\nframework designed to address data heterogeneity. FedMRL incorporates a novel\nloss function to facilitate fairness among clients, preventing bias in the\nfinal global model. Additionally, it employs a multi-agent reinforcement\nlearning (MARL) approach to calculate the proximal term $(\\mu)$ for the\npersonalized local objective function, ensuring convergence to the global\noptimum. Furthermore, FedMRL integrates an adaptive weight adjustment method\nusing a Self-organizing map (SOM) on the server side to counteract distribution\nshifts among clients' local data distributions. We assess our approach using\ntwo publicly available real-world medical datasets, and the results demonstrate\nthat FedMRL significantly outperforms state-of-the-art techniques, showing its\nefficacy in addressing data heterogeneity in federated learning. The code can\nbe found here~{\\url{https://github.com/Pranabiitp/FedMRL}}.\n","authors":["Pranab Sahoo","Ashutosh Tripathi","Sriparna Saha","Samrat Mondal"],"pdf_url":"https://arxiv.org/pdf/2407.05800v1.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.05796v1","updated":"2024-07-08T09:56:30Z","published":"2024-07-08T09:56:30Z","title":"Poisson Ordinal Network for Gleason Group Estimation Using Bi-Parametric\n  MRI","summary":"  The Gleason groups serve as the primary histological grading system for\nprostate cancer, providing crucial insights into the cancer's potential for\ngrowth and metastasis. In clinical practice, pathologists determine the Gleason\ngroups based on specimens obtained from ultrasound-guided biopsies. In this\nstudy, we investigate the feasibility of directly estimating the Gleason groups\nfrom MRI scans to reduce otherwise required biopsies. We identify two\ncharacteristics of this task, ordinality and the resulting dependent yet\nunknown variances between Gleason groups. In addition to the inter- / intra-\nobserver variability in a multi-step Gleason scoring process based on the\ninterpretation of Gleason patterns, our MR-based prediction is also subject to\nspecimen sampling variance and, to a lesser degree, varying MR imaging\nprotocols. To address this challenge, we propose a novel Poisson ordinal\nnetwork (PON). PONs model the prediction using a Poisson distribution and\nleverages Poisson encoding and Poisson focal loss to capture a learnable\ndependency between ordinal classes (here, Gleason groups), rather than relying\nsolely on the numerical ground-truth (e.g. Gleason Groups 1-5 or Gleason Scores\n6-10). To improve this modelling efficacy, PONs also employ contrastive\nlearning with a memory bank to regularise intra-class variance, decoupling the\nmemory requirement of contrast learning from the batch size. Experimental\nresults based on the images labelled by saturation biopsies from 265\nprior-biopsy-blind patients, across two tasks demonstrate the superiority and\neffectiveness of our proposed method.\n","authors":["Yinsong Xu","Yipei Wang","Ziyi Shen","Iani J. M. B. Gayo","Natasha Thorley","Shonit Punwani","Aidong Men","Dean Barratt","Qingchao Chen","Yipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2407.05796v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.05795v1","updated":"2024-07-08T09:55:36Z","published":"2024-07-08T09:55:36Z","title":"HyCIR: Boosting Zero-Shot Composed Image Retrieval with Synthetic Labels","summary":"  Composed Image Retrieval (CIR) aims to retrieve images based on a query image\nwith text. Current Zero-Shot CIR (ZS-CIR) methods try to solve CIR tasks\nwithout using expensive triplet-labeled training datasets. However, the gap\nbetween ZS-CIR and triplet-supervised CIR is still large. In this work, we\npropose Hybrid CIR (HyCIR), which uses synthetic labels to boost the\nperformance of ZS-CIR. A new label Synthesis pipeline for CIR (SynCir) is\nproposed, in which only unlabeled images are required. First, image pairs are\nextracted based on visual similarity. Second, query text is generated for each\nimage pair based on vision-language model and LLM. Third, the data is further\nfiltered in language space based on semantic similarity. To improve ZS-CIR\nperformance, we propose a hybrid training strategy to work with both ZS-CIR\nsupervision and synthetic CIR triplets. Two kinds of contrastive learning are\nadopted. One is to use large-scale unlabeled image dataset to learn an\nimage-to-text mapping with good generalization. The other is to use synthetic\nCIR triplets to learn a better mapping for CIR tasks. Our approach achieves\nSOTA zero-shot performance on the common CIR benchmarks: CIRR and CIRCO.\n","authors":["Yingying Jiang","Hanchao Jia","Xiaobing Wang","Peng Hao"],"pdf_url":"https://arxiv.org/pdf/2407.05795v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2309.12325v3","updated":"2024-07-08T09:54:09Z","published":"2023-08-11T10:49:05Z","title":"FUTURE-AI: International consensus guideline for trustworthy and\n  deployable artificial intelligence in healthcare","summary":"  Despite major advances in artificial intelligence (AI) for medicine and\nhealthcare, the deployment and adoption of AI technologies remain limited in\nreal-world clinical practice. In recent years, concerns have been raised about\nthe technical, clinical, ethical and legal risks associated with medical AI. To\nincrease real world adoption, it is essential that medical AI tools are trusted\nand accepted by patients, clinicians, health organisations and authorities.\nThis work describes the FUTURE-AI guideline as the first international\nconsensus framework for guiding the development and deployment of trustworthy\nAI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and\ncurrently comprises 118 inter-disciplinary experts from 51 countries\nrepresenting all continents, including AI scientists, clinicians, ethicists,\nand social scientists. Over a two-year period, the consortium defined guiding\nprinciples and best practices for trustworthy AI through an iterative process\ncomprising an in-depth literature review, a modified Delphi survey, and online\nconsensus meetings. The FUTURE-AI framework was established based on 6 guiding\nprinciples for trustworthy AI in healthcare, i.e. Fairness, Universality,\nTraceability, Usability, Robustness and Explainability. Through consensus, a\nset of 28 best practices were defined, addressing technical, clinical, legal\nand socio-ethical dimensions. The recommendations cover the entire lifecycle of\nmedical AI, from design, development and validation to regulation, deployment,\nand monitoring. FUTURE-AI is a risk-informed, assumption-free guideline which\nprovides a structured approach for constructing medical AI tools that will be\ntrusted, deployed and adopted in real-world practice. Researchers are\nencouraged to take the recommendations into account in proof-of-concept stages\nto facilitate future translation towards clinical practice of medical AI.\n","authors":["Karim Lekadir","Aasa Feragen","Abdul Joseph Fofanah","Alejandro F Frangi","Alena Buyx","Anais Emelie","Andrea Lara","Antonio R Porras","An-Wen Chan","Arcadi Navarro","Ben Glocker","Benard O Botwe","Bishesh Khanal","Brigit Beger","Carol C Wu","Celia Cintas","Curtis P Langlotz","Daniel Rueckert","Deogratias Mzurikwao","Dimitrios I Fotiadis","Doszhan Zhussupov","Enzo Ferrante","Erik Meijering","Eva Weicken","Fabio A GonzÃ¡lez","Folkert W Asselbergs","Fred Prior","Gabriel P Krestin","Gary Collins","Geletaw S Tegenaw","Georgios Kaissis","Gianluca Misuraca","Gianna Tsakou","Girish Dwivedi","Haridimos Kondylakis","Harsha Jayakody","Henry C Woodruf","Horst Joachim Mayer","Hugo JWL Aerts","Ian Walsh","Ioanna Chouvarda","IrÃ¨ne Buvat","Isabell Tributsch","Islem Rekik","James Duncan","Jayashree Kalpathy-Cramer","Jihad Zahir","Jinah Park","John Mongan","Judy W Gichoya","Julia A Schnabel","Kaisar Kushibar","Katrine Riklund","Kensaku Mori","Kostas Marias","Lameck M Amugongo","Lauren A Fromont","Lena Maier-Hein","Leonor CerdÃ¡ Alberich","Leticia Rittner","Lighton Phiri","Linda Marrakchi-Kacem","LluÃ­s Donoso-Bach","Luis MartÃ­-BonmatÃ­","M Jorge Cardoso","Maciej Bobowicz","Mahsa Shabani","Manolis Tsiknakis","Maria A Zuluaga","Maria Bielikova","Marie-Christine Fritzsche","Marina Camacho","Marius George Linguraru","Markus Wenzel","Marleen De Bruijne","Martin G Tolsgaard","Marzyeh Ghassemi","Md Ashrafuzzaman","Melanie Goisauf","Mohammad Yaqub","MÃ³nica Cano AbadÃ­a","Mukhtar M E Mahmoud","Mustafa Elattar","Nicola Rieke","Nikolaos Papanikolaou","Noussair Lazrak","Oliver DÃ­az","Olivier Salvado","Oriol Pujol","Ousmane Sall","Pamela Guevara","Peter Gordebeke","Philippe Lambin","Pieta Brown","Purang Abolmaesumi","Qi Dou","Qinghua Lu","Richard Osuala","Rose Nakasi","S Kevin Zhou","Sandy Napel","Sara Colantonio","Shadi Albarqouni","Smriti Joshi","Stacy Carter","Stefan Klein","Steffen E Petersen","Susanna AussÃ³","Suyash Awate","Tammy Riklin Raviv","Tessa Cook","Tinashe E M Mutsvangwa","Wendy A Rogers","Wiro J Niessen","XÃ¨nia Puig-Bosch","Yi Zeng","Yunusa G Mohammed","Yves Saint James Aquino","Zohaib Salahuddin","Martijn P A Starmans"],"pdf_url":"https://arxiv.org/pdf/2309.12325v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06517v2","updated":"2024-07-08T09:52:45Z","published":"2024-06-10T17:56:21Z","title":"Genomics-guided Representation Learning for Pathologic Pan-cancer Tumor\n  Microenvironment Subtype Prediction","summary":"  The characterization of Tumor MicroEnvironment (TME) is challenging due to\nits complexity and heterogeneity. Relatively consistent TME characteristics\nembedded within highly specific tissue features, render them difficult to\npredict. The capability to accurately classify TME subtypes is of critical\nsignificance for clinical tumor diagnosis and precision medicine. Based on the\nobservation that tumors with different origins share similar microenvironment\npatterns, we propose PathoTME, a genomics-guided Siamese representation\nlearning framework employing Whole Slide Image (WSI) for pan-cancer TME\nsubtypes prediction. Specifically, we utilize Siamese network to leverage\ngenomic information as a regularization factor to assist WSI embeddings\nlearning during the training phase. Additionally, we employ Domain Adversarial\nNeural Network (DANN) to mitigate the impact of tissue type variations. To\neliminate domain bias, a dynamic WSI prompt is designed to further unleash the\nmodel's capabilities. Our model achieves better performance than other\nstate-of-the-art methods across 23 cancer types on TCGA dataset. Our code is\navailable at https://github.com/Mengflz/PathoTME.\n","authors":["Fangliangzi Meng","Hongrun Zhang","Ruodan Yan","Guohui Chuai","Chao Li","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2406.06517v2.pdf","comment":"MICCAI2024"},{"id":"http://arxiv.org/abs/2407.05782v1","updated":"2024-07-08T09:45:20Z","published":"2024-07-08T09:45:20Z","title":"Sequential Contrastive Audio-Visual Learning","summary":"  Contrastive learning has emerged as a powerful technique in audio-visual\nrepresentation learning, leveraging the natural co-occurrence of audio and\nvisual modalities in extensive web-scale video datasets to achieve significant\nadvancements. However, conventional contrastive audio-visual learning\nmethodologies often rely on aggregated representations derived through temporal\naggregation, which neglects the intrinsic sequential nature of the data. This\noversight raises concerns regarding the ability of standard approaches to\ncapture and utilize fine-grained information within sequences, information that\nis vital for distinguishing between semantically similar yet distinct examples.\nIn response to this limitation, we propose sequential contrastive audio-visual\nlearning (SCAV), which contrasts examples based on their non-aggregated\nrepresentation space using sequential distances. Retrieval experiments with the\nVGGSound and Music datasets demonstrate the effectiveness of SCAV, showing 2-3x\nrelative improvements against traditional aggregation-based contrastive\nlearning and other methods from the literature. We also show that models\ntrained with SCAV exhibit a high degree of flexibility regarding the metric\nemployed for retrieval, allowing them to operate on a spectrum of\nefficiency-accuracy trade-offs, potentially making them applicable in multiple\nscenarios, from small- to large-scale retrieval.\n","authors":["Ioannis Tsiamas","Santiago Pascual","Chunghsin Yeh","Joan SerrÃ "],"pdf_url":"https://arxiv.org/pdf/2407.05782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10003v2","updated":"2024-07-08T09:40:03Z","published":"2023-07-19T14:23:26Z","title":"TbExplain: A Text-based Explanation Method for Scene Classification\n  Models with the Statistical Prediction Correction","summary":"  The field of Explainable Artificial Intelligence (XAI) aims to improve the\ninterpretability of black-box machine learning models. Building a heatmap based\non the importance value of input features is a popular method for explaining\nthe underlying functions of such models in producing their predictions.\nHeatmaps are almost understandable to humans, yet they are not without flaws.\nNon-expert users, for example, may not fully understand the logic of heatmaps\n(the logic in which relevant pixels to the model's prediction are highlighted\nwith different intensities or colors). Additionally, objects and regions of the\ninput image that are relevant to the model prediction are frequently not\nentirely differentiated by heatmaps. In this paper, we propose a framework\ncalled TbExplain that employs XAI techniques and a pre-trained object detector\nto present text-based explanations of scene classification models. Moreover,\nTbExplain incorporates a novel method to correct predictions and textually\nexplain them based on the statistics of objects in the input image when the\ninitial prediction is unreliable. To assess the trustworthiness and validity of\nthe text-based explanations, we conducted a qualitative experiment, and the\nfindings indicated that these explanations are sufficiently reliable.\nFurthermore, our quantitative and qualitative experiments on TbExplain with\nscene classification datasets reveal an improvement in classification accuracy\nover ResNet variants.\n","authors":["Amirhossein Aminimehr","Pouya Khani","Amirali Molaei","Amirmohammad Kazemeini","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2307.10003v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05771v1","updated":"2024-07-08T09:27:34Z","published":"2024-07-08T09:27:34Z","title":"Multi-times Monte Carlo Rendering for Inter-reflection Reconstruction","summary":"  Inverse rendering methods have achieved remarkable performance in\nreconstructing high-fidelity 3D objects with disentangled geometries,\nmaterials, and environmental light. However, they still face huge challenges in\nreflective surface reconstruction. Although recent methods model the light\ntrace to learn specularity, the ignorance of indirect illumination makes it\nhard to handle inter-reflections among multiple smooth objects. In this work,\nwe propose Ref-MC2 that introduces the multi-time Monte Carlo sampling which\ncomprehensively computes the environmental illumination and meanwhile considers\nthe reflective light from object surfaces. To address the computation challenge\nas the times of Monte Carlo sampling grow, we propose a specularity-adaptive\nsampling strategy, significantly reducing the computational complexity. Besides\nthe computational resource, higher geometry accuracy is also required because\ngeometric errors accumulate multiple times. Therefore, we further introduce a\nreflection-aware surface model to initialize the geometry and refine it during\ninverse rendering. We construct a challenging dataset containing scenes with\nmultiple objects and inter-reflections. Experiments show that our method\noutperforms other inverse rendering methods on various object groups. We also\nshow downstream applications, e.g., relighting and material editing, to\nillustrate the disentanglement ability of our method.\n","authors":["Tengjie Zhu","Zhuo Chen","Jingnan Gao","Yichao Yan","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2407.05771v1.pdf","comment":"10 pages,6 figures,NeurIPS 2024 Submitted"},{"id":"http://arxiv.org/abs/2407.05769v1","updated":"2024-07-08T09:25:45Z","published":"2024-07-08T09:25:45Z","title":"Boosting 3D Object Detection with Semantic-Aware Multi-Branch Framework","summary":"  In autonomous driving, LiDAR sensors are vital for acquiring 3D point clouds,\nproviding reliable geometric information. However, traditional sampling methods\nof preprocessing often ignore semantic features, leading to detail loss and\nground point interference in 3D object detection. To address this, we propose a\nmulti-branch two-stage 3D object detection framework using a Semantic-aware\nMulti-branch Sampling (SMS) module and multi-view consistency constraints. The\nSMS module includes random sampling, Density Equalization Sampling (DES) for\nenhancing distant objects, and Ground Abandonment Sampling (GAS) to focus on\nnon-ground points. The sampled multi-view points are processed through a\nConsistent KeyPoint Selection (CKPS) module to generate consistent keypoint\nmasks for efficient proposal sampling. The first-stage detector uses\nmulti-branch parallel learning with multi-view consistency loss for feature\naggregation, while the second-stage detector fuses multi-view data through a\nMulti-View Fusion Pooling (MVFP) module to precisely predict 3D objects. The\nexperimental results on KITTI 3D object detection benchmark dataset show that\nour method achieves excellent detection performance improvement for a variety\nof backbones, especially for low-performance backbones with the simple network\nstructures.\n","authors":["Hao Jing","Anhong Wang","Lijun Zhao","Yakun Yang","Donghan Bu","Jing Zhang","Yifan Zhang","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2407.05769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05767v1","updated":"2024-07-08T09:19:40Z","published":"2024-07-08T09:19:40Z","title":"Nonrigid Reconstruction of Freehand Ultrasound without a Tracker","summary":"  Reconstructing 2D freehand Ultrasound (US) frames into 3D space without using\na tracker has recently seen advances with deep learning. Predicting good\nframe-to-frame rigid transformations is often accepted as the learning\nobjective, especially when the ground-truth labels from spatial tracking\ndevices are inherently rigid transformations. Motivated by a) the observed\nnonrigid deformation due to soft tissue motion during scanning, and b) the\nhighly sensitive prediction of rigid transformation, this study investigates\nthe methods and their benefits in predicting nonrigid transformations for\nreconstructing 3D US. We propose a novel co-optimisation algorithm for\nsimultaneously estimating rigid transformations among US frames, supervised by\nground-truth from a tracker, and a nonrigid deformation, optimised by a\nregularised registration network. We show that these two objectives can be\neither optimised using meta-learning or combined by weighting. A fast scattered\ndata interpolation is also developed for enabling frequent reconstruction and\nregistration of non-parallel US frames, during training. With a new data set\ncontaining over 357,000 frames in 720 scans, acquired from 60 subjects, the\nexperiments demonstrate that, due to an expanded thus easier-to-optimise\nsolution space, the generalisation is improved with the added deformation\nestimation, with respect to the rigid ground-truth. The global pixel\nreconstruction error (assessing accumulative prediction) is lowered from 18.48\nto 16.51 mm, compared with baseline rigid-transformation-predicting methods.\nUsing manually identified landmarks, the proposed co-optimisation also shows\npotentials in compensating nonrigid tissue motion at inference, which is not\nmeasurable by tracker-provided ground-truth. The code and data used in this\npaper are made publicly available at https://github.com/QiLi111/NR-Rec-FUS.\n","authors":["Qi Li","Ziyi Shen","Qianye Yang","Dean C. Barratt","Matthew J. Clarkson","Tom Vercauteren","Yipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2407.05767v1.pdf","comment":"Accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.05765v1","updated":"2024-07-08T09:16:42Z","published":"2024-07-08T09:16:42Z","title":"Enlarging Feature Support Overlap for Domain Generalization","summary":"  Deep models often struggle with out-of-distribution (OOD) generalization,\nlimiting their real-world applicability beyond controlled laboratory settings.\nInvariant risk minimization (IRM) addresses this issue by learning invariant\nfeatures and minimizing the risk across different domains. Thus, it avoids the\npitfalls of pseudo-invariant features and spurious causality associated with\nempirical risk minimization (ERM). However, according to the support overlap\ntheorem, ERM and IRM may fail to address the OOD problem when pseudo-invariant\nfeatures have insufficient support overlap. To this end, we propose a novel\nmethod to enlarge feature support overlap for domain generalization.\nSpecifically, we introduce Bayesian random semantic data augmentation to\nincrease sample diversity and overcome the deficiency of IRM. Experiments on\nseveral challenging OOD generalization benchmarks demonstrate that our approach\nsurpasses existing models, delivering superior performance and robustness. The\ncode is available at \\url{https://github.com/YaoyaoZhu19/BSDG}.\n","authors":["Yaoyao Zhu","Xiuding Cai","Dong Miao","Yu Yao","Zhongliang Fu"],"pdf_url":"https://arxiv.org/pdf/2407.05765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06501v2","updated":"2024-07-08T09:14:46Z","published":"2024-03-11T08:17:56Z","title":"SeSame: Simple, Easy 3D Object Detection with Point-Wise Semantics","summary":"  In autonomous driving, 3D object detection provides more precise information\nfor downstream tasks, including path planning and motion estimation, compared\nto 2D object detection. In this paper, we propose SeSame: a method aimed at\nenhancing semantic information in existing LiDAR-only based 3D object\ndetection. This addresses the limitation of existing 3D detectors, which\nprimarily focus on object presence and classification, thus lacking in\ncapturing relationships between elemental units that constitute the data, akin\nto semantic segmentation. Experiments demonstrate the effectiveness of our\nmethod with performance improvements on the KITTI object detection benchmark.\nOur code is available at https://github.com/HAMA-DL-dev/SeSame\n","authors":["Hayeon O","Chanuk Yang","Kunsoo Huh"],"pdf_url":"https://arxiv.org/pdf/2403.06501v2.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.05761v1","updated":"2024-07-08T09:13:30Z","published":"2024-07-08T09:13:30Z","title":"Interpretability of Uncertainty: Exploring Cortical Lesion Segmentation\n  in Multiple Sclerosis","summary":"  Uncertainty quantification (UQ) has become critical for evaluating the\nreliability of artificial intelligence systems, especially in medical image\nsegmentation. This study addresses the interpretability of instance-wise\nuncertainty values in deep learning models for focal lesion segmentation in\nmagnetic resonance imaging, specifically cortical lesion (CL) segmentation in\nmultiple sclerosis. CL segmentation presents several challenges, including the\ncomplexity of manual segmentation, high variability in annotation, data\nscarcity, and class imbalance, all of which contribute to aleatoric and\nepistemic uncertainty. We explore how UQ can be used not only to assess\nprediction reliability but also to provide insights into model behavior, detect\nbiases, and verify the accuracy of UQ methods. Our research demonstrates the\npotential of instance-wise uncertainty values to offer post hoc global model\nexplanations, serving as a sanity check for the model. The implementation is\navailable at https://github.com/NataliiaMolch/interpret-lesion-unc.\n","authors":["Nataliia Molchanova","Alessandro Cagol","Pedro M. Gordaliza","Mario Ocampo-Pineda","Po-Jui Lu","Matthias Weigel","Xinjie Chen","Adrien Depeursinge","Cristina Granziera","Henning MÃ¼ller","Meritxell Bach Cuadra"],"pdf_url":"https://arxiv.org/pdf/2407.05761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05758v1","updated":"2024-07-08T09:08:42Z","published":"2024-07-08T09:08:42Z","title":"Potential of Multimodal Large Language Models for Data Mining of Medical\n  Images and Free-text Reports","summary":"  Medical images and radiology reports are crucial for diagnosing medical\nconditions, highlighting the importance of quantitative analysis for clinical\ndecision-making. However, the diversity and cross-source heterogeneity of these\ndata challenge the generalizability of current data-mining methods. Multimodal\nlarge language models (MLLMs) have recently transformed many domains,\nsignificantly affecting the medical field. Notably, Gemini-Vision-series\n(Gemini) and GPT-4-series (GPT-4) models have epitomized a paradigm shift in\nArtificial General Intelligence (AGI) for computer vision, showcasing their\npotential in the biomedical domain. In this study, we evaluated the performance\nof the Gemini, GPT-4, and 4 popular large models for an exhaustive evaluation\nacross 14 medical imaging datasets, including 5 medical imaging categories\n(dermatology, radiology, dentistry, ophthalmology, and endoscopy), and 3\nradiology report datasets. The investigated tasks encompass disease\nclassification, lesion segmentation, anatomical localization, disease\ndiagnosis, report generation, and lesion detection. Our experimental results\ndemonstrated that Gemini-series models excelled in report generation and lesion\ndetection but faces challenges in disease classification and anatomical\nlocalization. Conversely, GPT-series models exhibited proficiency in lesion\nsegmentation and anatomical localization but encountered difficulties in\ndisease diagnosis and lesion detection. Additionally, both the Gemini series\nand GPT series contain models that have demonstrated commendable generation\nefficiency. While both models hold promise in reducing physician workload,\nalleviating pressure on limited healthcare resources, and fostering\ncollaboration between clinical practitioners and artificial intelligence\ntechnologies, substantial enhancements and comprehensive validations remain\nimperative before clinical deployment.\n","authors":["Yutong Zhang","Yi Pan","Tianyang Zhong","Peixin Dong","Kangni Xie","Yuxiao Liu","Hanqi Jiang","Zhengliang Liu","Shijie Zhao","Tuo Zhang","Xi Jiang","Dinggang Shen","Tianming Liu","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.05758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06407v2","updated":"2024-07-08T08:56:07Z","published":"2024-03-11T03:38:48Z","title":"Can LLMs' Tuning Methods Work in Medical Multimodal Domain?","summary":"  While Large Language Models (LLMs) excel in world knowledge understanding,\nadapting them to specific subfields requires precise adjustments. Due to the\nmodel's vast scale, traditional global fine-tuning methods for large models can\nbe computationally expensive and impact generalization. To address this\nchallenge, a range of innovative Parameters-Efficient Fine-Tuning (PEFT)\nmethods have emerged and achieved remarkable success in both LLMs and Large\nVision-Language Models (LVLMs). In the medical domain, fine-tuning a medical\nVision-Language Pretrained (VLP) model is essential for adapting it to specific\ntasks. Can the fine-tuning methods for large models be transferred to the\nmedical field to enhance transfer learning efficiency? In this paper, we delve\ninto the fine-tuning methods of LLMs and conduct extensive experiments to\ninvestigate the impact of fine-tuning methods for large models on the existing\nmultimodal model in the medical domain from the training data level and the\nmodel structure level. We show the different impacts of fine-tuning methods for\nlarge models on medical VLMs and develop the most efficient ways to fine-tune\nmedical VLP models. We hope this research can guide medical domain researchers\nin optimizing VLMs' training costs, fostering the broader application of VLMs\nin healthcare fields. The code and dataset have been released at\nhttps://github.com/TIMMY-CHAN/MILE.\n","authors":["Jiawei Chen","Yue Jiang","Dingkang Yang","Mingcheng Li","Jinjie Wei","Ziyun Qian","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06407v2.pdf","comment":"Accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.01894v2","updated":"2024-07-08T08:50:00Z","published":"2024-07-02T02:30:23Z","title":"Adaptive Modality Balanced Online Knowledge Distillation for\n  Brain-Eye-Computer based Dim Object Detection","summary":"  Advanced cognition can be extracted from the human brain using brain-computer\ninterfaces. Integrating these interfaces with computer vision techniques, which\npossess efficient feature extraction capabilities, can achieve more robust and\naccurate detection of dim targets in aerial images. However, existing target\ndetection methods primarily concentrate on homogeneous data, lacking efficient\nand versatile processing capabilities for heterogeneous multimodal data. In\nthis paper, we first build a brain-eye-computer based object detection system\nfor aerial images under few-shot conditions. This system detects suspicious\ntargets using region proposal networks, evokes the event-related potential\n(ERP) signal in electroencephalogram (EEG) through the eye-tracking-based slow\nserial visual presentation (ESSVP) paradigm, and constructs the EEG-image data\npairs with eye movement data. Then, an adaptive modality balanced online\nknowledge distillation (AMBOKD) method is proposed to recognize dim objects\nwith the EEG-image data. AMBOKD fuses EEG and image features using a multi-head\nattention module, establishing a new modality with comprehensive features. To\nenhance the performance and robust capability of the fusion modality,\nsimultaneous training and mutual learning between modalities are enabled by\nend-to-end online knowledge distillation. During the learning process, an\nadaptive modality balancing module is proposed to ensure multimodal equilibrium\nby dynamically adjusting the weights of the importance and the training\ngradients across various modalities. The effectiveness and superiority of our\nmethod are demonstrated by comparing it with existing state-of-the-art methods.\nAdditionally, experiments conducted on public datasets and system validations\nin real-world scenarios demonstrate the reliability and practicality of the\nproposed system and the designed method.\n","authors":["Zixing Li","Chao Yan","Zhen Lan","Xiaojia Xiang","Han Zhou","Jun Lai","Dengqing Tang"],"pdf_url":"https://arxiv.org/pdf/2407.01894v2.pdf","comment":"18 pages,15 figures"},{"id":"http://arxiv.org/abs/2407.05736v1","updated":"2024-07-08T08:43:32Z","published":"2024-07-08T08:43:32Z","title":"TransMA: an explainable multi-modal deep learning model for predicting\n  properties of ionizable lipid nanoparticles in mRNA delivery","summary":"  As the primary mRNA delivery vehicles, ionizable lipid nanoparticles (LNPs)\nexhibit excellent safety, high transfection efficiency, and strong immune\nresponse induction. However, the screening process for LNPs is time-consuming\nand costly. To expedite the identification of high-transfection-efficiency mRNA\ndrug delivery systems, we propose an explainable LNPs transfection efficiency\nprediction model, called TransMA. TransMA employs a multi-modal molecular\nstructure fusion architecture, wherein the fine-grained atomic spatial\nrelationship extractor named molecule 3D Transformer captures three-dimensional\nspatial features of the molecule, and the coarse-grained atomic sequence\nextractor named molecule Mamba captures one-dimensional molecular features. We\ndesign the mol-attention mechanism block, enabling it to align coarse and\nfine-grained atomic features and captures relationships between atomic spatial\nand sequential structures. TransMA achieves state-of-the-art performance in\npredicting transfection efficiency using the scaffold and cliff data splitting\nmethods on the current largest LNPs dataset, including Hela and RAW cell lines.\nMoreover, we find that TransMA captures the relationship between subtle\nstructural changes and significant transfection efficiency variations,\nproviding valuable insights for LNPs design. Additionally, TransMA's\npredictions on external transfection efficiency data maintain a consistent\norder with actual transfection efficiencies, demonstrating its robust\ngeneralization capability. The code, model and data are made publicly available\nat https://github.com/wklix/TransMA/tree/master. We hope that high-accuracy\ntransfection prediction models in the future can aid in LNPs design and initial\nscreening, thereby assisting in accelerating the mRNA design process.\n","authors":["Kun Wu","Zixu Wang","Xiulong Yang","Yangyang Chen","Zhenqi Han","Jialu Zhang","Lizhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.05736v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.05735v1","updated":"2024-07-08T08:40:15Z","published":"2024-07-08T08:40:15Z","title":"An Earth Rover dataset recorded at the ICRA@40 party","summary":"  The ICRA conference is celebrating its $40^{th}$ anniversary in Rotterdam in\nSeptember 2024, with as highlight the Happy Birthday ICRA Party at the iconic\nHolland America Line Cruise Terminal. One month later the IROS conference will\ntake place, which will include the Earth Rover Challenge. In this challenge\nopen-world autonomous navigation models are studied truly open-world settings.\n  As part of the Earth Rover Challenge several real-world navigation sets in\nseveral cities world-wide, like Auckland, Australia and Wuhan, China. The only\ndataset recorded in the Netherlands is the small village Oudewater. The\nproposal is to record a dataset with the robot used in the Earth Rover\nChallenge in Rotterdam, in front of the Holland America Line Cruise Terminal,\nbefore the festivities of the Happy Birthday ICRA Party start.\n","authors":["Qi Zhang","Zhihao Lin","Arnoud Visser"],"pdf_url":"https://arxiv.org/pdf/2407.05735v1.pdf","comment":"2 page, submitted as Late-Breaking extended abstract to IEEE\n  Conference on Robotics and Automation"},{"id":"http://arxiv.org/abs/2309.14737v3","updated":"2024-07-08T08:29:08Z","published":"2023-09-26T08:03:10Z","title":"Volumetric Semantically Consistent 3D Panoptic Mapping","summary":"  We introduce an online 2D-to-3D semantic instance mapping algorithm aimed at\ngenerating comprehensive, accurate, and efficient semantic 3D maps suitable for\nautonomous agents in unstructured environments. The proposed approach is based\non a Voxel-TSDF representation used in recent algorithms. It introduces novel\nways of integrating semantic prediction confidence during mapping, producing\nsemantic and instance-consistent 3D regions. Further improvements are achieved\nby graph optimization-based semantic labeling and instance refinement. The\nproposed method achieves accuracy superior to the state of the art on public\nlarge-scale datasets, improving on a number of widely used metrics. We also\nhighlight a downfall in the evaluation of recent studies: using the ground\ntruth trajectory as input instead of a SLAM-estimated one substantially affects\nthe accuracy, creating a large gap between the reported results and the actual\nperformance on real-world data.\n","authors":["Yang Miao","Iro Armeni","Marc Pollefeys","Daniel Barath"],"pdf_url":"https://arxiv.org/pdf/2309.14737v3.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.05726v1","updated":"2024-07-08T08:29:02Z","published":"2024-07-08T08:29:02Z","title":"Gait Patterns as Biomarkers: A Video-Based Approach for Classifying\n  Scoliosis","summary":"  Scoliosis poses significant diagnostic challenges, particularly in\nadolescents, where early detection is crucial for effective treatment.\nTraditional diagnostic and follow-up methods, which rely on physical\nexaminations and radiography, face limitations due to the need for clinical\nexpertise and the risk of radiation exposure, thus restricting their use for\nwidespread early screening. In response, we introduce a novel, video-based,\nnon-invasive method for scoliosis classification using gait analysis, which\ncircumvents these limitations. This study presents Scoliosis1K, the first\nlarge-scale dataset tailored for video-based scoliosis classification,\nencompassing over one thousand adolescents. Leveraging this dataset, we\ndeveloped ScoNet, an initial model that encountered challenges in dealing with\nthe complexities of real-world data. This led to the creation of ScoNet-MT, an\nenhanced model incorporating multi-task learning, which exhibits promising\ndiagnostic accuracy for application purposes. Our findings demonstrate that\ngait can be a non-invasive biomarker for scoliosis, revolutionizing screening\npractices with deep learning and setting a precedent for non-invasive\ndiagnostic methodologies. The dataset and code are publicly available at\nhttps://zhouzi180.github.io/Scoliosis1K/.\n","authors":["Zirui Zhou","Junhao Liang","Zizhao Peng","Chao Fan","Fengwei An","Shiqi Yu"],"pdf_url":"https://arxiv.org/pdf/2407.05726v1.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2401.10461v2","updated":"2024-07-08T08:25:47Z","published":"2024-01-19T03:01:07Z","title":"Learning to Robustly Reconstruct Low-light Dynamic Scenes from Spike\n  Streams","summary":"  As a neuromorphic sensor with high temporal resolution, spike camera can\ngenerate continuous binary spike streams to capture per-pixel light intensity.\nWe can use reconstruction methods to restore scene details in high-speed\nscenarios. However, due to limited information in spike streams, low-light\nscenes are difficult to effectively reconstruct. In this paper, we propose a\nbidirectional recurrent-based reconstruction framework, including a\nLight-Robust Representation (LR-Rep) and a fusion module, to better handle such\nextreme conditions. LR-Rep is designed to aggregate temporal information in\nspike streams, and a fusion module is utilized to extract temporal features.\nAdditionally, we have developed a reconstruction benchmark for high-speed\nlow-light scenes. Light sources in the scenes are carefully aligned to\nreal-world conditions. Experimental results demonstrate the superiority of our\nmethod, which also generalizes well to real spike streams. Related codes and\nproposed datasets will be released after publication.\n","authors":["Liwen Hu","Ziluo Ding","Mianzhi Liu","Lei Ma","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2401.10461v2.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.05713v1","updated":"2024-07-08T08:13:16Z","published":"2024-07-08T08:13:16Z","title":"Short-term Object Interaction Anticipation with Disentangled Object\n  Detection @ Ego4D Short Term Object Interaction Anticipation Challenge","summary":"  Short-term object interaction anticipation is an important task in egocentric\nvideo analysis, including precise predictions of future interactions and their\ntimings as well as the categories and positions of the involved active objects.\nTo alleviate the complexity of this task, our proposed method, SOIA-DOD,\neffectively decompose it into 1) detecting active object and 2) classifying\ninteraction and predicting their timing. Our method first detects all potential\nactive objects in the last frame of egocentric video by fine-tuning a\npre-trained YOLOv9. Then, we combine these potential active objects as query\nwith transformer encoder, thereby identifying the most promising next active\nobject and predicting its future interaction and time-to-contact. Experimental\nresults demonstrate that our method outperforms state-of-the-art models on the\nchallenge test set, achieving the best performance in predicting next active\nobjects and their interactions. Finally, our proposed ranked the third overall\ntop-5 mAP when including time-to-contact predictions. The source code is\navailable at https://github.com/KeenyJin/SOIA-DOD.\n","authors":["Hyunjin Cho","Dong Un Kang","Se Young Chun"],"pdf_url":"https://arxiv.org/pdf/2407.05713v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2407.05712v1","updated":"2024-07-08T08:12:57Z","published":"2024-07-08T08:12:57Z","title":"MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices","summary":"  Existing neural head avatars methods have achieved significant progress in\nthe image quality and motion range of portrait animation. However, these\nmethods neglect the computational overhead, and to the best of our knowledge,\nnone is designed to run on mobile devices. This paper presents MobilePortrait,\na lightweight one-shot neural head avatars method that reduces learning\ncomplexity by integrating external knowledge into both the motion modeling and\nimage synthesis, enabling real-time inference on mobile devices. Specifically,\nwe introduce a mixed representation of explicit and implicit keypoints for\nprecise motion modeling and precomputed visual features for enhanced foreground\nand background synthesis. With these two key designs and using simple U-Nets as\nbackbones, our method achieves state-of-the-art performance with less than\none-tenth the computational demand. It has been validated to reach speeds of\nover 100 FPS on mobile devices and support both video and audio-driven inputs.\n","authors":["Jianwen Jiang","Gaojie Lin","Zhengkun Rong","Chao Liang","Yongming Zhu","Jiaqi Yang","Tianyun Zhong"],"pdf_url":"https://arxiv.org/pdf/2407.05712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05709v1","updated":"2024-07-08T08:10:16Z","published":"2024-07-08T08:10:16Z","title":"Heterogeneous window transformer for image denoising","summary":"  Deep networks can usually depend on extracting more structural information to\nimprove denoising results. However, they may ignore correlation between pixels\nfrom an image to pursue better denoising performance. Window transformer can\nuse long- and short-distance modeling to interact pixels to address mentioned\nproblem. To make a tradeoff between distance modeling and denoising time, we\npropose a heterogeneous window transformer (HWformer) for image denoising.\nHWformer first designs heterogeneous global windows to capture global context\ninformation for improving denoising effects. To build a bridge between long and\nshort-distance modeling, global windows are horizontally and vertically shifted\nto facilitate diversified information without increasing denoising time. To\nprevent the information loss phenomenon of independent patches, sparse idea is\nguided a feed-forward network to extract local information of neighboring\npatches. The proposed HWformer only takes 30% of popular Restormer in terms of\ndenoising time.\n","authors":["Chunwei Tian","Menghua Zheng","Chia-Wen Lin","Zhiwu Li","David Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.05709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00469v2","updated":"2024-07-08T08:07:41Z","published":"2024-03-30T20:25:16Z","title":"SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs","summary":"  We introduce a novel problem, i.e., the localization of an input image within\na multi-modal reference map represented by a database of 3D scene graphs. These\ngraphs comprise multiple modalities, including object-level point clouds,\nimages, attributes, and relationships between objects, offering a lightweight\nand efficient alternative to conventional methods that rely on extensive image\ndatabases. Given the available modalities, the proposed method SceneGraphLoc\nlearns a fixed-sized embedding for each node (i.e., representing an object\ninstance) in the scene graph, enabling effective matching with the objects\nvisible in the input query image. This strategy significantly outperforms other\ncross-modal methods, even without incorporating images into the map embeddings.\nWhen images are leveraged, SceneGraphLoc achieves performance close to that of\nstate-of-the-art techniques depending on large image databases, while requiring\nthree orders-of-magnitude less storage and operating orders-of-magnitude\nfaster. The code will be made public.\n","authors":["Yang Miao","Francis Engelmann","Olga Vysotska","Federico Tombari","Marc Pollefeys","DÃ¡niel BÃ©la BarÃ¡th"],"pdf_url":"https://arxiv.org/pdf/2404.00469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05703v1","updated":"2024-07-08T08:06:06Z","published":"2024-07-08T08:06:06Z","title":"LGRNet: Local-Global Reciprocal Network for Uterine Fibroid Segmentation\n  in Ultrasound Videos","summary":"  Regular screening and early discovery of uterine fibroid are crucial for\npreventing potential malignant transformations and ensuring timely, life-saving\ninterventions. To this end, we collect and annotate the first ultrasound video\ndataset with 100 videos for uterine fibroid segmentation (UFUV). We also\npresent Local-Global Reciprocal Network (LGRNet) to efficiently and effectively\npropagate the long-term temporal context which is crucial to help distinguish\nbetween uninformative noisy surrounding tissues and target lesion regions.\nSpecifically, the Cyclic Neighborhood Propagation (CNP) is introduced to\npropagate the inter-frame local temporal context in a cyclic manner. Moreover,\nto aggregate global temporal context, we first condense each frame into a set\nof frame bottleneck queries and devise Hilbert Selective Scan (HilbertSS) to\nboth efficiently path connect each frame and preserve the locality bias. A\ndistribute layer is then utilized to disseminate back the global context for\nreciprocal refinement. Extensive experiments on UFUV and three public Video\nPolyp Segmentation (VPS) datasets demonstrate consistent improvements compared\nto state-of-the-art segmentation methods, indicating the effectiveness and\nversatility of LGRNet. Code, checkpoints, and dataset are available at\nhttps://github.com/bio-mlhui/LGRNet\n","authors":["Huihui Xu","Yijun Yang","Angelica I Aviles-Rivero","Guang Yang","Jing Qin","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.05703v1.pdf","comment":"MICCAI2024 Early Accept"},{"id":"http://arxiv.org/abs/2402.19007v2","updated":"2024-07-08T07:58:13Z","published":"2024-02-29T10:03:57Z","title":"DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in\n  Dynamic Environments","summary":"  Zero-Shot Object Navigation (ZSON) requires agents to autonomously locate and\napproach unseen objects in unfamiliar environments and has emerged as a\nparticularly challenging task within the domain of Embodied AI. Existing\ndatasets for developing ZSON algorithms lack consideration of dynamic\nobstacles, object attribute diversity, and scene texts, thus exhibiting\nnoticeable discrepancies from real-world situations. To address these issues,\nwe propose a Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic\nEnvironments (DOZE) that comprises ten high-fidelity 3D scenes with over 18k\ntasks, aiming to mimic complex, dynamic real-world scenarios. Specifically,\nDOZE scenes feature multiple moving humanoid obstacles, a wide array of\nopen-vocabulary objects, diverse distinct-attribute objects, and valuable\ntextual hints. Besides, different from existing datasets that only provide\ncollision checking between the agent and static obstacles, we enhance DOZE by\nintegrating capabilities for detecting collisions between the agent and moving\nobstacles. This novel functionality enables the evaluation of the agents'\ncollision avoidance abilities in dynamic environments. We test four\nrepresentative ZSON methods on DOZE, revealing substantial room for improvement\nin existing approaches concerning navigation efficiency, safety, and object\nrecognition accuracy. Our dataset can be found at\nhttps://DOZE-Dataset.github.io/.\n","authors":["Ji Ma","Hongming Dai","Yao Mu","Pengying Wu","Hao Wang","Xiaowei Chi","Yang Fei","Shanghang Zhang","Chang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.19007v2.pdf","comment":"This version of the paper has been accepted for publication in IEEE\n  Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2405.12710v2","updated":"2024-07-08T07:54:24Z","published":"2024-05-21T11:59:36Z","title":"Text-Video Retrieval with Global-Local Semantic Consistent Learning","summary":"  Adapting large-scale image-text pre-training models, e.g., CLIP, to the video\ndomain represents the current state-of-the-art for text-video retrieval. The\nprimary approaches involve transferring text-video pairs to a common embedding\nspace and leveraging cross-modal interactions on specific entities for semantic\nalignment. Though effective, these paradigms entail prohibitive computational\ncosts, leading to inefficient retrieval. To address this, we propose a simple\nyet effective method, Global-Local Semantic Consistent Learning (GLSCL), which\ncapitalizes on latent shared semantics across modalities for text-video\nretrieval. Specifically, we introduce a parameter-free global interaction\nmodule to explore coarse-grained alignment. Then, we devise a shared local\ninteraction module that employs several learnable queries to capture latent\nsemantic concepts for learning fine-grained alignment. Furthermore, an\nInter-Consistency Loss (ICL) is devised to accomplish the concept alignment\nbetween the visual query and corresponding textual query, and an\nIntra-Diversity Loss (IDL) is developed to repulse the distribution within\nvisual (textual) queries to generate more discriminative concepts. Extensive\nexperiments on five widely used benchmarks (i.e., MSR-VTT, MSVD, DiDeMo, LSMDC,\nand ActivityNet) substantiate the superior effectiveness and efficiency of the\nproposed method. Remarkably, our method achieves comparable performance with\nSOTA as well as being nearly 220 times faster in terms of computational cost.\nCode is available at: https://github.com/zchoi/GLSCL.\n","authors":["Haonan Zhang","Pengpeng Zeng","Lianli Gao","Jingkuan Song","Yihang Duan","Xinyu Lyu","Hengtao Shen"],"pdf_url":"https://arxiv.org/pdf/2405.12710v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2407.05688v1","updated":"2024-07-08T07:43:06Z","published":"2024-07-08T07:43:06Z","title":"Learning with Alignments: Tackling the Inter- and Intra-domain Shifts\n  for Cross-multidomain Facial Expression Recognition","summary":"  Facial Expression Recognition (FER) holds significant importance in\nhuman-computer interactions. Existing cross-domain FER methods often transfer\nknowledge solely from a single labeled source domain to an unlabeled target\ndomain, neglecting the comprehensive information across multiple sources.\nNevertheless, cross-multidomain FER (CMFER) is very challenging for (i) the\ninherent inter-domain shifts across multiple domains and (ii) the intra-domain\nshifts stemming from the ambiguous expressions and low inter-class\ndistinctions. In this paper, we propose a novel Learning with Alignments CMFER\nframework, named LA-CMFER, to handle both inter- and intra-domain shifts.\nSpecifically, LA-CMFER is constructed with a global branch and a local branch\nto extract features from the full images and local subtle expressions,\nrespectively. Based on this, LA-CMFER presents a dual-level inter-domain\nalignment method to force the model to prioritize hard-to-align samples in\nknowledge transfer at a sample level while gradually generating a\nwell-clustered feature space with the guidance of class attributes at a cluster\nlevel, thus narrowing the inter-domain shifts. To address the intra-domain\nshifts, LA-CMFER introduces a multi-view intra-domain alignment method with a\nmulti-view clustering consistency constraint where a prediction similarity\nmatrix is built to pursue consistency between the global and local views, thus\nrefining pseudo labels and eliminating latent noise. Extensive experiments on\nsix benchmark datasets have validated the superiority of our LA-CMFER.\n","authors":["Yuxiang Yang","Lu Wen","Xinyi Zeng","Yuanyuan Xu","Xi Wu","Jiliu Zhou","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05687v1","updated":"2024-07-08T07:42:32Z","published":"2024-07-08T07:42:32Z","title":"Learning Lane Graphs from Aerial Imagery Using Transformers","summary":"  The robust and safe operation of automated vehicles underscores the critical\nneed for detailed and accurate topological maps. At the heart of this\nrequirement is the construction of lane graphs, which provide essential\ninformation on lane connectivity, vital for navigating complex urban\nenvironments autonomously. While transformer-based models have been effective\nin creating map topologies from vehicle-mounted sensor data, their potential\nfor generating such graphs from aerial imagery remains untapped. This work\nintroduces a novel approach to generating successor lane graphs from aerial\nimagery, utilizing the advanced capabilities of transformer models. We frame\nsuccessor lane graphs as a collection of maximal length paths and predict them\nusing a Detection Transformer (DETR) architecture. We demonstrate the efficacy\nof our method through extensive experiments on the diverse and large-scale\nUrbanLaneGraph dataset, illustrating its accuracy in generating successor lane\ngraphs and highlighting its potential for enhancing autonomous vehicle\nnavigation in complex environments.\n","authors":["Martin BÃ¼chner","Simon Dorer","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2407.05687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15485v3","updated":"2024-07-08T07:41:50Z","published":"2024-06-17T11:00:04Z","title":"SegHist: A General Segmentation-based Framework for Chinese Historical\n  Document Text Line Detection","summary":"  Text line detection is a key task in historical document analysis facing many\nchallenges of arbitrary-shaped text lines, dense texts, and text lines with\nhigh aspect ratios, etc. In this paper, we propose a general framework for\nhistorical document text detection (SegHist), enabling existing\nsegmentation-based text detection methods to effectively address the\nchallenges, especially text lines with high aspect ratios. Integrating the\nSegHist framework with the commonly used method DB++, we develop DB-SegHist.\nThis approach achieves SOTA on the CHDAC, MTHv2, and competitive results on\nHDRC datasets, with a significant improvement of 1.19% on the most challenging\nCHDAC dataset which features more text lines with high aspect ratios. Moreover,\nour method attains SOTA on rotated MTHv2 and rotated HDRC, demonstrating its\nrotational robustness. The code is available at\nhttps://github.com/LumionHXJ/SegHist.\n","authors":["Xingjian Hu","Baole Wei","Liangcai Gao","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2406.15485v3.pdf","comment":"Accepted by ICDAR2024 (poster)"},{"id":"http://arxiv.org/abs/2312.13631v2","updated":"2024-07-08T07:34:29Z","published":"2023-12-21T07:48:38Z","title":"Diff-Oracle: Deciphering Oracle Bone Scripts with Controllable Diffusion\n  Model","summary":"  Deciphering oracle bone scripts plays an important role in Chinese\narchaeology and philology. However, a significant challenge remains due to the\nscarcity of oracle character images. To overcome this issue, we propose\nDiff-Oracle, a novel approach based on diffusion models to generate a diverse\nrange of controllable oracle characters. Unlike traditional diffusion models\nthat operate primarily on text prompts, Diff-Oracle incorporates a style\nencoder that utilizes style reference images to control the generation style.\nThis encoder extracts style prompts from existing oracle character images,\nwhere style details are converted into a text embedding format via a pretrained\nlanguage-vision model. On the other hand, a content encoder is integrated\nwithin Diff-Oracle to capture specific content details from content reference\nimages, ensuring that the generated characters accurately represent the\nintended glyphs. To effectively train Diff-Oracle, we pre-generate pixel-level\npaired oracle character images (i.e., style and content images) by an\nimage-to-image translation model. Extensive qualitative and quantitative\nexperiments are conducted on datasets Oracle-241 and OBC306. While\nsignificantly surpassing present generative methods in terms of image\ngeneration, Diff-Oracle substantially benefits downstream oracle character\nrecognition, outperforming all existing SOTAs by a large margin. In particular,\non the challenging OBC306 dataset, Diff-Oracle leads to an accuracy gain of\n7.70% in the zero-shot setting and is able to recognize unseen oracle character\nimages with the accuracy of 84.62%, achieving a new benchmark for deciphering\noracle bone scripts.\n","authors":["Jing Li","Qiu-Feng Wang","Siyuan Wang","Rui Zhang","Kaizhu Huang","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2312.13631v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05683v1","updated":"2024-07-08T07:33:52Z","published":"2024-07-08T07:33:52Z","title":"RadiomicsFill-Mammo: Synthetic Mammogram Mass Manipulation with\n  Radiomics Features","summary":"  Motivated by the question, \"Can we generate tumors with desired attributes?''\nthis study leverages radiomics features to explore the feasibility of\ngenerating synthetic tumor images. Characterized by its low-dimensional yet\nbiologically meaningful markers, radiomics bridges the gap between complex\nmedical imaging data and actionable clinical insights. We present\nRadiomicsFill-Mammo, the first of the RadiomicsFill series, an innovative\ntechnique that generates realistic mammogram mass images mirroring specific\nradiomics attributes using masked images and opposite breast images, leveraging\na recent stable diffusion model. This approach also allows for the\nincorporation of essential clinical variables, such as BI-RADS and breast\ndensity, alongside radiomics features as conditions for mass generation.\nResults indicate that RadiomicsFill-Mammo effectively generates diverse and\nrealistic tumor images based on various radiomics conditions. Results also\ndemonstrate a significant improvement in mass detection capabilities,\nleveraging RadiomicsFill-Mammo as a strategy to generate simulated samples.\nFurthermore, RadiomicsFill-Mammo not only advances medical imaging research but\nalso opens new avenues for enhancing treatment planning and tumor simulation.\nOur code is available at https://github.com/nainye/RadiomicsFill.\n","authors":["Inye Na","Jonghun Kim","Eun Sook Ko","Hyunjin Park"],"pdf_url":"https://arxiv.org/pdf/2407.05683v1.pdf","comment":"Accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.05680v1","updated":"2024-07-08T07:28:24Z","published":"2024-07-08T07:28:24Z","title":"Fine-Grained Multi-View Hand Reconstruction Using Inverse Rendering","summary":"  Reconstructing high-fidelity hand models with intricate textures plays a\ncrucial role in enhancing human-object interaction and advancing real-world\napplications. Despite the state-of-the-art methods excelling in texture\ngeneration and image rendering, they often face challenges in accurately\ncapturing geometric details. Learning-based approaches usually offer better\nrobustness and faster inference, which tend to produce smoother results and\nrequire substantial amounts of training data. To address these issues, we\npresent a novel fine-grained multi-view hand mesh reconstruction method that\nleverages inverse rendering to restore hand poses and intricate details.\nFirstly, our approach predicts a parametric hand mesh model through Graph\nConvolutional Networks (GCN) based method from multi-view images. We further\nintroduce a novel Hand Albedo and Mesh (HAM) optimization module to refine both\nthe hand mesh and textures, which is capable of preserving the mesh topology.\nIn addition, we suggest an effective mesh-based neural rendering scheme to\nsimultaneously generate photo-realistic image and optimize mesh geometry by\nfusing the pre-trained rendering network with vertex features. We conduct the\ncomprehensive experiments on InterHand2.6M, DeepHandMesh and dataset collected\nby ourself, whose promising results show that our proposed approach outperforms\nthe state-of-the-art methods on both reconstruction accuracy and rendering\nquality. Code and dataset are publicly available at\nhttps://github.com/agnJason/FMHR.\n","authors":["Qijun Gan","Wentong Li","Jinwei Ren","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.05680v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2407.05679v1","updated":"2024-07-08T07:26:08Z","published":"2024-07-08T07:26:08Z","title":"BEVWorld: A Multimodal World Model for Autonomous Driving via Unified\n  BEV Latent Space","summary":"  World models are receiving increasing attention in autonomous driving for\ntheir ability to predict potential future scenarios. In this paper, we present\nBEVWorld, a novel approach that tokenizes multimodal sensor inputs into a\nunified and compact Bird's Eye View (BEV) latent space for environment\nmodeling. The world model consists of two parts: the multi-modal tokenizer and\nthe latent BEV sequence diffusion model. The multi-modal tokenizer first\nencodes multi-modality information and the decoder is able to reconstruct the\nlatent BEV tokens into LiDAR and image observations by ray-casting rendering in\na self-supervised manner. Then the latent BEV sequence diffusion model predicts\nfuture scenarios given action tokens as conditions. Experiments demonstrate the\neffectiveness of BEVWorld in autonomous driving tasks, showcasing its\ncapability in generating future scenes and benefiting downstream tasks such as\nperception and motion prediction. Code will be available at\nhttps://github.com/zympsyche/BevWorld.\n","authors":["Yumeng Zhang","Shi Gong","Kaixin Xiong","Xiaoqing Ye","Xiao Tan","Fan Wang","Jizhou Huang","Hua Wu","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05679v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.05671v1","updated":"2024-07-08T07:10:17Z","published":"2024-07-08T07:10:17Z","title":"MSTF: Multiscale Transformer for Incomplete Trajectory Prediction","summary":"  Motion forecasting plays a pivotal role in autonomous driving systems,\nenabling vehicles to execute collision warnings and rational local-path\nplanning based on predictions of the surrounding vehicles. However, prevalent\nmethods often assume complete observed trajectories, neglecting the potential\nimpact of missing values induced by object occlusion, scope limitation, and\nsensor failures. Such oversights inevitably compromise the accuracy of\ntrajectory predictions. To tackle this challenge, we propose an end-to-end\nframework, termed Multiscale Transformer (MSTF), meticulously crafted for\nincomplete trajectory prediction. MSTF integrates a Multiscale Attention Head\n(MAH) and an Information Increment-based Pattern Adaptive (IIPA) module.\nSpecifically, the MAH component concurrently captures multiscale motion\nrepresentation of trajectory sequence from various temporal granularities,\nutilizing a multi-head attention mechanism. This approach facilitates the\nmodeling of global dependencies in motion across different scales, thereby\nmitigating the adverse effects of missing values. Additionally, the IIPA module\nadaptively extracts continuity representation of motion across time steps by\nanalyzing missing patterns in the data. The continuity representation\ndelineates motion trend at a higher level, guiding MSTF to generate predictions\nconsistent with motion continuity. We evaluate our proposed MSTF model using\ntwo large-scale real-world datasets. Experimental results demonstrate that MSTF\nsurpasses state-of-the-art (SOTA) models in the task of incomplete trajectory\nprediction, showcasing its efficacy in addressing the challenges posed by\nmissing values in motion forecasting for autonomous driving systems.\n","authors":["Zhanwen Liu","Chao Li","Nan Yang","Yang Wang","Jiaqi Ma","Guangliang Cheng","Xiangmo Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.05671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05666v1","updated":"2024-07-08T07:03:22Z","published":"2024-07-08T07:03:22Z","title":"Enhancing Neural Radiance Fields with Depth and Normal Completion Priors\n  from Sparse Views","summary":"  Neural Radiance Fields (NeRF) are an advanced technology that creates highly\nrealistic images by learning about scenes through a neural network model.\nHowever, NeRF often encounters issues when there are not enough images to work\nwith, leading to problems in accurately rendering views. The main issue is that\nNeRF lacks sufficient structural details to guide the rendering process\naccurately. To address this, we proposed a Depth and Normal Dense Completion\nPriors for NeRF (CP\\_NeRF) framework. This framework enhances view rendering by\nadding depth and normal dense completion priors to the NeRF optimization\nprocess. Before optimizing NeRF, we obtain sparse depth maps using the\nStructure from Motion (SfM) technique used to get camera poses. Based on the\nsparse depth maps and a normal estimator, we generate sparse normal maps for\ntraining a normal completion prior with precise standard deviations. During\noptimization, we apply depth and normal completion priors to transform sparse\ndata into dense depth and normal maps with their standard deviations. We use\nthese dense maps to guide ray sampling, assist distance sampling and construct\na normal loss function for better training accuracy. To improve the rendering\nof NeRF's normal outputs, we incorporate an optical centre position embedder\nthat helps synthesize more accurate normals through volume rendering.\nAdditionally, we employ a normal patch matching technique to choose accurate\nrendered normal maps, ensuring more precise supervision for the model. Our\nmethod is superior to leading techniques in rendering detailed indoor scenes,\neven with limited input views.\n","authors":["Jiawei Guo","HungChyun Chou","Ning Ding"],"pdf_url":"https://arxiv.org/pdf/2407.05666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03492v4","updated":"2024-07-08T07:03:12Z","published":"2023-06-06T08:19:30Z","title":"Industrial Anomaly Detection and Localization Using Weakly-Supervised\n  Residual Transformers","summary":"  Recent advancements in industrial Anomaly Detection (AD) have shown that\nincorporating a few anomalous samples during training can significantly boost\naccuracy. However, this performance improvement comes at a high cost: extensive\nannotation efforts, which are often impractical in real-world applications. In\nthis work, we propose a novel framework called ``Weakly-supervised RESidual\nTransformer`` (WeakREST), which aims to achieve high AD accuracy while\nminimizing the need for extensive annotations. First, we reformulate the\npixel-wise anomaly localization task into a block-wise classification problem.\nBy shifting the focus to block-wise level, we can drastically reduce the amount\nof required annotations without compromising on the accuracy of anomaly\ndetection Secondly, we design a residual-based transformer model, termed\n``Positional Fast Anomaly Residuals`` (PosFAR), to classify the image blocks in\nreal time. We further propose to label the anomalous regions using only\nbounding boxes or image tags as weaker labels, leading to a semi-supervised\nlearning setting. On the benchmark dataset MVTec-AD, our proposed WeakREST\nframework achieves a remarkable Average Precision (AP) of 83.0%, significantly\noutperforming the previous best result of 75.8% in the unsupervised setting. In\nthe supervised AD setting, WeakREST further improves performance, attaining an\nAP of 87.6% compared to the previous best of 78.6%. Notably, even when\nutilizing weaker labels based on bounding boxes, WeakREST surpasses recent\nleading methods that rely on pixel-wise supervision, achieving an AP of 87.1%\nagainst the prior best of 78.6% on MVTec-AD. This precision advantage is also\nconsistently observed on other well-known AD datasets, such as BTAD and KSDD2.\n","authors":["Hanxi Li","Jingqi Wu","Lin Yuanbo Wu","Hao Chen","Mingwen Wang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2306.03492v4.pdf","comment":"14 pages,7 figures"},{"id":"http://arxiv.org/abs/2407.05657v1","updated":"2024-07-08T06:32:44Z","published":"2024-07-08T06:32:44Z","title":"DMSD-CDFSAR: Distillation from Mixed-Source Domain for Cross-Domain\n  Few-shot Action Recognition","summary":"  Few-shot action recognition is an emerging field in computer vision,\nprimarily focused on meta-learning within the same domain. However, challenges\narise in real-world scenario deployment, as gathering extensive labeled data\nwithin a specific domain is laborious and time-intensive. Thus, attention\nshifts towards cross-domain few-shot action recognition, requiring the model to\ngeneralize across domains with significant deviations. Therefore, we propose a\nnovel approach, ``Distillation from Mixed-Source Domain\", tailored to address\nthis conundrum. Our method strategically integrates insights from both labeled\ndata of the source domain and unlabeled data of the target domain during the\ntraining. The ResNet18 is used as the backbone to extract spatial features from\nthe source and target domains. We design two branches for meta-training: the\noriginal-source and the mixed-source branches. In the first branch, a Domain\nTemporal Encoder is employed to capture temporal features for both the source\nand target domains. Additionally, a Domain Temporal Decoder is employed to\nreconstruct all extracted features. In the other branch, a Domain Mixed Encoder\nis used to handle labeled source domain data and unlabeled target domain data,\ngenerating mixed-source domain features. We incorporate a pre-training stage\nbefore meta-training, featuring a network architecture similar to that of the\nfirst branch. Lastly, we introduce a dual distillation mechanism to refine the\nclassification probabilities of source domain features, aligning them with\nthose of mixed-source domain features. This iterative process enriches the\ninsights of the original-source branch with knowledge from the mixed-source\nbranch, thereby enhancing the model's generalization capabilities. Our code is\navailable at URL: \\url{https://xxxx/xxxx/xxxx.git}\n","authors":["Fei Guo","YiKang Wang","Han Qi","Li Zhu","Jing Sun"],"pdf_url":"https://arxiv.org/pdf/2407.05657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02595v2","updated":"2024-07-08T06:30:01Z","published":"2024-05-04T07:39:25Z","title":"Vision-based 3D occupancy prediction in autonomous driving: a review and\n  outlook","summary":"  In recent years, autonomous driving has garnered escalating attention for its\npotential to relieve drivers' burdens and improve driving safety. Vision-based\n3D occupancy prediction, which predicts the spatial occupancy status and\nsemantics of 3D voxel grids around the autonomous vehicle from image inputs, is\nan emerging perception task suitable for cost-effective perception system of\nautonomous driving. Although numerous studies have demonstrated the greater\nadvantages of 3D occupancy prediction over object-centric perception tasks,\nthere is still a lack of a dedicated review focusing on this rapidly developing\nfield. In this paper, we first introduce the background of vision-based 3D\noccupancy prediction and discuss the challenges in this task. Secondly, we\nconduct a comprehensive survey of the progress in vision-based 3D occupancy\nprediction from three aspects: feature enhancement, deployment friendliness and\nlabel efficiency, and provide an in-depth analysis of the potentials and\nchallenges of each category of methods. Finally, we present a summary of\nprevailing research trends and propose some inspiring future outlooks. To\nprovide a valuable reference for researchers, a regularly updated collection of\nrelated papers, datasets, and codes is organized at\nhttps://github.com/zya3d/Awesome-3D-Occupancy-Prediction.\n","authors":["Yanan Zhang","Jinqing Zhang","Zengran Wang","Junhao Xu","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2405.02595v2.pdf","comment":"20 pages, 20 figures"},{"id":"http://arxiv.org/abs/2407.05650v1","updated":"2024-07-08T06:22:10Z","published":"2024-07-08T06:22:10Z","title":"The Dynamic Net Architecture: Learning Robust and Holistic Visual\n  Representations Through Self-Organizing Networks","summary":"  We present a novel intelligent-system architecture called \"Dynamic Net\nArchitecture\" (DNA) that relies on recurrence-stabilized networks and discuss\nit in application to vision. Our architecture models a (cerebral cortical) area\nwherein elementary feature neurons encode details of visual structures, and\ncoherent nets of such neurons model holistic object structures. By interpreting\nsmaller or larger coherent pieces of an area network as complex features, our\nmodel encodes hierarchical feature representations essentially different than\nartificial neural networks (ANNs).\n  DNA models operate on a dynamic connectionism principle, wherein neural\nactivations stemming from initial afferent signals undergo stabilization\nthrough a self-organizing mechanism facilitated by Hebbian plasticity alongside\nperiodically tightening inhibition. In contrast to ANNs, which rely on\nfeed-forward connections and backpropagation of error, we posit that this\nprocessing paradigm leads to highly robust representations, as by employing\ndynamic lateral connections, irrelevant details in neural activations are\nfiltered out, freeing further processing steps from distracting noise and\npremature decisions.\n  We empirically demonstrate the viability of the DNA by composing line\nfragments into longer lines and show that the construction of nets representing\nlines remains robust even with the introduction of up to $59\\%$ noise at each\nspatial location. Furthermore, we demonstrate the model's capability to\nreconstruct anticipated features from partially obscured inputs and that it can\ngeneralize to patterns not observed during training. In this work, we limit the\nDNA to one cortical area and focus on its internals while providing insights\ninto a standalone area's strengths and shortcomings. Additionally, we provide\nan outlook on how future work can implement invariant object recognition by\ncombining multiple areas.\n","authors":["Pascal J. Sager","Jan M. Deriu","Benjamin F. Grewe","Thilo Stadelmann","Christoph von der Malsburg"],"pdf_url":"https://arxiv.org/pdf/2407.05650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05647v1","updated":"2024-07-08T06:18:04Z","published":"2024-07-08T06:18:04Z","title":"Learning to Adapt Category Consistent Meta-Feature of CLIP for Few-Shot\n  Classification","summary":"  The recent CLIP-based methods have shown promising zero-shot and few-shot\nperformance on image classification tasks. Existing approaches such as CoOp and\nTip-Adapter only focus on high-level visual features that are fully aligned\nwith textual features representing the ``Summary\" of the image. However, the\ngoal of few-shot learning is to classify unseen images of the same category\nwith few labeled samples. Especially, in contrast to high-level\nrepresentations, local representations (LRs) at low-level are more consistent\nbetween seen and unseen samples. Based on this point, we propose the\nMeta-Feature Adaption method (MF-Adapter) that combines the complementary\nstrengths of both LRs and high-level semantic representations. Specifically, we\nintroduce the Meta-Feature Unit (MF-Unit), which is a simple yet effective\nlocal similarity metric to measure category-consistent local context in an\ninductive manner. Then we train an MF-Adapter to map image features to MF-Unit\nfor adequately generalizing the intra-class knowledge between unseen images and\nthe support set. Extensive experiments show that our proposed method is\nsuperior to the state-of-the-art CLIP downstream few-shot classification\nmethods, even showing stronger performance on a set of challenging visual\nclassification tasks.\n","authors":["Jiaying Shi","Xuetong Xue","Shenghui Xu"],"pdf_url":"https://arxiv.org/pdf/2407.05647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05645v1","updated":"2024-07-08T06:14:37Z","published":"2024-07-08T06:14:37Z","title":"OneDiff: A Generalist Model for Image Difference","summary":"  In computer vision, Image Difference Captioning (IDC) is crucial for\naccurately describing variations between closely related images. Traditional\nIDC methods often rely on specialist models, which restrict their applicability\nacross varied contexts. This paper introduces the OneDiff model, a novel\ngeneralist approach that utilizes a robust vision-language model architecture,\nintegrating a siamese image encoder with a Visual Delta Module. This innovative\nconfiguration allows for the precise detection and articulation of fine-grained\ndifferences between image pairs. OneDiff is trained through a dual-phase\nstrategy, encompassing Coupled Sample Training and multi-task learning across a\ndiverse array of data types, supported by our newly developed DiffCap Dataset.\nThis dataset merges real-world and synthetic data, enhancing the training\nprocess and bolstering the model's robustness. Extensive testing on diverse IDC\nbenchmarks, such as Spot-the-Diff, CLEVR-Change, and Birds-to-Words, shows that\nOneDiff consistently outperforms existing state-of-the-art models in accuracy\nand adaptability, achieving improvements of up to 85\\% CIDEr points in average.\nBy setting a new benchmark in IDC, OneDiff paves the way for more versatile and\neffective applications in detecting and describing visual differences. The\ncode, models, and data will be made publicly available.\n","authors":["Erdong Hu","Longteng Guo","Tongtian Yue","Zijia Zhao","Shuning Xue","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2407.05645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18459v2","updated":"2024-07-08T06:06:53Z","published":"2024-06-26T16:10:31Z","title":"DiffuseHigh: Training-free Progressive High-Resolution Image Synthesis\n  through Structure Guidance","summary":"  Recent surge in large-scale generative models has spurred the development of\nvast fields in computer vision. In particular, text-to-image diffusion models\nhave garnered widespread adoption across diverse domain due to their potential\nfor high-fidelity image generation. Nonetheless, existing large-scale diffusion\nmodels are confined to generate images of up to 1K resolution, which is far\nfrom meeting the demands of contemporary commercial applications. Directly\nsampling higher-resolution images often yields results marred by artifacts such\nas object repetition and distorted shapes. Addressing the aforementioned issues\ntypically necessitates training or fine-tuning models on higher resolution\ndatasets. However, this undertaking poses a formidable challenge due to the\ndifficulty in collecting large-scale high-resolution contents and substantial\ncomputational resources. While several preceding works have proposed\nalternatives, they often fail to produce convincing results. In this work, we\nprobe the generative ability of diffusion models at higher resolution beyond\nits original capability and propose a novel progressive approach that fully\nutilizes generated low-resolution image to guide the generation of higher\nresolution image. Our method obviates the need for additional training or\nfine-tuning which significantly lowers the burden of computational costs.\nExtensive experiments and results validate the efficiency and efficacy of our\nmethod. Project page: https://yhyun225.github.io/DiffusHigh/\n","authors":["Younghyun Kim","Geunmin Hwang","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2406.18459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05638v1","updated":"2024-07-08T06:05:19Z","published":"2024-07-08T06:05:19Z","title":"HPFF: Hierarchical Locally Supervised Learning with Patch Feature Fusion","summary":"  Traditional deep learning relies on end-to-end backpropagation for training,\nbut it suffers from drawbacks such as high memory consumption and not aligning\nwith biological neural networks. Recent advancements have introduced locally\nsupervised learning, which divides networks into modules with isolated\ngradients and trains them locally. However, this approach can lead to\nperformance lag due to limited interaction between these modules, and the\ndesign of auxiliary networks occupies a certain amount of GPU memory. To\novercome these limitations, we propose a novel model called HPFF that performs\nhierarchical locally supervised learning and patch-level feature computation on\nthe auxiliary networks. Hierarchical Locally Supervised Learning (HiLo) enables\nthe network to learn features at different granularity levels along their\nrespective local paths. Specifically, the network is divided into two-level\nlocal modules: independent local modules and cascade local modules. The cascade\nlocal modules combine two adjacent independent local modules, incorporating\nboth updates within the modules themselves and information exchange between\nadjacent modules. Patch Feature Fusion (PFF) reduces GPU memory usage by\nsplitting the input features of the auxiliary networks into patches for\ncomputation. By averaging these patch-level features, it enhances the network's\nability to focus more on those patterns that are prevalent across multiple\npatches. Furthermore, our method exhibits strong generalization capabilities\nand can be seamlessly integrated with existing techniques. We conduct\nexperiments on CIFAR-10, STL-10, SVHN, and ImageNet datasets, and the results\ndemonstrate that our proposed HPFF significantly outperforms previous\napproaches, consistently achieving state-of-the-art performance across\ndifferent datasets. Our code is available at:\n\\url{https://github.com/Zeudfish/HPFF}.\n","authors":["Junhao Su","Chenghao He","Feiyu Zhu","Xiaojie Xu","Dongzhi Guan","Chenyang Si"],"pdf_url":"https://arxiv.org/pdf/2407.05638v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2207.02466v5","updated":"2024-07-08T06:03:19Z","published":"2022-07-06T06:26:17Z","title":"GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty\n  Estimation","summary":"  The inherent ambiguity in ground-truth annotations of 3D bounding boxes,\ncaused by occlusions, signal missing, or manual annotation errors, can confuse\ndeep 3D object detectors during training, thus deteriorating detection\naccuracy. However, existing methods overlook such issues to some extent and\ntreat the labels as deterministic. In this paper, we formulate the label\nuncertainty problem as the diversity of potentially plausible bounding boxes of\nobjects. Then, we propose GLENet, a generative framework adapted from\nconditional variational autoencoders, to model the one-to-many relationship\nbetween a typical 3D object and its potential ground-truth bounding boxes with\nlatent variables. The label uncertainty generated by GLENet is a plug-and-play\nmodule and can be conveniently integrated into existing deep 3D detectors to\nbuild probabilistic detectors and supervise the learning of the localization\nuncertainty. Besides, we propose an uncertainty-aware quality estimator\narchitecture in probabilistic detectors to guide the training of the IoU-branch\nwith predicted localization uncertainty. We incorporate the proposed methods\ninto various popular base 3D detectors and demonstrate significant and\nconsistent performance gains on both KITTI and Waymo benchmark datasets.\nEspecially, the proposed GLENet-VR outperforms all published LiDAR-based\napproaches by a large margin and achieves the top rank among single-modal\nmethods on the challenging KITTI test set. The source code and pre-trained\nmodels are publicly available at \\url{https://github.com/Eaphan/GLENet}.\n","authors":["Yifan Zhang","Qijian Zhang","Zhiyu Zhu","Junhui Hou","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2207.02466v5.pdf","comment":"Accepted to IJCV 2023"},{"id":"http://arxiv.org/abs/2311.17893v2","updated":"2024-07-08T05:33:12Z","published":"2023-11-29T18:47:17Z","title":"Betrayed by Attention: A Simple yet Effective Approach for\n  Self-supervised Video Object Segmentation","summary":"  In this paper, we propose a simple yet effective approach for self-supervised\nvideo object segmentation (VOS). Our key insight is that the inherent\nstructural dependencies present in DINO-pretrained Transformers can be\nleveraged to establish robust spatio-temporal correspondences in videos.\nFurthermore, simple clustering on this correspondence cue is sufficient to\nyield competitive segmentation results. Previous self-supervised VOS techniques\nmajorly resort to auxiliary modalities or utilize iterative slot attention to\nassist in object discovery, which restricts their general applicability and\nimposes higher computational requirements. To deal with these challenges, we\ndevelop a simplified architecture that capitalizes on the emerging objectness\nfrom DINO-pretrained Transformers, bypassing the need for additional modalities\nor slot attention. Specifically, we first introduce a single spatio-temporal\nTransformer block to process the frame-wise DINO features and establish\nspatio-temporal dependencies in the form of self-attention. Subsequently,\nutilizing these attention maps, we implement hierarchical clustering to\ngenerate object segmentation masks. To train the spatio-temporal block in a\nfully self-supervised manner, we employ semantic and dynamic motion consistency\ncoupled with entropy normalization. Our method demonstrates state-of-the-art\nperformance across multiple unsupervised VOS benchmarks and particularly excels\nin complex real-world multi-object video segmentation tasks such as\nDAVIS-17-Unsupervised and YouTube-VIS-19. The code and model checkpoints will\nbe released at https://github.com/shvdiwnkozbw/SSL-UVOS.\n","authors":["Shuangrui Ding","Rui Qian","Haohang Xu","Dahua Lin","Hongkai Xiong"],"pdf_url":"https://arxiv.org/pdf/2311.17893v2.pdf","comment":"ECCV 2024 Camera Ready"},{"id":"http://arxiv.org/abs/2407.05623v1","updated":"2024-07-08T05:31:51Z","published":"2024-07-08T05:31:51Z","title":"Momentum Auxiliary Network for Supervised Local Learning","summary":"  Deep neural networks conventionally employ end-to-end backpropagation for\ntheir training process, which lacks biological credibility and triggers a\nlocking dilemma during network parameter updates, leading to significant GPU\nmemory use. Supervised local learning, which segments the network into multiple\nlocal blocks updated by independent auxiliary networks. However, these methods\ncannot replace end-to-end training due to lower accuracy, as gradients only\npropagate within their local block, creating a lack of information exchange\nbetween blocks. To address this issue and establish information transfer across\nblocks, we propose a Momentum Auxiliary Network (MAN) that establishes a\ndynamic interaction mechanism. The MAN leverages an exponential moving average\n(EMA) of the parameters from adjacent local blocks to enhance information flow.\nThis auxiliary network, updated through EMA, helps bridge the informational gap\nbetween blocks. Nevertheless, we observe that directly applying EMA parameters\nhas certain limitations due to feature discrepancies among local blocks. To\novercome this, we introduce learnable biases, further boosting performance. We\nhave validated our method on four image classification datasets (CIFAR-10,\nSTL-10, SVHN, ImageNet), attaining superior performance and substantial memory\nsavings. Notably, our method can reduce GPU memory usage by more than 45\\% on\nthe ImageNet dataset compared to end-to-end training, while achieving higher\nperformance. The Momentum Auxiliary Network thus offers a new perspective for\nsupervised local learning. Our code is available at:\n\\url{https://github.com/JunhaoSu0/MAN}.\n","authors":["Junhao Su","Changpeng Cai","Feiyu Zhu","Chenghao He","Xiaojie Xu","Dongzhi Guan","Chenyang Si"],"pdf_url":"https://arxiv.org/pdf/2407.05623v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.03146v2","updated":"2024-07-08T05:21:59Z","published":"2024-05-31T02:56:43Z","title":"Enhancing Class Fairness in Classification with A Two-Player Game\n  Approach","summary":"  Data augmentation is widely applied and has shown its benefits in different\nmachine learning tasks. However, as recently observed in some downstream tasks,\ndata augmentation may introduce an unfair impact on classifications. While it\ncan improve the performance of some classes, it can actually be detrimental for\nother classes, which can be problematic in some application domains. In this\npaper, to counteract this phenomenon, we propose a FAir Classification approach\nwith a Two-player game (FACT). We first formulate the training of a classifier\nwith data augmentation as a fair optimization problem, which can be further\nwritten as an adversarial two-player game. Following this formulation, we\npropose a novel multiplicative weight optimization algorithm, for which we\ntheoretically prove that it can converge to a solution that is fair over\nclasses. Interestingly, our formulation also reveals that this fairness issue\nover classes is not due to data augmentation only, but is in fact a general\nphenomenon. Our empirical experiments demonstrate that the performance of our\nlearned classifiers is indeed more fairly distributed over classes in five\ndatasets, with only limited impact on the average accuracy.\n","authors":["Yunpeng Jiang","Paul Weng","Yutong Ban"],"pdf_url":"https://arxiv.org/pdf/2407.03146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04116v2","updated":"2024-07-08T05:11:38Z","published":"2024-03-07T00:12:08Z","title":"Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis","summary":"  X-ray is widely applied for transmission imaging due to its stronger\npenetration than natural light. When rendering novel view X-ray projections,\nexisting methods mainly based on NeRF suffer from long training time and slow\ninference speed. In this paper, we propose a 3D Gaussian splatting-based\nframework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we\nredesign a radiative Gaussian point cloud model inspired by the isotropic\nnature of X-ray imaging. Our model excludes the influence of view direction\nwhen learning to predict the radiation intensity of 3D points. Based on this\nmodel, we develop a Differentiable Radiative Rasterization (DRR) with CUDA\nimplementation. Secondly, we customize an Angle-pose Cuboid Uniform\nInitialization (ACUI) strategy that directly uses the parameters of the X-ray\nscanner to compute the camera information and then uniformly samples point\npositions within a cuboid enclosing the scanned object. Experiments show that\nour X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying\nless than 15% training time and over 73x inference speed. The application on\nsparse-view CT reconstruction also reveals the practical values of our method.\nCode is publicly available at https://github.com/caiyuanhao1998/X-Gaussian . A\nvideo demo of the training process visualization is at\nhttps://www.youtube.com/watch?v=gDVf_Ngeghg .\n","authors":["Yuanhao Cai","Yixun Liang","Jiahao Wang","Angtian Wang","Yulun Zhang","Xiaokang Yang","Zongwei Zhou","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2403.04116v2.pdf","comment":"ECCV 2024; The first 3D Gaussian Splatting-based method for X-ray 3D\n  reconstruction"},{"id":"http://arxiv.org/abs/2407.05616v1","updated":"2024-07-08T05:05:43Z","published":"2024-07-08T05:05:43Z","title":"Explainable Image Recognition via Enhanced Slot-attention Based\n  Classifier","summary":"  The imperative to comprehend the behaviors of deep learning models is of\nutmost importance. In this realm, Explainable Artificial Intelligence (XAI) has\nemerged as a promising avenue, garnering increasing interest in recent years.\nDespite this, most existing methods primarily depend on gradients or input\nperturbation, which often fails to embed explanations directly within the\nmodel's decision-making process. Addressing this gap, we introduce ESCOUTER, a\nvisually explainable classifier based on the modified slot attention mechanism.\nESCOUTER distinguishes itself by not only delivering high classification\naccuracy but also offering more transparent insights into the reasoning behind\nits decisions. It differs from prior approaches in two significant aspects: (a)\nESCOUTER incorporates explanations into the final confidence scores for each\ncategory, providing a more intuitive interpretation, and (b) it offers positive\nor negative explanations for all categories, elucidating \"why an image belongs\nto a certain category\" or \"why it does not.\" A novel loss function specifically\nfor ESCOUTER is designed to fine-tune the model's behavior, enabling it to\ntoggle between positive and negative explanations. Moreover, an area loss is\nalso designed to adjust the size of the explanatory regions for a more precise\nexplanation. Our method, rigorously tested across various datasets and XAI\nmetrics, outperformed previous state-of-the-art methods, solidifying its\neffectiveness as an explanatory tool.\n","authors":["Bowen Wang","Liangzhi Li","Jiahao Zhang","Yuta Nakashima","Hajime Nagahara"],"pdf_url":"https://arxiv.org/pdf/2407.05616v1.pdf","comment":"16 pages, 12 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.06178v1","updated":"2024-07-08T17:52:23Z","published":"2024-07-08T17:52:23Z","title":"Transfer Learning with Self-Supervised Vision Transformers for Snake\n  Identification","summary":"  We present our approach for the SnakeCLEF 2024 competition to predict snake\nspecies from images. We explore and use Meta's DINOv2 vision transformer model\nfor feature extraction to tackle species' high variability and visual\nsimilarity in a dataset of 182,261 images. We perform exploratory analysis on\nembeddings to understand their structure, and train a linear classifier on the\nembeddings to predict species. Despite achieving a score of 39.69, our results\nshow promise for DINOv2 embeddings in snake identification. All code for this\nproject is available at https://github.com/dsgt-kaggle-clef/snakeclef-2024.\n","authors":["Anthony Miyaguchi","Murilo Gustineli","Austin Fischer","Ryan Lundqvist"],"pdf_url":"https://arxiv.org/pdf/2407.06178v1.pdf","comment":"Paper submitted to CLEF 2024 CEUR-WS"},{"id":"http://arxiv.org/abs/2407.06060v1","updated":"2024-07-08T16:01:04Z","published":"2024-07-08T16:01:04Z","title":"MERGE -- A Bimodal Dataset for Static Music Emotion Recognition","summary":"  The Music Emotion Recognition (MER) field has seen steady developments in\nrecent years, with contributions from feature engineering, machine learning,\nand deep learning. The landscape has also shifted from audio-centric systems to\nbimodal ensembles that combine audio and lyrics. However, a severe lack of\npublic and sizeable bimodal databases has hampered the development and\nimprovement of bimodal audio-lyrics systems. This article proposes three new\naudio, lyrics, and bimodal MER research datasets, collectively called MERGE,\ncreated using a semi-automatic approach. To comprehensively assess the proposed\ndatasets and establish a baseline for benchmarking, we conducted several\nexperiments for each modality, using feature engineering, machine learning, and\ndeep learning methodologies. In addition, we propose and validate fixed\ntrain-validate-test splits. The obtained results confirm the viability of the\nproposed datasets, achieving the best overall result of 79.21% F1-score for\nbimodal classification using a deep neural network.\n","authors":["Pedro Lima Louro","Hugo Redinho","Ricardo Santos","Ricardo Malheiro","Renato Panda","Rui Pedro Paiva"],"pdf_url":"https://arxiv.org/pdf/2407.06060v1.pdf","comment":"16 pages, 4 figures, 13 tables, submitted to IEEE Transactions on\n  Affective Computing"},{"id":"http://arxiv.org/abs/2303.11666v2","updated":"2024-07-08T15:55:13Z","published":"2023-03-21T08:20:43Z","title":"A Survey on Causal Inference for Recommendation","summary":"  Causal inference has recently garnered significant interest among recommender\nsystem (RS) researchers due to its ability to dissect cause-and-effect\nrelationships and its broad applicability across multiple fields. It offers a\nframework to model the causality in recommender systems like confounding\neffects and deal with counterfactual problems such as offline policy evaluation\nand data augmentation. Although there are already some valuable surveys on\ncausal recommendations, they typically classify approaches based on the\npractical issues faced in RS, a classification that may disperse and fragment\nthe unified causal theories. Considering RS researchers' unfamiliarity with\ncausality, it is necessary yet challenging to comprehensively review relevant\nstudies from a coherent causal theoretical perspective, thereby facilitating a\ndeeper integration of causal inference in RS. This survey provides a systematic\nreview of up-to-date papers in this area from a causal theory standpoint and\ntraces the evolutionary development of RS methods within the same causal\nstrategy. Firstly, we introduce the fundamental concepts of causal inference as\nthe basis of the following review. Subsequently, we propose a novel\ntheory-driven taxonomy, categorizing existing methods based on the causal\ntheory employed - namely, those based on the potential outcome framework, the\nstructural causal model, and general counterfactuals. The review then delves\ninto the technical details of how existing methods apply causal inference to\naddress particular recommender issues. Finally, we highlight some promising\ndirections for future research in this field. Representative papers and\nopen-source resources will be progressively available at\nhttps://github.com/Chrissie-Law/Causal-Inference-for-Recommendation.\n","authors":["Huishi Luo","Fuzhen Zhuang","Ruobing Xie","Hengshu Zhu","Deqing Wang","Zhulin An","Yongjun Xu"],"pdf_url":"https://arxiv.org/pdf/2303.11666v2.pdf","comment":"Accepted by The Innovation"},{"id":"http://arxiv.org/abs/2407.04472v2","updated":"2024-07-08T14:50:49Z","published":"2024-07-05T12:42:31Z","title":"EventChat: Implementation and user-centric evaluation of a large\n  language model-driven conversational recommender system for exploring leisure\n  events in an SME context","summary":"  Large language models (LLMs) present an enormous evolution in the strategic\npotential of conversational recommender systems (CRS). Yet to date, research\nhas predominantly focused upon technical frameworks to implement LLM-driven\nCRS, rather than end-user evaluations or strategic implications for firms,\nparticularly from the perspective of a small to medium enterprises (SME) that\nmakeup the bedrock of the global economy. In the current paper, we detail the\ndesign of an LLM-driven CRS in an SME setting, and its subsequent performance\nin the field using both objective system metrics and subjective user\nevaluations. While doing so, we additionally outline a short-form revised\nResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly\nevolving field. Our results reveal good system performance from a user\nexperience perspective (85.5% recommendation accuracy) but underscore latency,\ncost, and quality issues challenging business viability. Notably, with a median\ncost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and\nresponse time emerge as crucial areas for achieving a more user-friendly and\neconomically viable LLM-driven CRS for SME settings. One major driver of these\ncosts is the use of an advanced LLM as a ranker within the retrieval-augmented\ngeneration (RAG) technique. Our results additionally indicate that relying\nsolely on approaches such as Prompt-based learning with ChatGPT as the\nunderlying LLM makes it challenging to achieve satisfying quality in a\nproduction environment. Strategic considerations for SMEs deploying an\nLLM-driven CRS are outlined, particularly considering trade-offs in the current\ntechnical landscape.\n","authors":["Hannes Kunstmann","Joseph Ollier","Joel Persson","Florian von Wangenheim"],"pdf_url":"https://arxiv.org/pdf/2407.04472v2.pdf","comment":"27 pages, 3 tables, 5 figures, pre-print manuscript, updated version\n  of manuscript due to typo (previous version, Figure 5 was incorrectly named\n  Figure 6)"},{"id":"http://arxiv.org/abs/2406.18114v2","updated":"2024-07-08T12:02:19Z","published":"2024-06-26T07:02:49Z","title":"Knowledge Graph Enhanced Retrieval-Augmented Generation for Failure Mode\n  and Effects Analysis","summary":"  Failure mode and effects analysis (FMEA) is a critical tool for mitigating\npotential failures, particular during ramp-up phases of new products. However,\nits effectiveness is often limited by the missing reasoning capabilities of the\nFMEA tools, which are usually tabular structured. Meanwhile, large language\nmodels (LLMs) offer novel prospects for fine-tuning on custom datasets for\nreasoning within FMEA contexts. However, LLMs face challenges in tasks that\nrequire factual knowledge, a gap that retrieval-augmented generation (RAG)\napproaches aim to fill. RAG retrieves information from a non-parametric data\nstore and uses a language model to generate responses. Building on this idea,\nwe propose to advance the non-parametric data store with a knowledge graph\n(KG). By enhancing the RAG framework with a KG, our objective is to leverage\nanalytical and semantic question-answering capabilities on FMEA data. This\npaper contributes by presenting a new ontology for FMEA observations, an\nalgorithm for creating vector embeddings from the FMEA KG, and a KG enhanced\nRAG framework. Our approach is validated through a human study and we measure\nthe performance of the context retrieval recall and precision.\n","authors":["Lukas Bahr","Christoph Wehner","Judith Wewerka","JosÃ© Bittencourt","Ute Schmid","RÃ¼diger Daub"],"pdf_url":"https://arxiv.org/pdf/2406.18114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05836v1","updated":"2024-07-08T11:36:17Z","published":"2024-07-08T11:36:17Z","title":"Academic Article Recommendation Using Multiple Perspectives","summary":"  We argue that Content-based filtering (CBF) and Graph-based methods (GB)\ncomplement one another in Academic Search recommendations. The scientific\nliterature can be viewed as a conversation between authors and the audience.\nCBF uses abstracts to infer authors' positions, and GB uses citations to infer\nresponses from the audience. In this paper, we describe nine differences\nbetween CBF and GB, as well as synergistic opportunities for hybrid\ncombinations. Two embeddings will be used to illustrate these opportunities:\n(1) Specter, a CBF method based on BERT-like deepnet encodings of abstracts,\nand (2) ProNE, a GB method based on spectral clustering of more than 200M\npapers and 2B citations from Semantic Scholar.\n","authors":["Kenneth Church","Omar Alonso","Peter Vickers","Jiameng Sun","Abteen Ebrahimi","Raman Chandrasekar"],"pdf_url":"https://arxiv.org/pdf/2407.05836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10251v2","updated":"2024-07-08T08:52:56Z","published":"2024-06-10T08:23:52Z","title":"The Impact of Quantization on Retrieval-Augmented Generation: An\n  Analysis of Small LLMs","summary":"  Post-training quantization reduces the computational demand of Large Language\nModels (LLMs) but can weaken some of their capabilities. Since LLM abilities\nemerge with scale, smaller LLMs are more sensitive to quantization. In this\npaper, we explore how quantization affects smaller LLMs' ability to perform\nretrieval-augmented generation (RAG), specifically in longer contexts. We chose\npersonalization for evaluation because it is a challenging domain to perform\nusing RAG as it requires long-context reasoning over multiple documents. We\ncompare the original FP16 and the quantized INT4 performance of multiple 7B and\n8B LLMs on two tasks while progressively increasing the number of retrieved\ndocuments to test how quantized models fare against longer contexts. To better\nunderstand the effect of retrieval, we evaluate three retrieval models in our\nexperiments. Our findings reveal that if a 7B LLM performs the task well,\nquantization does not impair its performance and long-context reasoning\ncapabilities. We conclude that it is possible to utilize RAG with quantized\nsmaller LLMs.\n","authors":["Mert Yazan","Suzan Verberne","Frederik Situmeang"],"pdf_url":"https://arxiv.org/pdf/2406.10251v2.pdf","comment":"Accepted to the IR-RAG Workshop at SIGIR 2024"},{"id":"http://arxiv.org/abs/2407.05627v1","updated":"2024-07-08T05:42:29Z","published":"2024-07-08T05:42:29Z","title":"New Directions in Text Classification Research: Maximizing The\n  Performance of Sentiment Classification from Limited Data","summary":"  The stakeholders' needs in sentiment analysis for various issues, whether\npositive or negative, are speed and accuracy. One new challenge in sentiment\nanalysis tasks is the limited training data, which often leads to suboptimal\nmachine learning models and poor performance on test data. This paper discusses\nthe problem of text classification based on limited training data (300 to 600\nsamples) into three classes: positive, negative, and neutral. A benchmark\ndataset is provided for training and testing data on the issue of Kaesang\nPangarep's appointment as Chairman of PSI. External data for aggregation and\naugmentation purposes are provided, consisting of two datasets: the topic of\nCovid Vaccination sentiment and an open topic. The official score used is the\nF1-score, which balances precision and recall among the three classes,\npositive, negative, and neutral. A baseline score is provided as a reference\nfor researchers for unoptimized classification methods. The optimized score is\nprovided as a reference for the target score to be achieved by any proposed\nmethod. Both scoring (baseline and optimized) use the SVM method, which is\nwidely reported as the state-of-the-art in conventional machine learning\nmethods. The F1-scores achieved by the baseline and optimized methods are\n40.83% and 51.28%, respectively.\n","authors":["Surya Agustian","Muhammad Irfan Syah","Nurul Fatiara","Rahmad Abdillah"],"pdf_url":"https://arxiv.org/pdf/2407.05627v1.pdf","comment":"9 pages, in Indonesian language. intro to a shared task in sentiment\n  classification"},{"id":"http://arxiv.org/abs/2407.06298v1","updated":"2024-07-08T18:07:33Z","published":"2024-07-08T18:07:33Z","title":"Multi-Label Plant Species Classification with Self-Supervised Vision\n  Transformers","summary":"  We present a transfer learning approach using a self-supervised Vision\nTransformer (DINOv2) for the PlantCLEF 2024 competition, focusing on the\nmulti-label plant species classification. Our method leverages both base and\nfine-tuned DINOv2 models to extract generalized feature embeddings. We train\nclassifiers to predict multiple plant species within a single image using these\nrich embeddings. To address the computational challenges of the large-scale\ndataset, we employ Spark for distributed data processing, ensuring efficient\nmemory management and processing across a cluster of workers. Our data\nprocessing pipeline transforms images into grids of tiles, classifying each\ntile, and aggregating these predictions into a consolidated set of\nprobabilities. Our results demonstrate the efficacy of combining transfer\nlearning with advanced data processing techniques for multi-label image\nclassification tasks. Our code is available at\nhttps://github.com/dsgt-kaggle-clef/plantclef-2024.\n","authors":["Murilo Gustineli","Anthony Miyaguchi","Ian Stalter"],"pdf_url":"https://arxiv.org/pdf/2407.06298v1.pdf","comment":"Paper submitted to CLEF 2024 CEUR-WS"},{"id":"http://arxiv.org/abs/2407.07923v1","updated":"2024-07-08T15:24:58Z","published":"2024-07-08T15:24:58Z","title":"New Method for Keyword Extraction for Patent Claims","summary":"  The search for prior art is crucial in patent application processing, it\nconsists in retrieving other documents relevant to the invention of the\napplication. Most methods feed a search engine with keywords that are extracted\nby frequency-analysis methods. We suggest and demonstrate a new method that\nrelies on the way information is provided in patent claims.\n","authors":["Julien Rossi"],"pdf_url":"https://arxiv.org/pdf/2407.07923v1.pdf","comment":"Master's thesis"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.06190v1","updated":"2024-07-08T17:59:54Z","published":"2024-07-08T17:59:54Z","title":"4D Contrastive Superflows are Dense 3D Representation Learners","summary":"  In the realm of autonomous driving, accurate 3D perception is the foundation.\nHowever, developing such models relies on extensive human annotations -- a\nprocess that is both costly and labor-intensive. To address this challenge from\na data representation learning perspective, we introduce SuperFlow, a novel\nframework designed to harness consecutive LiDAR-camera pairs for establishing\nspatiotemporal pretraining objectives. SuperFlow stands out by integrating two\nkey designs: 1) a dense-to-sparse consistency regularization, which promotes\ninsensitivity to point cloud density variations during feature learning, and 2)\na flow-based contrastive learning module, carefully crafted to extract\nmeaningful temporal cues from readily available sensor calibrations. To further\nboost learning efficiency, we incorporate a plug-and-play view consistency\nmodule that enhances the alignment of the knowledge distilled from camera\nviews. Extensive comparative and ablation studies across 11 heterogeneous LiDAR\ndatasets validate our effectiveness and superiority. Additionally, we observe\nseveral interesting emerging properties by scaling up the 2D and 3D backbones\nduring pretraining, shedding light on the future research of 3D foundation\nmodels for LiDAR-based perception.\n","authors":["Xiang Xu","Lingdong Kong","Hui Shuai","Wenwei Zhang","Liang Pan","Kai Chen","Ziwei Liu","Qingshan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.06190v1.pdf","comment":"ECCV 2024; 36 pages, 11 figures, 11 tables; Code at\n  https://github.com/Xiangxu-0103/SuperFlow"},{"id":"http://arxiv.org/abs/2407.06183v1","updated":"2024-07-08T17:56:00Z","published":"2024-07-08T17:56:00Z","title":"Stepping on the Edge: Curvature Aware Learning Rate Tuners","summary":"  Curvature information -- particularly, the largest eigenvalue of the loss\nHessian, known as the sharpness -- often forms the basis for learning rate\ntuners. However, recent work has shown that the curvature information undergoes\ncomplex dynamics during training, going from a phase of increasing sharpness to\neventual stabilization. We analyze the closed-loop feedback effect between\nlearning rate tuning and curvature. We find that classical learning rate tuners\nmay yield greater one-step loss reduction, yet they ultimately underperform in\nthe long term when compared to constant learning rates in the full batch\nregime. These models break the stabilization of the sharpness, which we explain\nusing a simplified model of the joint dynamics of the learning rate and the\ncurvature. To further investigate these effects, we introduce a new learning\nrate tuning method, Curvature Dynamics Aware Tuning (CDAT), which prioritizes\nlong term curvature stabilization over instantaneous progress on the objective.\nIn the full batch regime, CDAT shows behavior akin to prefixed warm-up\nschedules on deep learning objectives, outperforming tuned constant learning\nrates. In the mini batch regime, we observe that stochasticity introduces\nconfounding effects that explain the previous success of some learning rate\ntuners at appropriate batch sizes. Our findings highlight the critical role of\nunderstanding the joint dynamics of the learning rate and curvature, beyond\ngreedy minimization, to diagnose failures and design effective adaptive\nlearning rate tuners.\n","authors":["Vincent Roulet","Atish Agarwala","Jean-Bastien Grill","Grzegorz Swirszcz","Mathieu Blondel","Fabian Pedregosa"],"pdf_url":"https://arxiv.org/pdf/2407.06183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14734v2","updated":"2024-07-08T17:55:24Z","published":"2024-05-23T16:01:46Z","title":"SimPO: Simple Preference Optimization with a Reference-Free Reward","summary":"  Direct Preference Optimization (DPO) is a widely used offline preference\noptimization algorithm that reparameterizes reward functions in reinforcement\nlearning from human feedback (RLHF) to enhance simplicity and training\nstability. In this work, we propose SimPO, a simpler yet more effective\napproach. The effectiveness of SimPO is attributed to a key design: using the\naverage log probability of a sequence as the implicit reward. This reward\nformulation better aligns with model generation and eliminates the need for a\nreference model, making it more compute and memory efficient. Additionally, we\nintroduce a target reward margin to the Bradley-Terry objective to encourage a\nlarger margin between the winning and losing responses, further enhancing the\nalgorithm's performance. We compare SimPO to DPO and its latest variants across\nvarious state-of-the-art training setups, including both base and\ninstruction-tuned models like Mistral and Llama3. We evaluated on extensive\ninstruction-following benchmarks, including AlpacaEval 2, MT-Bench, and the\nrecent challenging Arena-Hard benchmark. Our results demonstrate that SimPO\nconsistently and significantly outperforms existing approaches without\nsubstantially increasing response length. Specifically, SimPO outperforms DPO\nby up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our\ntop-performing model, built on Llama3-8B-Instruct, achieves a remarkable 53.7\nlength-controlled win rate on AlpacaEval 2 -- surpassing Claude 3 Opus on the\nleaderboard, and a 36.5 win rate on Arena-Hard -- making it the strongest 8B\nopen-source model.\n","authors":["Yu Meng","Mengzhou Xia","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14734v2.pdf","comment":"Code: https://github.com/princeton-nlp/SimPO. v2 updates: additional\n  baselines (RRHF, SLiC-HF, CPO); a new setting Llama3-Instruct-v0.2 (Appendix\n  G); more analyses (Section 4.4 & Appendix H)"},{"id":"http://arxiv.org/abs/2407.06178v1","updated":"2024-07-08T17:52:23Z","published":"2024-07-08T17:52:23Z","title":"Transfer Learning with Self-Supervised Vision Transformers for Snake\n  Identification","summary":"  We present our approach for the SnakeCLEF 2024 competition to predict snake\nspecies from images. We explore and use Meta's DINOv2 vision transformer model\nfor feature extraction to tackle species' high variability and visual\nsimilarity in a dataset of 182,261 images. We perform exploratory analysis on\nembeddings to understand their structure, and train a linear classifier on the\nembeddings to predict species. Despite achieving a score of 39.69, our results\nshow promise for DINOv2 embeddings in snake identification. All code for this\nproject is available at https://github.com/dsgt-kaggle-clef/snakeclef-2024.\n","authors":["Anthony Miyaguchi","Murilo Gustineli","Austin Fischer","Ryan Lundqvist"],"pdf_url":"https://arxiv.org/pdf/2407.06178v1.pdf","comment":"Paper submitted to CLEF 2024 CEUR-WS"},{"id":"http://arxiv.org/abs/2405.08698v2","updated":"2024-07-08T17:48:43Z","published":"2024-05-14T15:37:56Z","title":"Byzantine-Resilient Secure Aggregation for Federated Learning Without\n  Privacy Compromises","summary":"  Federated learning (FL) shows great promise in large scale machine learning,\nbut brings new risks in terms of privacy and security. We propose ByITFL, a\nnovel scheme for FL that provides resilience against Byzantine users while\nkeeping the users' data private from the federator and private from other\nusers. The scheme builds on the preexisting non-private FLTrust scheme, which\ntolerates malicious users through trust scores (TS) that attenuate or amplify\nthe users' gradients. The trust scores are based on the ReLU function, which we\napproximate by a polynomial. The distributed and privacy-preserving computation\nin ByITFL is designed using a combination of Lagrange coded computing,\nverifiable secret sharing and re-randomization steps. ByITFL is the first\nByzantine resilient scheme for FL with full information-theoretic privacy.\n","authors":["Yue Xia","Christoph Hofmeister","Maximilian Egger","Rawad Bitar"],"pdf_url":"https://arxiv.org/pdf/2405.08698v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06169v1","updated":"2024-07-08T17:48:39Z","published":"2024-07-08T17:48:39Z","title":"Potential Based Diffusion Motion Planning","summary":"  Effective motion planning in high dimensional spaces is a long-standing open\nproblem in robotics. One class of traditional motion planning algorithms\ncorresponds to potential-based motion planning. An advantage of potential based\nmotion planning is composability -- different motion constraints can be easily\ncombined by adding corresponding potentials. However, constructing motion paths\nfrom potentials requires solving a global optimization across configuration\nspace potential landscape, which is often prone to local minima. We propose a\nnew approach towards learning potential based motion planning, where we train a\nneural network to capture and learn an easily optimizable potentials over\nmotion planning trajectories. We illustrate the effectiveness of such approach,\nsignificantly outperforming both classical and recent learned motion planning\napproaches and avoiding issues with local minima. We further illustrate its\ninherent composability, enabling us to generalize to a multitude of different\nmotion constraints.\n","authors":["Yunhao Luo","Chen Sun","Joshua B. Tenenbaum","Yilun Du"],"pdf_url":"https://arxiv.org/pdf/2407.06169v1.pdf","comment":"ICML 2024. Project page and code at\n  https://energy-based-model.github.io/potential-motion-plan/"},{"id":"http://arxiv.org/abs/2407.06167v1","updated":"2024-07-08T17:45:40Z","published":"2024-07-08T17:45:40Z","title":"DÎµpS: Delayed Îµ-Shrinking for Faster Once-For-All\n  Training","summary":"  CNNs are increasingly deployed across different hardware, dynamic\nenvironments, and low-power embedded devices. This has led to the design and\ntraining of CNN architectures with the goal of maximizing accuracy subject to\nsuch variable deployment constraints. As the number of deployment scenarios\ngrows, there is a need to find scalable solutions to design and train\nspecialized CNNs. Once-for-all training has emerged as a scalable approach that\njointly co-trains many models (subnets) at once with a constant training cost\nand finds specialized CNNs later. The scalability is achieved by training the\nfull model and simultaneously reducing it to smaller subnets that share model\nweights (weight-shared shrinking). However, existing once-for-all training\napproaches incur huge training costs reaching 1200 GPU hours. We argue this is\nbecause they either start the process of shrinking the full model too early or\ntoo late. Hence, we propose Delayed $\\epsilon$-Shrinking (D$\\epsilon$pS) that\nstarts the process of shrinking the full model when it is partially trained\n(~50%) which leads to training cost improvement and better in-place knowledge\ndistillation to smaller models. The proposed approach also consists of novel\nheuristics that dynamically adjust subnet learning rates incrementally (E),\nleading to improved weight-shared knowledge distillation from larger to smaller\nsubnets as well. As a result, DEpS outperforms state-of-the-art once-for-all\ntraining techniques across different datasets including CIFAR10/100,\nImageNet-100, and ImageNet-1k on accuracy and cost. It achieves 1.83% higher\nImageNet-1k top1 accuracy or the same accuracy with 1.3x reduction in FLOPs and\n2.5x drop in training cost (GPU*hrs)\n","authors":["Aditya Annavajjala","Alind Khare","Animesh Agrawal","Igor Fedorov","Hugo Latapie","Myungjin Lee","Alexey Tumanov"],"pdf_url":"https://arxiv.org/pdf/2407.06167v1.pdf","comment":"Accepted to the 18th European Conference on Computer Vision (ECCV\n  2024)"},{"id":"http://arxiv.org/abs/2406.04313v3","updated":"2024-07-08T17:42:41Z","published":"2024-06-06T17:57:04Z","title":"Improving Alignment and Robustness with Circuit Breakers","summary":"  AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that interrupts the models as they respond with harmful outputs\nwith \"circuit breakers.\" Existing techniques aimed at improving alignment, such\nas refusal training, are often bypassed. Techniques such as adversarial\ntraining try to plug these holes by countering specific attacks. As an\nalternative to refusal training and adversarial training, circuit-breaking\ndirectly controls the representations that are responsible for harmful outputs\nin the first place. Our technique can be applied to both text-only and\nmultimodal language models to prevent the generation of harmful outputs without\nsacrificing utility -- even in the presence of powerful unseen attacks.\nNotably, while adversarial robustness in standalone image recognition remains\nan open challenge, circuit breakers allow the larger multimodal system to\nreliably withstand image \"hijacks\" that aim to produce harmful content.\nFinally, we extend our approach to AI agents, demonstrating considerable\nreductions in the rate of harmful actions when they are under attack. Our\napproach represents a significant step forward in the development of reliable\nsafeguards to harmful behavior and adversarial attacks.\n","authors":["Andy Zou","Long Phan","Justin Wang","Derek Duenas","Maxwell Lin","Maksym Andriushchenko","Rowan Wang","Zico Kolter","Matt Fredrikson","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2406.04313v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17119v2","updated":"2024-07-08T17:23:22Z","published":"2024-06-24T20:13:23Z","title":"Accelerating Phase Field Simulations Through a Hybrid Adaptive Fourier\n  Neural Operator with U-Net Backbone","summary":"  Prolonged contact between a corrosive liquid and metal alloys can cause\nprogressive dealloying. For such liquid-metal dealloying (LMD) process, phase\nfield models have been developed. However, the governing equations often\ninvolve coupled non-linear partial differential equations (PDE), which are\nchallenging to solve numerically. In particular, stiffness in the PDEs requires\nan extremely small time steps (e.g. $10^{-12}$ or smaller). This computational\nbottleneck is especially problematic when running LMD simulation until a late\ntime horizon is required. This motivates the development of surrogate models\ncapable of leaping forward in time, by skipping several consecutive time steps\nat-once. In this paper, we propose U-Shaped Adaptive Fourier Neural Operators\n(U-AFNO), a machine learning (ML) model inspired by recent advances in neural\noperator learning. U-AFNO employs U-Nets for extracting and reconstructing\nlocal features within the physical fields, and passes the latent space through\na vision transformer (ViT) implemented in the Fourier space (AFNO). We use\nU-AFNOs to learn the dynamics mapping the field at a current time step into a\nlater time step. We also identify global quantities of interest (QoI)\ndescribing the corrosion process (e.g. the deformation of the liquid-metal\ninterface) and show that our proposed U-AFNO model is able to accurately\npredict the field dynamics, in-spite of the chaotic nature of LMD. Our model\nreproduces the key micro-structure statistics and QoIs with a level of accuracy\non-par with the high-fidelity numerical solver. We also investigate the\nopportunity of using hybrid simulations, in which we alternate forward leap in\ntime using the U-AFNO with high-fidelity time stepping. We demonstrate that\nwhile advantageous for some surrogate model design choices, our proposed U-AFNO\nmodel in fully auto-regressive settings consistently outperforms hybrid\nschemes.\n","authors":["Christophe Bonneville","Nathan Bieberdorf","Arun Hegde","Mark Asta","Habib N. Najm","Laurent Capolungo","Cosmin Safta"],"pdf_url":"https://arxiv.org/pdf/2406.17119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05968v2","updated":"2024-07-08T17:20:19Z","published":"2024-05-09T17:59:55Z","title":"A Universal Growth Rate for Learning with Smooth Surrogate Losses","summary":"  This paper presents a comprehensive analysis of the growth rate of\n$H$-consistency bounds (and excess error bounds) for various surrogate losses\nused in classification. We prove a square-root growth rate near zero for smooth\nmargin-based surrogate losses in binary classification, providing both upper\nand lower bounds under mild assumptions. This result also translates to excess\nerror bounds. Our lower bound requires weaker conditions than those in previous\nwork for excess error bounds, and our upper bound is entirely novel. Moreover,\nwe extend this analysis to multi-class classification with a series of novel\nresults, demonstrating a universal square-root growth rate for smooth comp-sum\nand constrained losses, covering common choices for training neural networks in\nmulti-class classification. Given this universal rate, we turn to the question\nof choosing among different surrogate losses. We first examine how\n$H$-consistency bounds vary across surrogates based on the number of classes.\nNext, ignoring constants and focusing on behavior near zero, we identify\nminimizability gaps as the key differentiating factor in these bounds. Thus, we\nthoroughly analyze these gaps, to guide surrogate loss selection, covering:\ncomparisons across different comp-sum losses, conditions where gaps become\nzero, and general conditions leading to small gaps. Additionally, we\ndemonstrate the key role of minimizability gaps in comparing excess error\nbounds and $H$-consistency bounds.\n","authors":["Anqi Mao","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2405.05968v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06124v1","updated":"2024-07-08T17:00:28Z","published":"2024-07-08T17:00:28Z","title":"Structured Generations: Using Hierarchical Clusters to guide Diffusion\n  Models","summary":"  This paper introduces Diffuse-TreeVAE, a deep generative model that\nintegrates hierarchical clustering into the framework of Denoising Diffusion\nProbabilistic Models (DDPMs). The proposed approach generates new images by\nsampling from a root embedding of a learned latent tree VAE-based structure, it\nthen propagates through hierarchical paths, and utilizes a second-stage DDPM to\nrefine and generate distinct, high-quality images for each data cluster. The\nresult is a model that not only improves image clarity but also ensures that\nthe generated samples are representative of their respective clusters,\naddressing the limitations of previous VAE-based methods and advancing the\nstate of clustering-based generative modeling.\n","authors":["Jorge da Silva Goncalves","Laura Manduchi","Moritz Vandenhirtz","Julia Vogt"],"pdf_url":"https://arxiv.org/pdf/2407.06124v1.pdf","comment":"8 pages, 7 figures, Structured Probabilistic Inference & Generative\n  Modeling workshop of ICML 2024"},{"id":"http://arxiv.org/abs/2307.13885v6","updated":"2024-07-08T17:00:16Z","published":"2023-07-26T01:10:29Z","title":"Characterizing Data Point Vulnerability via Average-Case Robustness","summary":"  Studying the robustness of machine learning models is important to ensure\nconsistent model behaviour across real-world settings. To this end, adversarial\nrobustness is a standard framework, which views robustness of predictions\nthrough a binary lens: either a worst-case adversarial misclassification exists\nin the local region around an input, or it does not. However, this binary\nperspective does not account for the degrees of vulnerability, as data points\nwith a larger number of misclassified examples in their neighborhoods are more\nvulnerable. In this work, we consider a complementary framework for robustness,\ncalled average-case robustness, which measures the fraction of points in a\nlocal region that provides consistent predictions. However, computing this\nquantity is hard, as standard Monte Carlo approaches are inefficient especially\nfor high-dimensional inputs. In this work, we propose the first analytical\nestimators for average-case robustness for multi-class classifiers. We show\nempirically that our estimators are accurate and efficient for standard deep\nlearning models and demonstrate their usefulness for identifying vulnerable\ndata points, as well as quantifying robustness bias of models. Overall, our\ntools provide a complementary view to robustness, improving our ability to\ncharacterize model behaviour.\n","authors":["Tessa Han","Suraj Srinivas","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2307.13885v6.pdf","comment":"UAI 2024"},{"id":"http://arxiv.org/abs/2405.16642v2","updated":"2024-07-08T17:00:07Z","published":"2024-05-26T17:38:44Z","title":"Fast TRAC: A Parameter-Free Optimizer for Lifelong Reinforcement\n  Learning","summary":"  A key challenge in lifelong reinforcement learning (RL) is the loss of\nplasticity, where previous learning progress hinders an agent's adaptation to\nnew tasks. While regularization and resetting can help, they require precise\nhyperparameter selection at the outset and environment-dependent adjustments.\nBuilding on the principled theory of online convex optimization, we present a\nparameter-free optimizer for lifelong RL, called TRAC, which requires no tuning\nor prior knowledge about the distribution shifts. Extensive experiments on\nProcgen, Atari, and Gym Control environments show that TRAC works surprisingly\nwell-mitigating loss of plasticity and rapidly adapting to challenging\ndistribution shifts-despite the underlying optimization problem being nonconvex\nand nonstationary.\n","authors":["Aneesh Muppidi","Zhiyu Zhang","Heng Yang"],"pdf_url":"https://arxiv.org/pdf/2405.16642v2.pdf","comment":"Code and Website:\n  https://computationalrobotics.seas.harvard.edu/TRAC/"},{"id":"http://arxiv.org/abs/2407.01512v2","updated":"2024-07-08T16:59:38Z","published":"2024-07-01T17:55:35Z","title":"Open-TeleVision: Teleoperation with Immersive Active Visual Feedback","summary":"  Teleoperation serves as a powerful method for collecting on-robot data\nessential for robot learning from demonstrations. The intuitiveness and ease of\nuse of the teleoperation system are crucial for ensuring high-quality, diverse,\nand scalable data. To achieve this, we propose an immersive teleoperation\nsystem Open-TeleVision that allows operators to actively perceive the robot's\nsurroundings in a stereoscopic manner. Additionally, the system mirrors the\noperator's arm and hand movements on the robot, creating an immersive\nexperience as if the operator's mind is transmitted to a robot embodiment. We\nvalidate the effectiveness of our system by collecting data and training\nimitation learning policies on four long-horizon, precise tasks (Can Sorting,\nCan Insertion, Folding, and Unloading) for 2 different humanoid robots and\ndeploy them in the real world. The system is open-sourced at:\nhttps://robot-tv.github.io/\n","authors":["Xuxin Cheng","Jialong Li","Shiqi Yang","Ge Yang","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.01512v2.pdf","comment":"Website: https://robot-tv.github.io/"},{"id":"http://arxiv.org/abs/2407.06121v1","updated":"2024-07-08T16:58:57Z","published":"2024-07-08T16:58:57Z","title":"Periodic agent-state based Q-learning for POMDPs","summary":"  The standard approach for Partially Observable Markov Decision Processes\n(POMDPs) is to convert them to a fully observed belief-state MDP. However, the\nbelief state depends on the system model and is therefore not viable in\nreinforcement learning (RL) settings. A widely used alternative is to use an\nagent state, which is a model-free, recursively updateable function of the\nobservation history. Examples include frame stacking and recurrent neural\nnetworks. Since the agent state is model-free, it is used to adapt standard RL\nalgorithms to POMDPs. However, standard RL algorithms like Q-learning learn a\nstationary policy. Our main thesis that we illustrate via examples is that\nbecause the agent state does not satisfy the Markov property, non-stationary\nagent-state based policies can outperform stationary ones. To leverage this\nfeature, we propose PASQL (periodic agent-state based Q-learning), which is a\nvariant of agent-state-based Q-learning that learns periodic policies. By\ncombining ideas from periodic Markov chains and stochastic approximation, we\nrigorously establish that PASQL converges to a cyclic limit and characterize\nthe approximation error of the converged periodic policy. Finally, we present a\nnumerical experiment to highlight the salient features of PASQL and demonstrate\nthe benefit of learning periodic policies over stationary policies.\n","authors":["Amit Sinha","Mathieu Geist","Aditya Mahajan"],"pdf_url":"https://arxiv.org/pdf/2407.06121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06120v1","updated":"2024-07-08T16:57:26Z","published":"2024-07-08T16:57:26Z","title":"Sketchy Moment Matching: Toward Fast and Provable Data Selection for\n  Finetuning","summary":"  We revisit data selection in a modern context of finetuning from a\nfundamental perspective. Extending the classical wisdom of variance\nminimization in low dimensions to high-dimensional finetuning, our\ngeneralization analysis unveils the importance of additionally reducing bias\ninduced by low-rank approximation. Inspired by the variance-bias tradeoff in\nhigh dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a\nscalable data selection scheme with two stages. (i) First, the bias is\ncontrolled using gradient sketching that explores the finetuning parameter\nspace for an informative low-dimensional subspace $\\mathcal{S}$; (ii) then the\nvariance is reduced over $\\mathcal{S}$ via moment matching between the original\nand selected datasets. Theoretically, we show that gradient sketching is fast\nand provably accurate: selecting $n$ samples by reducing variance over\n$\\mathcal{S}$ preserves the fast-rate generalization $O(\\dim(\\mathcal{S})/n)$,\nindependent of the parameter dimension. Empirically, we concretize the\nvariance-bias balance via synthetic experiments and demonstrate the\neffectiveness of SkMM for finetuning in real vision tasks.\n","authors":["Yijun Dong","Hoang Phan","Xiang Pan","Qi Lei"],"pdf_url":"https://arxiv.org/pdf/2407.06120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02539v2","updated":"2024-07-08T16:50:48Z","published":"2024-07-02T00:44:06Z","title":"Research on Autonomous Robots Navigation based on Reinforcement Learning","summary":"  Reinforcement learning continuously optimizes decision-making based on\nreal-time feedback reward signals through continuous interaction with the\nenvironment, demonstrating strong adaptive and self-learning capabilities. In\nrecent years, it has become one of the key methods to achieve autonomous\nnavigation of robots. In this work, an autonomous robot navigation method based\non reinforcement learning is introduced. We use the Deep Q Network (DQN) and\nProximal Policy Optimization (PPO) models to optimize the path planning and\ndecision-making process through the continuous interaction between the robot\nand the environment, and the reward signals with real-time feedback. By\ncombining the Q-value function with the deep neural network, deep Q network can\nhandle high-dimensional state space, so as to realize path planning in complex\nenvironments. Proximal policy optimization is a strategy gradient-based method,\nwhich enables robots to explore and utilize environmental information more\nefficiently by optimizing policy functions. These methods not only improve the\nrobot's navigation ability in the unknown environment, but also enhance its\nadaptive and self-learning capabilities. Through multiple training and\nsimulation experiments, we have verified the effectiveness and robustness of\nthese models in various complex scenarios.\n","authors":["Zixiang Wang","Hao Yan","Yining Wang","Zhengjia Xu","Zhuoyue Wang","Zhizhong Wu"],"pdf_url":"https://arxiv.org/pdf/2407.02539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05540v2","updated":"2024-07-08T16:39:35Z","published":"2024-06-08T18:11:30Z","title":"A Fine-tuning Dataset and Benchmark for Large Language Models for\n  Protein Understanding","summary":"  The parallels between protein sequences and natural language in their\nsequential structures have inspired the application of large language models\n(LLMs) to protein understanding. Despite the success of LLMs in NLP, their\neffectiveness in comprehending protein sequences remains an open question,\nlargely due to the absence of datasets linking protein sequences to descriptive\ntext. Researchers have then attempted to adapt LLMs for protein understanding\nby integrating a protein sequence encoder with a pre-trained LLM. However, this\nadaptation raises a fundamental question: \"Can LLMs, originally designed for\nNLP, effectively comprehend protein sequences as a form of language?\" Current\ndatasets fall short in addressing this question due to the lack of a direct\ncorrelation between protein sequences and corresponding text descriptions,\nlimiting the ability to train and evaluate LLMs for protein understanding\neffectively. To bridge this gap, we introduce ProteinLMDataset, a dataset\nspecifically designed for further self-supervised pretraining and supervised\nfine-tuning (SFT) of LLMs to enhance their capability for protein sequence\ncomprehension. Specifically, ProteinLMDataset includes 17.46 billion tokens for\npretraining and 893,000 instructions for SFT. Additionally, we present\nProteinLMBench, the first benchmark dataset consisting of 944 manually verified\nmultiple-choice questions for assessing the protein understanding capabilities\nof LLMs. ProteinLMBench incorporates protein-related details and sequences in\nmultiple languages, establishing a new standard for evaluating LLMs' abilities\nin protein comprehension. The large language model InternLM2-7B, pretrained and\nfine-tuned on the ProteinLMDataset, outperforms GPT-4 on ProteinLMBench,\nachieving the highest accuracy score.\n","authors":["Yiqing Shen","Zan Chen","Michail Mamalakis","Luhan He","Haiyang Xia","Tianbin Li","Yanzhou Su","Junjun He","Yu Guang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.05540v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06099v1","updated":"2024-07-08T16:38:52Z","published":"2024-07-08T16:38:52Z","title":"Physics-Informed Machine Learning Towards A Real-Time Spacecraft Thermal\n  Simulator","summary":"  Modeling thermal states for complex space missions, such as the surface\nexploration of airless bodies, requires high computation, whether used in\nground-based analysis for spacecraft design or during onboard reasoning for\nautonomous operations. For example, a finite-element thermal model with\nhundreds of elements can take significant time to simulate, which makes it\nunsuitable for onboard reasoning during time-sensitive scenarios such as\ndescent and landing, proximity operations, or in-space assembly. Further, the\nlack of fast and accurate thermal modeling drives thermal designs to be more\nconservative and leads to spacecraft with larger mass and higher power budgets.\nThe emerging paradigm of physics-informed machine learning (PIML) presents a\nclass of hybrid modeling architectures that address this challenge by combining\nsimplified physics models with machine learning (ML) models resulting in models\nwhich maintain both interpretability and robustness. Such techniques enable\ndesigns with reduced mass and power through onboard thermal-state estimation\nand control and may lead to improved onboard handling of off-nominal states,\nincluding unplanned down-time. The PIML model or hybrid model presented here\nconsists of a neural network which predicts reduced nodalizations (distribution\nand size of coarse mesh) given on-orbit thermal load conditions, and\nsubsequently a (relatively coarse) finite-difference model operates on this\nmesh to predict thermal states. We compare the computational performance and\naccuracy of the hybrid model to a data-driven neural net model, and a\nhigh-fidelity finite-difference model of a prototype Earth-orbiting small\nspacecraft. The PIML based active nodalization approach provides significantly\nbetter generalization than the neural net model and coarse mesh model, while\nreducing computing cost by up to 1.7x compared to the high-fidelity model.\n","authors":["Manaswin Oddiraju","Zaki Hasnain","Saptarshi Bandyopadhyay","Eric Sunada","Souma Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2407.06099v1.pdf","comment":"Accepted for presentation at the AIAA Aviation 2024 Forum"},{"id":"http://arxiv.org/abs/2407.06092v1","updated":"2024-07-08T16:31:49Z","published":"2024-07-08T16:31:49Z","title":"Assessing Cardiomegaly in Dogs Using a Simple CNN Model","summary":"  This paper introduces DogHeart, a dataset comprising 1400 training, 200\nvalidation, and 400 test images categorized as small, normal, and large based\non VHS score. A custom CNN model is developed, featuring a straightforward\narchitecture with 4 convolutional layers and 4 fully connected layers. Despite\nthe absence of data augmentation, the model achieves a 72\\% accuracy in\nclassifying cardiomegaly severity. The study contributes to automated\nassessment of cardiac conditions in dogs, highlighting the potential for early\ndetection and intervention in veterinary care.\n","authors":["Nikhil Deekonda"],"pdf_url":"https://arxiv.org/pdf/2407.06092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20993v2","updated":"2024-07-08T16:26:03Z","published":"2024-05-31T16:38:35Z","title":"Information limits and Thouless-Anderson-Palmer equations for spiked\n  matrix models with structured noise","summary":"  We consider a prototypical problem of Bayesian inference for a structured\nspiked model: a low-rank signal is corrupted by additive noise. While both\ninformation-theoretic and algorithmic limits are well understood when the noise\nis a Gaussian Wigner matrix, the more realistic case of structured noise still\nproves to be challenging. To capture the structure while maintaining\nmathematical tractability, a line of work has focused on rotationally invariant\nnoise. However, existing studies either provide sub-optimal algorithms or are\nlimited to special cases of noise ensembles. In this paper, using tools from\nstatistical physics (replica method) and random matrix theory (generalized\nspherical integrals) we establish the first characterization of the\ninformation-theoretic limits for a noise matrix drawn from a general trace\nensemble. Remarkably, our analysis unveils the asymptotic equivalence between\nthe rotationally invariant model and a surrogate Gaussian one. Finally, we show\nhow to saturate the predicted statistical limits using an efficient algorithm\ninspired by the theory of adaptive Thouless-Anderson-Palmer (TAP) equations.\n","authors":["Jean Barbier","Francesco Camilli","Marco Mondelli","Yizhou Xu"],"pdf_url":"https://arxiv.org/pdf/2405.20993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00853v2","updated":"2024-07-08T16:15:08Z","published":"2024-06-02T20:18:40Z","title":"A Tutorial on Doubly Robust Learning for Causal Inference","summary":"  Doubly robust learning offers a robust framework for causal inference from\nobservational data by integrating propensity score and outcome modeling.\nDespite its theoretical appeal, practical adoption remains limited due to\nperceived complexity and inaccessible software. This tutorial aims to demystify\ndoubly robust methods and demonstrate their application using the EconML\npackage. We provide an introduction to causal inference, discuss the principles\nof outcome modeling and propensity scores, and illustrate the doubly robust\napproach through simulated case studies. By simplifying the methodology and\noffering practical coding examples, we intend to make doubly robust learning\naccessible to researchers and practitioners in data science and statistics.\n","authors":["Hlynur DavÃ­Ã° Hlynsson"],"pdf_url":"https://arxiv.org/pdf/2406.00853v2.pdf","comment":"This paper is being withdrawn due to significant revisions and\n  updates required based on new findings and internal review"},{"id":"http://arxiv.org/abs/2407.06060v1","updated":"2024-07-08T16:01:04Z","published":"2024-07-08T16:01:04Z","title":"MERGE -- A Bimodal Dataset for Static Music Emotion Recognition","summary":"  The Music Emotion Recognition (MER) field has seen steady developments in\nrecent years, with contributions from feature engineering, machine learning,\nand deep learning. The landscape has also shifted from audio-centric systems to\nbimodal ensembles that combine audio and lyrics. However, a severe lack of\npublic and sizeable bimodal databases has hampered the development and\nimprovement of bimodal audio-lyrics systems. This article proposes three new\naudio, lyrics, and bimodal MER research datasets, collectively called MERGE,\ncreated using a semi-automatic approach. To comprehensively assess the proposed\ndatasets and establish a baseline for benchmarking, we conducted several\nexperiments for each modality, using feature engineering, machine learning, and\ndeep learning methodologies. In addition, we propose and validate fixed\ntrain-validate-test splits. The obtained results confirm the viability of the\nproposed datasets, achieving the best overall result of 79.21% F1-score for\nbimodal classification using a deep neural network.\n","authors":["Pedro Lima Louro","Hugo Redinho","Ricardo Santos","Ricardo Malheiro","Renato Panda","Rui Pedro Paiva"],"pdf_url":"https://arxiv.org/pdf/2407.06060v1.pdf","comment":"16 pages, 4 figures, 13 tables, submitted to IEEE Transactions on\n  Affective Computing"},{"id":"http://arxiv.org/abs/2407.06057v1","updated":"2024-07-08T15:59:44Z","published":"2024-07-08T15:59:44Z","title":"Variational Best-of-N Alignment","summary":"  Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on a controlled\ngeneration task suggest that while variational BoN is not as effective as BoN\nin aligning language models, it is close to BoN performance as vBoN appears\nmore often on the Pareto frontier of reward and KL divergence compared to\nmodels trained with KL-constrained RL objective.\n","authors":["Afra Amini","Tim Vieira","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2407.06057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08670v4","updated":"2024-07-08T15:56:39Z","published":"2023-06-14T17:59:15Z","title":"Simple Opinion Dynamics for No-Regret Learning","summary":"  We study a cooperative multi-agent bandit setting in the distributed GOSSIP\nmodel: in every round, each of $n$ agents chooses an action from a common set,\nobserves the action's corresponding reward, and subsequently exchanges\ninformation with a single randomly chosen neighbor, which may inform its choice\nin the next round. We introduce and analyze families of memoryless and\ntime-independent protocols for this setting, inspired by opinion dynamics that\nare well-studied for other algorithmic tasks in the GOSSIP model. For\nstationary reward settings, we prove for the first time that these simple\nprotocols exhibit best-of-both-worlds behavior, simultaneously obtaining\nconstant cumulative regret scaling like $R(T)/T = \\widetilde O(1/T)$, and also\nreaching consensus on the highest-mean action within $\\widetilde O(\\sqrt{n})$\nrounds. We obtain these results by showing a new connection between the global\nevolution of these decentralized protocols and a class of zero-sum\nmultiplicative weights update} processes. Using this connection, we establish a\ngeneral framework for analyzing the population-level regret and other\nproperties of our protocols. Finally, we show our protocols are also\nsurprisingly robust to adversarial rewards, and in this regime we obtain\nsublinear regret scaling like $R(T)/T = \\widetilde O(1/\\sqrt{T})$ as long as\nthe number of rounds does not grow too fast as a function of $n$.\n","authors":["John Lazarsfeld","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2306.08670v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06053v1","updated":"2024-07-08T15:55:12Z","published":"2024-07-08T15:55:12Z","title":"Learning local equivariant representations for quantum operators","summary":"  Predicting quantum operator matrices such as Hamiltonian, overlap, and\ndensity matrices in the density functional theory (DFT) framework is crucial\nfor understanding material properties. Current methods often focus on\nindividual operators and struggle with efficiency and scalability for large\nsystems. Here we introduce a novel deep learning model, SLEM (Strictly\nLocalized Equivariant Message-passing) for predicting multiple quantum\noperators, that achieves state-of-the-art accuracy while dramatically improving\ncomputational efficiency. SLEM's key innovation is its strict locality-based\ndesign, constructing local, equivariant representations for quantum tensors\nwhile preserving physical symmetries. This enables complex many-body dependence\nwithout expanding the effective receptive field, leading to superior data\nefficiency and transferability. Using an innovative SO(2) convolution\ntechnique, SLEM reduces the computational complexity of high-order tensor\nproducts and is therefore capable of handling systems requiring the $f$ and $g$\norbitals in their basis sets. We demonstrate SLEM's capabilities across diverse\n2D and 3D materials, achieving high accuracy even with limited training data.\nSLEM's design facilitates efficient parallelization, potentially extending DFT\nsimulations to systems with device-level sizes, opening new possibilities for\nlarge-scale quantum simulations and high-throughput materials discovery.\n","authors":["Zhanghao Zhouyin","Zixi Gan","Shishir Kumar Pandey","Linfeng Zhang","Qiangqiang Gu"],"pdf_url":"https://arxiv.org/pdf/2407.06053v1.pdf","comment":"11 pages, 5 figures and 4 tables"},{"id":"http://arxiv.org/abs/2305.16269v3","updated":"2024-07-08T15:32:52Z","published":"2023-05-25T17:25:14Z","title":"UDPM: Upsampling Diffusion Probabilistic Models","summary":"  Denoising Diffusion Probabilistic Models (DDPM) have recently gained\nsignificant attention. DDPMs compose a Markovian process that begins in the\ndata domain and gradually adds noise until reaching pure white noise. DDPMs\ngenerate high-quality samples from complex data distributions by defining an\ninverse process and training a deep neural network to learn this mapping.\nHowever, these models are inefficient because they require many diffusion steps\nto produce aesthetically pleasing samples. Additionally, unlike generative\nadversarial networks (GANs), the latent space of diffusion models is less\ninterpretable. In this work, we propose to generalize the denoising diffusion\nprocess into an Upsampling Diffusion Probabilistic Model (UDPM). In the forward\nprocess, we reduce the latent variable dimension through downsampling, followed\nby the traditional noise perturbation. As a result, the reverse process\ngradually denoises and upsamples the latent variable to produce a sample from\nthe data distribution. We formalize the Markovian diffusion processes of UDPM\nand demonstrate its generation capabilities on the popular FFHQ, AFHQv2, and\nCIFAR10 datasets. UDPM generates images with as few as three network\nevaluations, whose overall computational cost is less than a single DDPM or EDM\nstep, while achieving an FID score of 6.86. This surpasses current\nstate-of-the-art efficient diffusion models that use a single denoising step\nfor sampling. Additionally, UDPM offers an interpretable and interpolable\nlatent space, which gives it an advantage over traditional DDPMs. Our code is\navailable online: \\url{https://github.com/shadyabh/UDPM/}\n","authors":["Shady Abu-Hussein","Raja Giryes"],"pdf_url":"https://arxiv.org/pdf/2305.16269v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06018v1","updated":"2024-07-08T15:08:41Z","published":"2024-07-08T15:08:41Z","title":"Leveraging Transformers for Weakly Supervised Object Localization in\n  Unconstrained Videos","summary":"  Weakly-Supervised Video Object Localization (WSVOL) involves localizing an\nobject in videos using only video-level labels, also referred to as tags.\nState-of-the-art WSVOL methods like Temporal CAM (TCAM) rely on class\nactivation mapping (CAM) and typically require a pre-trained CNN classifier.\nHowever, their localization accuracy is affected by their tendency to minimize\nthe mutual information between different instances of a class and exploit\ntemporal information during training for downstream tasks, e.g., detection and\ntracking. In the absence of bounding box annotation, it is challenging to\nexploit precise information about objects from temporal cues because the model\nstruggles to locate objects over time. To address these issues, a novel method\ncalled transformer based CAM for videos (TrCAM-V), is proposed for WSVOL. It\nconsists of a DeiT backbone with two heads for classification and localization.\nThe classification head is trained using standard classification loss (CL),\nwhile the localization head is trained using pseudo-labels that are extracted\nusing a pre-trained CLIP model. From these pseudo-labels, the high and low\nactivation values are considered to be foreground and background regions,\nrespectively. Our TrCAM-V method allows training a localization network by\nsampling pseudo-pixels on the fly from these regions. Additionally, a\nconditional random field (CRF) loss is employed to align the object boundaries\nwith the foreground map. During inference, the model can process individual\nframes for real-time localization applications. Extensive experiments on\nchallenging YouTube-Objects unconstrained video datasets show that our TrCAM-V\nmethod achieves new state-of-the-art performance in terms of classification and\nlocalization accuracy.\n","authors":["Shakeeb Murtaza","Marco Pedersoli","Aydin Sarraf","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2407.06018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16614v2","updated":"2024-07-08T15:08:16Z","published":"2023-05-26T04:20:02Z","title":"Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings","summary":"  This paper proposes the Phy-DRL: a physics-regulated deep reinforcement\nlearning (DRL) framework for safety-critical autonomous systems. The Phy-DRL\nhas three distinguished invariant-embedding designs: i) residual action policy\n(i.e., integrating data-driven-DRL action policy and physics-model-based action\npolicy), ii) automatically constructed safety-embedded reward, and iii)\nphysics-model-guided neural network (NN) editing, including link editing and\nactivation editing. Theoretically, the Phy-DRL exhibits 1) a mathematically\nprovable safety guarantee and 2) strict compliance of critic and actor networks\nwith physics knowledge about the action-value function and action policy.\nFinally, we evaluate the Phy-DRL on a cart-pole system and a quadruped robot.\nThe experiments validate our theoretical results and demonstrate that Phy-DRL\nfeatures guaranteed safety compared to purely data-driven DRL and solely\nmodel-based design while offering remarkably fewer learning parameters and fast\ntraining towards safety guarantee.\n","authors":["Hongpeng Cao","Yanbing Mao","Lui Sha","Marco Caccamo"],"pdf_url":"https://arxiv.org/pdf/2305.16614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06015v1","updated":"2024-07-08T15:06:03Z","published":"2024-07-08T15:06:03Z","title":"Simulation-based Benchmarking for Causal Structure Learning in Gene\n  Perturbation Experiments","summary":"  Causal structure learning (CSL) refers to the task of learning causal\nrelationships from data. Advances in CSL now allow learning of causal graphs in\ndiverse application domains, which has the potential to facilitate data-driven\ncausal decision-making. Real-world CSL performance depends on a number of\n$\\textit{context-specific}$ factors, including context-specific data\ndistributions and non-linear dependencies, that are important in practical\nuse-cases. However, our understanding of how to assess and select CSL methods\nin specific contexts remains limited. To address this gap, we present\n$\\textit{CausalRegNet}$, a multiplicative effect structural causal model that\nallows for generating observational and interventional data incorporating\ncontext-specific properties, with a focus on the setting of gene perturbation\nexperiments. Using real-world gene perturbation data, we show that CausalRegNet\ngenerates accurate distributions and scales far better than current simulation\nframeworks. We illustrate the use of CausalRegNet in assessing CSL methods in\nthe context of interventional experiments in biology.\n","authors":["Luka KovaÄeviÄ","Izzy Newsham","Sach Mukherjee","John Whittaker"],"pdf_url":"https://arxiv.org/pdf/2407.06015v1.pdf","comment":"16 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2407.04472v2","updated":"2024-07-08T14:50:49Z","published":"2024-07-05T12:42:31Z","title":"EventChat: Implementation and user-centric evaluation of a large\n  language model-driven conversational recommender system for exploring leisure\n  events in an SME context","summary":"  Large language models (LLMs) present an enormous evolution in the strategic\npotential of conversational recommender systems (CRS). Yet to date, research\nhas predominantly focused upon technical frameworks to implement LLM-driven\nCRS, rather than end-user evaluations or strategic implications for firms,\nparticularly from the perspective of a small to medium enterprises (SME) that\nmakeup the bedrock of the global economy. In the current paper, we detail the\ndesign of an LLM-driven CRS in an SME setting, and its subsequent performance\nin the field using both objective system metrics and subjective user\nevaluations. While doing so, we additionally outline a short-form revised\nResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly\nevolving field. Our results reveal good system performance from a user\nexperience perspective (85.5% recommendation accuracy) but underscore latency,\ncost, and quality issues challenging business viability. Notably, with a median\ncost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and\nresponse time emerge as crucial areas for achieving a more user-friendly and\neconomically viable LLM-driven CRS for SME settings. One major driver of these\ncosts is the use of an advanced LLM as a ranker within the retrieval-augmented\ngeneration (RAG) technique. Our results additionally indicate that relying\nsolely on approaches such as Prompt-based learning with ChatGPT as the\nunderlying LLM makes it challenging to achieve satisfying quality in a\nproduction environment. Strategic considerations for SMEs deploying an\nLLM-driven CRS are outlined, particularly considering trade-offs in the current\ntechnical landscape.\n","authors":["Hannes Kunstmann","Joseph Ollier","Joel Persson","Florian von Wangenheim"],"pdf_url":"https://arxiv.org/pdf/2407.04472v2.pdf","comment":"27 pages, 3 tables, 5 figures, pre-print manuscript, updated version\n  of manuscript due to typo (previous version, Figure 5 was incorrectly named\n  Figure 6)"},{"id":"http://arxiv.org/abs/2403.04482v2","updated":"2024-07-08T14:49:14Z","published":"2024-03-07T13:33:30Z","title":"On the Topology Awareness and Generalization Performance of Graph Neural\n  Networks","summary":"  Many computer vision and machine learning problems are modelled as learning\ntasks on graphs where graph neural networks GNNs have emerged as a dominant\ntool for learning representations of graph structured data A key feature of\nGNNs is their use of graph structures as input enabling them to exploit the\ngraphs inherent topological properties known as the topology awareness of GNNs\nDespite the empirical successes of GNNs the influence of topology awareness on\ngeneralization performance remains unexplored, particularly for node level\ntasks that diverge from the assumption of data being independent and\nidentically distributed IID The precise definition and characterization of the\ntopology awareness of GNNs especially concerning different topological features\nare still unclear This paper introduces a comprehensive framework to\ncharacterize the topology awareness of GNNs across any topological feature\nUsing this framework we investigate the effects of topology awareness on GNN\ngeneralization performance Contrary to the prevailing belief that enhancing the\ntopology awareness of GNNs is always advantageous our analysis reveals a\ncritical insight improving the topology awareness of GNNs may inadvertently\nlead to unfair generalization across structural groups which might not be\ndesired in some scenarios Additionally we conduct a case study using the\nintrinsic graph metric the shortest path distance on various benchmark datasets\nThe empirical results of this case study confirm our theoretical insights\nMoreover we demonstrate the practical applicability of our framework by using\nit to tackle the cold start problem in graph active learning\n","authors":["Junwei Su","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2403.04482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04538v2","updated":"2024-07-08T14:44:06Z","published":"2024-07-05T14:24:37Z","title":"PDiscoFormer: Relaxing Part Discovery Constraints with Vision\n  Transformers","summary":"  Computer vision methods that explicitly detect object parts and reason on\nthem are a step towards inherently interpretable models. Existing approaches\nthat perform part discovery driven by a fine-grained classification task make\nvery restrictive assumptions on the geometric properties of the discovered\nparts; they should be small and compact. Although this prior is useful in some\ncases, in this paper we show that pre-trained transformer-based vision models,\nsuch as self-supervised DINOv2 ViT, enable the relaxation of these constraints.\nIn particular, we find that a total variation (TV) prior, which allows for\nmultiple connected components of any size, substantially outperforms previous\nwork. We test our approach on three fine-grained classification benchmarks:\nCUB, PartImageNet and Oxford Flowers, and compare our results to previously\npublished methods as well as a re-implementation of the state-of-the-art method\nPDiscoNet with a transformer-based backbone. We consistently obtain substantial\nimprovements across the board, both on part discovery metrics and the\ndownstream classification task, showing that the strong inductive biases in\nself-supervised ViT models require to rethink the geometric priors that can be\nused for unsupervised part discovery.\n","authors":["Ananthu Aniraj","Cassio F. Dantas","Dino Ienco","Diego Marcos"],"pdf_url":"https://arxiv.org/pdf/2407.04538v2.pdf","comment":"Accepted as a main conference paper at the European Conference of\n  Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2407.05986v1","updated":"2024-07-08T14:26:30Z","published":"2024-07-08T14:26:30Z","title":"KidSat: satellite imagery to map childhood poverty dataset and benchmark","summary":"  Satellite imagery has emerged as an important tool to analyse demographic,\nhealth, and development indicators. While various deep learning models have\nbeen built for these tasks, each is specific to a particular problem, with few\nstandard benchmarks available. We propose a new dataset pairing satellite\nimagery and high-quality survey data on child poverty to benchmark satellite\nfeature representations. Our dataset consists of 33,608 images, each 10 km\n$\\times$ 10 km, from 19 countries in Eastern and Southern Africa in the time\nperiod 1997-2022. As defined by UNICEF, multidimensional child poverty covers\nsix dimensions and it can be calculated from the face-to-face Demographic and\nHealth Surveys (DHS) Program . As part of the benchmark, we test spatial as\nwell as temporal generalization, by testing on unseen locations, and on data\nafter the training years. Using our dataset we benchmark multiple models, from\nlow-level satellite imagery models such as MOSAIKS , to deep learning\nfoundation models, which include both generic vision models such as\nSelf-Distillation with no Labels (DINOv2) models and specific satellite imagery\nmodels such as SatMAE. We provide open source code for building the satellite\ndataset, obtaining ground truth data from DHS and running various models\nassessed in our work.\n","authors":["Makkunda Sharma","Fan Yang","Duy-Nhat Vo","Esra Suel","Swapnil Mishra","Samir Bhatt","Oliver Fiala","William Rudgard","Seth Flaxman"],"pdf_url":"https://arxiv.org/pdf/2407.05986v1.pdf","comment":"15 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.18682v2","updated":"2024-07-08T14:26:16Z","published":"2024-06-26T18:39:08Z","title":"The Multilingual Alignment Prism: Aligning Global and Local Preferences\n  to Reduce Harm","summary":"  A key concern with the concept of \"alignment\" is the implicit question of\n\"alignment to what?\". AI systems are increasingly used across the world, yet\nsafety alignment is often focused on homogeneous monolingual settings.\nAdditionally, preference training and safety measures often overfit to harms\ncommon in Western-centric datasets. Here, we explore the viability of different\nalignment approaches when balancing dual objectives: addressing and optimizing\nfor a non-homogeneous set of languages and cultural preferences while\nminimizing both global and local harms. We collect the first set of human\nannotated red-teaming prompts in different languages distinguishing between\nglobal and local harm, which serve as a laboratory for understanding the\nreliability of alignment techniques when faced with preference distributions\nthat are non-stationary across geographies and languages. While this setting is\nseldom covered by the literature to date, which primarily centers on English\nharm mitigation, it captures real-world interactions with AI systems around the\nworld. We establish a new precedent for state-of-the-art alignment techniques\nacross 6 languages with minimal degradation in general performance. Our work\nprovides important insights into cross-lingual transfer and novel optimization\napproaches to safeguard AI systems designed to serve global populations.\n","authors":[" Aakanksha","Arash Ahmadian","Beyza Ermis","Seraphina Goldfarb-Tarrant","Julia Kreutzer","Marzieh Fadaee","Sara Hooker"],"pdf_url":"https://arxiv.org/pdf/2406.18682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05982v1","updated":"2024-07-08T14:25:39Z","published":"2024-07-08T14:25:39Z","title":"MTL-Split: Multi-Task Learning for Edge Devices using Split Computing","summary":"  Split Computing (SC), where a Deep Neural Network (DNN) is intelligently\nsplit with a part of it deployed on an edge device and the rest on a remote\nserver is emerging as a promising approach. It allows the power of DNNs to be\nleveraged for latency-sensitive applications that do not allow the entire DNN\nto be deployed remotely, while not having sufficient computation bandwidth\navailable locally. In many such embedded systems scenarios, such as those in\nthe automotive domain, computational resource constraints also necessitate\nMulti-Task Learning (MTL), where the same DNN is used for multiple inference\ntasks instead of having dedicated DNNs for each task, which would need more\ncomputing bandwidth. However, how to partition such a multi-tasking DNN to be\ndeployed within a SC framework has not been sufficiently studied. This paper\nstudies this problem, and MTL-Split, our novel proposed architecture, shows\nencouraging results on both synthetic and real-world data. The source code is\navailable at https://github.com/intelligolabs/MTL-Split.\n","authors":["Luigi Capogrosso","Enrico Fraccaroli","Samarjit Chakraborty","Franco Fummi","Marco Cristani"],"pdf_url":"https://arxiv.org/pdf/2407.05982v1.pdf","comment":"Accepted at the 61st Design Automation Conference (DAC 2024)"},{"id":"http://arxiv.org/abs/2401.12196v2","updated":"2024-07-08T14:24:40Z","published":"2024-01-22T18:36:29Z","title":"Learning Dynamics from Multicellular Graphs with Deep Neural Networks","summary":"  Multicellular self-assembly into functional structures is a dynamic process\nthat is critical in the development and diseases, including embryo development,\norgan formation, tumor invasion, and others. Being able to infer collective\ncell migratory dynamics from their static configuration is valuable for both\nunderstanding and predicting these complex processes. However, the\nidentification of structural features that can indicate multicellular motion\nhas been difficult, and existing metrics largely rely on physical instincts.\nHere we show that using a graph neural network (GNN), the motion of\nmulticellular collectives can be inferred from a static snapshot of cell\npositions, in both experimental and synthetic datasets.\n","authors":["Haiqian Yang","Florian Meyer","Shaoxun Huang","Liu Yang","Cristiana Lungu","Monilola A. Olayioye","Markus J. Buehler","Ming Guo"],"pdf_url":"https://arxiv.org/pdf/2401.12196v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.15745v3","updated":"2024-07-08T14:21:59Z","published":"2022-05-31T12:31:21Z","title":"HyperMAML: Few-Shot Adaptation of Deep Models with Hypernetworks","summary":"  The aim of Few-Shot learning methods is to train models which can easily\nadapt to previously unseen tasks, based on small amounts of data. One of the\nmost popular and elegant Few-Shot learning approaches is Model-Agnostic\nMeta-Learning (MAML). The main idea behind this method is to learn the general\nweights of the meta-model, which are further adapted to specific problems in a\nsmall number of gradient steps. However, the model's main limitation lies in\nthe fact that the update procedure is realized by gradient-based optimisation.\nIn consequence, MAML cannot always modify weights to the essential level in one\nor even a few gradient iterations. On the other hand, using many gradient steps\nresults in a complex and time-consuming optimization procedure, which is hard\nto train in practice, and may lead to overfitting. In this paper, we propose\nHyperMAML, a novel generalization of MAML, where the training of the update\nprocedure is also part of the model. Namely, in HyperMAML, instead of updating\nthe weights with gradient descent, we use for this purpose a trainable\nHypernetwork. Consequently, in this framework, the model can generate\nsignificant updates whose range is not limited to a fixed number of gradient\nsteps. Experiments show that HyperMAML consistently outperforms MAML and\nperforms comparably to other state-of-the-art techniques in a number of\nstandard Few-Shot learning benchmarks.\n","authors":["M. PrzewiÄÅºlikowski","P. Przybysz","J. Tabor","M. ZiÄba","P. Spurek"],"pdf_url":"https://arxiv.org/pdf/2205.15745v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05973v1","updated":"2024-07-08T14:16:05Z","published":"2024-07-08T14:16:05Z","title":"Active Label Refinement for Robust Training of Imbalanced Medical Image\n  Classification Tasks in the Presence of High Label Noise","summary":"  The robustness of supervised deep learning-based medical image classification\nis significantly undermined by label noise. Although several methods have been\nproposed to enhance classification performance in the presence of noisy labels,\nthey face some challenges: 1) a struggle with class-imbalanced datasets,\nleading to the frequent overlooking of minority classes as noisy samples; 2) a\nsingular focus on maximizing performance using noisy datasets, without\nincorporating experts-in-the-loop for actively cleaning the noisy labels. To\nmitigate these challenges, we propose a two-phase approach that combines\nLearning with Noisy Labels (LNL) and active learning. This approach not only\nimproves the robustness of medical image classification in the presence of\nnoisy labels, but also iteratively improves the quality of the dataset by\nrelabeling the important incorrect labels, under a limited annotation budget.\nFurthermore, we introduce a novel Variance of Gradients approach in LNL phase,\nwhich complements the loss-based sample selection by also sampling\nunder-represented samples. Using two imbalanced noisy medical classification\ndatasets, we demonstrate that that our proposed technique is superior to its\npredecessors at handling class imbalance by not misidentifying clean samples\nfrom minority classes as mostly noisy samples.\n","authors":["Bidur Khanal","Tianhong Dai","Binod Bhattarai","Cristian Linte"],"pdf_url":"https://arxiv.org/pdf/2407.05973v1.pdf","comment":"Accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.05966v1","updated":"2024-07-08T14:05:03Z","published":"2024-07-08T14:05:03Z","title":"On Bellman equations for continuous-time policy evaluation I:\n  discretization and approximation","summary":"  We study the problem of computing the value function from a\ndiscretely-observed trajectory of a continuous-time diffusion process. We\ndevelop a new class of algorithms based on easily implementable numerical\nschemes that are compatible with discrete-time reinforcement learning (RL) with\nfunction approximation. We establish high-order numerical accuracy as well as\nthe approximation error guarantees for the proposed approach. In contrast to\ndiscrete-time RL problems where the approximation factor depends on the\neffective horizon, we obtain a bounded approximation factor using the\nunderlying elliptic structures, even if the effective horizon diverges to\ninfinity.\n","authors":["Wenlong Mou","Yuhua Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.05966v1.pdf","comment":"WM and YZ contributed equally to this work"},{"id":"http://arxiv.org/abs/2407.05965v1","updated":"2024-07-08T14:04:58Z","published":"2024-07-08T14:04:58Z","title":"T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models","summary":"  The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset using LLMs and jailbreaking prompt attacks. Based on our evaluation\nresults, we draw several important findings, including: 1) no single model\nexcels in all aspects, with different models showing various strengths; 2) the\ncorrelation between GPT-4 assessments and manual reviews is generally high; 3)\nthere is a trade-off between the usability and safety of text-to-video\ngenerative models. This indicates that as the field of video generation rapidly\nadvances, safety risks are set to surge, highlighting the urgency of\nprioritizing video safety. We hope that T2VSafetyBench can provide insights for\nbetter understanding the safety of video generation in the era of generative\nAI.\n","authors":["Yibo Miao","Yifan Zhu","Yinpeng Dong","Lijia Yu","Jun Zhu","Xiao-Shan Gao"],"pdf_url":"https://arxiv.org/pdf/2407.05965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10246v2","updated":"2024-07-08T13:44:56Z","published":"2023-07-17T06:54:36Z","title":"Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding\n  (Survey)","summary":"  Can we obtain insights about the brain using AI models? How is the\ninformation in deep learning models related to brain recordings? Can we improve\nAI models with the help of brain recordings? Such questions can be tackled by\nstudying brain recordings like functional magnetic resonance imaging (fMRI). As\na first step, the neuroscience community has contributed several large\ncognitive neuroscience datasets related to passive reading/listening/viewing of\nconcept words, narratives, pictures, and movies. Encoding and decoding models\nusing these datasets have also been proposed in the past two decades. These\nmodels serve as additional tools for basic cognitive science and neuroscience\nresearch. Encoding models aim at generating fMRI brain representations given a\nstimulus automatically. They have several practical applications in evaluating\nand diagnosing neurological conditions and thus may also help design therapies\nfor brain damage. Decoding models solve the inverse problem of reconstructing\nthe stimuli given the fMRI. They are useful for designing brain-machine or\nbrain-computer interfaces. Inspired by the effectiveness of deep learning\nmodels for natural language processing, computer vision, and speech, several\nneural encoding and decoding models have been recently proposed. In this\nsurvey, we will first discuss popular representations of language, vision and\nspeech stimuli, and present a summary of neuroscience datasets. Further, we\nwill review popular deep learning based encoding and decoding architectures and\nnote their benefits and limitations. Finally, we will conclude with a summary\nand discussion about future trends. Given the large amount of recently\npublished work in the computational cognitive neuroscience (CCN) community, we\nbelieve that this survey enables an entry point for DNN researchers to\ndiversify into CCN research.\n","authors":["Subba Reddy Oota","Zijiao Chen","Manish Gupta","Raju S. Bapi","Gael Jobard","Frederic Alexandre","Xavier Hinaut"],"pdf_url":"https://arxiv.org/pdf/2307.10246v2.pdf","comment":"47 pages, 23 figures"},{"id":"http://arxiv.org/abs/2407.05934v1","updated":"2024-07-08T13:41:21Z","published":"2024-07-08T13:41:21Z","title":"Graph Anomaly Detection with Noisy Labels by Reinforcement Learning","summary":"  Graph anomaly detection (GAD) has been widely applied in many areas, e.g.,\nfraud detection in finance and robot accounts in social networks. Existing\nmethods are dedicated to identifying the outlier nodes that deviate from normal\nones. While they heavily rely on high-quality annotation, which is hard to\nobtain in real-world scenarios, this could lead to severely degraded\nperformance based on noisy labels. Thus, we are motivated to cut the edges of\nsuspicious nodes to alleviate the impact of noise. However, it remains\ndifficult to precisely identify the nodes with noisy labels. Moreover, it is\nhard to quantitatively evaluate the regret of cutting the edges, which may have\neither positive or negative influences. To this end, we propose a novel\nframework REGAD, i.e., REinforced Graph Anomaly Detector. Specifically, we aim\nto maximize the performance improvement (AUC) of a base detector by cutting\nnoisy edges approximated through the nodes with high-confidence labels. (i) We\ndesign a tailored action and search space to train a policy network to\ncarefully prune edges step by step, where only a few suspicious edges are\nprioritized in each step. (ii) We design a policy-in-the-loop mechanism to\niteratively optimize the policy based on the feedback from base detector. The\noverall performance is evaluated by the cumulative rewards. Extensive\nexperiments are conducted on three datasets under different anomaly ratios. The\nresults indicate the superior performance of our proposed REGAD.\n","authors":["Zhu Wang","Shuang Zhou","Junnan Dong","Chang Yang","Xiao Huang","Shengjie Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.05934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16426v2","updated":"2024-07-08T13:35:12Z","published":"2024-06-24T08:20:43Z","title":"Fault Detection for agents on power grid topology optimization: A\n  Comprehensive analysis","summary":"  The topology optimization of transmission networks using Deep Reinforcement\nLearning (DRL) has increasingly come into focus. Various researchers have\nproposed different DRL agents, which are often benchmarked on the Grid2Op\nenvironment from the Learning to Run a Power Network (L2RPN) challenges. The\nenvironments have many advantages with their realistic chronics and underlying\npower flow backends. However, the interpretation of agent survival or failure\nis not always clear, as there are a variety of potential causes. In this work,\nwe focus on the failures of the power grid to identify patterns and detect them\na priori. We collect the failed chronics of three different agents on the WCCI\n2022 L2RPN environment, totaling about 40k data points. By clustering, we are\nable to detect five distinct clusters, identifying different failure types.\nFurther, we propose a multi-class prediction approach to detect failures\nbeforehand and evaluate five different models. Here, the Light\nGradient-Boosting Machine (LightGBM) shows the best performance, with an\naccuracy of 86%. It also correctly identifies in 91% of the time failure and\nsurvival observations. Finally, we provide a detailed feature importance\nanalysis that identifies critical features and regions in the grid.\n","authors":["Malte Lehna","Mohamed Hassouna","Dmitry Degtyar","Sven Tomforde","Christoph Scholz"],"pdf_url":"https://arxiv.org/pdf/2406.16426v2.pdf","comment":"11 Pages plus references and appendix. The appendix consist of\n  additional material of the paper and is not included in the initial\n  submission"},{"id":"http://arxiv.org/abs/2407.05921v1","updated":"2024-07-08T13:28:47Z","published":"2024-07-08T13:28:47Z","title":"TAPVid-3D: A Benchmark for Tracking Any Point in 3D","summary":"  We introduce a new benchmark, TAPVid-3D, for evaluating the task of\nlong-range Tracking Any Point in 3D (TAP-3D). While point tracking in two\ndimensions (TAP) has many benchmarks measuring performance on real-world\nvideos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To\nthis end, leveraging existing footage, we build a new benchmark for 3D point\ntracking featuring 4,000+ real-world videos, composed of three different data\nsources spanning a variety of object types, motion patterns, and indoor and\noutdoor environments. To measure performance on the TAP-3D task, we formulate a\ncollection of metrics that extend the Jaccard-based metric used in TAP to\nhandle the complexities of ambiguous depth scales across models, occlusions,\nand multi-track spatio-temporal smoothness. We manually verify a large sample\nof trajectories to ensure correct video annotations, and assess the current\nstate of the TAP-3D task by constructing competitive baselines using existing\ntracking models. We anticipate this benchmark will serve as a guidepost to\nimprove our ability to understand precise 3D motion and surface deformation\nfrom monocular video. Code for dataset download, generation, and model\nevaluation is available at https://tapvid3d.github.io\n","authors":["Skanda Koppula","Ignacio Rocco","Yi Yang","Joe Heyward","JoÃ£o Carreira","Andrew Zisserman","Gabriel Brostow","Carl Doersch"],"pdf_url":"https://arxiv.org/pdf/2407.05921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05920v1","updated":"2024-07-08T13:27:41Z","published":"2024-07-08T13:27:41Z","title":"LPGD: A General Framework for Backpropagation through Embedded\n  Optimization Layers","summary":"  Embedding parameterized optimization problems as layers into machine learning\narchitectures serves as a powerful inductive bias. Training such architectures\nwith stochastic gradient descent requires care, as degenerate derivatives of\nthe embedded optimization problem often render the gradients uninformative. We\npropose Lagrangian Proximal Gradient Descent (LPGD) a flexible framework for\ntraining architectures with embedded optimization layers that seamlessly\nintegrates into automatic differentiation libraries. LPGD efficiently computes\nmeaningful replacements of the degenerate optimization layer derivatives by\nre-running the forward solver oracle on a perturbed input. LPGD captures\nvarious previously proposed methods as special cases, while fostering deep\nlinks to traditional optimization methods. We theoretically analyze our method\nand demonstrate on historical and synthetic data that LPGD converges faster\nthan gradient descent even in a differentiable setup.\n","authors":["Anselm Paulus","Georg Martius","VÃ­t Musil"],"pdf_url":"https://arxiv.org/pdf/2407.05920v1.pdf","comment":"ICML 2024 conference paper"},{"id":"http://arxiv.org/abs/2407.05919v1","updated":"2024-07-08T13:25:28Z","published":"2024-07-08T13:25:28Z","title":"Fostering Trust and Quantifying Value of AI and ML","summary":"  Artificial Intelligence (AI) and Machine Learning (ML) providers have a\nresponsibility to develop valid and reliable systems. Much has been discussed\nabout trusting AI and ML inferences (the process of running live data through a\ntrained AI model to make a prediction or solve a task), but little has been\ndone to define what that means. Those in the space of ML- based products are\nfamiliar with topics such as transparency, explainability, safety, bias, and so\nforth. Yet, there are no frameworks to quantify and measure those. Producing\never more trustworthy machine learning inferences is a path to increase the\nvalue of products (i.e., increased trust in the results) and to engage in\nconversations with users to gather feedback to improve products. In this paper,\nwe begin by examining the dynamic of trust between a provider (Trustor) and\nusers (Trustees). Trustors are required to be trusting and trustworthy, whereas\ntrustees need not be trusting nor trustworthy. The challenge for trustors is to\nprovide results that are good enough to make a trustee increase their level of\ntrust above a minimum threshold for: 1- doing business together; 2-\ncontinuation of service. We conclude by defining and proposing a framework, and\na set of viable metrics, to be used for computing a trust score and objectively\nunderstand how trustworthy a machine learning system can claim to be, plus\ntheir behavior over time.\n","authors":["Dalmo Cirne","Veena Calambur"],"pdf_url":"https://arxiv.org/pdf/2407.05919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01848v2","updated":"2024-07-08T13:18:17Z","published":"2024-07-01T23:16:34Z","title":"UniFIDES: Universal Fractional Integro-Differential Equation Solvers","summary":"  The development of data-driven approaches for solving differential equations\nhas been followed by a plethora of applications in science and engineering\nacross a multitude of disciplines and remains a central focus of active\nscientific inquiry. However, a large body of natural phenomena incorporates\nmemory effects that are best described via fractional integro-differential\nequations (FIDEs), in which the integral or differential operators accept\nnon-integer orders. Addressing the challenges posed by nonlinear FIDEs is a\nrecognized difficulty, necessitating the application of generic methods with\nimmediate practical relevance. This work introduces the Universal Fractional\nIntegro-Differential Equation Solvers (UniFIDES), a comprehensive machine\nlearning platform designed to expeditiously solve a variety of FIDEs in both\nforward and inverse directions, without the need for ad hoc manipulation of the\nequations. The effectiveness of UniFIDES is demonstrated through a collection\nof integer-order and fractional problems in science and engineering. Our\nresults highlight UniFIDES' ability to accurately solve a wide spectrum of\nintegro-differential equations and offer the prospect of using machine learning\nplatforms universally for discovering and describing dynamical and complex\nsystems.\n","authors":["Milad Saadat","Deepak Mangal","Safa Jamali"],"pdf_url":"https://arxiv.org/pdf/2407.01848v2.pdf","comment":"27 pages, 9 figures, regular article"},{"id":"http://arxiv.org/abs/2407.05895v1","updated":"2024-07-08T13:01:53Z","published":"2024-07-08T13:01:53Z","title":"Link Representation Learning for Probabilistic Travel Time Estimation","summary":"  Travel time estimation is a crucial application in navigation apps and web\nmapping services. Current deterministic and probabilistic methods primarily\nfocus on modeling individual trips, assuming independence among trips. However,\nin real-world scenarios, we often observe strong inter-trip correlations due to\nfactors such as weather conditions, traffic management, and road works. In this\npaper, we propose to model trip-level link travel time using a Gaussian\nhierarchical model, which can characterize both inter-trip and intra-trip\ncorrelations. The joint distribution of travel time of multiple trips becomes a\nmultivariate Gaussian parameterized by learnable link representations. To\neffectively use the sparse GPS trajectories, we also propose a data\naugmentation method based on trip sub-sampling, which allows for fine-grained\ngradient backpropagation in learning link representations. During inference, we\nestimate the probability distribution of the travel time of a queried trip\nconditional on the completed trips that are spatiotemporally adjacent. We refer\nto the overall framework as ProbTTE. We evaluate ProbTTE on two real-world GPS\ntrajectory datasets, and the results demonstrate its superior performance\ncompared to state-of-the-art deterministic and probabilistic baselines.\nAdditionally, we find that the learned link representations align well with the\nphysical geometry of the network, making them suitable as input for other\napplications.\n","authors":["Chen Xu","Qiang Wang","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2407.05895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10352v6","updated":"2024-07-08T12:52:12Z","published":"2023-07-19T21:21:18Z","title":"Properties of Discrete Sliced Wasserstein Losses","summary":"  The Sliced Wasserstein (SW) distance has become a popular alternative to the\nWasserstein distance for comparing probability measures. Widespread\napplications include image processing, domain adaptation and generative\nmodelling, where it is common to optimise some parameters in order to minimise\nSW, which serves as a loss function between discrete probability measures\n(since measures admitting densities are numerically unattainable). All these\noptimisation problems bear the same sub-problem, which is minimising the Sliced\nWasserstein energy. In this paper we study the properties of $\\mathcal{E}: Y\n\\longmapsto \\mathrm{SW}_2^2(\\gamma_Y, \\gamma_Z)$, i.e. the SW distance between\ntwo uniform discrete measures with the same amount of points as a function of\nthe support $Y \\in \\mathbb{R}^{n \\times d}$ of one of the measures. We\ninvestigate the regularity and optimisation properties of this energy, as well\nas its Monte-Carlo approximation $\\mathcal{E}_p$ (estimating the expectation in\nSW using only $p$ samples) and show convergence results on the critical points\nof $\\mathcal{E}_p$ to those of $\\mathcal{E}$, as well as an almost-sure uniform\nconvergence and a uniform Central Limit result on the process\n$\\mathcal{E}_p(Y)$. Finally, we show that in a certain sense, Stochastic\nGradient Descent methods minimising $\\mathcal{E}$ and $\\mathcal{E}_p$ converge\ntowards (Clarke) critical points of these energies.\n","authors":["Eloi Tanguy","RÃ©mi Flamary","Julie Delon"],"pdf_url":"https://arxiv.org/pdf/2307.10352v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05887v1","updated":"2024-07-08T12:47:03Z","published":"2024-07-08T12:47:03Z","title":"Generation and De-Identification of Indian Clinical Discharge Summaries\n  using LLMs","summary":"  The consequences of a healthcare data breach can be devastating for the\npatients, providers, and payers. The average financial impact of a data breach\nin recent months has been estimated to be close to USD 10 million. This is\nespecially significant for healthcare organizations in India that are managing\nrapid digitization while still establishing data governance procedures that\nalign with the letter and spirit of the law. Computer-based systems for\nde-identification of personal information are vulnerable to data drift, often\nrendering them ineffective in cross-institution settings. Therefore, a rigorous\nassessment of existing de-identification against local health datasets is\nimperative to support the safe adoption of digital health initiatives in India.\nUsing a small set of de-identified patient discharge summaries provided by an\nIndian healthcare institution, in this paper, we report the nominal performance\nof de-identification algorithms (based on language models) trained on publicly\navailable non-Indian datasets, pointing towards a lack of cross-institutional\ngeneralization. Similarly, experimentation with off-the-shelf de-identification\nsystems reveals potential risks associated with the approach. To overcome data\nscarcity, we explore generating synthetic clinical reports (using publicly\navailable and Indian summaries) by performing in-context learning over Large\nLanguage Models (LLMs). Our experiments demonstrate the use of generated\nreports as an effective strategy for creating high-performing de-identification\nsystems with good generalization capabilities.\n","authors":["Sanjeet Singh","Shreya Gupta","Niralee Gupta","Naimish Sharma","Lokesh Srivastava","Vibhu Agarwal","Ashutosh Modi"],"pdf_url":"https://arxiv.org/pdf/2407.05887v1.pdf","comment":"Accepted at BioNLP Workshop at ACL 2024; 21 pages (9 pages main\n  content)"},{"id":"http://arxiv.org/abs/2407.05876v1","updated":"2024-07-08T12:37:07Z","published":"2024-07-08T12:37:07Z","title":"Efficiently Training Neural Networks for Imperfect Information Games by\n  Sampling Information Sets","summary":"  In imperfect information games, the evaluation of a game state not only\ndepends on the observable world but also relies on hidden parts of the\nenvironment. As accessing the obstructed information trivialises state\nevaluations, one approach to tackle such problems is to estimate the value of\nthe imperfect state as a combination of all states in the information set,\ni.e., all possible states that are consistent with the current imperfect\ninformation. In this work, the goal is to learn a function that maps from the\nimperfect game information state to its expected value. However, constructing a\nperfect training set, i.e. an enumeration of the whole information set for\nnumerous imperfect states, is often infeasible. To compute the expected values\nfor an imperfect information game like \\textit{Reconnaissance Blind Chess}, one\nwould need to evaluate thousands of chess positions just to obtain the training\ntarget for a single state. Still, the expected value of a state can already be\napproximated with appropriate accuracy from a much smaller set of evaluations.\nThus, in this paper, we empirically investigate how a budget of perfect\ninformation game evaluations should be distributed among training samples to\nmaximise the return. Our results show that sampling a small number of states,\nin our experiments roughly 3, for a larger number of separate positions is\npreferable over repeatedly sampling a smaller quantity of states. Thus, we find\nthat in our case, the quantity of different samples seems to be more important\nthan higher target quality.\n","authors":["Timo Bertram","Johannes FÃ¼rnkranz","Martin MÃ¼ller"],"pdf_url":"https://arxiv.org/pdf/2407.05876v1.pdf","comment":"KI 2024 - 47th German Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2407.05872v1","updated":"2024-07-08T12:32:51Z","published":"2024-07-08T12:32:51Z","title":"Scaling Exponents Across Parameterizations and Optimizers","summary":"  Robust and effective scaling of models from small to large width typically\nrequires the precise adjustment of many algorithmic and architectural details,\nsuch as parameterization and optimizer choices. In this work, we propose a new\nperspective on parameterization by investigating a key assumption in prior work\nabout the alignment between parameters and data and derive new theoretical\nresults under weaker assumptions and a broader set of optimizers. Our extensive\nempirical investigation includes tens of thousands of models trained with all\ncombinations of three optimizers, four parameterizations, several alignment\nassumptions, more than a dozen learning rates, and fourteen model sizes up to\n26.8B parameters. We find that the best learning rate scaling prescription\nwould often have been excluded by the assumptions in prior work. Our results\nshow that all parameterizations, not just maximal update parameterization\n(muP), can achieve hyperparameter transfer; moreover, our novel per-layer\nlearning rate prescription for standard parameterization outperforms muP.\nFinally, we demonstrate that an overlooked aspect of parameterization, the\nepsilon parameter in Adam, must be scaled correctly to avoid gradient underflow\nand propose Adam-atan2, a new numerically stable, scale-invariant version of\nAdam that eliminates the epsilon hyperparameter entirely.\n","authors":["Katie Everett","Lechao Xiao","Mitchell Wortsman","Alexander A. Alemi","Roman Novak","Peter J. Liu","Izzeddin Gur","Jascha Sohl-Dickstein","Leslie Pack Kaelbling","Jaehoon Lee","Jeffrey Pennington"],"pdf_url":"https://arxiv.org/pdf/2407.05872v1.pdf","comment":"63 pages, International Conference on Machine Learning 2024"},{"id":"http://arxiv.org/abs/2407.05864v1","updated":"2024-07-08T12:29:29Z","published":"2024-07-08T12:29:29Z","title":"Neural Network-based Information Set Weighting for Playing\n  Reconnaissance Blind Chess","summary":"  In imperfect information games, the game state is generally not fully\nobservable to players. Therefore, good gameplay requires policies that deal\nwith the different information that is hidden from each player. To combat this,\neffective algorithms often reason about information sets; the sets of all\npossible game states that are consistent with a player's observations. While\nthere is no way to distinguish between the states within an information set,\nthis property does not imply that all states are equally likely to occur in\nplay. We extend previous research on assigning weights to the states in an\ninformation set in order to facilitate better gameplay in the imperfect\ninformation game of Reconnaissance Blind Chess. For this, we train two\ndifferent neural networks which estimate the likelihood of each state in an\ninformation set from historical game data. Experimentally, we find that a\nSiamese neural network is able to achieve higher accuracy and is more efficient\nthan a classical convolutional neural network for the given domain. Finally, we\nevaluate an RBC-playing agent that is based on the generated weightings and\ncompare different parameter settings that influence how strongly it should rely\non them. The resulting best player is ranked 5th on the public leaderboard.\n","authors":["Timo Bertram","Johannes FÃ¼rnkranz","Martin MÃ¼ller"],"pdf_url":"https://arxiv.org/pdf/2407.05864v1.pdf","comment":"Extended version of IEEE Conference on Games 2023 paper"},{"id":"http://arxiv.org/abs/2202.01602v4","updated":"2024-07-08T12:11:38Z","published":"2022-02-03T14:19:23Z","title":"The Disagreement Problem in Explainable Machine Learning: A\n  Practitioner's Perspective","summary":"  As various post hoc explanation methods are increasingly being leveraged to\nexplain complex models in high-stakes settings, it becomes critical to develop\na deeper understanding of if and when the explanations output by these methods\ndisagree with each other, and how such disagreements are resolved in practice.\nHowever, there is little to no research that provides answers to these critical\nquestions. In this work, we introduce and study the disagreement problem in\nexplainable machine learning. More specifically, we formalize the notion of\ndisagreement between explanations, analyze how often such disagreements occur\nin practice, and how practitioners resolve these disagreements. We first\nconduct interviews with data scientists to understand what constitutes\ndisagreement between explanations generated by different methods for the same\nmodel prediction and introduce a novel quantitative framework to formalize this\nunderstanding. We then leverage this framework to carry out a rigorous\nempirical analysis with four real-world datasets, six state-of-the-art post hoc\nexplanation methods, and six different predictive models, to measure the extent\nof disagreement between the explanations generated by various popular\nexplanation methods. In addition, we carry out an online user study with data\nscientists to understand how they resolve the aforementioned disagreements. Our\nresults indicate that (1) state-of-the-art explanation methods often disagree\nin terms of the explanations they output, and (2) machine learning\npractitioners often employ ad hoc heuristics when resolving such disagreements.\nThese findings suggest that practitioners may be relying on misleading\nexplanations when making consequential decisions. They also underscore the\nimportance of developing principled frameworks for effectively evaluating and\ncomparing explanations output by various explanation techniques.\n","authors":["Satyapriya Krishna","Tessa Han","Alex Gu","Steven Wu","Shahin Jabbari","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2202.01602v4.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2402.09132v4","updated":"2024-07-08T12:10:58Z","published":"2024-02-14T12:28:38Z","title":"Exploring the Adversarial Capabilities of Large Language Models","summary":"  The proliferation of large language models (LLMs) has sparked widespread and\ngeneral interest due to their strong language generation capabilities, offering\ngreat potential for both industry and research. While previous research delved\ninto the security and privacy issues of LLMs, the extent to which these models\ncan exhibit adversarial behavior remains largely unexplored. Addressing this\ngap, we investigate whether common publicly available LLMs have inherent\ncapabilities to perturb text samples to fool safety measures, so-called\nadversarial examples resp.~attacks. More specifically, we investigate whether\nLLMs are inherently able to craft adversarial examples out of benign samples to\nfool existing safe rails. Our experiments, which focus on hate speech\ndetection, reveal that LLMs succeed in finding adversarial perturbations,\neffectively undermining hate speech detection systems. Our findings carry\nsignificant implications for (semi-)autonomous systems relying on LLMs,\nhighlighting potential challenges in their interaction with existing systems\nand safety measures.\n","authors":["Lukas Struppek","Minh Hieu Le","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2402.09132v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06549v5","updated":"2024-07-08T12:05:50Z","published":"2023-10-10T11:51:12Z","title":"Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield\n  but Also a Catalyst for Model Inversion Attacks","summary":"  Label smoothing -- using softened labels instead of hard ones -- is a widely\nadopted regularization method for deep learning, showing diverse benefits such\nas enhanced generalization and calibration. Its implications for preserving\nmodel privacy, however, have remained unexplored. To fill this gap, we\ninvestigate the impact of label smoothing on model inversion attacks (MIAs),\nwhich aim to generate class-representative samples by exploiting the knowledge\nencoded in a classifier, thereby inferring sensitive information about its\ntraining data. Through extensive analyses, we uncover that traditional label\nsmoothing fosters MIAs, thereby increasing a model's privacy leakage. Even\nmore, we reveal that smoothing with negative factors counters this trend,\nimpeding the extraction of class-related information and leading to privacy\npreservation, beating state-of-the-art defenses. This establishes a practical\nand powerful novel way for enhancing model resilience against MIAs.\n","authors":["Lukas Struppek","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.06549v5.pdf","comment":"Published as a conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2407.05841v1","updated":"2024-07-08T11:38:49Z","published":"2024-07-08T11:38:49Z","title":"An Empirical Comparison of Vocabulary Expansion and Initialization\n  Approaches for Language Models","summary":"  Language Models (LMs) excel in natural language processing tasks for English\nbut show reduced performance in most other languages. This problem is commonly\ntackled by continually pre-training and fine-tuning these models for said\nlanguages. A significant issue in this process is the limited vocabulary\ncoverage in the original model's tokenizer, leading to inadequate\nrepresentation of new languages and necessitating an expansion of the\ntokenizer. The initialization of the embeddings corresponding to new vocabulary\nitems presents a further challenge. Current strategies require cross-lingual\nembeddings and lack a solid theoretical foundation as well as comparisons with\nstrong baselines. In this paper, we first establish theoretically that\ninitializing within the convex hull of existing embeddings is a good\ninitialization, followed by a novel but simple approach, Constrained Word2Vec\n(CW2V), which does not require cross-lingual embeddings. Our study evaluates\ndifferent initialization methods for expanding RoBERTa and LLaMA 2 across four\nlanguages and five tasks. The results show that CW2V performs equally well or\neven better than more advanced techniques. Additionally, simpler approaches\nlike multivariate initialization perform on par with these advanced methods\nindicating that efficient large-scale multilingual continued pretraining can be\nachieved even with simpler initialization methods.\n","authors":["Nandini Mundra","Aditya Nanda Kishore","Raj Dabre","Ratish Puduppully","Anoop Kunchukuttan","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2407.05841v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.05832v1","updated":"2024-07-08T11:25:30Z","published":"2024-07-08T11:25:30Z","title":"A Data-Driven Machine Learning Approach for Detecting Albedo Anomalies\n  on the Lunar Surface","summary":"  This study introduces a data-driven approach using machine learning (ML)\ntechniques to explore and predict albedo anomalies on the Moon's surface. The\nresearch leverages diverse planetary datasets, including\nhigh-spatial-resolution albedo maps and element maps (LPFe, LPK, LPTh, LPTi)\nderived from laser and gamma-ray measurements. The primary objective is to\nidentify relationships between chemical elements and albedo, thereby expanding\nour understanding of planetary surfaces and offering predictive capabilities\nfor areas with incomplete datasets. To bridge the gap in resolution between the\nalbedo and element maps, we employ Gaussian blurring techniques, including an\ninnovative adaptive Gaussian blur. Our methodology culminates in the deployment\nof an Extreme Gradient Boosting Regression Model, optimized to predict full\nalbedo based on elemental composition. Furthermore, we present an interactive\nanalytical tool to visualize prediction errors, delineating their spatial and\nchemical characteristics. The findings not only pave the way for a more\ncomprehensive understanding of the Moon's surface but also provide a framework\nfor similar studies on other celestial bodies.\n","authors":["Sofia Strukova","Sergei Gleyzer","Patrick Peplowski","Jason P. Terry"],"pdf_url":"https://arxiv.org/pdf/2407.05832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10040v2","updated":"2024-07-08T11:20:42Z","published":"2024-05-16T12:22:41Z","title":"SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation","summary":"  It is often desirable to distill the capabilities of large language models\n(LLMs) into smaller student models due to compute and memory constraints. One\nway to do this for classification tasks is via dataset synthesis, which can be\naccomplished by generating examples of each label from the LLM. Prior\napproaches to synthesis use few-shot prompting, which relies on the LLM's\nparametric knowledge to generate usable examples. However, this leads to issues\nof repetition, bias towards popular entities, and stylistic differences from\nhuman text. In this work, we propose Synthesize by Retrieval and Refinement\n(SynthesizRR), which uses retrieval augmentation to introduce variety into the\ndataset synthesis process: as retrieved passages vary, the LLM is seeded with\ndifferent content to generate its examples. We empirically study the synthesis\nof six datasets, covering topic classification, sentiment analysis, tone\ndetection, and humor, requiring complex synthesis strategies. We find that\nSynthesizRR greatly improves lexical and semantic diversity, similarity to\nhuman-written text, and distillation performance, when compared to 32-shot\nprompting and four prior approaches. We release our extensive codebase at\nhttps://github.com/amazon-science/synthesizrr\n","authors":["Abhishek Divekar","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2405.10040v2.pdf","comment":"Code available at https://github.com/amazon-science/synthesizrr"},{"id":"http://arxiv.org/abs/2309.10301v2","updated":"2024-07-08T11:11:51Z","published":"2023-09-19T04:04:59Z","title":"Prominent Roles of Conditionally Invariant Components in Domain\n  Adaptation: Theory and Algorithms","summary":"  Domain adaptation (DA) is a statistical learning problem that arises when the\ndistribution of the source data used to train a model differs from that of the\ntarget data used to evaluate the model. While many DA algorithms have\ndemonstrated considerable empirical success, blindly applying these algorithms\ncan often lead to worse performance on new datasets. To address this, it is\ncrucial to clarify the assumptions under which a DA algorithm has good target\nperformance. In this work, we focus on the assumption of the presence of\nconditionally invariant components (CICs), which are relevant for prediction\nand remain conditionally invariant across the source and target data. We\ndemonstrate that CICs, which can be estimated through conditional invariant\npenalty (CIP), play three prominent roles in providing target risk guarantees\nin DA. First, we propose a new algorithm based on CICs, importance-weighted\nconditional invariant penalty (IW-CIP), which has target risk guarantees beyond\nsimple settings such as covariate shift and label shift. Second, we show that\nCICs help identify large discrepancies between source and target risks of other\nDA algorithms. Finally, we demonstrate that incorporating CICs into the domain\ninvariant projection (DIP) algorithm can address its failure scenario caused by\nlabel-flipping features. We support our new algorithms and theoretical findings\nvia numerical experiments on synthetic data, MNIST, CelebA, Camelyon17, and\nDomainNet datasets.\n","authors":["Keru Wu","Yuansi Chen","Wooseok Ha","Bin Yu"],"pdf_url":"https://arxiv.org/pdf/2309.10301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06322v3","updated":"2024-07-08T11:02:47Z","published":"2023-11-10T09:10:09Z","title":"Post-training Quantization for Text-to-Image Diffusion Models with\n  Progressive Calibration and Activation Relaxing","summary":"  High computational overhead is a troublesome problem for diffusion models.\nRecent studies have leveraged post-training quantization (PTQ) to compress\ndiffusion models. However, most of them only focus on unconditional models,\nleaving the quantization of widely-used pretrained text-to-image models, e.g.,\nStable Diffusion, largely unexplored. In this paper, we propose a novel\npost-training quantization method PCR (Progressive Calibration and Relaxing)\nfor text-to-image diffusion models, which consists of a progressive calibration\nstrategy that considers the accumulated quantization error across timesteps,\nand an activation relaxing strategy that improves the performance with\nnegligible cost. Additionally, we demonstrate the previous metrics for\ntext-to-image diffusion model quantization are not accurate due to the\ndistribution gap. To tackle the problem, we propose a novel QDiffBench\nbenchmark, which utilizes data in the same domain for more accurate evaluation.\nBesides, QDiffBench also considers the generalization performance of the\nquantized model outside the calibration dataset. Extensive experiments on\nStable Diffusion and Stable Diffusion XL demonstrate the superiority of our\nmethod and benchmark. Moreover, we are the first to achieve quantization for\nStable Diffusion XL while maintaining the performance.\n","authors":["Siao Tang","Xin Wang","Hong Chen","Chaoyu Guan","Zewen Wu","Yansong Tang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.06322v3.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2404.01964v2","updated":"2024-07-08T11:00:51Z","published":"2024-04-02T13:57:30Z","title":"CAM-Based Methods Can See through Walls","summary":"  CAM-based methods are widely-used post-hoc interpretability method that\nproduce a saliency map to explain the decision of an image classification\nmodel. The saliency map highlights the important areas of the image relevant to\nthe prediction. In this paper, we show that most of these methods can\nincorrectly attribute an important score to parts of the image that the model\ncannot see. We show that this phenomenon occurs both theoretically and\nexperimentally. On the theory side, we analyze the behavior of GradCAM on a\nsimple masked CNN model at initialization. Experimentally, we train a VGG-like\nmodel constrained to not use the lower part of the image and nevertheless\nobserve positive scores in the unseen part of the image. This behavior is\nevaluated quantitatively on two new datasets. We believe that this is\nproblematic, potentially leading to mis-interpretation of the model's behavior.\n","authors":["Magamed Taimeskhanov","Ronan Sicre","Damien Garreau"],"pdf_url":"https://arxiv.org/pdf/2404.01964v2.pdf","comment":"Accepted for publication at ECML 2024 (28 pages, 9 figures)"},{"id":"http://arxiv.org/abs/2207.01789v2","updated":"2024-07-08T10:58:33Z","published":"2022-07-05T03:18:17Z","title":"Improved Global Guarantees for the Nonconvex Burer--Monteiro\n  Factorization via Rank Overparameterization","summary":"  We consider minimizing a twice-differentiable, $L$-smooth, and $\\mu$-strongly\nconvex objective $\\phi$ over an $n\\times n$ positive semidefinite matrix\n$M\\succeq0$, under the assumption that the minimizer $M^{\\star}$ has low rank\n$r^{\\star}\\ll n$. Following the Burer--Monteiro approach, we instead minimize\nthe nonconvex objective $f(X)=\\phi(XX^{T})$ over a factor matrix $X$ of size\n$n\\times r$. This substantially reduces the number of variables from $O(n^{2})$\nto as few as $O(n)$ and also enforces positive semidefiniteness for free, but\nat the cost of giving up the convexity of the original problem. In this paper,\nwe prove that if the search rank $r\\ge r^{\\star}$ is overparameterized by a\n\\emph{constant factor} with respect to the true rank $r^{\\star}$, namely as in\n$r>\\frac{1}{4}(L/\\mu-1)^{2}r^{\\star}$, then despite nonconvexity, local\noptimization is guaranteed to globally converge from any initial point to the\nglobal optimum. This significantly improves upon a previous rank\noverparameterization threshold of $r\\ge n$, which we show is sharp in the\nabsence of smoothness and strong convexity, but would increase the number of\nvariables back up to $O(n^{2})$. Conversely, without rank overparameterization,\nwe prove that such a global guarantee is possible if and only if $\\phi$ is\nalmost perfectly conditioned, with a condition number of $L/\\mu<3$. Therefore,\nwe conclude that a small amount of overparameterization can lead to large\nimprovements in theoretical guarantees for the nonconvex Burer--Monteiro\nfactorization.\n","authors":["Richard Y. Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.01789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05816v1","updated":"2024-07-08T10:53:49Z","published":"2024-07-08T10:53:49Z","title":"Graph Reasoning Networks","summary":"  Graph neural networks (GNNs) are the predominant approach for graph-based\nmachine learning. While neural networks have shown great performance at\nlearning useful representations, they are often criticized for their limited\nhigh-level reasoning abilities. In this work, we present Graph Reasoning\nNetworks (GRNs), a novel approach to combine the strengths of fixed and learned\ngraph representations and a reasoning module based on a differentiable\nsatisfiability solver. While results on real-world datasets show comparable\nperformance to GNN, experiments on synthetic datasets demonstrate the potential\nof the newly proposed method.\n","authors":["Markus Zopf","Francesco Alesiani"],"pdf_url":"https://arxiv.org/pdf/2407.05816v1.pdf","comment":"Presented at the workshop on graphs and more complex structures for\n  learning and reasoning at AAAI 2022"},{"id":"http://arxiv.org/abs/2209.14272v3","updated":"2024-07-08T10:50:56Z","published":"2022-09-28T17:36:47Z","title":"Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and\n  First Results","summary":"  Humor is a substantial element of human social behavior, affect, and\ncognition. Its automatic understanding can facilitate a more naturalistic\nhuman-AI interaction. Current methods of humor detection have been exclusively\nbased on staged data, making them inadequate for \"real-world\" applications. We\ncontribute to addressing this deficiency by introducing the novel\nPassau-Spontaneous Football Coach Humor (Passau-SFCH) dataset, comprising about\n11 hours of recordings. The Passau-SFCH dataset is annotated for the presence\nof humor and its dimensions (sentiment and direction) as proposed in Martin's\nHumor Style Questionnaire. We conduct a series of experiments employing\npretrained Transformers, convolutional neural networks, and expert-designed\nfeatures. The performance of each modality (text, audio, video) for spontaneous\nhumor recognition is analyzed and their complementarity is investigated. Our\nfindings suggest that for the automatic analysis of humor and its sentiment,\nfacial expressions are most promising, while humor direction can be best\nmodeled via text-based features. Further, we experiment with different\nmultimodal approaches to humor recognition, including decision-level fusion and\nMulT, a multimodal Transformer approach. In this context, we propose a novel\nmultimodal architecture that yields the best overall results. Finally, we make\nour code publicly available at https://www.github.com/lc0197/passau-sfch. The\nPassau-SFCH dataset is available upon request.\n","authors":["Lukas Christ","Shahin Amiriparian","Alexander Kathan","Niklas MÃ¼ller","Andreas KÃ¶nig","BjÃ¶rn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2209.14272v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible (Major Revision)"},{"id":"http://arxiv.org/abs/2407.05800v1","updated":"2024-07-08T10:10:07Z","published":"2024-07-08T10:10:07Z","title":"FedMRL: Data Heterogeneity Aware Federated Multi-agent Deep\n  Reinforcement Learning for Medical Imaging","summary":"  Despite recent advancements in federated learning (FL) for medical image\ndiagnosis, addressing data heterogeneity among clients remains a significant\nchallenge for practical implementation. A primary hurdle in FL arises from the\nnon-IID nature of data samples across clients, which typically results in a\ndecline in the performance of the aggregated global model. In this study, we\nintroduce FedMRL, a novel federated multi-agent deep reinforcement learning\nframework designed to address data heterogeneity. FedMRL incorporates a novel\nloss function to facilitate fairness among clients, preventing bias in the\nfinal global model. Additionally, it employs a multi-agent reinforcement\nlearning (MARL) approach to calculate the proximal term $(\\mu)$ for the\npersonalized local objective function, ensuring convergence to the global\noptimum. Furthermore, FedMRL integrates an adaptive weight adjustment method\nusing a Self-organizing map (SOM) on the server side to counteract distribution\nshifts among clients' local data distributions. We assess our approach using\ntwo publicly available real-world medical datasets, and the results demonstrate\nthat FedMRL significantly outperforms state-of-the-art techniques, showing its\nefficacy in addressing data heterogeneity in federated learning. The code can\nbe found here~{\\url{https://github.com/Pranabiitp/FedMRL}}.\n","authors":["Pranab Sahoo","Ashutosh Tripathi","Sriparna Saha","Samrat Mondal"],"pdf_url":"https://arxiv.org/pdf/2407.05800v1.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.05793v1","updated":"2024-07-08T09:55:31Z","published":"2024-07-08T09:55:31Z","title":"A Primal-Dual Online Learning Approach for Dynamic Pricing of\n  Sequentially Displayed Complementary Items under Sale Constraints","summary":"  We address the challenging problem of dynamically pricing complementary items\nthat are sequentially displayed to customers. An illustrative example is the\nonline sale of flight tickets, where customers navigate through multiple web\npages. Initially, they view the ticket cost, followed by ancillary expenses\nsuch as insurance and additional luggage fees. Coherent pricing policies for\ncomplementary items are essential because optimizing the pricing of each item\nindividually is ineffective. Our scenario also involves a sales constraint,\nwhich specifies a minimum number of items to sell, and uncertainty regarding\ncustomer demand curves. To tackle this problem, we originally formulate it as a\nMarkov Decision Process with constraints. Leveraging online learning tools, we\ndesign a primal-dual online optimization algorithm. We empirically evaluate our\napproach using synthetic settings randomly generated from real-world data,\ncovering various configurations from stationary to non-stationary, and compare\nits performance in terms of constraints violation and regret against well-known\nbaselines optimizing each state singularly.\n","authors":["Francesco Emanuele Stradi","Filippo Cipriani","Lorenzo Ciampiconi","Marco Leonardi","Alessandro Rozza","Nicola Gatti"],"pdf_url":"https://arxiv.org/pdf/2407.05793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12325v3","updated":"2024-07-08T09:54:09Z","published":"2023-08-11T10:49:05Z","title":"FUTURE-AI: International consensus guideline for trustworthy and\n  deployable artificial intelligence in healthcare","summary":"  Despite major advances in artificial intelligence (AI) for medicine and\nhealthcare, the deployment and adoption of AI technologies remain limited in\nreal-world clinical practice. In recent years, concerns have been raised about\nthe technical, clinical, ethical and legal risks associated with medical AI. To\nincrease real world adoption, it is essential that medical AI tools are trusted\nand accepted by patients, clinicians, health organisations and authorities.\nThis work describes the FUTURE-AI guideline as the first international\nconsensus framework for guiding the development and deployment of trustworthy\nAI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and\ncurrently comprises 118 inter-disciplinary experts from 51 countries\nrepresenting all continents, including AI scientists, clinicians, ethicists,\nand social scientists. Over a two-year period, the consortium defined guiding\nprinciples and best practices for trustworthy AI through an iterative process\ncomprising an in-depth literature review, a modified Delphi survey, and online\nconsensus meetings. The FUTURE-AI framework was established based on 6 guiding\nprinciples for trustworthy AI in healthcare, i.e. Fairness, Universality,\nTraceability, Usability, Robustness and Explainability. Through consensus, a\nset of 28 best practices were defined, addressing technical, clinical, legal\nand socio-ethical dimensions. The recommendations cover the entire lifecycle of\nmedical AI, from design, development and validation to regulation, deployment,\nand monitoring. FUTURE-AI is a risk-informed, assumption-free guideline which\nprovides a structured approach for constructing medical AI tools that will be\ntrusted, deployed and adopted in real-world practice. Researchers are\nencouraged to take the recommendations into account in proof-of-concept stages\nto facilitate future translation towards clinical practice of medical AI.\n","authors":["Karim Lekadir","Aasa Feragen","Abdul Joseph Fofanah","Alejandro F Frangi","Alena Buyx","Anais Emelie","Andrea Lara","Antonio R Porras","An-Wen Chan","Arcadi Navarro","Ben Glocker","Benard O Botwe","Bishesh Khanal","Brigit Beger","Carol C Wu","Celia Cintas","Curtis P Langlotz","Daniel Rueckert","Deogratias Mzurikwao","Dimitrios I Fotiadis","Doszhan Zhussupov","Enzo Ferrante","Erik Meijering","Eva Weicken","Fabio A GonzÃ¡lez","Folkert W Asselbergs","Fred Prior","Gabriel P Krestin","Gary Collins","Geletaw S Tegenaw","Georgios Kaissis","Gianluca Misuraca","Gianna Tsakou","Girish Dwivedi","Haridimos Kondylakis","Harsha Jayakody","Henry C Woodruf","Horst Joachim Mayer","Hugo JWL Aerts","Ian Walsh","Ioanna Chouvarda","IrÃ¨ne Buvat","Isabell Tributsch","Islem Rekik","James Duncan","Jayashree Kalpathy-Cramer","Jihad Zahir","Jinah Park","John Mongan","Judy W Gichoya","Julia A Schnabel","Kaisar Kushibar","Katrine Riklund","Kensaku Mori","Kostas Marias","Lameck M Amugongo","Lauren A Fromont","Lena Maier-Hein","Leonor CerdÃ¡ Alberich","Leticia Rittner","Lighton Phiri","Linda Marrakchi-Kacem","LluÃ­s Donoso-Bach","Luis MartÃ­-BonmatÃ­","M Jorge Cardoso","Maciej Bobowicz","Mahsa Shabani","Manolis Tsiknakis","Maria A Zuluaga","Maria Bielikova","Marie-Christine Fritzsche","Marina Camacho","Marius George Linguraru","Markus Wenzel","Marleen De Bruijne","Martin G Tolsgaard","Marzyeh Ghassemi","Md Ashrafuzzaman","Melanie Goisauf","Mohammad Yaqub","MÃ³nica Cano AbadÃ­a","Mukhtar M E Mahmoud","Mustafa Elattar","Nicola Rieke","Nikolaos Papanikolaou","Noussair Lazrak","Oliver DÃ­az","Olivier Salvado","Oriol Pujol","Ousmane Sall","Pamela Guevara","Peter Gordebeke","Philippe Lambin","Pieta Brown","Purang Abolmaesumi","Qi Dou","Qinghua Lu","Richard Osuala","Rose Nakasi","S Kevin Zhou","Sandy Napel","Sara Colantonio","Shadi Albarqouni","Smriti Joshi","Stacy Carter","Stefan Klein","Steffen E Petersen","Susanna AussÃ³","Suyash Awate","Tammy Riklin Raviv","Tessa Cook","Tinashe E M Mutsvangwa","Wendy A Rogers","Wiro J Niessen","XÃ¨nia Puig-Bosch","Yi Zeng","Yunusa G Mohammed","Yves Saint James Aquino","Zohaib Salahuddin","Martijn P A Starmans"],"pdf_url":"https://arxiv.org/pdf/2309.12325v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05789v1","updated":"2024-07-08T09:51:02Z","published":"2024-07-08T09:51:02Z","title":"CANDID DAC: Leveraging Coupled Action Dimensions with Importance\n  Differences in DAC","summary":"  High-dimensional action spaces remain a challenge for dynamic algorithm\nconfiguration (DAC). Interdependencies and varying importance between action\ndimensions are further known key characteristics of DAC problems. We argue that\nthese Coupled Action Dimensions with Importance Differences (CANDID) represent\naspects of the DAC problem that are not yet fully explored. To address this\ngap, we introduce a new white-box benchmark within the DACBench suite that\nsimulates the properties of CANDID. Further, we propose sequential policies as\nan effective strategy for managing these properties. Such policies factorize\nthe action space and mitigate exponential growth by learning a policy per\naction dimension. At the same time, these policies accommodate the\ninterdependence of action dimensions by fostering implicit coordination. We\nshow this in an experimental study of value-based policies on our new\nbenchmark. This study demonstrates that sequential policies significantly\noutperform independent learning of factorized policies in CANDID action spaces.\nIn addition, they overcome the scalability limitations associated with learning\na single policy across all action dimensions. The code used for our experiments\nis available under https://github.com/PhilippBordne/candidDAC.\n","authors":["Philipp Bordne","M. Asif Hasan","Eddie Bergman","Noor Awad","AndrÃ© Biedenkapp"],"pdf_url":"https://arxiv.org/pdf/2407.05789v1.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.05788v1","updated":"2024-07-08T09:49:38Z","published":"2024-07-08T09:49:38Z","title":"Automated Computational Energy Minimization of ML Algorithms using\n  Constrained Bayesian Optimization","summary":"  Bayesian optimization (BO) is an efficient framework for optimization of\nblack-box objectives when function evaluations are costly and gradient\ninformation is not easily accessible. BO has been successfully applied to\nautomate the task of hyperparameter optimization (HPO) in machine learning (ML)\nmodels with the primary objective of optimizing predictive performance on\nheld-out data. In recent years, however, with ever-growing model sizes, the\nenergy cost associated with model training has become an important factor for\nML applications. Here we evaluate Constrained Bayesian Optimization (CBO) with\nthe primary objective of minimizing energy consumption and subject to the\nconstraint that the generalization performance is above some threshold. We\nevaluate our approach on regression and classification tasks and demonstrate\nthat CBO achieves lower energy consumption without compromising the predictive\nperformance of ML models.\n","authors":["Pallavi Mitra","Felix Biessmann"],"pdf_url":"https://arxiv.org/pdf/2407.05788v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2401.11849v2","updated":"2024-07-08T09:47:59Z","published":"2024-01-22T11:08:36Z","title":"Self-Labeling the Job Shop Scheduling Problem","summary":"  In this work, we propose a Self-Supervised training strategy specifically\ndesigned for combinatorial problems. One of the main obstacles in applying\nsupervised paradigms to such problems is the requirement of expensive target\nsolutions as ground-truth, often produced with costly exact solvers. Inspired\nby Semi- and Self-Supervised learning, we show that it is possible to easily\ntrain generative models by sampling multiple solutions and using the best one\naccording to the problem objective as a pseudo-label. In this way, we\niteratively improve the model generation capability by relying only on its\nself-supervision, completely removing the need for optimality information. We\nprove the effectiveness of this Self-Labeling strategy on the Job Shop\nScheduling (JSP), a complex combinatorial problem that is receiving much\nattention from the Reinforcement Learning community. We propose a generative\nmodel based on the well-known Pointer Network and train it with our strategy.\nExperiments on popular benchmarks demonstrate the potential of this approach as\nthe resulting models outperform constructive heuristics and current\nstate-of-the-art learning proposals for the JSP.\n","authors":["Andrea Corsini","Angelo Porrello","Simone Calderara","Mauro Dell'Amico"],"pdf_url":"https://arxiv.org/pdf/2401.11849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05782v1","updated":"2024-07-08T09:45:20Z","published":"2024-07-08T09:45:20Z","title":"Sequential Contrastive Audio-Visual Learning","summary":"  Contrastive learning has emerged as a powerful technique in audio-visual\nrepresentation learning, leveraging the natural co-occurrence of audio and\nvisual modalities in extensive web-scale video datasets to achieve significant\nadvancements. However, conventional contrastive audio-visual learning\nmethodologies often rely on aggregated representations derived through temporal\naggregation, which neglects the intrinsic sequential nature of the data. This\noversight raises concerns regarding the ability of standard approaches to\ncapture and utilize fine-grained information within sequences, information that\nis vital for distinguishing between semantically similar yet distinct examples.\nIn response to this limitation, we propose sequential contrastive audio-visual\nlearning (SCAV), which contrasts examples based on their non-aggregated\nrepresentation space using sequential distances. Retrieval experiments with the\nVGGSound and Music datasets demonstrate the effectiveness of SCAV, showing 2-3x\nrelative improvements against traditional aggregation-based contrastive\nlearning and other methods from the literature. We also show that models\ntrained with SCAV exhibit a high degree of flexibility regarding the metric\nemployed for retrieval, allowing them to operate on a spectrum of\nefficiency-accuracy trade-offs, potentially making them applicable in multiple\nscenarios, from small- to large-scale retrieval.\n","authors":["Ioannis Tsiamas","Santiago Pascual","Chunghsin Yeh","Joan SerrÃ "],"pdf_url":"https://arxiv.org/pdf/2407.05782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05781v1","updated":"2024-07-08T09:41:42Z","published":"2024-07-08T09:41:42Z","title":"Regret Analysis of Multi-task Representation Learning for\n  Linear-Quadratic Adaptive Control","summary":"  Representation learning is a powerful tool that enables learning over large\nmultitudes of agents or domains by enforcing that all agents operate on a\nshared set of learned features. However, many robotics or controls applications\nthat would benefit from collaboration operate in settings with changing\nenvironments and goals, whereas most guarantees for representation learning are\nstated for static settings. Toward rigorously establishing the benefit of\nrepresentation learning in dynamic settings, we analyze the regret of\nmulti-task representation learning for linear-quadratic control. This setting\nintroduces unique challenges. Firstly, we must account for and balance the\n$\\textit{misspecification}$ introduced by an approximate representation.\nSecondly, we cannot rely on the parameter update schemes of single-task online\nLQR, for which least-squares often suffices, and must devise a novel scheme to\nensure sufficient improvement. We demonstrate that for settings where\nexploration is \"benign\", the regret of any agent after $T$ timesteps scales as\n$\\tilde O(\\sqrt{T/H})$, where $H$ is the number of agents. In settings with\n\"difficult\" exploration, the regret scales as $\\tilde{\\mathcal O}(\\sqrt{d_u\nd_\\theta} \\sqrt{T} + T^{3/4}/H^{1/5})$, where $d_x$ is the state-space\ndimension, $d_u$ is the input dimension, and $d_\\theta$ is the task-specific\nparameter count. In both cases, by comparing to the minimax single-task regret\n$\\tilde{\\mathcal O}(\\sqrt{d_x d_u^2}\\sqrt{T})$, we see a benefit of a large\nnumber of agents. Notably, in the difficult exploration case, by sharing a\nrepresentation across tasks, the effective task-specific parameter count can\noften be small $d_\\theta < d_x d_u$. Lastly, we provide numerical validation of\nthe trends we predict.\n","authors":["Bruce D. Lee","Leonardo F. Toso","Thomas T. Zhang","James Anderson","Nikolai Matni"],"pdf_url":"https://arxiv.org/pdf/2407.05781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10003v2","updated":"2024-07-08T09:40:03Z","published":"2023-07-19T14:23:26Z","title":"TbExplain: A Text-based Explanation Method for Scene Classification\n  Models with the Statistical Prediction Correction","summary":"  The field of Explainable Artificial Intelligence (XAI) aims to improve the\ninterpretability of black-box machine learning models. Building a heatmap based\non the importance value of input features is a popular method for explaining\nthe underlying functions of such models in producing their predictions.\nHeatmaps are almost understandable to humans, yet they are not without flaws.\nNon-expert users, for example, may not fully understand the logic of heatmaps\n(the logic in which relevant pixels to the model's prediction are highlighted\nwith different intensities or colors). Additionally, objects and regions of the\ninput image that are relevant to the model prediction are frequently not\nentirely differentiated by heatmaps. In this paper, we propose a framework\ncalled TbExplain that employs XAI techniques and a pre-trained object detector\nto present text-based explanations of scene classification models. Moreover,\nTbExplain incorporates a novel method to correct predictions and textually\nexplain them based on the statistics of objects in the input image when the\ninitial prediction is unreliable. To assess the trustworthiness and validity of\nthe text-based explanations, we conducted a qualitative experiment, and the\nfindings indicated that these explanations are sufficiently reliable.\nFurthermore, our quantitative and qualitative experiments on TbExplain with\nscene classification datasets reveal an improvement in classification accuracy\nover ResNet variants.\n","authors":["Amirhossein Aminimehr","Pouya Khani","Amirali Molaei","Amirmohammad Kazemeini","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2307.10003v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07965v2","updated":"2024-07-08T09:21:00Z","published":"2024-03-12T11:56:38Z","title":"Conditional computation in neural networks: principles and research\n  trends","summary":"  This article summarizes principles and ideas from the emerging area of\napplying \\textit{conditional computation} methods to the design of neural\nnetworks. In particular, we focus on neural networks that can dynamically\nactivate or de-activate parts of their computational graph conditionally on\ntheir input. Examples include the dynamic selection of, e.g., input tokens,\nlayers (or sets of layers), and sub-modules inside each layer (e.g., channels\nin a convolutional filter). We first provide a general formalism to describe\nthese techniques in an uniform way. Then, we introduce three notable\nimplementations of these principles: mixture-of-experts (MoEs) networks, token\nselection mechanisms, and early-exit neural networks. The paper aims to provide\na tutorial-like introduction to this growing field. To this end, we analyze the\nbenefits of these modular designs in terms of efficiency, explainability, and\ntransfer learning, with a focus on emerging applicative areas ranging from\nautomated scientific discovery to semantic communication.\n","authors":["Simone Scardapane","Alessandro Baiocchi","Alessio Devoto","Valerio Marsocci","Pasquale Minervini","Jary Pomponi"],"pdf_url":"https://arxiv.org/pdf/2403.07965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19015v2","updated":"2024-07-08T09:07:51Z","published":"2024-06-27T09:00:05Z","title":"Lithium-Ion Battery System Health Monitoring and Fault Analysis from\n  Field Data Using Gaussian Processes","summary":"  Health monitoring, fault analysis, and detection are critical for the safe\nand sustainable operation of battery systems. We apply Gaussian process\nresistance models on lithium iron phosphate battery field data to effectively\nseparate the time-dependent and operating point-dependent resistance. The data\nset contains 29 battery systems returned to the manufacturer for warranty, each\nwith eight cells in series, totaling 232 cells and 131 million data rows. We\ndevelop probabilistic fault detection rules using recursive spatiotemporal\nGaussian processes. These processes allow the quick processing of over a\nmillion data points, enabling advanced online monitoring and furthering the\nunderstanding of battery pack failure in the field. The analysis underlines\nthat often, only a single cell shows abnormal behavior or a knee point,\nconsistent with weakest-link failure for cells connected in series, amplified\nby local resistive heating. The results further the understanding of how\nbatteries degrade and fail in the field and demonstrate the potential of\nefficient online monitoring based on data. We open-source the code and publish\nthe large data set upon completion of the review of this article.\n","authors":["Joachim Schaeffer","Eric Lenz","Duncan Gulla","Martin Z. Bazant","Richard D. Braatz","Rolf Findeisen"],"pdf_url":"https://arxiv.org/pdf/2406.19015v2.pdf","comment":"The supplementary information is only available in v1 due to its\n  large size; v2 and v1 are otherwise identical"},{"id":"http://arxiv.org/abs/2402.04836v2","updated":"2024-07-08T08:57:35Z","published":"2024-02-07T13:32:53Z","title":"On the Completeness of Invariant Geometric Deep Learning Models","summary":"  Invariant models, one important class of geometric deep learning models, are\ncapable of generating meaningful geometric representations by leveraging\ninformative geometric features in point clouds. These models are characterized\nby their simplicity, good experimental results and computational efficiency.\nHowever, their theoretical expressive power still remains unclear, restricting\na deeper understanding of the potential of such models. In this work, we\nconcentrate on characterizing the theoretical expressiveness of a wide range of\ninvariant models. We first rigorously bound the expressiveness of the most\nclassic invariant model, message-passing neural networks incorporating distance\n(DisGNN), restricting its unidentifiable cases to be only highly symmetric\npoint clouds. We then show that GeoNGNN, the geometric counterpart of one of\nthe simplest subgraph graph neural networks (subgraph GNNs), can effectively\nbreak these corner cases' symmetry and thus achieve E(3)-completeness. By\nleveraging GeoNGNN as a theoretical tool, we further prove that: 1) most\nsubgraph GNNs developed in traditional graph learning can be seamlessly\nextended to geometric scenarios with E(3)-completeness; 2) DimeNet, GemNet and\nSphereNet, three well-established invariant models, are also all capable of\nachieving E(3)-completeness. Our theoretical results fill the gap in the\ntheoretical power of invariant models, contributing to a rigorous and\ncomprehensive understanding of their capabilities. We also empirically\nevaluated GeoNGNN, the simplest model within the large E(3)-complete family we\nestablished, which achieves competitive results to models relying on high-order\ninvariant/equivariant representations on molecule-relevant tasks.\n","authors":["Zian Li","Xiyuan Wang","Shijia Kang","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.04836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05749v1","updated":"2024-07-08T08:55:25Z","published":"2024-07-08T08:55:25Z","title":"LDGCN: An Edge-End Lightweight Dual GCN Based on Single-Channel EEG for\n  Driver Drowsiness Monitoring","summary":"  Driver drowsiness electroencephalography (EEG) signal monitoring can timely\nalert drivers of their drowsiness status, thereby reducing the probability of\ntraffic accidents. Graph convolutional networks (GCNs) have shown significant\nadvancements in processing the non-stationary, time-varying, and non-Euclidean\nnature of EEG signals. However, the existing single-channel EEG adjacency graph\nconstruction process lacks interpretability, which hinders the ability of GCNs\nto effectively extract adjacency graph features, thus affecting the performance\nof drowsiness monitoring. To address this issue, we propose an edge-end\nlightweight dual graph convolutional network (LDGCN). Specifically, we are the\nfirst to incorporate neurophysiological knowledge to design a Baseline\nDrowsiness Status Adjacency Graph (BDSAG), which characterizes driver\ndrowsiness status. Additionally, to express more features within limited EEG\ndata, we introduce the Augmented Graph-level Module (AGM). This module captures\nglobal and local information at the graph level, ensuring that BDSAG features\nremain intact while enhancing effective feature expression capability.\nFurthermore, to deploy our method on the fourth-generation Raspberry Pi, we\nutilize Adaptive Pruning Optimization (APO) on both channels and neurons,\nreducing inference latency by almost half. Experiments on benchmark datasets\ndemonstrate that LDGCN offers the best trade-off between monitoring performance\nand hardware resource utilization compared to existing state-of-the-art\nalgorithms. All our source code can be found at\nhttps://github.com/BryantDom/Driver-Drowsiness-Monitoring.\n","authors":["Jingwei Huang","Chuansheng Wang","Jiayan Huang","Haoyi Fan","Antoni Grau","Fuquan Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.05749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05732v1","updated":"2024-07-08T08:36:44Z","published":"2024-07-08T08:36:44Z","title":"FairPFN: Transformers Can do Counterfactual Fairness","summary":"  Machine Learning systems are increasingly prevalent across healthcare, law\nenforcement, and finance but often operate on historical data, which may carry\nbiases against certain demographic groups. Causal and counterfactual fairness\nprovides an intuitive way to define fairness that closely aligns with legal\nstandards. Despite its theoretical benefits, counterfactual fairness comes with\nseveral practical limitations, largely related to the reliance on domain\nknowledge and approximate causal discovery techniques in constructing a causal\nmodel. In this study, we take a fresh perspective on counterfactually fair\nprediction, building upon recent work in in context learning (ICL) and prior\nfitted networks (PFNs) to learn a transformer called FairPFN. This model is\npretrained using synthetic fairness data to eliminate the causal effects of\nprotected attributes directly from observational data, removing the requirement\nof access to the correct causal model in practice. In our experiments, we\nthoroughly assess the effectiveness of FairPFN in eliminating the causal impact\nof protected attributes on a series of synthetic case studies and real world\ndatasets. Our findings pave the way for a new and promising research area:\ntransformers for causal and counterfactual fairness.\n","authors":["Jake Robertson","Noah Hollmann","Noor Awad","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2407.05732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19670v2","updated":"2024-07-08T08:28:34Z","published":"2024-06-28T05:44:47Z","title":"Function+Data Flow: A Framework to Specify Machine Learning Pipelines\n  for Digital Twinning","summary":"  The development of digital twins (DTs) for physical systems increasingly\nleverages artificial intelligence (AI), particularly for combining data from\ndifferent sources or for creating computationally efficient, reduced-dimension\nmodels. Indeed, even in very different application domains, twinning employs\ncommon techniques such as model order reduction and modelization with hybrid\ndata (that is, data sourced from both physics-based models and sensors).\nDespite this apparent generality, current development practices are ad-hoc,\nmaking the design of AI pipelines for digital twinning complex and\ntime-consuming. Here we propose Function+Data Flow (FDF), a domain-specific\nlanguage (DSL) to describe AI pipelines within DTs. FDF aims to facilitate the\ndesign and validation of digital twins. Specifically, FDF treats functions as\nfirst-class citizens, enabling effective manipulation of models learned with\nAI. We illustrate the benefits of FDF on two concrete use cases from different\ndomains: predicting the plastic strain of a structure and modeling the\nelectromagnetic behavior of a bearing.\n","authors":["Eduardo de Conto","Blaise Genest","Arvind Easwaran"],"pdf_url":"https://arxiv.org/pdf/2406.19670v2.pdf","comment":"9 pages, 10 figures, to be published in AIware'24"},{"id":"http://arxiv.org/abs/2407.03118v2","updated":"2024-07-08T08:07:35Z","published":"2024-07-03T14:04:05Z","title":"Can machine learning solve the challenge of adaptive learning and the\n  individualization of learning paths? A field experiment in an online learning\n  platform","summary":"  The individualization of learning contents based on digital technologies\npromises large individual and social benefits. However, it remains an open\nquestion how this individualization can be implemented. To tackle this question\nwe conduct a randomized controlled trial on a large digital self-learning\nplatform. We develop an algorithm based on two convolutional neural networks\nthat assigns tasks to $4,365$ learners according to their learning paths.\nLearners are randomized into three groups: two treatment groups -- a\ngroup-based adaptive treatment group and an individual adaptive treatment group\n-- and one control group. We analyze the difference between the three groups\nwith respect to effort learners provide and their performance on the platform.\nOur null results shed light on the multiple challenges associated with the\nindividualization of learning paths.\n","authors":["Marius KÃ¶ppel","Tim Klausmann","Isabell Zipperle","Daniel Schunk"],"pdf_url":"https://arxiv.org/pdf/2407.03118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05704v1","updated":"2024-07-08T08:06:45Z","published":"2024-07-08T08:06:45Z","title":"Narrowing the Gap between Adversarial and Stochastic MDPs via Policy\n  Optimization","summary":"  In this paper, we consider the problem of learning in adversarial Markov\ndecision processes [MDPs] with an oblivious adversary in a full-information\nsetting. The agent interacts with an environment during $T$ episodes, each of\nwhich consists of $H$ stages, and each episode is evaluated with respect to a\nreward function that will be revealed only at the end of the episode. We\npropose an algorithm, called APO-MVP, that achieves a regret bound of order\n$\\tilde{\\mathcal{O}}(\\mathrm{poly}(H)\\sqrt{SAT})$, where $S$ and $A$ are sizes\nof the state and action spaces, respectively. This result improves upon the\nbest-known regret bound by a factor of $\\sqrt{S}$, bridging the gap between\nadversarial and stochastic MDPs, and matching the minimax lower bound\n$\\Omega(\\sqrt{H^3SAT})$ as far as the dependencies in $S,A,T$ are concerned.\nThe proposed algorithm and analysis completely avoid the typical tool given by\noccupancy measures; instead, it performs policy optimization based only on\ndynamic programming and on a black-box online linear optimization strategy run\nover estimated advantage functions, making it easy to implement. The analysis\nleverages two recent techniques: policy optimization based on online linear\noptimization strategies (Jonckheere et al., 2023) and a refined martingale\nanalysis of the impact on values of estimating transitions kernels (Zhang et\nal., 2023).\n","authors":["Daniil Tiapkin","Evgenii Chzhen","Gilles Stoltz"],"pdf_url":"https://arxiv.org/pdf/2407.05704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09776v2","updated":"2024-07-08T08:06:00Z","published":"2024-06-14T07:22:39Z","title":"Faster Convergence on Heterogeneous Federated Edge Learning: An Adaptive\n  Clustered Data Sharing Approach","summary":"  Federated Edge Learning (FEEL) emerges as a pioneering distributed machine\nlearning paradigm for the 6G Hyper-Connectivity, harnessing data from the\nInternet of Things (IoT) devices while upholding data privacy. However, current\nFEEL algorithms struggle with non-independent and non-identically distributed\n(non-IID) data, leading to elevated communication costs and compromised model\naccuracy. To address these statistical imbalances within FEEL, we introduce a\nclustered data sharing framework, mitigating data heterogeneity by selectively\nsharing partial data from cluster heads to trusted associates through\nsidelink-aided multicasting. The collective communication pattern is integral\nto FEEL training, where both cluster formation and the efficiency of\ncommunication and computation impact training latency and accuracy\nsimultaneously. To tackle the strictly coupled data sharing and resource\noptimization, we decompose the overall optimization problem into the clients\nclustering and effective data sharing subproblems. Specifically, a\ndistribution-based adaptive clustering algorithm (DACA) is devised basing on\nthree deductive cluster forming conditions, which ensures the maximum sharing\nyield. Meanwhile, we design a stochastic optimization based joint computed\nfrequency and shared data volume optimization (JFVO) algorithm, determining the\noptimal resource allocation with an uncertain objective function. The\nexperiments show that the proposed framework facilitates FEEL on non-IID\ndatasets with faster convergence rate and higher model accuracy in a limited\ncommunication environment.\n","authors":["Gang Hu","Yinglei Teng","Nan Wang","Zhu Han"],"pdf_url":"https://arxiv.org/pdf/2406.09776v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05694v1","updated":"2024-07-08T07:53:06Z","published":"2024-07-08T07:53:06Z","title":"On the Limitations of Compute Thresholds as a Governance Strategy","summary":"  At face value, this essay is about understanding a fairly esoteric governance\ntool called compute thresholds. However, in order to grapple with whether these\nthresholds will achieve anything, we must first understand how they came to be.\nThis requires engaging with a decades-old debate at the heart of computer\nscience progress, namely, is bigger always better? Hence, this essay may be of\ninterest not only to policymakers and the wider public but also to computer\nscientists interested in understanding the role of compute in unlocking\nbreakthroughs. Does a certain inflection point of compute result in changes to\nthe risk profile of a model? This discussion is increasingly urgent given the\nwide adoption of governance approaches that suggest greater compute equates\nwith higher propensity for harm. Several leading frontier AI companies have\nreleased responsible scaling policies. Both the White House Executive Orders on\nAI Safety (EO) and the EU AI Act encode the use of FLOP or floating-point\noperations as a way to identify more powerful systems. What is striking about\nthe choice of compute thresholds to-date is that no models currently deployed\nin the wild fulfill the current criteria set by the EO. This implies that the\nemphasis is often not on auditing the risks and harms incurred by currently\ndeployed models - but rather is based upon the belief that future levels of\ncompute will introduce unforeseen new risks. A key conclusion of this essay is\nthat compute thresholds as currently implemented are shortsighted and likely to\nfail to mitigate risk. Governance that is overly reliant on compute fails to\nunderstand that the relationship between compute and risk is highly uncertain\nand rapidly changing. It also overestimates our ability to predict what\nabilities emerge at different scales. This essay ends with recommendations for\na better way forward.\n","authors":["Sara Hooker"],"pdf_url":"https://arxiv.org/pdf/2407.05694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12561v4","updated":"2024-07-08T07:52:09Z","published":"2022-12-23T19:37:39Z","title":"An active learning method for solving competitive multi-agent\n  decision-making and control problems","summary":"  To identify a stationary action profile for a population of competitive\nagents, each executing private strategies, we introduce a novel active-learning\nscheme where a centralized external observer (or entity) can probe the agents'\nreactions and recursively update simple local parametric estimates of the\naction-reaction mappings. Under very general working assumptions (not even\nassuming that a stationary profile exists), sufficient conditions are\nestablished to assess the asymptotic properties of the proposed active learning\nmethodology so that, if the parameters characterizing the action-reaction\nmappings converge, a stationary action profile is achieved. Such conditions\nhence act also as certificates for the existence of such a profile. Extensive\nnumerical simulations involving typical competitive multi-agent control and\ndecision-making problems illustrate the practical effectiveness of the proposed\nlearning-based approach.\n","authors":["Filippo Fabiani","Alberto Bemporad"],"pdf_url":"https://arxiv.org/pdf/2212.12561v4.pdf","comment":"Python package available at https://github.com/bemporad/gnep-learn"},{"id":"http://arxiv.org/abs/2407.05693v1","updated":"2024-07-08T07:47:30Z","published":"2024-07-08T07:47:30Z","title":"Sub-SA: Strengthen In-context Learning via Submodular Selective\n  Annotation","summary":"  In-context learning (ICL) leverages in-context examples as prompts for the\npredictions of Large Language Models (LLMs). These prompts play a crucial role\nin achieving strong performance. However, the selection of suitable prompts\nfrom a large pool of labeled examples often entails significant annotation\ncosts. To address this challenge, we propose \\textbf{Sub-SA}\n(\\textbf{Sub}modular \\textbf{S}elective \\textbf{A}nnotation), a submodule-based\nselective annotation method. The aim of Sub-SA is to reduce annotation costs\nwhile improving the quality of in-context examples and minimizing the time\nconsumption of the selection process. In Sub-SA, we design a submodular\nfunction that facilitates effective subset selection for annotation and\ndemonstrates the characteristics of monotonically and submodularity from the\ntheoretical perspective. Specifically, we propose \\textbf{RPR} (\\textbf{R}eward\nand \\textbf{P}enalty \\textbf{R}egularization) to better balance the diversity\nand representativeness of the unlabeled dataset attributed to a reward term and\na penalty term, respectively. Consequently, the selection for annotations can\nbe effectively addressed with a simple yet effective greedy search algorithm\nbased on the submodular function. Finally, we apply the similarity prompt\nretrieval to get the examples for ICL.\n","authors":["Jian Qian","Miao Sun","Sifan Zhou","Ziyu Zhao","Ruizhi Hun","Patrick Chiang"],"pdf_url":"https://arxiv.org/pdf/2407.05693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05684v1","updated":"2024-07-08T07:34:35Z","published":"2024-07-08T07:34:35Z","title":"Multi-Fidelity Bayesian Neural Network for Uncertainty Quantification in\n  Transonic Aerodynamic Loads","summary":"  Multi-fidelity models are becoming more prevalent in engineering,\nparticularly in aerospace, as they combine both the computational efficiency of\nlow-fidelity models with the high accuracy of higher-fidelity simulations.\nVarious state-of-the-art techniques exist for fusing data from different\nfidelity sources, including Co-Kriging and transfer learning in neural\nnetworks. This paper aims to implement a multi-fidelity Bayesian neural network\nmodel that applies transfer learning to fuse data generated by models at\ndifferent fidelities. Bayesian neural networks use probability distributions\nover network weights, enabling them to provide predictions along with estimates\nof their confidence. This approach harnesses the predictive and data fusion\ncapabilities of neural networks while also quantifying uncertainty. The results\ndemonstrate that the multi-fidelity Bayesian model outperforms the\nstate-of-the-art Co-Kriging in terms of overall accuracy and robustness on\nunseen data.\n","authors":["Andrea Vaiuso","Gabriele Immordino","Marcello Righi","Andrea Da Ronch"],"pdf_url":"https://arxiv.org/pdf/2407.05684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04385v3","updated":"2024-07-08T07:24:49Z","published":"2024-01-09T07:14:45Z","title":"Machine unlearning through fine-grained model parameters perturbation","summary":"  Machine unlearning techniques, which involve retracting data records and\nreducing influence of said data on trained models, help with the user privacy\nprotection objective but incur significant computational costs. Weight\nperturbation-based unlearning is a general approach, but it typically involves\nglobally modifying the parameters. We propose fine-grained Top-K and Random-k\nparameters perturbed inexact machine unlearning strategies that address the\nprivacy needs while keeping the computational costs tractable.\n  In order to demonstrate the efficacy of our strategies we also tackle the\nchallenge of evaluating the effectiveness of machine unlearning by considering\nthe model's generalization performance across both unlearning and remaining\ndata. To better assess the unlearning effect and model generalization, we\npropose novel metrics, namely, the forgetting rate and memory retention rate.\nHowever, for inexact machine unlearning, current metrics are inadequate in\nquantifying the degree of forgetting that occurs after unlearning strategies\nare applied. To address this, we introduce SPD-GAN, which subtly perturbs the\ndistribution of data targeted for unlearning. Then, we evaluate the degree of\nunlearning by measuring the performance difference of the models on the\nperturbed unlearning data before and after the unlearning process. By\nimplementing these innovative techniques and metrics, we achieve\ncomputationally efficacious privacy protection in machine learning applications\nwithout significant sacrifice of model performance. Furthermore, this approach\nprovides a novel method for evaluating the degree of unlearning.\n","authors":["Zhiwei Zuo","Zhuo Tang","Kenli Li","Anwitaman Datta"],"pdf_url":"https://arxiv.org/pdf/2401.04385v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05664v1","updated":"2024-07-08T06:59:29Z","published":"2024-07-08T06:59:29Z","title":"How DNNs break the Curse of Dimensionality: Compositionality and\n  Symmetry Learning","summary":"  We show that deep neural networks (DNNs) can efficiently learn any\ncomposition of functions with bounded $F_{1}$-norm, which allows DNNs to break\nthe curse of dimensionality in ways that shallow networks cannot. More\nspecifically, we derive a generalization bound that combines a covering number\nargument for compositionality, and the $F_{1}$-norm (or the related Barron\nnorm) for large width adaptivity. We show that the global minimizer of the\nregularized loss of DNNs can fit for example the composition of two functions\n$f^{*}=h\\circ g$ from a small number of observations, assuming $g$ is\nsmooth/regular and reduces the dimensionality (e.g. $g$ could be the modulo map\nof the symmetries of $f^{*}$), so that $h$ can be learned in spite of its low\nregularity. The measures of regularity we consider is the Sobolev norm with\ndifferent levels of differentiability, which is well adapted to the $F_{1}$\nnorm. We compute scaling laws empirically and observe phase transitions\ndepending on whether $g$ or $h$ is harder to learn, as predicted by our theory.\n","authors":["Arthur Jacot","Seok Hoan Choi","Yuxiao Wen"],"pdf_url":"https://arxiv.org/pdf/2407.05664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06441v3","updated":"2024-07-08T06:54:37Z","published":"2023-12-11T15:18:51Z","title":"Revisiting Graph-Based Fraud Detection in Sight of Heterophily and\n  Spectrum","summary":"  Graph-based fraud detection (GFD) can be regarded as a challenging\nsemi-supervised node binary classification task. In recent years, Graph Neural\nNetworks (GNN) have been widely applied to GFD, characterizing the anomalous\npossibility of a node by aggregating neighbor information. However, fraud\ngraphs are inherently heterophilic, thus most of GNNs perform poorly due to\ntheir assumption of homophily. In addition, due to the existence of heterophily\nand class imbalance problem, the existing models do not fully utilize the\nprecious node label information. To address the above issues, this paper\nproposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector\nincludes a hybrid filtering module and a local environmental constraint module,\nthe two modules are utilized to solve heterophily and label utilization problem\nrespectively. The first module starts from the perspective of the spectral\ndomain, and solves the heterophily problem to a certain extent. Specifically,\nit divides the spectrum into various mixed-frequency bands based on the\ncorrelation between spectrum energy distribution and heterophily. Then in order\nto make full use of the node label information, a local environmental\nconstraint module is adaptively designed. The comprehensive experimental\nresults on four real-world fraud detection datasets denote that SEC-GFD\noutperforms other competitive graph-based fraud detectors. We release our code\nat https://github.com/Sunxkissed/SEC-GFD.\n","authors":["Fan Xu","Nan Wang","Hao Wu","Xuezhi Wen","Xibin Zhao","Hai Wan"],"pdf_url":"https://arxiv.org/pdf/2312.06441v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05658v1","updated":"2024-07-08T06:35:13Z","published":"2024-07-08T06:35:13Z","title":"Random Features Hopfield Networks generalize retrieval to previously\n  unseen examples","summary":"  It has been recently shown that a learning transition happens when a Hopfield\nNetwork stores examples generated as superpositions of random features, where\nnew attractors corresponding to such features appear in the model. In this work\nwe reveal that the network also develops attractors corresponding to previously\nunseen examples generated with the same set of features. We explain this\nsurprising behaviour in terms of spurious states of the learned features: we\nargue that, increasing the number of stored examples beyond the learning\ntransition, the model also learns to mix the features to represent both stored\nand previously unseen examples. We support this claim with the computation of\nthe phase diagram of the model.\n","authors":["Silvio Kalaj","Clarissa Lauditi","Gabriele Perugini","Carlo Lucibello","Enrico M. Malatesta","Matteo Negri"],"pdf_url":"https://arxiv.org/pdf/2407.05658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05656v1","updated":"2024-07-08T06:29:46Z","published":"2024-07-08T06:29:46Z","title":"Multi-label Learning with Random Circular Vectors","summary":"  The extreme multi-label classification~(XMC) task involves learning a\nclassifier that can predict from a large label set the most relevant subset of\nlabels for a data instance. While deep neural networks~(DNNs) have demonstrated\nremarkable success in XMC problems, the task is still challenging because it\nmust deal with a large number of output labels, which make the DNN training\ncomputationally expensive. This paper addresses the issue by exploring the use\nof random circular vectors, where each vector component is represented as a\ncomplex amplitude. In our framework, we can develop an output layer and loss\nfunction of DNNs for XMC by representing the final output layer as a fully\nconnected layer that directly predicts a low-dimensional circular vector\nencoding a set of labels for a data instance. We conducted experiments on\nsynthetic datasets to verify that circular vectors have better label encoding\ncapacity and retrieval ability than normal real-valued vectors. Then, we\nconducted experiments on actual XMC datasets and found that these appealing\nproperties of circular vectors contribute to significant improvements in task\nperformance compared with a previous model using random real-valued vectors,\nwhile reducing the size of the output layers by up to 99%.\n","authors":["Ken Nishida","Kojiro Machi","Kazuma Onishi","Katsuhiko Hayashi","Hidetaka Kamigaito"],"pdf_url":"https://arxiv.org/pdf/2407.05656v1.pdf","comment":"11 pages, 6 figures, 3 tables; accepted to workshop RepL4NLP held in\n  conjunction with ACL 2024"},{"id":"http://arxiv.org/abs/2407.05650v1","updated":"2024-07-08T06:22:10Z","published":"2024-07-08T06:22:10Z","title":"The Dynamic Net Architecture: Learning Robust and Holistic Visual\n  Representations Through Self-Organizing Networks","summary":"  We present a novel intelligent-system architecture called \"Dynamic Net\nArchitecture\" (DNA) that relies on recurrence-stabilized networks and discuss\nit in application to vision. Our architecture models a (cerebral cortical) area\nwherein elementary feature neurons encode details of visual structures, and\ncoherent nets of such neurons model holistic object structures. By interpreting\nsmaller or larger coherent pieces of an area network as complex features, our\nmodel encodes hierarchical feature representations essentially different than\nartificial neural networks (ANNs).\n  DNA models operate on a dynamic connectionism principle, wherein neural\nactivations stemming from initial afferent signals undergo stabilization\nthrough a self-organizing mechanism facilitated by Hebbian plasticity alongside\nperiodically tightening inhibition. In contrast to ANNs, which rely on\nfeed-forward connections and backpropagation of error, we posit that this\nprocessing paradigm leads to highly robust representations, as by employing\ndynamic lateral connections, irrelevant details in neural activations are\nfiltered out, freeing further processing steps from distracting noise and\npremature decisions.\n  We empirically demonstrate the viability of the DNA by composing line\nfragments into longer lines and show that the construction of nets representing\nlines remains robust even with the introduction of up to $59\\%$ noise at each\nspatial location. Furthermore, we demonstrate the model's capability to\nreconstruct anticipated features from partially obscured inputs and that it can\ngeneralize to patterns not observed during training. In this work, we limit the\nDNA to one cortical area and focus on its internals while providing insights\ninto a standalone area's strengths and shortcomings. Additionally, we provide\nan outlook on how future work can implement invariant object recognition by\ncombining multiple areas.\n","authors":["Pascal J. Sager","Jan M. Deriu","Benjamin F. Grewe","Thilo Stadelmann","Christoph von der Malsburg"],"pdf_url":"https://arxiv.org/pdf/2407.05650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05649v1","updated":"2024-07-08T06:21:56Z","published":"2024-07-08T06:21:56Z","title":"Graph Attention with Random Rewiring","summary":"  Graph Neural Networks (GNNs) have become fundamental in graph-structured deep\nlearning. Key paradigms of modern GNNs include message passing, graph rewiring,\nand Graph Transformers. This paper introduces Graph-Rewiring Attention with\nStochastic Structures (GRASS), a novel GNN architecture that combines the\nadvantages of these three paradigms. GRASS rewires the input graph by\nsuperimposing a random regular graph, enhancing long-range information\npropagation while preserving structural features of the input graph. It also\nemploys a unique additive attention mechanism tailored for graph-structured\ndata, providing a graph inductive bias while remaining computationally\nefficient. Our empirical evaluations demonstrate that GRASS achieves\nstate-of-the-art performance on multiple benchmark datasets, confirming its\npractical efficacy.\n","authors":["Tongzhou Liao","BarnabÃ¡s PÃ³czos"],"pdf_url":"https://arxiv.org/pdf/2407.05649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05639v1","updated":"2024-07-08T06:07:51Z","published":"2024-07-08T06:07:51Z","title":"Deep Learning-based Anomaly Detection and Log Analysis for Computer\n  Networks","summary":"  Computer network anomaly detection and log analysis, as an important topic in\nthe field of network security, has been a key task to ensure network security\nand system reliability. First, existing network anomaly detection and log\nanalysis methods are often challenged by high-dimensional data and complex\nnetwork topologies, resulting in unstable performance and high false-positive\nrates. In addition, traditional methods are usually difficult to handle\ntime-series data, which is crucial for anomaly detection and log analysis.\nTherefore, we need a more efficient and accurate method to cope with these\nproblems. To compensate for the shortcomings of current methods, we propose an\ninnovative fusion model that integrates Isolation Forest, GAN (Generative\nAdversarial Network), and Transformer with each other, and each of them plays a\nunique role. Isolation Forest is used to quickly identify anomalous data\npoints, and GAN is used to generate synthetic data with the real data\ndistribution characteristics to augment the training dataset, while the\nTransformer is used for modeling and context extraction on time series data.\nThe synergy of these three components makes our model more accurate and robust\nin anomaly detection and log analysis tasks. We validate the effectiveness of\nthis fusion model in an extensive experimental evaluation. Experimental results\nshow that our model significantly improves the accuracy of anomaly detection\nwhile reducing the false alarm rate, which helps to detect potential network\nproblems in advance. The model also performs well in the log analysis task and\nis able to quickly identify anomalous behaviors, which helps to improve the\nstability of the system. The significance of this study is that it introduces\nadvanced deep learning techniques, which work anomaly detection and log\nanalysis.\n","authors":["Shuzhan Wang","Ruxue Jiang","Zhaoqi Wang","Yan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.05639v1.pdf","comment":"38 pages"},{"id":"http://arxiv.org/abs/2405.09802v2","updated":"2024-07-08T06:06:34Z","published":"2024-05-16T04:21:09Z","title":"Analysis and Predictive Modeling of Solar Coronal Holes Using Computer\n  Vision and LSTM Networks","summary":"  In the era of space exploration, coronal holes on the sun play a significant\nrole due to their impact on satellites and aircraft through their open magnetic\nfields and increased solar wind emissions. This study employs computer vision\ntechniques to detect coronal hole regions and estimate their sizes using\nimagery from the Solar Dynamics Observatory (SDO). Additionally, we utilize\ndeep learning methods, specifically Long Short-Term Memory (LSTM) networks, to\nanalyze trends in the area of coronal holes and predict their areas across\nvarious solar regions over a span of seven days. By examining time series data,\nwe aim to identify patterns in coronal hole behavior and understand their\npotential effects on space weather. This research enhances our ability to\nanticipate and prepare for space weather events that could affect Earth's\ntechnological systems.\n","authors":["Juyoung Yun","Jungmin Shin"],"pdf_url":"https://arxiv.org/pdf/2405.09802v2.pdf","comment":"Accepted to the first joint European Space Agency SPAICE Conference\n  2024"},{"id":"http://arxiv.org/abs/2407.05633v1","updated":"2024-07-08T05:58:49Z","published":"2024-07-08T05:58:49Z","title":"AdaPI: Facilitating DNN Model Adaptivity for Efficient Private Inference\n  in Edge Computing","summary":"  Private inference (PI) has emerged as a promising solution to execute\ncomputations on encrypted data, safeguarding user privacy and model parameters\nin edge computing. However, existing PI methods are predominantly developed\nconsidering constant resource constraints, overlooking the varied and dynamic\nresource constraints in diverse edge devices, like energy budgets.\nConsequently, model providers have to design specialized models for different\ndevices, where all of them have to be stored on the edge server, resulting in\ninefficient deployment. To fill this gap, this work presents AdaPI, a novel\napproach that achieves adaptive PI by allowing a model to perform well across\nedge devices with diverse energy budgets. AdaPI employs a PI-aware training\nstrategy that optimizes the model weights alongside weight-level and\nfeature-level soft masks. These soft masks are subsequently transformed into\nmultiple binary masks to enable adjustments in communication and computation\nworkloads. Through sequentially training the model with increasingly dense\nbinary masks, AdaPI attains optimal accuracy for each energy budget, which\noutperforms the state-of-the-art PI methods by 7.3\\% in terms of test accuracy\non CIFAR-100. The code of AdaPI can be accessed via\nhttps://github.com/jiahuiiiiii/AdaPI.\n","authors":["Tong Zhou","Jiahui Zhao","Yukui Luo","Xi Xie","Wujie Wen","Caiwen Ding","Xiaolin Xu"],"pdf_url":"https://arxiv.org/pdf/2407.05633v1.pdf","comment":"ICCAD 2024 accepted publication"},{"id":"http://arxiv.org/abs/2407.04272v2","updated":"2024-07-08T05:53:10Z","published":"2024-07-05T05:55:18Z","title":"Accelerating Communication in Deep Learning Recommendation Model\n  Training with Dual-Level Adaptive Lossy Compression","summary":"  DLRM is a state-of-the-art recommendation system model that has gained\nwidespread adoption across various industry applications. The large size of\nDLRM models, however, necessitates the use of multiple devices/GPUs for\nefficient training. A significant bottleneck in this process is the\ntime-consuming all-to-all communication required to collect embedding data from\nall devices. To mitigate this, we introduce a method that employs error-bounded\nlossy compression to reduce the communication data size and accelerate DLRM\ntraining. We develop a novel error-bounded lossy compression algorithm,\ninformed by an in-depth analysis of embedding data features, to achieve high\ncompression ratios. Moreover, we introduce a dual-level adaptive strategy for\nerror-bound adjustment, spanning both table-wise and iteration-wise aspects, to\nbalance the compression benefits with the potential impacts on accuracy. We\nfurther optimize our compressor for PyTorch tensors on GPUs, minimizing\ncompression overhead. Evaluation shows that our method achieves a 1.38$\\times$\ntraining speedup with a minimal accuracy impact.\n","authors":["Hao Feng","Boyuan Zhang","Fanjiang Ye","Min Si","Ching-Hsiang Chu","Jiannan Tian","Chunxing Yin","Summer Deng","Yuchen Hao","Pavan Balaji","Tong Geng","Dingwen Tao"],"pdf_url":"https://arxiv.org/pdf/2407.04272v2.pdf","comment":"accepted by SC '24"},{"id":"http://arxiv.org/abs/2407.05627v1","updated":"2024-07-08T05:42:29Z","published":"2024-07-08T05:42:29Z","title":"New Directions in Text Classification Research: Maximizing The\n  Performance of Sentiment Classification from Limited Data","summary":"  The stakeholders' needs in sentiment analysis for various issues, whether\npositive or negative, are speed and accuracy. One new challenge in sentiment\nanalysis tasks is the limited training data, which often leads to suboptimal\nmachine learning models and poor performance on test data. This paper discusses\nthe problem of text classification based on limited training data (300 to 600\nsamples) into three classes: positive, negative, and neutral. A benchmark\ndataset is provided for training and testing data on the issue of Kaesang\nPangarep's appointment as Chairman of PSI. External data for aggregation and\naugmentation purposes are provided, consisting of two datasets: the topic of\nCovid Vaccination sentiment and an open topic. The official score used is the\nF1-score, which balances precision and recall among the three classes,\npositive, negative, and neutral. A baseline score is provided as a reference\nfor researchers for unoptimized classification methods. The optimized score is\nprovided as a reference for the target score to be achieved by any proposed\nmethod. Both scoring (baseline and optimized) use the SVM method, which is\nwidely reported as the state-of-the-art in conventional machine learning\nmethods. The F1-scores achieved by the baseline and optimized methods are\n40.83% and 51.28%, respectively.\n","authors":["Surya Agustian","Muhammad Irfan Syah","Nurul Fatiara","Rahmad Abdillah"],"pdf_url":"https://arxiv.org/pdf/2407.05627v1.pdf","comment":"9 pages, in Indonesian language. intro to a shared task in sentiment\n  classification"},{"id":"http://arxiv.org/abs/2310.17110v3","updated":"2024-07-08T05:39:38Z","published":"2023-10-26T02:37:43Z","title":"LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on\n  Dynamic Graphs?","summary":"  In an era marked by the increasing adoption of Large Language Models (LLMs)\nfor various tasks, there is a growing focus on exploring LLMs' capabilities in\nhandling web data, particularly graph data. Dynamic graphs, which capture\ntemporal network evolution patterns, are ubiquitous in real-world web data.\nEvaluating LLMs' competence in understanding spatial-temporal information on\ndynamic graphs is essential for their adoption in web applications, which\nremains unexplored in the literature. In this paper, we bridge the gap via\nproposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic\ngraphs, to the best of our knowledge, for the first time. Specifically, we\npropose the LLM4DyG benchmark, which includes nine specially designed tasks\nconsidering the capability evaluation of LLMs from both temporal and spatial\ndimensions. Then, we conduct extensive experiments to analyze the impacts of\ndifferent data generators, data statistics, prompting techniques, and LLMs on\nthe model performance. Finally, we propose Disentangled Spatial-Temporal\nThoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporal\nunderstanding abilities. Our main observations are: 1) LLMs have preliminary\nspatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graph\ntasks show increasing difficulties for LLMs as the graph size and density\nincrease, while not sensitive to the time span and data generation mechanism,\n3) the proposed DST2 prompting method can help to improve LLMs'\nspatial-temporal understanding abilities on dynamic graphs for most tasks. The\ndata and codes are publicly available at Github.\n","authors":["Zeyang Zhang","Xin Wang","Ziwei Zhang","Haoyang Li","Yijian Qin","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.17110v3.pdf","comment":"Accepted by ACM KDD'24"},{"id":"http://arxiv.org/abs/2407.05622v1","updated":"2024-07-08T05:30:34Z","published":"2024-07-08T05:30:34Z","title":"On the Complexity of Learning Sparse Functions with Statistical and\n  Gradient Queries","summary":"  The goal of this paper is to investigate the complexity of gradient\nalgorithms when learning sparse functions (juntas). We introduce a type of\nStatistical Queries ($\\mathsf{SQ}$), which we call Differentiable Learning\nQueries ($\\mathsf{DLQ}$), to model gradient queries on a specified loss with\nrespect to an arbitrary model. We provide a tight characterization of the query\ncomplexity of $\\mathsf{DLQ}$ for learning the support of a sparse function over\ngeneric product distributions. This complexity crucially depends on the loss\nfunction. For the squared loss, $\\mathsf{DLQ}$ matches the complexity of\nCorrelation Statistical Queries $(\\mathsf{CSQ})$--potentially much worse than\n$\\mathsf{SQ}$. But for other simple loss functions, including the $\\ell_1$\nloss, $\\mathsf{DLQ}$ always achieves the same complexity as $\\mathsf{SQ}$. We\nalso provide evidence that $\\mathsf{DLQ}$ can indeed capture learning with\n(stochastic) gradient descent by showing it correctly describes the complexity\nof learning with a two-layer neural network in the mean field regime and linear\nscaling.\n","authors":["Nirmit Joshi","Theodor Misiakiewicz","Nathan Srebro"],"pdf_url":"https://arxiv.org/pdf/2407.05622v1.pdf","comment":"43 pages, 1 table, 1 figure"},{"id":"http://arxiv.org/abs/2407.03146v2","updated":"2024-07-08T05:21:59Z","published":"2024-05-31T02:56:43Z","title":"Enhancing Class Fairness in Classification with A Two-Player Game\n  Approach","summary":"  Data augmentation is widely applied and has shown its benefits in different\nmachine learning tasks. However, as recently observed in some downstream tasks,\ndata augmentation may introduce an unfair impact on classifications. While it\ncan improve the performance of some classes, it can actually be detrimental for\nother classes, which can be problematic in some application domains. In this\npaper, to counteract this phenomenon, we propose a FAir Classification approach\nwith a Two-player game (FACT). We first formulate the training of a classifier\nwith data augmentation as a fair optimization problem, which can be further\nwritten as an adversarial two-player game. Following this formulation, we\npropose a novel multiplicative weight optimization algorithm, for which we\ntheoretically prove that it can converge to a solution that is fair over\nclasses. Interestingly, our formulation also reveals that this fairness issue\nover classes is not due to data augmentation only, but is in fact a general\nphenomenon. Our empirical experiments demonstrate that the performance of our\nlearned classifiers is indeed more fairly distributed over classes in five\ndatasets, with only limited impact on the average accuracy.\n","authors":["Yunpeng Jiang","Paul Weng","Yutong Ban"],"pdf_url":"https://arxiv.org/pdf/2407.03146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05615v1","updated":"2024-07-08T05:03:46Z","published":"2024-07-08T05:03:46Z","title":"OSN: Infinite Representations of Dynamic 3D Scenes from Monocular Videos","summary":"  It has long been challenging to recover the underlying dynamic 3D scene\nrepresentations from a monocular RGB video. Existing works formulate this\nproblem into finding a single most plausible solution by adding various\nconstraints such as depth priors and strong geometry constraints, ignoring the\nfact that there could be infinitely many 3D scene representations corresponding\nto a single dynamic video. In this paper, we aim to learn all plausible 3D\nscene configurations that match the input video, instead of just inferring a\nspecific one. To achieve this ambitious goal, we introduce a new framework,\ncalled OSN. The key to our approach is a simple yet innovative object scale\nnetwork together with a joint optimization module to learn an accurate scale\nrange for every dynamic 3D object. This allows us to sample as many faithful 3D\nscene configurations as possible. Extensive experiments show that our method\nsurpasses all baselines and achieves superior accuracy in dynamic novel view\nsynthesis on multiple synthetic and real-world datasets. Most notably, our\nmethod demonstrates a clear advantage in learning fine-grained 3D scene\ngeometry. Our code and data are available at https://github.com/vLAR-group/OSN\n","authors":["Ziyang Song","Jinxi Li","Bo Yang"],"pdf_url":"https://arxiv.org/pdf/2407.05615v1.pdf","comment":"ICML 2024. Code and data are available at:\n  https://github.com/vLAR-group/OSN"},{"id":"http://arxiv.org/abs/2406.12837v3","updated":"2024-07-08T04:55:34Z","published":"2024-06-18T17:55:15Z","title":"LayerMerge: Neural Network Depth Compression through Layer Pruning and\n  Merging","summary":"  Recent works show that reducing the number of layers in a convolutional\nneural network can enhance efficiency while maintaining the performance of the\nnetwork. Existing depth compression methods remove redundant non-linear\nactivation functions and merge the consecutive convolution layers into a single\nlayer. However, these methods suffer from a critical drawback; the kernel size\nof the merged layers becomes larger, significantly undermining the latency\nreduction gained from reducing the depth of the network. We show that this\nproblem can be addressed by jointly pruning convolution layers and activation\nfunctions. To this end, we propose LayerMerge, a novel depth compression method\nthat selects which activation layers and convolution layers to remove, to\nachieve a desired inference speed-up while minimizing performance loss. Since\nthe corresponding selection problem involves an exponential search space, we\nformulate a novel surrogate optimization problem and efficiently solve it via\ndynamic programming. Empirical results demonstrate that our method consistently\noutperforms existing depth compression and layer pruning methods on various\nnetwork architectures, both on image classification and generation tasks. We\nrelease the code at https://github.com/snu-mllab/LayerMerge.\n","authors":["Jinuk Kim","Marwa El Halabi","Mingi Ji","Hyun Oh Song"],"pdf_url":"https://arxiv.org/pdf/2406.12837v3.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2407.05593v1","updated":"2024-07-08T04:15:43Z","published":"2024-07-08T04:15:43Z","title":"Unmasking Trees for Tabular Data","summary":"  We herein describe UnmaskingTrees, a method and open-source software package\nfor tabular data generation and, especially, imputation. Our experiments\nsuggest that training gradient-boosted trees to incrementally unmask features\noffers a simple, strong baseline for imputation.\n","authors":["Calvin McCarter"],"pdf_url":"https://arxiv.org/pdf/2407.05593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05591v1","updated":"2024-07-08T04:08:35Z","published":"2024-07-08T04:08:35Z","title":"On the Power of Convolution Augmented Transformer","summary":"  The transformer architecture has catalyzed revolutionary advances in language\nmodeling. However, recent architectural recipes, such as state-space models,\nhave bridged the performance gap. Motivated by this, we examine the benefits of\nConvolution-Augmented Transformer (CAT) for recall, copying, and length\ngeneralization tasks. CAT incorporates convolutional filters in the K/Q/V\nembeddings of an attention layer. Through CAT, we show that the locality of the\nconvolution synergizes with the global view of the attention. Unlike comparable\narchitectures, such as Mamba or transformer, CAT can provably solve the\nassociative recall (AR) and copying tasks using a single layer while also\nenjoying guaranteed length generalization. We also establish computational\ntradeoffs between convolution and attention by characterizing how convolution\ncan mitigate the need for full attention by summarizing the context window and\ncreating salient summary tokens to attend. Evaluations on real datasets\ncorroborate our findings and demonstrate that CAT and its variations indeed\nenhance the language modeling performance.\n","authors":["Mingchen Li","Xuechen Zhang","Yixiao Huang","Samet Oymak"],"pdf_url":"https://arxiv.org/pdf/2407.05591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01991v2","updated":"2024-07-08T03:33:43Z","published":"2023-10-03T12:03:06Z","title":"Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward\n  Reasoning in Math Word Problems","summary":"  While forward reasoning (i.e., find the answer given the question) has been\nexplored extensively in recent literature, backward reasoning is relatively\nunexplored. We examine the backward reasoning capabilities of LLMs on Math Word\nProblems (MWPs): given a mathematical question and its answer, with some\ndetails omitted from the question, can LLMs effectively retrieve the missing\ninformation? On modifying three benchmark datasets for this task, to evaluate\nthis task: GSM8k, SVAMP, and MultiArith, we find a significant drop in the\naccuracy of models on this task compared to forward reasoning across SOTA LLMs\n(GPT4, GPT3.5, PaLM-2, and LLaMa). Motivated by the fact backward reasoning can\nbe seen as the ''inverse'' of forward reasoning, we propose variations of three\ndifferent forward reasoning strategies to improve performance. Rephrase\nreformulates the given problem into a forward reasoning problem, PAL-Tools\ncombines the idea of Program-Aided LLMs to produce a set of equations that can\nbe solved by an external solver, and Check your Work exploits the availability\nof natural verifier of high accuracy in the forward direction, interleaving\nsolving and verification steps. Finally, realizing that each of our base\nmethods correctly solves a different set of problems, we propose a novel\nBayesian formulation for creating an ensemble over the base methods to further\nboost the accuracy. Extensive experimentation demonstrates successive\nimprovement in the performance of LLMs on the backward reasoning task, using\nour strategies, with our ensemble-based method resulting in significant\nperformance gains compared to the SOTA forward reasoning strategies we adapt.\n","authors":["Aniruddha Deb","Neeva Oza","Sarthak Singla","Dinesh Khandelwal","Dinesh Garg","Parag Singla"],"pdf_url":"https://arxiv.org/pdf/2310.01991v2.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.05580v1","updated":"2024-07-08T03:30:25Z","published":"2024-07-08T03:30:25Z","title":"$\\mathrm{E^{2}CFD}$: Towards Effective and Efficient Cost Function\n  Design for Safe Reinforcement Learning via Large Language Model","summary":"  Different classes of safe reinforcement learning algorithms have shown\nsatisfactory performance in various types of safety requirement scenarios.\nHowever, the existing methods mainly address one or several classes of specific\nsafety requirement scenario problems and cannot be applied to arbitrary safety\nrequirement scenarios. In addition, the optimization objectives of existing\nreinforcement learning algorithms are misaligned with the task requirements.\nBased on the need to address these issues, we propose $\\mathrm{E^{2}CFD}$, an\neffective and efficient cost function design framework. $\\mathrm{E^{2}CFD}$\nleverages the capabilities of a large language model (LLM) to comprehend\nvarious safety scenarios and generate corresponding cost functions. It\nincorporates the \\textit{fast performance evaluation (FPE)} method to\nfacilitate rapid and iterative updates to the generated cost function. Through\nthis iterative process, $\\mathrm{E^{2}CFD}$ aims to obtain the most suitable\ncost function for policy training, tailored to the specific tasks within the\nsafety scenario. Experiments have proven that the performance of policies\ntrained using this framework is superior to traditional safe reinforcement\nlearning algorithms and policies trained with carefully designed cost\nfunctions.\n","authors":["Zepeng Wang","Chao Ma","Linjiang Zhou","Libing Wu","Lei Yang","Xiaochuan Shi","Guojun Peng"],"pdf_url":"https://arxiv.org/pdf/2407.05580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06692v3","updated":"2024-07-08T02:52:05Z","published":"2024-01-12T16:56:54Z","title":"An Experimental Design Framework for Label-Efficient Supervised\n  Finetuning of Large Language Models","summary":"  Supervised finetuning (SFT) on instruction datasets has played a crucial role\nin achieving the remarkable zero-shot generalization capabilities observed in\nmodern large language models (LLMs). However, the annotation efforts required\nto produce high quality responses for instructions are becoming prohibitively\nexpensive, especially as the number of tasks spanned by instruction datasets\ncontinues to increase. Active learning is effective in identifying useful\nsubsets of samples to annotate from an unlabeled pool, but its high\ncomputational cost remains a barrier to its widespread applicability in the\ncontext of LLMs. To mitigate the annotation cost of SFT and circumvent the\ncomputational bottlenecks of active learning, we propose using experimental\ndesign. Experimental design techniques select the most informative samples to\nlabel, and typically maximize some notion of uncertainty and/or diversity. In\nour work, we implement a framework that evaluates several existing and novel\nexperimental design techniques and find that these methods consistently yield\nsignificant gains in label efficiency with little computational overhead. On\ngenerative tasks, our methods achieve the same generalization performance with\nonly $50\\%$ of annotation cost required by random sampling.\n","authors":["Gantavya Bhatt","Yifang Chen","Arnav M. Das","Jifan Zhang","Sang T. Truong","Stephen Mussmann","Yinglun Zhu","Jeffrey Bilmes","Simon S. Du","Kevin Jamieson","Jordan T. Ash","Robert D. Nowak"],"pdf_url":"https://arxiv.org/pdf/2401.06692v3.pdf","comment":"Accepted to Findings of the Association for Computational\n  Linguistics: ACL 2024"},{"id":"http://arxiv.org/abs/2407.04495v2","updated":"2024-07-08T02:48:15Z","published":"2024-07-05T13:35:14Z","title":"Speed-accuracy trade-off for the diffusion models: Wisdom from\n  nonequilibrium thermodynamics and optimal transport","summary":"  We discuss a connection between a generative model, called the diffusion\nmodel, and nonequilibrium thermodynamics for the Fokker-Planck equation, called\nstochastic thermodynamics. Based on the techniques of stochastic\nthermodynamics, we derive the speed-accuracy trade-off for the diffusion\nmodels, which is a trade-off relationship between the speed and accuracy of\ndata generation in diffusion models. Our result implies that the entropy\nproduction rate in the forward process affects the errors in data generation.\nFrom a stochastic thermodynamic perspective, our results provide quantitative\ninsight into how best to generate data in diffusion models. The optimal\nlearning protocol is introduced by the conservative force in stochastic\nthermodynamics and the geodesic of space by the 2-Wasserstein distance in\noptimal transport theory. We numerically illustrate the validity of the\nspeed-accuracy trade-off for the diffusion models with different noise\nschedules such as the cosine schedule, the conditional optimal transport, and\nthe optimal transport.\n","authors":["Kotaro Ikeda","Tomoya Uda","Daisuke Okanohara","Sosuke Ito"],"pdf_url":"https://arxiv.org/pdf/2407.04495v2.pdf","comment":"26 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.04308v2","updated":"2024-07-08T02:37:44Z","published":"2024-07-05T07:23:51Z","title":"SSP-GNN: Learning to Track via Bilevel Optimization","summary":"  We propose a graph-based tracking formulation for multi-object tracking (MOT)\nwhere target detections contain kinematic information and re-identification\nfeatures (attributes). Our method applies a successive shortest paths (SSP)\nalgorithm to a tracking graph defined over a batch of frames. The edge costs in\nthis tracking graph are computed via a message-passing network, a graph neural\nnetwork (GNN) variant. The parameters of the GNN, and hence, the tracker, are\nlearned end-to-end on a training set of example ground-truth tracks and\ndetections. Specifically, learning takes the form of bilevel optimization\nguided by our novel loss function. We evaluate our algorithm on simulated\nscenarios to understand its sensitivity to scenario aspects and model\nhyperparameters. Across varied scenario complexities, our method compares\nfavorably to a strong baseline.\n","authors":["Griffin Golias","Masa Nakura-Fan","Vitaly Ablavsky"],"pdf_url":"https://arxiv.org/pdf/2407.04308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00700v3","updated":"2024-07-08T01:59:10Z","published":"2023-12-01T16:33:57Z","title":"GIFT: Generative Interpretable Fine-Tuning","summary":"  We present Generative Interpretable Fine-Tuning (GIFT) for\nparameter-efficient fine-tuning of pretrained Transformer backbones, which can\nbe formulated as a simple factorized matrix multiplication in the parameter\nspace or equivalently in the activation/representation space, and thus embraces\nbuilt-in interpretability. For a layer with weights $\\omega\\in\n\\mathbb{R}^{d_{out}\\times d_{in}}$, our proposed GIFT learns the fine-tuned\nweights $\\hat{\\omega}$ directly from $\\omega$ as $\\hat{\\omega}=\\omega\\cdot\n(\\mathbb{I}+\\phi_{d_{in}\\times r}\\cdot\\psi_{r\\times d_{in}})$. $\\Theta=(\\phi,\n\\psi)$ are the learnable parameters of the two linear layers. $\\Theta$ can be\nshared by all layers selected for fine-tuning (e.g., all the Query and Value\nlayers), or can be layer-type specific (e.g., different $\\Theta$'s used for\nQuery and Value), resulting in significantly fewer trainable parameters\ncompared to layer-specific Low-Rank Adaptation (LoRA). We perform comprehensive\nevaluations on natural language tasks (commonsense and arithmetic reasoning,\ninstruction tuning, and sequence classification), and fine-grained visual\nclassification tasks. We obtain the best performance and parameter efficiency\namong baselines on commonsense reasoning, instruction tuning and visual\nrecognition benchmarks. Compared to LoRA, we obtain 5.9% absolute increase in\naverage accuracy with 53.8 times reduction of parameters on Commonsense170k\nusing Llama-3 (8B), and 5.4% absolute increase in the win rate with 4 times\nreduction of parameters using Llama-2 (7B) during instruction tuning. Our GIFT\nalso obtains a slightly higher win rate on instruction tuning than GPT 3.5\n(Turbo 1106). We show the output of the first linear layer (i.e., $\\omega\\cdot\n\\phi$) is surprisingly interpretable, which can play the role of a\ntoken-clustering head as a by-product to localize meaningful objects/parts in\nimages for computer vision tasks.\n","authors":["Chinmay Savadikar","Xi Song","Tianfu Wu"],"pdf_url":"https://arxiv.org/pdf/2312.00700v3.pdf","comment":"Project page and code: https://savadikarc.github.io/gift"},{"id":"http://arxiv.org/abs/2407.03125v2","updated":"2024-07-08T01:22:37Z","published":"2024-07-03T14:07:41Z","title":"Foundations and Frontiers of Graph Learning Theory","summary":"  Recent advancements in graph learning have revolutionized the way to\nunderstand and analyze data with complex structures. Notably, Graph Neural\nNetworks (GNNs), i.e. neural network architectures designed for learning graph\nrepresentations, have become a popular paradigm. With these models being\nusually characterized by intuition-driven design or highly intricate\ncomponents, placing them within the theoretical analysis framework to distill\nthe core concepts, helps understand the key principles that drive the\nfunctionality better and guide further development. Given this surge in\ninterest, this article provides a comprehensive summary of the theoretical\nfoundations and breakthroughs concerning the approximation and learning\nbehaviors intrinsic to prevalent graph learning models. Encompassing\ndiscussions on fundamental aspects such as expressiveness power,\ngeneralization, optimization, and unique phenomena such as over-smoothing and\nover-squashing, this piece delves into the theoretical foundations and frontier\ndriving the evolution of graph learning. In addition, this article also\npresents several challenges and further initiates discussions on possible\nsolutions.\n","authors":["Yu Huang","Min Zhou","Menglin Yang","Zhen Wang","Muhan Zhang","Jie Wang","Hong Xie","Hao Wang","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.03125v2.pdf","comment":"35pages,273references. Github link:\n  https://github.com/minehly/awesome-paper-for-graph-learning-theory"},{"id":"http://arxiv.org/abs/2311.09402v2","updated":"2024-07-08T00:56:36Z","published":"2023-11-15T21:58:01Z","title":"Synthetically Enhanced: Unveiling Synthetic Data's Potential in Medical\n  Imaging Research","summary":"  Chest X-rays (CXR) are essential for diagnosing a variety of conditions, but\nwhen used on new populations, model generalizability issues limit their\nefficacy. Generative AI, particularly denoising diffusion probabilistic models\n(DDPMs), offers a promising approach to generating synthetic images, enhancing\ndataset diversity. This study investigates the impact of synthetic data\nsupplementation on the performance and generalizability of medical imaging\nresearch. The study employed DDPMs to create synthetic CXRs conditioned on\ndemographic and pathological characteristics from the CheXpert dataset. These\nsynthetic images were used to supplement training datasets for pathology\nclassifiers, with the aim of improving their performance. The evaluation\ninvolved three datasets (CheXpert, MIMIC-CXR, and Emory Chest X-ray) and\nvarious experiments, including supplementing real data with synthetic data,\ntraining with purely synthetic data, and mixing synthetic data with external\ndatasets. Performance was assessed using the area under the receiver operating\ncurve (AUROC). Adding synthetic data to real datasets resulted in a notable\nincrease in AUROC values (up to 0.02 in internal and external test sets with\n1000% supplementation, p-value less than 0.01 in all instances). When\nclassifiers were trained exclusively on synthetic data, they achieved\nperformance levels comparable to those trained on real data with 200%-300% data\nsupplementation. The combination of real and synthetic data from different\nsources demonstrated enhanced model generalizability, increasing model AUROC\nfrom 0.76 to 0.80 on the internal test set (p-value less than 0.01). In\nconclusion, synthetic data supplementation significantly improves the\nperformance and generalizability of pathology classifiers in medical imaging.\n","authors":["Bardia Khosravi","Frank Li","Theo Dapamede","Pouria Rouzrokh","Cooper U. Gamble","Hari M. Trivedi","Cody C. Wyles","Andrew B. Sellergren","Saptarshi Purkayastha","Bradley J. Erickson","Judy W. Gichoya"],"pdf_url":"https://arxiv.org/pdf/2311.09402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15786v2","updated":"2024-07-08T00:28:52Z","published":"2024-06-22T08:41:48Z","title":"What Matters in Transformers? Not All Attention is Needed","summary":"  Scaling Transformer-based large language models (LLMs) has demonstrated\npromising performance across various tasks. However, this scaling also\nintroduces redundant structures, posing challenges for real-world deployment.\nDespite some recognition of redundancy in LLMs, the variability of redundancy\nacross different structures, such as MLP and Attention layers, is\nunder-explored. In this work, we investigate the varying redundancy across\ndifferent modules within Transformers, including Blocks, MLP, and Attention\nlayers, using a similarity-based metric. This metric operates on the premise\nthat redundant structures produce outputs highly similar to their inputs.\nSurprisingly, while attention layers are essential for transformers and\ndistinguish them from other mainstream architectures, we found that a large\nproportion of attention layers exhibit excessively high similarity and can be\nsafely pruned without degrading performance, leading to reduced memory and\ncomputation costs. Additionally, we further propose a method that jointly drops\nAttention and MLP layers, achieving improved performance and dropping ratios.\nExtensive experiments demonstrate the effectiveness of our methods, e.g.,\nLlama-3-70B maintains comparable performance even after pruning half of the\nattention layers. Our findings provide valuable insights for future network\narchitecture design. The code will be released at:\n\\url{https://github.com/Shwai-He/LLM-Drop}.\n","authors":["Shwai He","Guoheng Sun","Zheyu Shen","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2406.15786v2.pdf","comment":"15 pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2401.11646v2","updated":"2024-07-08T00:27:04Z","published":"2024-01-22T01:45:34Z","title":"Nonparametric Density Estimation via Variance-Reduced Sketching","summary":"  Nonparametric density models are of great interest in various scientific and\nengineering disciplines. Classical density kernel methods, while numerically\nrobust and statistically sound in low-dimensional settings, become inadequate\neven in moderate higher-dimensional settings due to the curse of\ndimensionality. In this paper, we introduce a new framework called\nVariance-Reduced Sketching (VRS), specifically designed to estimate\nmultivariable density functions with a reduced curse of dimensionality. Our\nframework conceptualizes multivariable functions as infinite-size matrices, and\nfacilitates a new sketching technique motivated by numerical linear algebra\nliterature to reduce the variance in density estimation problems. We\ndemonstrate the robust numerical performance of VRS through a series of\nsimulated experiments and real-world data applications. Notably, VRS shows\nremarkable improvement over existing neural network estimators and classical\nkernel methods in numerous density models. Additionally, we offer theoretical\njustifications for VRS to support its ability to deliver nonparametric density\nestimation with a reduced curse of dimensionality.\n","authors":["Yifan Peng","Yuehaw Khoo","Daren Wang"],"pdf_url":"https://arxiv.org/pdf/2401.11646v2.pdf","comment":"62 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.05527v1","updated":"2024-07-08T00:21:17Z","published":"2024-07-08T00:21:17Z","title":"Rethinking Image Skip Connections in StyleGAN2","summary":"  Various models based on StyleGAN have gained significant traction in the\nfield of image synthesis, attributed to their robust training stability and\nsuperior performances. Within the StyleGAN framework, the adoption of image\nskip connection is favored over the traditional residual connection. However,\nthis preference is just based on empirical observations; there has not been any\nin-depth mathematical analysis on it yet. To rectify this situation, this brief\naims to elucidate the mathematical meaning of the image skip connection and\nintroduce a groundbreaking methodology, termed the image squeeze connection,\nwhich significantly improves the quality of image synthesis. Specifically, we\nanalyze the image skip connection technique to reveal its problem and introduce\nthe proposed method which not only effectively boosts the GAN performance but\nalso reduces the required number of network parameters. Extensive experiments\non various datasets demonstrate that the proposed method consistently enhances\nthe performance of state-of-the-art models based on StyleGAN. We believe that\nour findings represent a vital advancement in the field of image synthesis,\nsuggesting a novel direction for future research and applications.\n","authors":["Seung Park","Yong-Goo Shin"],"pdf_url":"https://arxiv.org/pdf/2407.05527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05526v1","updated":"2024-07-08T00:19:43Z","published":"2024-07-08T00:19:43Z","title":"Can Machines Learn the True Probabilities?","summary":"  When there exists uncertainty, AI machines are designed to make decisions so\nas to reach the best expected outcomes. Expectations are based on true facts\nabout the objective environment the machines interact with, and those facts can\nbe encoded into AI models in the form of true objective probability functions.\nAccordingly, AI models involve probabilistic machine learning in which the\nprobabilities should be objectively interpreted. We prove under some basic\nassumptions when machines can learn the true objective probabilities, if any,\nand when machines cannot learn them.\n","authors":["Jinsook Kim"],"pdf_url":"https://arxiv.org/pdf/2407.05526v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.06060v1","updated":"2024-07-08T16:01:04Z","published":"2024-07-08T16:01:04Z","title":"MERGE -- A Bimodal Dataset for Static Music Emotion Recognition","summary":"  The Music Emotion Recognition (MER) field has seen steady developments in\nrecent years, with contributions from feature engineering, machine learning,\nand deep learning. The landscape has also shifted from audio-centric systems to\nbimodal ensembles that combine audio and lyrics. However, a severe lack of\npublic and sizeable bimodal databases has hampered the development and\nimprovement of bimodal audio-lyrics systems. This article proposes three new\naudio, lyrics, and bimodal MER research datasets, collectively called MERGE,\ncreated using a semi-automatic approach. To comprehensively assess the proposed\ndatasets and establish a baseline for benchmarking, we conducted several\nexperiments for each modality, using feature engineering, machine learning, and\ndeep learning methodologies. In addition, we propose and validate fixed\ntrain-validate-test splits. The obtained results confirm the viability of the\nproposed datasets, achieving the best overall result of 79.21% F1-score for\nbimodal classification using a deep neural network.\n","authors":["Pedro Lima Louro","Hugo Redinho","Ricardo Santos","Ricardo Malheiro","Renato Panda","Rui Pedro Paiva"],"pdf_url":"https://arxiv.org/pdf/2407.06060v1.pdf","comment":"16 pages, 4 figures, 13 tables, submitted to IEEE Transactions on\n  Affective Computing"},{"id":"http://arxiv.org/abs/2407.06001v1","updated":"2024-07-08T14:53:07Z","published":"2024-07-08T14:53:07Z","title":"Pseudo-triplet Guided Few-shot Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) is a challenging task that aims to retrieve\nthe target image based on a multimodal query, i.e., a reference image and its\ncorresponding modification text. While previous supervised or zero-shot\nlearning paradigms all fail to strike a good trade-off between time-consuming\nannotation cost and retrieval performance, recent researchers introduced the\ntask of few-shot CIR (FS-CIR) and proposed a textual inversion-based network\nbased on pretrained CLIP model to realize it. Despite its promising\nperformance, the approach suffers from two key limitations: insufficient\nmultimodal query composition training and indiscriminative training triplet\nselection. To address these two limitations, in this work, we propose a novel\ntwo-stage pseudo triplet guided few-shot CIR scheme, dubbed PTG-FSCIR. In the\nfirst stage, we employ a masked training strategy and advanced image caption\ngenerator to construct pseudo triplets from pure image data to enable the model\nto acquire primary knowledge related to multimodal query composition. In the\nsecond stage, based on active learning, we design a pseudo modification\ntext-based query-target distance metric to evaluate the challenging score for\neach unlabeled sample. Meanwhile, we propose a robust top range-based random\nsampling strategy according to the 3-$\\sigma$ rule in statistics, to sample the\nchallenging samples for fine-tuning the pretrained model. Notably, our scheme\nis plug-and-play and compatible with any existing supervised CIR models. We\ntested our scheme across three backbones on three public datasets (i.e.,\nFashionIQ, CIRR, and Birds-to-Words), achieving maximum improvements of 26.4%,\n25.5% and 21.6% respectively, demonstrating our scheme's effectiveness.\n","authors":["Bohan Hou","Haoqiang Lin","Haokun Wen","Meng Liu","Xuemeng Song"],"pdf_url":"https://arxiv.org/pdf/2407.06001v1.pdf","comment":"15 pages, 5 figures,"},{"id":"http://arxiv.org/abs/2403.08551v4","updated":"2024-07-08T13:22:14Z","published":"2024-03-13T14:02:54Z","title":"GaussianImage: 1000 FPS Image Representation and Compression by 2D\n  Gaussian Splatting","summary":"  Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 2000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding. Code is available at\nhttps://github.com/Xinjie-Q/GaussianImage.\n","authors":["Xinjie Zhang","Xingtong Ge","Tongda Xu","Dailan He","Yan Wang","Hongwei Qin","Guo Lu","Jing Geng","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08551v4.pdf","comment":"Accepted by ECCV 2024. Project\n  Page:https://xingtongge.github.io/GaussianImage-page; Code:\n  https://github.com/Xinjie-Q/GaussianImage"},{"id":"http://arxiv.org/abs/2403.10883v2","updated":"2024-07-08T12:03:14Z","published":"2024-03-16T10:32:24Z","title":"Improving Adversarial Transferability of Vision-Language Pre-training\n  Models through Collaborative Multimodal Interaction","summary":"  Despite the substantial advancements in Vision-Language Pre-training (VLP)\nmodels, their susceptibility to adversarial attacks poses a significant\nchallenge. Existing work rarely studies the transferability of attacks on VLP\nmodels, resulting in a substantial performance gap from white-box attacks. We\nobserve that prior work overlooks the interaction mechanisms between\nmodalities, which plays a crucial role in understanding the intricacies of VLP\nmodels. In response, we propose a novel attack, called Collaborative Multimodal\nInteraction Attack (CMI-Attack), leveraging modality interaction through\nembedding guidance and interaction enhancement. Specifically, attacking text at\nthe embedding level while preserving semantics, as well as utilizing\ninteraction image gradients to enhance constraints on perturbations of texts\nand images. Significantly, in the image-text retrieval task on Flickr30K\ndataset, CMI-Attack raises the transfer success rates from ALBEF to TCL,\n$\\text{CLIP}_{\\text{ViT}}$ and $\\text{CLIP}_{\\text{CNN}}$ by 8.11%-16.75% over\nstate-of-the-art methods. Moreover, CMI-Attack also demonstrates superior\nperformance in cross-task generalization scenarios. Our work addresses the\nunderexplored realm of transfer attacks on VLP models, shedding light on the\nimportance of modality interaction for enhanced adversarial robustness.\n","authors":["Jiyuan Fu","Zhaoyu Chen","Kaixun Jiang","Haijing Guo","Jiafeng Wang","Shuyong Gao","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.10883v2.pdf","comment":"This work won first place in CVPR 2024 Workshop Challenge: Black-box\n  Adversarial Attacks on Vision Foundation Models"},{"id":"http://arxiv.org/abs/2407.05814v1","updated":"2024-07-08T10:51:03Z","published":"2024-07-08T10:51:03Z","title":"Cross-domain Few-shot In-context Learning for Enhancing Traffic Sign\n  Recognition","summary":"  Recent multimodal large language models (MLLM) such as GPT-4o and GPT-4v have\nshown great potential in autonomous driving. In this paper, we propose a\ncross-domain few-shot in-context learning method based on the MLLM for\nenhancing traffic sign recognition (TSR). We first construct a traffic sign\ndetection network based on Vision Transformer Adapter and an extraction module\nto extract traffic signs from the original road images. To reduce the\ndependence on training data and improve the performance stability of\ncross-country TSR, we introduce a cross-domain few-shot in-context learning\nmethod based on the MLLM. To enhance MLLM's fine-grained recognition ability of\ntraffic signs, the proposed method generates corresponding description texts\nusing template traffic signs. These description texts contain key information\nabout the shape, color, and composition of traffic signs, which can stimulate\nthe ability of MLLM to perceive fine-grained traffic sign categories. By using\nthe description texts, our method reduces the cross-domain differences between\ntemplate and real traffic signs. Our approach requires only simple and uniform\ntextual indications, without the need for large-scale traffic sign images and\nlabels. We perform comprehensive evaluations on the German traffic sign\nrecognition benchmark dataset, the Belgium traffic sign dataset, and two\nreal-world datasets taken from Japan. The experimental results show that our\nmethod significantly enhances the TSR performance.\n","authors":["Yaozong Gan","Guang Li","Ren Togo","Keisuke Maeda","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2407.05814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.14272v3","updated":"2024-07-08T10:50:56Z","published":"2022-09-28T17:36:47Z","title":"Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and\n  First Results","summary":"  Humor is a substantial element of human social behavior, affect, and\ncognition. Its automatic understanding can facilitate a more naturalistic\nhuman-AI interaction. Current methods of humor detection have been exclusively\nbased on staged data, making them inadequate for \"real-world\" applications. We\ncontribute to addressing this deficiency by introducing the novel\nPassau-Spontaneous Football Coach Humor (Passau-SFCH) dataset, comprising about\n11 hours of recordings. The Passau-SFCH dataset is annotated for the presence\nof humor and its dimensions (sentiment and direction) as proposed in Martin's\nHumor Style Questionnaire. We conduct a series of experiments employing\npretrained Transformers, convolutional neural networks, and expert-designed\nfeatures. The performance of each modality (text, audio, video) for spontaneous\nhumor recognition is analyzed and their complementarity is investigated. Our\nfindings suggest that for the automatic analysis of humor and its sentiment,\nfacial expressions are most promising, while humor direction can be best\nmodeled via text-based features. Further, we experiment with different\nmultimodal approaches to humor recognition, including decision-level fusion and\nMulT, a multimodal Transformer approach. In this context, we propose a novel\nmultimodal architecture that yields the best overall results. Finally, we make\nour code publicly available at https://www.github.com/lc0197/passau-sfch. The\nPassau-SFCH dataset is available upon request.\n","authors":["Lukas Christ","Shahin Amiriparian","Alexander Kathan","Niklas MÃ¼ller","Andreas KÃ¶nig","BjÃ¶rn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2209.14272v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible (Major Revision)"},{"id":"http://arxiv.org/abs/2407.05782v1","updated":"2024-07-08T09:45:20Z","published":"2024-07-08T09:45:20Z","title":"Sequential Contrastive Audio-Visual Learning","summary":"  Contrastive learning has emerged as a powerful technique in audio-visual\nrepresentation learning, leveraging the natural co-occurrence of audio and\nvisual modalities in extensive web-scale video datasets to achieve significant\nadvancements. However, conventional contrastive audio-visual learning\nmethodologies often rely on aggregated representations derived through temporal\naggregation, which neglects the intrinsic sequential nature of the data. This\noversight raises concerns regarding the ability of standard approaches to\ncapture and utilize fine-grained information within sequences, information that\nis vital for distinguishing between semantically similar yet distinct examples.\nIn response to this limitation, we propose sequential contrastive audio-visual\nlearning (SCAV), which contrasts examples based on their non-aggregated\nrepresentation space using sequential distances. Retrieval experiments with the\nVGGSound and Music datasets demonstrate the effectiveness of SCAV, showing 2-3x\nrelative improvements against traditional aggregation-based contrastive\nlearning and other methods from the literature. We also show that models\ntrained with SCAV exhibit a high degree of flexibility regarding the metric\nemployed for retrieval, allowing them to operate on a spectrum of\nefficiency-accuracy trade-offs, potentially making them applicable in multiple\nscenarios, from small- to large-scale retrieval.\n","authors":["Ioannis Tsiamas","Santiago Pascual","Chunghsin Yeh","Joan SerrÃ "],"pdf_url":"https://arxiv.org/pdf/2407.05782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10003v2","updated":"2024-07-08T09:40:03Z","published":"2023-07-19T14:23:26Z","title":"TbExplain: A Text-based Explanation Method for Scene Classification\n  Models with the Statistical Prediction Correction","summary":"  The field of Explainable Artificial Intelligence (XAI) aims to improve the\ninterpretability of black-box machine learning models. Building a heatmap based\non the importance value of input features is a popular method for explaining\nthe underlying functions of such models in producing their predictions.\nHeatmaps are almost understandable to humans, yet they are not without flaws.\nNon-expert users, for example, may not fully understand the logic of heatmaps\n(the logic in which relevant pixels to the model's prediction are highlighted\nwith different intensities or colors). Additionally, objects and regions of the\ninput image that are relevant to the model prediction are frequently not\nentirely differentiated by heatmaps. In this paper, we propose a framework\ncalled TbExplain that employs XAI techniques and a pre-trained object detector\nto present text-based explanations of scene classification models. Moreover,\nTbExplain incorporates a novel method to correct predictions and textually\nexplain them based on the statistics of objects in the input image when the\ninitial prediction is unreliable. To assess the trustworthiness and validity of\nthe text-based explanations, we conducted a qualitative experiment, and the\nfindings indicated that these explanations are sufficiently reliable.\nFurthermore, our quantitative and qualitative experiments on TbExplain with\nscene classification datasets reveal an improvement in classification accuracy\nover ResNet variants.\n","authors":["Amirhossein Aminimehr","Pouya Khani","Amirali Molaei","Amirmohammad Kazemeini","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2307.10003v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13049v3","updated":"2024-07-08T07:32:28Z","published":"2024-05-19T09:59:00Z","title":"SemEval-2024 Task 3: Multimodal Emotion Cause Analysis in Conversations","summary":"  The ability to understand emotions is an essential component of human-like\nartificial intelligence, as emotions greatly influence human cognition,\ndecision making, and social interactions. In addition to emotion recognition in\nconversations, the task of identifying the potential causes behind an\nindividual's emotional state in conversations, is of great importance in many\napplication scenarios. We organize SemEval-2024 Task 3, named Multimodal\nEmotion Cause Analysis in Conversations, which aims at extracting all pairs of\nemotions and their corresponding causes from conversations. Under different\nmodality settings, it consists of two subtasks: Textual Emotion-Cause Pair\nExtraction in Conversations (TECPE) and Multimodal Emotion-Cause Pair\nExtraction in Conversations (MECPE). The shared task has attracted 143\nregistrations and 216 successful submissions. In this paper, we introduce the\ntask, dataset and evaluation settings, summarize the systems of the top teams,\nand discuss the findings of the participants.\n","authors":["Fanfan Wang","Heqing Ma","Jianfei Yu","Rui Xia","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2405.13049v3.pdf","comment":"Accepted to the 18th International Workshop on Semantic Evaluation\n  (SemEval-2024). 12 pages, 3 figures, 4 Tables"},{"id":"http://arxiv.org/abs/2407.05645v1","updated":"2024-07-08T06:14:37Z","published":"2024-07-08T06:14:37Z","title":"OneDiff: A Generalist Model for Image Difference","summary":"  In computer vision, Image Difference Captioning (IDC) is crucial for\naccurately describing variations between closely related images. Traditional\nIDC methods often rely on specialist models, which restrict their applicability\nacross varied contexts. This paper introduces the OneDiff model, a novel\ngeneralist approach that utilizes a robust vision-language model architecture,\nintegrating a siamese image encoder with a Visual Delta Module. This innovative\nconfiguration allows for the precise detection and articulation of fine-grained\ndifferences between image pairs. OneDiff is trained through a dual-phase\nstrategy, encompassing Coupled Sample Training and multi-task learning across a\ndiverse array of data types, supported by our newly developed DiffCap Dataset.\nThis dataset merges real-world and synthetic data, enhancing the training\nprocess and bolstering the model's robustness. Extensive testing on diverse IDC\nbenchmarks, such as Spot-the-Diff, CLEVR-Change, and Birds-to-Words, shows that\nOneDiff consistently outperforms existing state-of-the-art models in accuracy\nand adaptability, achieving improvements of up to 85\\% CIDEr points in average.\nBy setting a new benchmark in IDC, OneDiff paves the way for more versatile and\neffective applications in detecting and describing visual differences. The\ncode, models, and data will be made publicly available.\n","authors":["Erdong Hu","Longteng Guo","Tongtian Yue","Zijia Zhao","Shuning Xue","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2407.05645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05551v1","updated":"2024-07-08T01:59:17Z","published":"2024-07-08T01:59:17Z","title":"Read, Watch and Scream! Sound Generation from Text and Video","summary":"  Multimodal generative models have shown impressive advances with the help of\npowerful diffusion models. Despite the progress, generating sound solely from\ntext poses challenges in ensuring comprehensive scene depiction and temporal\nalignment. Meanwhile, video-to-sound generation limits the flexibility to\nprioritize sound synthesis for specific objects within the scene. To tackle\nthese challenges, we propose a novel video-and-text-to-sound generation method,\ncalled ReWaS, where video serves as a conditional control for a text-to-audio\ngeneration model. Our method estimates the structural information of audio\n(namely, energy) from the video while receiving key content cues from a user\nprompt. We employ a well-performing text-to-sound model to consolidate the\nvideo control, which is much more efficient for training multimodal diffusion\nmodels with massive triplet-paired (audio-video-text) data. In addition, by\nseparating the generative components of audio, it becomes a more flexible\nsystem that allows users to freely adjust the energy, surrounding environment,\nand primary sound source according to their preferences. Experimental results\ndemonstrate that our method shows superiority in terms of quality,\ncontrollability, and training efficiency. Our demo is available at\nhttps://naver-ai.github.io/rewas\n","authors":["Yujin Jeong","Yunji Kim","Sanghyuk Chun","Jiyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2407.05551v1.pdf","comment":"Project page: https://naver-ai.github.io/rewas"}]},"2024-07-10T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.07895v1","updated":"2024-07-10T17:59:43Z","published":"2024-07-10T17:59:43Z","title":"LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large\n  Multimodal Models","summary":"  Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT\n","authors":["Feng Li","Renrui Zhang","Hao Zhang","Yuanhan Zhang","Bo Li","Wei Li","Zejun Ma","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2407.07895v1.pdf","comment":"Project Page:\n  https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/"},{"id":"http://arxiv.org/abs/2407.07890v1","updated":"2024-07-10T17:57:58Z","published":"2024-07-10T17:57:58Z","title":"Training on the Test Task Confounds Evaluation and Emergence","summary":"  We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of techniques to\ninclude task-relevant data in the pretraining stage of a language model. We\ndemonstrate that training on the test task confounds both relative model\nevaluations and claims about emergent capabilities. We argue that the seeming\nsuperiority of one model family over another may be explained by a different\ndegree of training on the test task. To this end, we propose an effective\nmethod to adjust for training on the test task by fine-tuning each model under\ncomparison on the same task-relevant data before evaluation. We then show that\ninstances of emergent behavior largely vanish once we adjust for training on\nthe test task. This also applies to reported instances of emergent behavior\nthat cannot be explained by the choice of evaluation metric. Our work promotes\na new perspective on the evaluation of large language models with broad\nimplications for benchmarking and the study of emergent capabilities.\n","authors":["Ricardo Dominguez-Olmedo","Florian E. Dorner","Moritz Hardt"],"pdf_url":"https://arxiv.org/pdf/2407.07890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08460v2","updated":"2024-07-10T17:57:01Z","published":"2024-05-14T09:31:31Z","title":"Is Your LLM Outdated? Evaluating LLMs at Temporal Generalization","summary":"  The rapid advancement of Large Language Models (LLMs) highlights the urgent\nneed for evolving evaluation methodologies that keep pace with improvements in\nlanguage comprehension and information processing. However, traditional\nbenchmarks, which are often static, fail to capture the continually changing\ninformation landscape, leading to a disparity between the perceived and actual\neffectiveness of LLMs in ever-changing real-world scenarios. Our study examines\ntemporal generalization, which includes the ability to understand, predict, and\ngenerate text relevant to past, present, and future contexts, revealing\nsignificant temporal biases in LLMs. We propose an evaluation framework, for\ndynamically generating benchmarks from recent real-world predictions.\nExperiments demonstrate that LLMs struggle with temporal generalization,\nshowing performance decline over time. These findings highlight the necessity\nfor improved training and updating processes to enhance adaptability and reduce\nbiases. Our code, dataset and benchmark are available at\nhttps://github.com/FreedomIntelligence/FreshBench.\n","authors":["Chenghao Zhu","Nuo Chen","Yufei Gao","Yunyi Zhang","Prayag Tiwari","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2405.08460v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.07880v1","updated":"2024-07-10T17:48:25Z","published":"2024-07-10T17:48:25Z","title":"Towards Robust Alignment of Language Models: Distributionally\n  Robustifying Direct Preference Optimization","summary":"  This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO.\n","authors":["Junkang Wu","Yuexiang Xie","Zhengyi Yang","Jiancan Wu","Jiawei Chen","Jinyang Gao","Bolin Ding","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2407.07880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07875v1","updated":"2024-07-10T17:41:10Z","published":"2024-07-10T17:41:10Z","title":"Generative Image as Action Models","summary":"  Image-generation diffusion models have been fine-tuned to unlock new\ncapabilities such as image-editing and novel view synthesis. Can we similarly\nunlock image-generation models for visuomotor control? We present GENIMA, a\nbehavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'\nas targets on RGB images. These images are fed into a controller that maps the\nvisual targets into a sequence of joint-positions. We study GENIMA on 25\nRLBench and 9 real-world manipulation tasks. We find that, by lifting actions\ninto image-space, internet pre-trained diffusion models can generate policies\nthat outperform state-of-the-art visuomotor approaches, especially in\nrobustness to scene perturbations and generalizing to novel objects. Our method\nis also competitive with 3D agents, despite lacking priors such as depth,\nkeypoints, or motion-planners.\n","authors":["Mohit Shridhar","Yat Long Lo","Stephen James"],"pdf_url":"https://arxiv.org/pdf/2407.07875v1.pdf","comment":"Project website, code, checkpoints: https://genima-robot.github.io/"},{"id":"http://arxiv.org/abs/2403.02502v2","updated":"2024-07-10T17:36:25Z","published":"2024-03-04T21:50:29Z","title":"Trial and Error: Exploration-Based Trajectory Optimization for LLM\n  Agents","summary":"  Large Language Models (LLMs) have become integral components in various\nautonomous agent systems. In this study, we present an exploration-based\ntrajectory optimization approach, referred to as ETO. This learning method is\ndesigned to enhance the performance of open LLM agents. Contrary to previous\nstudies that exclusively train on successful expert trajectories, our method\nallows agents to learn from their exploration failures. This leads to improved\nperformance through an iterative optimization framework. During the exploration\nphase, the agent interacts with the environment while completing given tasks,\ngathering failure trajectories to create contrastive trajectory pairs. In the\nsubsequent training phase, the agent utilizes these trajectory preference pairs\nto update its policy using contrastive learning methods like DPO. This\niterative cycle of exploration and training fosters continued improvement in\nthe agents. Our experiments on three complex tasks demonstrate that ETO\nconsistently surpasses baseline performance by a large margin. Furthermore, an\nexamination of task-solving efficiency and potential in scenarios lacking\nexpert trajectory underscores the effectiveness of our approach.\n","authors":["Yifan Song","Da Yin","Xiang Yue","Jie Huang","Sujian Li","Bill Yuchen Lin"],"pdf_url":"https://arxiv.org/pdf/2403.02502v2.pdf","comment":"Accepted to ACL 2024 Main Conference; Camera Ready"},{"id":"http://arxiv.org/abs/2311.05657v3","updated":"2024-07-10T17:36:02Z","published":"2023-11-09T00:30:13Z","title":"Agent Lumos: Unified and Modular Training for Open-Source Language\n  Agents","summary":"  Closed-source agents suffer from several issues such as a lack of\naffordability, transparency, and reproducibility, particularly on complex\ninteractive tasks. This motivates the development of open-source alternatives.\nWe introduce LUMOS, one of the first frameworks for training open-source\nLLM-based agents. LUMOS features a learnable, unified, and modular architecture\nwith a planning module that learns high-level subgoal generation, and a\ngrounding module trained to translate these into actions using various tools in\nthe execution module. The design allows for modular upgrades and wider\napplicability to diverse interactive tasks. To foster generalizable agent\nlearning, we collect large-scale, unified, and high-quality training\nannotations derived from diverse ground-truth reasoning rationales across\nvarious complex interactive tasks. On 9 datasets, LUMOS exhibits several key\nadvantages: (1) LUMOS excels multiple larger open-source agents on the held-out\ndatasets (unused for training) for each task type. LUMOS even surpasses GPT\nagents on QA and web tasks; (2) LUMOS outperforms open-source agents produced\nby chain-of-thoughts and unmodularized integrated training; and (3) LUMOS\neffectively generalizes to unseen tasks, outperforming 33B-scale agents and\ndomain-specific agents.\n","authors":["Da Yin","Faeze Brahman","Abhilasha Ravichander","Khyathi Chandu","Kai-Wei Chang","Yejin Choi","Bill Yuchen Lin"],"pdf_url":"https://arxiv.org/pdf/2311.05657v3.pdf","comment":"Accepted to ACL 2024 Main Conference; Camera Ready. Project website:\n  https://allenai.github.io/lumos/"},{"id":"http://arxiv.org/abs/2312.02249v2","updated":"2024-07-10T17:26:21Z","published":"2023-12-04T17:27:24Z","title":"Recursive Visual Programming","summary":"  Visual Programming (VP) has emerged as a powerful framework for Visual\nQuestion Answering (VQA). By generating and executing bespoke code for each\nquestion, these methods demonstrate impressive compositional and reasoning\ncapabilities, especially in few-shot and zero-shot scenarios. However, existing\nVP methods generate all code in a single function, resulting in code that is\nsuboptimal in terms of both accuracy and interpretability. Inspired by human\ncoding practices, we propose Recursive Visual Programming (RVP), which\nsimplifies generated routines, provides more efficient problem solving, and can\nmanage more complex data structures. RVP is inspired by human coding practices\nand approaches VQA tasks with an iterative recursive code generation approach,\nallowing decomposition of complicated problems into smaller parts. Notably, RVP\nis capable of dynamic type assignment, i.e., as the system recursively\ngenerates a new piece of code, it autonomously determines the appropriate\nreturn type and crafts the requisite code to generate that output. We show\nRVP's efficacy through extensive experiments on benchmarks including VSR, COVR,\nGQA, and NextQA, underscoring the value of adopting human-like recursive and\nmodular programming techniques for solving VQA tasks through coding.\n","authors":["Jiaxin Ge","Sanjay Subramanian","Baifeng Shi","Roei Herzig","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2312.02249v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07858v1","updated":"2024-07-10T17:20:59Z","published":"2024-07-10T17:20:59Z","title":"FACTS About Building Retrieval Augmented Generation-based Chatbots","summary":"  Enterprise chatbots, powered by generative AI, are emerging as key\napplications to enhance employee productivity. Retrieval Augmented Generation\n(RAG), Large Language Models (LLMs), and orchestration frameworks like\nLangchain and Llamaindex are crucial for building these chatbots. However,\ncreating effective enterprise chatbots is challenging and requires meticulous\nRAG pipeline engineering. This includes fine-tuning embeddings and LLMs,\nextracting documents from vector databases, rephrasing queries, reranking\nresults, designing prompts, honoring document access controls, providing\nconcise responses, including references, safeguarding personal information, and\nbuilding orchestration agents. We present a framework for building RAG-based\nchatbots based on our experience with three NVIDIA chatbots: for IT/HR\nbenefits, financial earnings, and general content. Our contributions are\nthree-fold: introducing the FACTS framework (Freshness, Architectures, Cost,\nTesting, Security), presenting fifteen RAG pipeline control points, and\nproviding empirical results on accuracy-latency tradeoffs between large and\nsmall LLMs. To the best of our knowledge, this is the first paper of its kind\nthat provides a holistic view of the factors as well as solutions for building\nsecure enterprise-grade chatbots.\"\n","authors":["Rama Akkiraju","Anbang Xu","Deepak Bora","Tan Yu","Lu An","Vishal Seth","Aaditya Shukla","Pritam Gundecha","Hridhay Mehta","Ashwin Jha","Prithvi Raj","Abhinav Balasubramanian","Murali Maram","Guru Muthusamy","Shivakesh Reddy Annepally","Sidney Knowles","Min Du","Nick Burnett","Sean Javiya","Ashok Marannan","Mamta Kumari","Surbhi Jha","Ethan Dereszenski","Anupam Chakraborty","Subhash Ranjan","Amina Terfai","Anoop Surya","Tracey Mercer","Vinodh Kumar Thanigachalam","Tamar Bar","Sanjana Krishnan","Samy Kilaru","Jasmine Jaksic","Nave Algarici","Jacob Liberman","Joey Conway","Sonu Nayyar","Justin Boitano"],"pdf_url":"https://arxiv.org/pdf/2407.07858v1.pdf","comment":"8 pages, 6 figures, 2 tables, Preprint submission to ACM CIKM 2024"},{"id":"http://arxiv.org/abs/2310.00796v3","updated":"2024-07-10T17:09:58Z","published":"2023-10-01T21:19:12Z","title":"SIP: Injecting a Structural Inductive Bias into a Seq2Seq Model by\n  Simulation","summary":"  Strong inductive biases enable learning from little data and help\ngeneralization outside of the training distribution. Popular neural\narchitectures such as Transformers lack strong structural inductive biases for\nseq2seq NLP tasks on their own. Consequently, they struggle with systematic\ngeneralization beyond the training distribution, e.g. with extrapolating to\nlonger inputs, even when pre-trained on large amounts of text. We show how a\nstructural inductive bias can be efficiently injected into a seq2seq model by\npre-training it to simulate structural transformations on synthetic data.\nSpecifically, we inject an inductive bias towards Finite State Transducers\n(FSTs) into a Transformer by pre-training it to simulate FSTs given their\ndescriptions. Our experiments show that our method imparts the desired\ninductive bias, resulting in improved systematic generalization and better\nfew-shot learning for FST-like tasks. Our analysis shows that fine-tuned models\naccurately capture the state dynamics of the unseen underlying FSTs, suggesting\nthat the simulation process is internalized by the fine-tuned model.\n","authors":["Matthias Lindemann","Alexander Koller","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2310.00796v3.pdf","comment":"ACL 2024 camera-ready"},{"id":"http://arxiv.org/abs/2402.14848v2","updated":"2024-07-10T17:01:37Z","published":"2024-02-19T16:04:53Z","title":"Same Task, More Tokens: the Impact of Input Length on the Reasoning\n  Performance of Large Language Models","summary":"  This paper explores the impact of extending input lengths on the capabilities\nof Large Language Models (LLMs). Despite LLMs advancements in recent times,\ntheir performance consistency across different input lengths is not well\nunderstood. We investigate this aspect by introducing a novel QA reasoning\nframework, specifically designed to assess the impact of input length. We\nisolate the effect of input length using multiple versions of the same sample,\neach being extended with padding of different lengths, types and locations. Our\nfindings show a notable degradation in LLMs' reasoning performance at much\nshorter input lengths than their technical maximum. We show that the\ndegradation trend appears in every version of our dataset, although at\ndifferent intensities. Additionally, our study reveals that the traditional\nmetric of next word prediction correlates negatively with performance of LLMs'\non our reasoning dataset. We analyse our results and identify failure modes\nthat can serve as useful guides for future research, potentially informing\nstrategies to address the limitations observed in LLMs.\n","authors":["Mosh Levy","Alon Jacoby","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2402.14848v2.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2407.07840v1","updated":"2024-07-10T17:00:29Z","published":"2024-07-10T17:00:29Z","title":"Decompose and Compare Consistency: Measuring VLMs' Answer Reliability\n  via Task-Decomposition Consistency Comparison","summary":"  Despite tremendous advancements, current state-of-the-art Vision-Language\nModels (VLMs) are still far from perfect. They tend to hallucinate and may\ngenerate biased responses. In such circumstances, having a way to assess the\nreliability of a given response generated by a VLM is quite useful. Existing\nmethods, such as estimating uncertainty using answer likelihoods or\nprompt-based confidence generation, often suffer from overconfidence. Other\nmethods use self-consistency comparison but are affected by confirmation\nbiases. To alleviate these, we propose \\textbf{De}compose and \\textbf{C}ompare\n\\textbf{C}onsistency (\\texttt{DeCC}) for reliability measurement. By comparing\nthe consistency between the direct answer generated using the VLM's internal\nreasoning process, and the indirect answers obtained by decomposing the\nquestion into sub-questions and reasoning over the sub-answers produced by the\nVLM, \\texttt{DeCC} measures the reliability of VLM's direct answer. Experiments\nacross six vision-language tasks with three VLMs show \\texttt{DeCC}'s\nreliability estimation achieves better correlation with task accuracy compared\nto the existing methods.\n","authors":["Qian Yang","Weixiang Yan","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.07840v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2405.19327v4","updated":"2024-07-10T16:55:47Z","published":"2024-05-29T17:57:16Z","title":"MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model\n  Series","summary":"  Large Language Models (LLMs) have made great strides in recent years to\nachieve unprecedented performance across different tasks. However, due to\ncommercial interest, the most competitive models like GPT, Gemini, and Claude\nhave been gated behind proprietary interfaces without disclosing the training\ndetails. Recently, many institutions have open-sourced several strong LLMs like\nLLaMA-3, comparable to existing closed-source LLMs. However, only the model's\nweights are provided with most details (e.g., intermediate checkpoints,\npre-training corpus, and training code, etc.) being undisclosed. To improve the\ntransparency of LLMs, the research community has formed to open-source truly\nopen LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training\ncorpus and training code) are being provided. These models have greatly\nadvanced the scientific study of these large models including their strengths,\nweaknesses, biases and risks. However, we observe that the existing truly open\nLLMs on reasoning, knowledge, and coding tasks are still inferior to existing\nstate-of-the-art LLMs with similar model sizes. To this end, we open-source\nMAP-Neo, a highly capable and transparent bilingual language model with 7B\nparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the\nfirst fully open-sourced bilingual LLM with comparable performance compared to\nexisting state-of-the-art LLMs. Moreover, we open-source all details to\nreproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning\npipeline, checkpoints, and well-optimized training/evaluation framework are\nprovided. Finally, we hope our MAP-Neo will enhance and strengthen the open\nresearch community and inspire more innovations and creativities to facilitate\nthe further improvements of LLMs.\n","authors":["Ge Zhang","Scott Qu","Jiaheng Liu","Chenchen Zhang","Chenghua Lin","Chou Leuang Yu","Danny Pan","Esther Cheng","Jie Liu","Qunshu Lin","Raven Yuan","Tuney Zheng","Wei Pang","Xinrun Du","Yiming Liang","Yinghao Ma","Yizhi Li","Ziyang Ma","Bill Lin","Emmanouil Benetos","Huan Yang","Junting Zhou","Kaijing Ma","Minghao Liu","Morry Niu","Noah Wang","Quehry Que","Ruibo Liu","Sine Liu","Shawn Guo","Soren Gao","Wangchunshu Zhou","Xinyue Zhang","Yizhi Zhou","Yubo Wang","Yuelin Bai","Yuhan Zhang","Yuxiang Zhang","Zenith Wang","Zhenzhu Yang","Zijian Zhao","Jiajun Zhang","Wanli Ouyang","Wenhao Huang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2405.19327v4.pdf","comment":"https://map-neo.github.io/"},{"id":"http://arxiv.org/abs/2404.04167v4","updated":"2024-07-10T16:51:17Z","published":"2024-04-05T15:20:02Z","title":"Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model","summary":"  In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models.\n","authors":["Xinrun Du","Zhouliang Yu","Songyang Gao","Ding Pan","Yuyang Cheng","Ziyang Ma","Ruibin Yuan","Xingwei Qu","Jiaheng Liu","Tianyu Zheng","Xinchen Luo","Guorui Zhou","Wenhu Chen","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.04167v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07810v1","updated":"2024-07-10T16:30:27Z","published":"2024-07-10T16:30:27Z","title":"Transformer Alignment in Large Language Models","summary":"  Large Language Models (LLMs) have made significant strides in natural\nlanguage processing, and a precise understanding of the internal mechanisms\ndriving their success is essential. We regard LLMs as transforming embeddings\nvia a discrete, coupled, nonlinear, dynamical system in high dimensions. This\nperspective motivates tracing the trajectories of individual tokens as they\npass through transformer blocks, and linearizing the system along these\ntrajectories through their Jacobian matrices. In our analysis of 38 openly\navailable LLMs, we uncover the alignment of top left and right singular vectors\nof Residual Jacobians, as well as the emergence of linearity and layer-wise\nexponential growth. Notably, we discover that increased alignment\n$\\textit{positively correlates}$ with model performance. Metrics evaluated\npost-training show significant improvement in comparison to measurements made\nwith randomly initialized weights, highlighting the significant effects of\ntraining in transformers. These findings reveal a remarkable level of\nregularity that has previously been overlooked, reinforcing the dynamical\ninterpretation and paving the way for deeper understanding and optimization of\nLLM architectures.\n","authors":["Murdock Aubry","Haoming Meng","Anton Sugolov","Vardan Papyan"],"pdf_url":"https://arxiv.org/pdf/2407.07810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07802v1","updated":"2024-07-10T16:20:53Z","published":"2024-07-10T16:20:53Z","title":"ROSA: Random Subspace Adaptation for Efficient Fine-Tuning","summary":"  Model training requires significantly more memory, compared with inference.\nParameter efficient fine-tuning (PEFT) methods provide a means of adapting\nlarge models to downstream tasks using less memory. However, existing methods\nsuch as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce\nlatency overhead at inference time or achieve subpar downstream performance\ncompared with full fine-tuning. In this work we propose Random Subspace\nAdaptation (ROSA), a method that outperforms previous PEFT methods by a\nsignificant margin, while maintaining a zero latency overhead during inference\ntime. In contrast to previous methods, ROSA is able to adapt subspaces of\narbitrarily large dimension, better approximating full-finetuning. We\ndemonstrate both theoretically and experimentally that this makes ROSA strictly\nmore expressive than LoRA, without consuming additional memory during runtime.\nAs PEFT methods are especially useful in the natural language processing\ndomain, where models operate on scales that make full fine-tuning very\nexpensive, we evaluate ROSA in two common NLP scenarios: natural language\ngeneration (NLG) and natural language understanding (NLU) with GPT-2 and\nRoBERTa, respectively. We show that on almost every GLUE task ROSA outperforms\nLoRA by a significant margin, while also outperforming LoRA on NLG tasks. Our\ncode is available at https://github.com/rosa-paper/rosa\n","authors":["Marawan Gamal Abdel Hameed","Aristides Milios","Siva Reddy","Guillaume Rabusseau"],"pdf_url":"https://arxiv.org/pdf/2407.07802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07801v1","updated":"2024-07-10T16:17:49Z","published":"2024-07-10T16:17:49Z","title":"AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning","summary":"  In recent years, advancements in representation learning and language models\nhave propelled Automated Captioning (AC) to new heights, enabling the\ngeneration of human-level descriptions. Leveraging these advancements, we\npropose \\textbf{AVCap}, an \\textbf{A}udio-\\textbf{V}isual \\textbf{Cap}tioning\nframework, a simple yet powerful baseline approach applicable to audio-visual\ncaptioning. AVCap utilizes audio-visual features as text tokens, which has many\nadvantages not only in performance but also in the extensibility and\nscalability of the model. AVCap is designed around three pivotal dimensions:\nthe exploration of optimal audio-visual encoder architectures, the adaptation\nof pre-trained models according to the characteristics of generated text, and\nthe investigation into the efficacy of modality fusion in captioning. Our\nmethod outperforms existing audio-visual captioning methods across all metrics\nand the code is available on https://github.com/JongSuk1/AVCap\n","authors":["Jongsuk Kim","Jiwon Shin","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.07801v1.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2407.07799v1","updated":"2024-07-10T16:16:02Z","published":"2024-07-10T16:16:02Z","title":"Attribute or Abstain: Large Language Models as Long Document Assistants","summary":"  LLMs can help humans working with long documents, but are known to\nhallucinate. Attribution can increase trust in LLM responses: The LLM provides\nevidence that supports its response, which enhances verifiability. Existing\napproaches to attribution have only been evaluated in RAG settings, where the\ninitial retrieval confounds LLM performance. This is crucially different from\nthe long document setting, where retrieval is not needed, but could help. Thus,\na long document specific evaluation of attribution is missing. To fill this\ngap, we present LAB, a benchmark of 6 diverse long document tasks with\nattribution, and experiment with different approaches to attribution on 4 LLMs\nof different sizes, both prompted and fine-tuned. We find that citation, i.e.\nresponse generation and evidence extraction in one step, mostly performs best.\nWe investigate whether the ``Lost in the Middle'' phenomenon exists for\nattribution, but do not find this. We also find that evidence quality can\npredict response quality on datasets with simple responses, but not so for\ncomplex responses, as models struggle with providing evidence for complex\nclaims. We release code and data for further investigation.\n","authors":["Jan Buchmann","Xiao Liu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2407.07799v1.pdf","comment":"Code and data:\n  https://github.com/UKPLab/arxiv2024-attribute-or-abstain"},{"id":"http://arxiv.org/abs/2407.07796v1","updated":"2024-07-10T16:14:34Z","published":"2024-07-10T16:14:34Z","title":"Evaluating Large Language Models with Grid-Based Game Competitions: An\n  Extensible LLM Benchmark and Leaderboard","summary":"  We introduce a novel and extensible benchmark for large language models\n(LLMs) through grid-based games such as Tic-Tac-Toe, Connect-Four, and Gomoku.\nThe open-source game simulation code, available on GitHub, allows LLMs to\ncompete and generates detailed data files in JSON, CSV, TXT, and PNG formats\nfor leaderboard rankings and further analysis. We present the results of games\namong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by\nAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and\nGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of\nresults from other LLMs. In total, we simulated 2,310 matches (5 sessions for\neach pair among 7 LLMs and a random player) across three types of games, using\nthree distinct prompt types: list, illustration, and image. The results\nrevealed significant variations in LLM performance across different games and\nprompt types, with analysis covering win and disqualification rates, missed\nopportunity analysis, and invalid move analysis. The details of the leaderboard\nand result matrix data are available as open-access data on GitHub. This study\nenhances our understanding of LLMs' capabilities in playing games they were not\nspecifically trained for, helping to assess their rule comprehension and\nstrategic thinking. On the path to Artificial General Intelligence (AGI), this\nstudy lays the groundwork for future exploration into their utility in complex\ndecision-making scenarios, illuminating their strategic thinking abilities and\noffering directions for further inquiry into the limits of LLMs within\ngame-based frameworks.\n","authors":["Oguzhan Topsakal","Colby Jacob Edell","Jackson Bailey Harper"],"pdf_url":"https://arxiv.org/pdf/2407.07796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07791v1","updated":"2024-07-10T16:08:46Z","published":"2024-07-10T16:08:46Z","title":"Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent\n  Communities","summary":"  The rapid adoption of large language models (LLMs) in multi-agent systems has\nhighlighted their impressive capabilities in various applications, such as\ncollaborative problem-solving and autonomous negotiation. However, the security\nimplications of these LLM-based multi-agent systems have not been thoroughly\ninvestigated, particularly concerning the spread of manipulated knowledge. In\nthis paper, we investigate this critical issue by constructing a detailed\nthreat model and a comprehensive simulation environment that mirrors real-world\nmulti-agent deployments in a trusted platform. Subsequently, we propose a novel\ntwo-stage attack method involving Persuasiveness Injection and Manipulated\nKnowledge Injection to systematically explore the potential for manipulated\nknowledge (i.e., counterfactual and toxic knowledge) spread without explicit\nprompt manipulation.\n  Our method leverages the inherent vulnerabilities of LLMs in handling world\nknowledge, which can be exploited by attackers to unconsciously spread\nfabricated information. Through extensive experiments, we demonstrate that our\nattack method can successfully induce LLM-based agents to spread both\ncounterfactual and toxic knowledge without degrading their foundational\ncapabilities during agent communication. Furthermore, we show that these\nmanipulations can persist through popular retrieval-augmented generation\nframeworks, where several benign agents store and retrieve manipulated chat\nhistories for future interactions. This persistence indicates that even after\nthe interaction has ended, the benign agents may continue to be influenced by\nmanipulated knowledge. Our findings reveal significant security risks in\nLLM-based multi-agent systems, emphasizing the imperative need for robust\ndefenses against manipulated knowledge spread, such as introducing ``guardian''\nagents and advanced fact-checking tools.\n","authors":["Tianjie Ju","Yiting Wang","Xinbei Ma","Pengzhou Cheng","Haodong Zhao","Yulong Wang","Lifeng Liu","Jian Xie","Zhuosheng Zhang","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2407.07791v1.pdf","comment":"18 Pages, working in progress"},{"id":"http://arxiv.org/abs/2407.07061v2","updated":"2024-07-10T15:57:21Z","published":"2024-07-09T17:33:24Z","title":"Internet of Agents: Weaving a Web of Heterogeneous Agents for\n  Collaborative Intelligence","summary":"  The rapid advancement of large language models (LLMs) has paved the way for\nthe development of highly capable autonomous agents. However, existing\nmulti-agent frameworks often struggle with integrating diverse capable\nthird-party agents due to reliance on agents defined within their own\necosystems. They also face challenges in simulating distributed environments,\nas most frameworks are limited to single-device setups. Furthermore, these\nframeworks often rely on hard-coded communication pipelines, limiting their\nadaptability to dynamic task requirements. Inspired by the concept of the\nInternet, we propose the Internet of Agents (IoA), a novel framework that\naddresses these limitations by providing a flexible and scalable platform for\nLLM-based multi-agent collaboration. IoA introduces an agent integration\nprotocol, an instant-messaging-like architecture design, and dynamic mechanisms\nfor agent teaming and conversation flow control. Through extensive experiments\non general assistant tasks, embodied AI tasks, and retrieval-augmented\ngeneration benchmarks, we demonstrate that IoA consistently outperforms\nstate-of-the-art baselines, showcasing its ability to facilitate effective\ncollaboration among heterogeneous agents. IoA represents a step towards linking\ndiverse agents in an Internet-like environment, where agents can seamlessly\ncollaborate to achieve greater intelligence and capabilities. Our codebase has\nbeen released at \\url{https://github.com/OpenBMB/IoA}.\n","authors":["Weize Chen","Ziming You","Ran Li","Yitong Guan","Chen Qian","Chenyang Zhao","Cheng Yang","Ruobing Xie","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2407.07061v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2407.07778v1","updated":"2024-07-10T15:52:44Z","published":"2024-07-10T15:52:44Z","title":"WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment","summary":"  AI systems make decisions in physical environments through primitive actions\nor affordances that are accessed via API calls. While deploying AI agents in\nthe real world involves numerous high-level actions, existing embodied\nsimulators offer a limited set of domain-salient APIs. This naturally brings up\nthe questions: how many primitive actions (APIs) are needed for a versatile\nembodied agent, and what should they look like? We explore this via a thought\nexperiment: assuming that wikiHow tutorials cover a wide variety of\nhuman-written tasks, what is the space of APIs needed to cover these\ninstructions? We propose a framework to iteratively induce new APIs by\ngrounding wikiHow instruction to situated agent policies. Inspired by recent\nsuccesses in large language models (LLMs) for embodied planning, we propose a\nfew-shot prompting to steer GPT-4 to generate Pythonic programs as agent\npolicies and bootstrap a universe of APIs by 1) reusing a seed set of APIs; and\nthen 2) fabricate new API calls when necessary. The focus of this thought\nexperiment is on defining these APIs rather than their executability. We apply\nthe proposed pipeline on instructions from wikiHow tutorials. On a small\nfraction (0.5%) of tutorials, we induce an action space of 300+ APIs necessary\nfor capturing the rich variety of tasks in the physical world. A detailed\nautomatic and human analysis of the induction output reveals that the proposed\npipeline enables effective reuse and creation of APIs. Moreover, a manual\nreview revealed that existing simulators support only a small subset of the\ninduced APIs (9 of the top 50 frequent APIs), motivating the development of\naction-rich embodied environments.\n","authors":["Jiefu Ou","Arda Uzunoglu","Benjamin Van Durme","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2407.07778v1.pdf","comment":"ACL 2024 NLRSE, 8 pages"},{"id":"http://arxiv.org/abs/2407.07771v1","updated":"2024-07-10T15:46:32Z","published":"2024-07-10T15:46:32Z","title":"Multi-task Prompt Words Learning for Social Media Content Generation","summary":"  The rapid development of the Internet has profoundly changed human life.\nHumans are increasingly expressing themselves and interacting with others on\nsocial media platforms. However, although artificial intelligence technology\nhas been widely used in many aspects of life, its application in social media\ncontent creation is still blank. To solve this problem, we propose a new prompt\nword generation framework based on multi-modal information fusion, which\ncombines multiple tasks including topic classification, sentiment analysis,\nscene recognition and keyword extraction to generate more comprehensive prompt\nwords. Subsequently, we use a template containing a set of prompt words to\nguide ChatGPT to generate high-quality tweets. Furthermore, in the absence of\neffective and objective evaluation criteria in the field of content generation,\nwe use the ChatGPT tool to evaluate the results generated by the algorithm,\nmaking large-scale evaluation of content generation algorithms possible.\nEvaluation results on extensive content generation demonstrate that our cue\nword generation framework generates higher quality content compared to manual\nmethods and other cueing techniques, while topic classification, sentiment\nanalysis, and scene recognition significantly enhance content clarity and its\nconsistency with the image.\n","authors":["Haochen Xue","Chong Zhang","Chengzhi Liu","Fangyu Wu","Xiaobo Jin"],"pdf_url":"https://arxiv.org/pdf/2407.07771v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.01523v2","updated":"2024-07-10T15:31:09Z","published":"2024-07-01T17:59:26Z","title":"MMLongBench-Doc: Benchmarking Long-context Document Understanding with\n  Visualizations","summary":"  Understanding documents with rich layouts and multi-modal components is a\nlong-standing and practical task. Recent Large Vision-Language Models (LVLMs)\nhave made remarkable strides in various tasks, particularly in single-page\ndocument understanding (DU). However, their abilities on long-context DU remain\nan open problem. This work presents MMLongBench-Doc, a long-context,\nmulti-modal benchmark comprising 1,062 expert-annotated questions. Distinct\nfrom previous datasets, it is constructed upon 130 lengthy PDF-formatted\ndocuments with an average of 49.4 pages and 20,971 textual tokens. Towards\ncomprehensive evaluation, answers to these questions rely on pieces of evidence\nfrom (1) different sources (text, image, chart, table, and layout structure)\nand (2) various locations (i.e. page number). Moreover, 33.2% of the questions\nare cross-page questions requiring evidence across multiple pages. 22.8% of the\nquestions are designed to be unanswerable for detecting potential\nhallucinations. Experiments on 14 LVLMs demonstrate that long-context DU\ngreatly challenges current models. Notably, the best-performing model, GPT-4o,\nachieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores\n31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse\nperformance than their LLM counterparts which are fed with lossy-parsed OCR\ndocuments. These results validate the necessity of future research toward more\ncapable long-context LVLMs. Project Page:\nhttps://mayubo2333.github.io/MMLongBench-Doc\n","authors":["Yubo Ma","Yuhang Zang","Liangyu Chen","Meiqi Chen","Yizhu Jiao","Xinze Li","Xinyuan Lu","Ziyu Liu","Yan Ma","Xiaoyi Dong","Pan Zhang","Liangming Pan","Yu-Gang Jiang","Jiaqi Wang","Yixin Cao","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2407.01523v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01616v3","updated":"2024-07-10T15:20:19Z","published":"2024-04-02T03:42:28Z","title":"Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems","summary":"  Large language models (LLMs) are trained on text-only data that go far beyond\nthe languages with paired speech and text data. At the same time, Dual Encoder\n(DE) based retrieval systems project queries and documents into the same\nembedding space and have demonstrated their success in retrieval and bi-text\nmining. To match speech and text in many languages, we propose using LLMs to\ninitialize multi-modal DE retrieval systems. Unlike traditional methods, our\nsystem doesn't require speech data during LLM pre-training and can exploit\nLLM's multilingual text understanding capabilities to match speech and text in\nlanguages unseen during retrieval training. Our multi-modal LLM-based retrieval\nsystem is capable of matching speech and text in 102 languages despite only\ntraining on 21 languages. Our system outperforms previous systems trained\nexplicitly on all 102 languages. We achieve a 10% absolute improvement in\nRecall@1 averaged across these languages. Additionally, our model demonstrates\ncross-lingual speech and text matching, which is further enhanced by readily\navailable machine translation data.\n","authors":["Frank Palma Gomez","Ramon Sanabria","Yun-hsuan Sung","Daniel Cer","Siddharth Dalmia","Gustavo Hernandez Abrego"],"pdf_url":"https://arxiv.org/pdf/2404.01616v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01858v2","updated":"2024-07-10T15:11:56Z","published":"2024-03-04T09:13:33Z","title":"An Improved Traditional Chinese Evaluation Suite for Foundation Model","summary":"  We present TMMLU+, a new benchmark designed for Traditional Chinese language\nunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66\nsubjects from elementary to professional level. It is six times larger and\nboasts a more balanced subject distribution than its predecessor, Taiwan\nMassive Multitask Language Understanding (TMMLU). We also benchmark\nclosed-source models and 26 open-weight Chinese large language models (LLMs) of\nparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings reveal\nthat (1.) Traditional Chinese models still trail behind their Simplified\nChinese counterparts, highlighting a need for more focused advancements in LLMs\ncatering to Traditional Chinese. (2.) Current LLMs still fall short of human\nperformance in average scores, indicating a potential need for future research\nto delve deeper into social science and humanities subjects. (3.) Among all the\ntokenization compression metrics examined, we identify that only the fertility\nscore uniquely demonstrates strong correlations with our benchmark results. We\nforesee that TMMLU+ will pinpoint areas for future model improvement, thereby\nnarrowing the gap between machine and human linguistic capabilities and\nsupporting researchers in developing Traditional Chinese LLMs. Our dataset,\nalong with the benchmark source code, is accessible at\nhuggingface.co/datasets/ikala/tmmluplus.\n","authors":["Zhi-Rui Tam","Ya-Ting Pai","Yen-Wei Lee","Jun-Da Chen","Wei-Min Chu","Sega Cheng","Hong-Han Shuai"],"pdf_url":"https://arxiv.org/pdf/2403.01858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07737v1","updated":"2024-07-10T15:07:58Z","published":"2024-07-10T15:07:58Z","title":"Fine-Tuning Large Language Models with User-Level Differential Privacy","summary":"  We investigate practical and scalable algorithms for training large language\nmodels (LLMs) with user-level differential privacy (DP) in order to provably\nsafeguard all the examples contributed by each user. We study two variants of\nDP-SGD with: (1) example-level sampling (ELS) and per-example gradient\nclipping, and (2) user-level sampling (ULS) and per-user gradient clipping. We\nderive a novel user-level DP accountant that allows us to compute provably\ntight privacy guarantees for ELS. Using this, we show that while ELS can\noutperform ULS in specific settings, ULS generally yields better results when\neach user has a diverse collection of examples. We validate our findings\nthrough experiments in synthetic mean estimation and LLM fine-tuning tasks\nunder fixed compute budgets. We find that ULS is significantly better in\nsettings where either (1) strong privacy guarantees are required, or (2) the\ncompute budget is large. Notably, our focus on LLM-compatible training\nalgorithms allows us to scale to models with hundreds of millions of parameters\nand datasets with hundreds of thousands of users.\n","authors":["Zachary Charles","Arun Ganesh","Ryan McKenna","H. Brendan McMahan","Nicole Mitchell","Krishna Pillutla","Keith Rush"],"pdf_url":"https://arxiv.org/pdf/2407.07737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14770v3","updated":"2024-07-10T15:03:44Z","published":"2023-05-24T06:19:14Z","title":"Using Natural Language Explanations to Rescale Human Judgments","summary":"  The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover human judgments. However, annotators' judgments for subjective tasks can\ndiffer in many ways: they may reflect different qualitative judgments about an\nexample, and they may be mapped to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric.\n","authors":["Manya Wadhwa","Jifan Chen","Junyi Jessy Li","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2305.14770v3.pdf","comment":"Data available at\n  https://github.com/ManyaWadhwa/explanation_based_rescaling"},{"id":"http://arxiv.org/abs/2407.07726v1","updated":"2024-07-10T14:57:46Z","published":"2024-07-10T14:57:46Z","title":"PaliGemma: A versatile 3B VLM for transfer","summary":"  PaliGemma is an open Vision-Language Model (VLM) that is based on the\nSigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to\nbe a versatile and broadly knowledgeable base model that is effective to\ntransfer. It achieves strong performance on a wide variety of open-world tasks.\nWe evaluate PaliGemma on almost 40 diverse tasks including standard VLM\nbenchmarks, but also more specialized tasks such as remote-sensing and\nsegmentation.\n","authors":["Lucas Beyer","Andreas Steiner","AndrÃ© Susano Pinto","Alexander Kolesnikov","Xiao Wang","Daniel Salz","Maxim Neumann","Ibrahim Alabdulmohsin","Michael Tschannen","Emanuele Bugliarello","Thomas Unterthiner","Daniel Keysers","Skanda Koppula","Fangyu Liu","Adam Grycner","Alexey Gritsenko","Neil Houlsby","Manoj Kumar","Keran Rong","Julian Eisenschlos","Rishabh Kabra","Matthias Bauer","Matko BoÅ¡njak","Xi Chen","Matthias Minderer","Paul Voigtlaender","Ioana Bica","Ivana Balazevic","Joan Puigcerver","Pinelopi Papalampidi","Olivier Henaff","Xi Xiong","Radu Soricut","Jeremiah Harmsen","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2407.07726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05449v2","updated":"2024-07-10T14:44:18Z","published":"2024-07-07T17:19:34Z","title":"SmurfCat at PAN 2024 TextDetox: Alignment of Multilingual Transformers\n  for Text Detoxification","summary":"  This paper presents a solution for the Multilingual Text Detoxification task\nin the PAN-2024 competition of the SmurfCat team. Using data augmentation\nthrough machine translation and a special filtering procedure, we collected an\nadditional multilingual parallel dataset for text detoxification. Using the\nobtained data, we fine-tuned several multilingual sequence-to-sequence models,\nsuch as mT0 and Aya, on a text detoxification task. We applied the ORPO\nalignment technique to the final model. Our final model has only 3.7 billion\nparameters and achieves state-of-the-art results for the Ukrainian language and\nnear state-of-the-art results for other languages. In the competition, our team\nachieved first place in the automated evaluation with a score of 0.52 and\nsecond place in the final human evaluation with a score of 0.74.\n","authors":["Elisei Rykov","Konstantin Zaytsev","Ivan Anisimov","Alexandr Voronin"],"pdf_url":"https://arxiv.org/pdf/2407.05449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11473v2","updated":"2024-07-10T14:36:06Z","published":"2024-06-17T12:38:38Z","title":"Promises, Outlooks and Challenges of Diffusion Language Modeling","summary":"  The modern autoregressive Large Language Models (LLMs) have achieved\noutstanding performance on NLP benchmarks, and they are deployed in the real\nworld. However, they still suffer from limitations of the autoregressive\ntraining paradigm. For example, autoregressive token generation is notably slow\nand can be prone to \\textit{exposure bias}. The diffusion-based language models\nwere proposed as an alternative to autoregressive generation to address some of\nthese limitations. We evaluate the recently proposed Score Entropy Discrete\nDiffusion (SEDD) approach and show it is a promising alternative to\nautoregressive generation but it has some short-comings too. We empirically\ndemonstrate the advantages and challenges of SEDD, and observe that SEDD\ngenerally matches autoregressive models in perplexity and on benchmarks such as\nHellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference\nlatency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD\nallows conditioning on tokens at abitrary positions, SEDD appears slightly\nweaker than GPT-2 for conditional generation given short prompts. Finally, we\nreproduced the main results from the original SEDD paper.\n","authors":["Justin Deschenaux","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2406.11473v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07683v1","updated":"2024-07-10T14:08:24Z","published":"2024-07-10T14:08:24Z","title":"The Language of Weather: Social Media Reactions to Weather Accounting\n  for Climatic and Linguistic Baselines","summary":"  This study explores how different weather conditions influence public\nsentiment on social media, focusing on Twitter data from the UK. By considering\nclimate and linguistic baselines, we improve the accuracy of weather-related\nsentiment analysis. Our findings show that emotional responses to weather are\ncomplex, influenced by combinations of weather variables and regional language\ndifferences. The results highlight the importance of context-sensitive methods\nfor better understanding public mood in response to weather, which can enhance\nimpact-based forecasting and risk communication in the context of climate\nchange.\n","authors":["James C. Young","Rudy Arthur","Hywel T. P. Williams"],"pdf_url":"https://arxiv.org/pdf/2407.07683v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.11757v2","updated":"2024-07-10T13:53:11Z","published":"2024-06-17T17:16:45Z","title":"STAR: SocioTechnical Approach to Red Teaming Language Models","summary":"  This research introduces STAR, a sociotechnical framework that improves on\ncurrent best practices for red teaming safety of large language models. STAR\nmakes two key contributions: it enhances steerability by generating\nparameterised instructions for human red teamers, leading to improved coverage\nof the risk surface. Parameterised instructions also provide more detailed\ninsights into model failures at no increased cost. Second, STAR improves signal\nquality by matching demographics to assess harms for specific groups, resulting\nin more sensitive annotations. STAR further employs a novel step of arbitration\nto leverage diverse viewpoints and improve label reliability, treating\ndisagreement not as noise but as a valuable contribution to signal quality.\n","authors":["Laura Weidinger","John Mellor","Bernat Guillen Pegueroles","Nahema Marchal","Ravin Kumar","Kristian Lum","Canfer Akbulut","Mark Diaz","Stevie Bergman","Mikel Rodriguez","Verena Rieser","William Isaac"],"pdf_url":"https://arxiv.org/pdf/2406.11757v2.pdf","comment":"8 pages, 5 figures, 5 pages appendix. * denotes equal contribution"},{"id":"http://arxiv.org/abs/2407.07666v1","updated":"2024-07-10T13:45:16Z","published":"2024-07-10T13:45:16Z","title":"A Proposed S.C.O.R.E. Evaluation Framework for Large Language Models :\n  Safety, Consensus, Objectivity, Reproducibility and Explainability","summary":"  A comprehensive qualitative evaluation framework for large language models\n(LLM) in healthcare that expands beyond traditional accuracy and quantitative\nmetrics needed. We propose 5 key aspects for evaluation of LLMs: Safety,\nConsensus, Objectivity, Reproducibility and Explainability (S.C.O.R.E.). We\nsuggest that S.C.O.R.E. may form the basis for an evaluation framework for\nfuture LLM-based models that are safe, reliable, trustworthy, and ethical for\nhealthcare and clinical applications.\n","authors":["Ting Fang Tan","Kabilan Elangovan","Jasmine Ong","Nigam Shah","Joseph Sung","Tien Yin Wong","Lan Xue","Nan Liu","Haibo Wang","Chang Fu Kuo","Simon Chesterman","Zee Kin Yeong","Daniel SW Ting"],"pdf_url":"https://arxiv.org/pdf/2407.07666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06808v2","updated":"2024-07-10T13:31:59Z","published":"2024-05-10T20:45:40Z","title":"Large Language Model in Financial Regulatory Interpretation","summary":"  This study explores the innovative use of Large Language Models (LLMs) as\nanalytical tools for interpreting complex financial regulations. The primary\nobjective is to design effective prompts that guide LLMs in distilling verbose\nand intricate regulatory texts, such as the Basel III capital requirement\nregulations, into a concise mathematical framework that can be subsequently\ntranslated into actionable code. This novel approach aims to streamline the\nimplementation of regulatory mandates within the financial reporting and risk\nmanagement systems of global banking institutions. A case study was conducted\nto assess the performance of various LLMs, demonstrating that GPT-4 outperforms\nother models in processing and collecting necessary information, as well as\nexecuting mathematical calculations. The case study utilized numerical\nsimulations with asset holdings -- including fixed income, equities, currency\npairs, and commodities -- to demonstrate how LLMs can effectively implement the\nBasel III capital adequacy requirements.\n  Keywords: Large Language Models, Prompt Engineering, LLMs in Finance, Basel\nIII, Minimum Capital Requirements, LLM Ethics\n","authors":["Zhiyu Cao","Zachary Feinstein"],"pdf_url":"https://arxiv.org/pdf/2405.06808v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07630v1","updated":"2024-07-10T13:09:23Z","published":"2024-07-10T13:09:23Z","title":"A Review of the Challenges with Massive Web-mined Corpora Used in Large\n  Language Models Pre-Training","summary":"  This article presents a comprehensive review of the challenges associated\nwith using massive web-mined corpora for the pre-training of large language\nmodels (LLMs). This review identifies key challenges in this domain, including\nchallenges such as noise (irrelevant or misleading information), duplication of\ncontent, the presence of low-quality or incorrect information, biases, and the\ninclusion of sensitive or personal information in web-mined corpora. Addressing\nthese issues is crucial for the development of accurate, reliable, and\nethically responsible language models. Through an examination of current\nmethodologies for data cleaning, pre-processing, bias detection and mitigation,\nwe highlight the gaps in existing approaches and suggest directions for future\nresearch. Our discussion aims to catalyze advancements in developing more\nsophisticated and ethically responsible LLMs.\n","authors":["MichaÅ PereÅkiewicz","RafaÅ PoÅwiata"],"pdf_url":"https://arxiv.org/pdf/2407.07630v1.pdf","comment":"8 pages, Icaisc 2024 conference"},{"id":"http://arxiv.org/abs/2407.07617v1","updated":"2024-07-10T12:56:17Z","published":"2024-07-10T12:56:17Z","title":"Psycho-linguistic Experiment on Universal Semantic Components of Verbal\n  Humor: System Description and Annotation","summary":"  Objective criteria for universal semantic components that distinguish a\nhumorous utterance from a non-humorous one are presently under debate. In this\narticle, we give an in-depth observation of our system of self-paced reading\nfor annotation of humor, that collects readers' annotations while they open a\ntext word by word. The system registers keys that readers press to open the\nnext word, choose a class (humorous versus non-humorous texts), change their\nchoice. We also touch upon our psycho-linguistic experiment conducted with the\nsystem and the data collected during it.\n","authors":["Elena Mikhalkova","Nadezhda Ganzherli","Julia Murzina"],"pdf_url":"https://arxiv.org/pdf/2407.07617v1.pdf","comment":"5 pages, 4 figures, preprint submitted to journal in 2023"},{"id":"http://arxiv.org/abs/2407.07612v1","updated":"2024-07-10T12:50:44Z","published":"2024-07-10T12:50:44Z","title":"Teaching Transformers Causal Reasoning through Axiomatic Training","summary":"  For text-based AI systems to interact in the real world, causal reasoning is\nan essential skill. Since interventional data is costly to generate, we study\nto what extent an agent can learn causal reasoning from passive data.\nSpecifically, we consider an axiomatic training setup where an agent learns\nfrom multiple demonstrations of a causal axiom (or rule), rather than\nincorporating the axiom as an inductive bias or inferring it from data values.\nA key question is whether the agent would learn to generalize from the axiom\ndemonstrations to new scenarios. For example, if a transformer model is trained\non demonstrations of the causal transitivity axiom over small graphs, would it\ngeneralize to applying the transitivity axiom over large graphs? Our results,\nbased on a novel axiomatic training scheme, indicate that such generalization\nis possible. We consider the task of inferring whether a variable causes\nanother variable, given a causal graph structure. We find that a 67 million\nparameter transformer model, when trained on linear causal chains (along with\nsome noisy variations) can generalize well to new kinds of graphs, including\nlonger causal chains, causal chains with reversed order, and graphs with\nbranching; even when it is not explicitly trained for such settings. Our model\nperforms at par (or even better) than many larger language models such as\nGPT-4, Gemini Pro, and Phi-3. Overall, our axiomatic training framework\nprovides a new paradigm of learning causal reasoning from passive data that can\nbe used to learn arbitrary axioms, as long as sufficient demonstrations can be\ngenerated.\n","authors":["Aniket Vashishtha","Abhinav Kumar","Abbavaram Gowtham Reddy","Vineeth N Balasubramanian","Amit Sharma"],"pdf_url":"https://arxiv.org/pdf/2407.07612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02301v2","updated":"2024-07-10T12:45:13Z","published":"2024-06-04T13:30:45Z","title":"mCoT: Multilingual Instruction Tuning for Reasoning Consistency in\n  Language Models","summary":"  Large language models (LLMs) with Chain-of-thought (CoT) have recently\nemerged as a powerful technique for eliciting reasoning to improve various\ndownstream tasks. As most research mainly focuses on English, with few\nexplorations in a multilingual context, the question of how reliable this\nreasoning capability is in different languages is still open. To address it\ndirectly, we study multilingual reasoning consistency across multiple\nlanguages, using popular open-source LLMs. First, we compile the first\nlarge-scale multilingual math reasoning dataset, mCoT-MATH, covering eleven\ndiverse languages. Then, we introduce multilingual CoT instruction tuning to\nboost reasoning capability across languages, thereby improving model\nconsistency. While existing LLMs show substantial variation across the\nlanguages we consider, and especially low performance for lesser resourced\nlanguages, our 7B parameter model mCoT achieves impressive consistency across\nlanguages, and superior or comparable performance to close- and open-source\nmodels even of much larger sizes.\n","authors":["Huiyuan Lai","Malvina Nissim"],"pdf_url":"https://arxiv.org/pdf/2406.02301v2.pdf","comment":"Accepted to ACL 2024 main (Corrected Figure 2 (a))"},{"id":"http://arxiv.org/abs/2407.07606v1","updated":"2024-07-10T12:45:02Z","published":"2024-07-10T12:45:02Z","title":"The Computational Learning of Construction Grammars: State of the Art\n  and Prospective Roadmap","summary":"  This paper documents and reviews the state of the art concerning\ncomputational models of construction grammar learning. It brings together prior\nwork on the computational learning of form-meaning pairings, which has so far\nbeen studied in several distinct areas of research. The goal of this paper is\nthreefold. First of all, it aims to synthesise the variety of methodologies\nthat have been proposed to date and the results that have been obtained.\nSecond, it aims to identify those parts of the challenge that have been\nsuccessfully tackled and reveal those that require further research. Finally,\nit aims to provide a roadmap which can help to boost and streamline future\nresearch efforts on the computational learning of large-scale, usage-based\nconstruction grammars.\n","authors":["Jonas Doumen","Veronica Juliana Schmalz","Katrien Beuls","Paul Van Eecke"],"pdf_url":"https://arxiv.org/pdf/2407.07606v1.pdf","comment":"Peer-reviewed author's draft of a journal article to appear in\n  Constructions and Frames (2025)"},{"id":"http://arxiv.org/abs/2407.07566v1","updated":"2024-07-10T11:51:26Z","published":"2024-07-10T11:51:26Z","title":"HebDB: a Weakly Supervised Dataset for Hebrew Speech Processing","summary":"  We present HebDB, a weakly supervised dataset for spoken language processing\nin the Hebrew language. HebDB offers roughly 2500 hours of natural and\nspontaneous speech recordings in the Hebrew language, consisting of a large\nvariety of speakers and topics. We provide raw recordings together with a\npre-processed, weakly supervised, and filtered version. The goal of HebDB is to\nfurther enhance research and development of spoken language processing tools\nfor the Hebrew language. Hence, we additionally provide two baseline systems\nfor Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a\nfully supervised model. We present the performance of these two methods\noptimized on HebDB and compare them to current multi-lingual ASR alternatives.\nResults suggest the proposed method reaches better results than the evaluated\nbaselines considering similar model sizes. Dataset, code, and models are\npublicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/.\n","authors":["Arnon Turetzky","Or Tal","Yael Segal-Feldman","Yehoshua Dissen","Ella Zeldes","Amit Roth","Eyal Cohen","Yosi Shrem","Bronya R. Chernyak","Olga Seleznova","Joseph Keshet","Yossi Adi"],"pdf_url":"https://arxiv.org/pdf/2407.07566v1.pdf","comment":"Accepted at Interspeech2024"},{"id":"http://arxiv.org/abs/2407.07565v1","updated":"2024-07-10T11:50:20Z","published":"2024-07-10T11:50:20Z","title":"On Leakage of Code Generation Evaluation Datasets","summary":"  In this paper we consider contamination by code generation test sets, in\nparticular in their use in modern large language models. We discuss three\npossible sources of such contamination and show findings supporting each of\nthem: (i) direct data leakage, (ii) indirect data leakage through the use of\nsynthetic data and (iii) overfitting to evaluation sets during model selection.\n  Key to our findings is a new dataset of 161 prompts with their associated\npython solutions, dataset which is released at\nhttps://huggingface.co/datasets/CohereForAI/lbpp .\n","authors":["Alexandre Matton","Tom Sherborne","Dennis Aumiller","Elena Tommasone","Milad Alizadeh","Jingyi He","Raymond Ma","Maxime Voisin","Ellen Gilsenan-McMahon","Matthias GallÃ©"],"pdf_url":"https://arxiv.org/pdf/2407.07565v1.pdf","comment":"4 main pages, 9 in total"},{"id":"http://arxiv.org/abs/2311.09404v2","updated":"2024-07-10T11:34:49Z","published":"2023-11-15T22:03:28Z","title":"To Translate or Not to Translate: A Systematic Investigation of\n  Translation-Based Cross-Lingual Transfer to Low-Resource Languages","summary":"  Perfect machine translation (MT) would render cross-lingual transfer (XLT) by\nmeans of multilingual language models (mLMs) superfluous. Given, on the one\nhand, the large body of work on improving XLT with mLMs and, on the other hand,\nrecent advances in massively multilingual MT, in this work, we systematically\nevaluate existing and propose new translation-based XLT approaches for transfer\nto low-resource languages. We show that all translation-based approaches\ndramatically outperform zero-shot XLT with mLMs -- with the combination of\nround-trip translation of the source-language training data and the translation\nof the target-language test instances at inference -- being generally the most\neffective. We next show that one can obtain further empirical gains by adding\nreliable translations to other high-resource languages to the training data.\nMoreover, we propose an effective translation-based XLT strategy even for\nlanguages not supported by the MT system. Finally, we show that model selection\nfor XLT based on target-language validation data obtained with MT outperforms\nmodel selection based on the source-language data. We believe our findings\nwarrant a broader inclusion of more robust translation-based baselines in XLT\nresearch.\n","authors":["Benedikt Ebing","Goran GlavaÅ¡"],"pdf_url":"https://arxiv.org/pdf/2311.09404v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01343v3","updated":"2024-07-10T11:33:28Z","published":"2024-03-31T07:11:48Z","title":"CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs","summary":"  Businesses and software platforms are increasingly turning to Large Language\nModels (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance\nwith file access or as reasoning agents for customer service. However, current\nLLM-based customer service models have limited integration with customer\nprofiles and lack the operational capabilities necessary for effective service.\nMoreover, existing API integrations emphasize diversity over the precision and\nerror avoidance essential in real-world customer service scenarios. To address\nthese issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile\nin existing System), designed to: (1) efficiently utilize existing databases or\nsystems for accessing user information or interacting with these systems\nfollowing existing guidelines; (2) provide accurate and reasonable responses or\ncarry out required operations in the system while avoiding harmful operations;\nand (3) leverage a combination of small and large LLMs to achieve satisfying\nperformance at a reasonable inference cost. We introduce a practical dataset,\nthe CPHOS-dataset, which includes a database, guiding files, and QA pairs\ncollected from CPHOS, an online platform that facilitates the organization of\nsimulated Physics Olympiads for high school teachers and students. We have\nconducted extensive experiments to validate the performance of our proposed\nCHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how\nLLMs can enhance or serve as alternatives to human customer service. Code for\nour proposed architecture and dataset can be found at\n{https://github.com/JingzheShi/CHOPS}.\n","authors":["Jingzhe Shi","Jialuo Li","Qinwei Ma","Zaiwen Yang","Huan Ma","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2404.01343v3.pdf","comment":"Accepted by COLM 2024"},{"id":"http://arxiv.org/abs/2407.07551v1","updated":"2024-07-10T11:26:10Z","published":"2024-07-10T11:26:10Z","title":"Arabic Automatic Story Generation with Large Language Models","summary":"  Large language models (LLMs) have recently emerged as a powerful tool for a\nwide range of language generation tasks. Nevertheless, this progress has been\nslower in Arabic. In this work, we focus on the task of generating stories from\nLLMs. For our training, we use stories acquired through machine translation\n(MT) as well as GPT-4. For the MT data, we develop a careful pipeline that\nensures we acquire high-quality stories. For our GPT-41 data, we introduce\ncrafted prompts that allow us to generate data well-suited to the Arabic\ncontext in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian\nand Moroccan). For example, we generate stories tailored to various Arab\ncountries on a wide host of topics. Our manual evaluation shows that our model\nfine-tuned on these training datasets can generate coherent stories that adhere\nto our instructions. We also conduct an extensive automatic and human\nevaluation comparing our models against state-of-the-art proprietary and\nopen-source models. Our datasets and models will be made publicly available at\nhttps: //github.com/UBC-NLP/arastories.\n","authors":["Ahmed Oumar El-Shangiti","Fakhraddin Alwajih","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2407.07551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15984v2","updated":"2024-07-10T11:08:37Z","published":"2024-05-24T23:56:36Z","title":"Evaluating the Adversarial Robustness of Retrieval-Based In-Context\n  Learning for Large Language Models","summary":"  With the emergence of large language models, such as LLaMA and OpenAI GPT-3,\nIn-Context Learning (ICL) gained significant attention due to its effectiveness\nand efficiency. However, ICL is very sensitive to the choice, order, and\nverbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented\nICL methods try to address this problem by leveraging retrievers to extract\nsemantically related examples as demonstrations. While this approach yields\nmore accurate results, its robustness against various types of adversarial\nattacks, including perturbations on test samples, demonstrations, and retrieved\ndata, remains under-explored. Our study reveals that retrieval-augmented models\ncan enhance robustness against test sample attacks, outperforming vanilla ICL\nwith a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit\noverconfidence in the demonstrations, leading to a 2% increase in ASR for\ndemonstration attacks. Adversarial training can help improve the robustness of\nICL methods to adversarial attacks; however, such a training scheme can be too\ncostly in the context of LLMs. As an alternative, we introduce an effective\ntraining-free adversarial defence method, DARD, which enriches the example pool\nwith those attacked samples. We show that DARD yields improvements in\nperformance and robustness, achieving a 15% reduction in ASR over the\nbaselines. Code and data are released to encourage further research:\nhttps://github.com/simonucl/adv-retreival-icl\n","authors":["Simon Chi Lok Yu","Jie He","Pasquale Minervini","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2405.15984v2.pdf","comment":"COLM 2024, 29 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.07531v1","updated":"2024-07-10T10:42:02Z","published":"2024-07-10T10:42:02Z","title":"Beyond Benchmarking: A New Paradigm for Evaluation and Assessment of\n  Large Language Models","summary":"  In current benchmarks for evaluating large language models (LLMs), there are\nissues such as evaluation content restriction, untimely updates, and lack of\noptimization guidance. In this paper, we propose a new paradigm for the\nmeasurement of LLMs: Benchmarking-Evaluation-Assessment. Our paradigm shifts\nthe \"location\" of LLM evaluation from the \"examination room\" to the \"hospital\".\nThrough conducting a \"physical examination\" on LLMs, it utilizes specific\ntask-solving as the evaluation content, performs deep attribution of existing\nproblems within LLMs, and provides recommendation for optimization.\n","authors":["Jin Liu","Qingquan Li","Wenlong Du"],"pdf_url":"https://arxiv.org/pdf/2407.07531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07864v3","updated":"2024-07-10T10:39:12Z","published":"2023-07-15T18:25:56Z","title":"CIDER: Context sensitive sentiment analysis for short-form text","summary":"  Researchers commonly perform sentiment analysis on large collections of short\ntexts like tweets, Reddit posts or newspaper headlines that are all focused on\na specific topic, theme or event. Usually, general-purpose sentiment analysis\nmethods are used. These perform well on average but miss the variation in\nmeaning that happens across different contexts, for example, the word \"active\"\nhas a very different intention and valence in the phrase \"active lifestyle\"\nversus \"active volcano\". This work presents a new approach, CIDER (Context\nInformed Dictionary and sEmantic Reasoner), which performs context-sensitive\nlinguistic analysis, where the valence of sentiment-laden terms is inferred\nfrom the whole corpus before being used to score the individual texts. In this\npaper, we detail the CIDER algorithm and demonstrate that it outperforms\nstate-of-the-art generalist unsupervised sentiment analysis techniques on a\nlarge collection of tweets about the weather. CIDER is also applicable to\nalternative (non-sentiment) linguistic scales. A case study on gender in the UK\nis presented, with the identification of highly gendered and sentiment-laden\ndays. We have made our implementation of CIDER available as a Python package:\nhttps://pypi.org/project/ciderpolarity/.\n","authors":["James C. Young","Rudy Arthur","Hywel T. P. Williams"],"pdf_url":"https://arxiv.org/pdf/2307.07864v3.pdf","comment":"20 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2305.06575v5","updated":"2024-07-10T09:53:59Z","published":"2023-05-11T05:19:47Z","title":"Chain-of-Dictionary Prompting Elicits Translation in Large Language\n  Models","summary":"  Large language models (LLMs) have shown surprisingly good performance in\nmultilingual neural machine translation (MNMT) even when trained without\nparallel data. Yet, despite the fact that the amount of training data is\ngigantic, they still struggle with translating rare words, particularly for\nlow-resource languages. Even worse, it is usually unrealistic to retrieve\nrelevant demonstrations for in-context learning with low-resource languages on\nLLMs, which restricts the practical use of LLMs for translation -- how should\nwe mitigate this problem? To this end, we present a novel method, CoD, which\naugments LLMs with prior knowledge with the chains of multilingual dictionaries\nfor a subset of input words to elicit translation abilities for LLMs. Extensive\nexperiments indicate that augmenting ChatGPT with CoD elicits large gains by up\nto 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in\nCyrillic script) on FLORES-200 full devtest set. We further demonstrate the\nimportance of chaining the multilingual dictionaries, as well as the\nsuperiority of CoD to few-shot demonstration for low-resource languages.\n","authors":["Hongyuan Lu","Haoran Yang","Haoyang Huang","Dongdong Zhang","Wai Lam","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2305.06575v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13244v2","updated":"2024-07-10T09:28:21Z","published":"2024-03-20T02:15:55Z","title":"Instruction Multi-Constraint Molecular Generation Using a\n  Teacher-Student Large Language Model","summary":"  While various models and computational tools have been proposed for structure\nand property analysis of molecules, generating molecules that conform to all\ndesired structures and properties remains a challenge. Here, we introduce a\nmulti-constraint molecular generation large language model, TSMMG, which, akin\nto a student, incorporates knowledge from various small models and tools,\nnamely, the 'teachers'. To train TSMMG, we construct a large set of\ntext-molecule pairs by extracting molecular knowledge from these 'teachers',\nenabling it to generate novel molecules that conform to the descriptions\nthrough various text prompts. We experimentally show that TSMMG remarkably\nperforms in generating molecules meeting complex, natural language-described\nproperty requirements across two-, three-, and four-constraint tasks, with an\naverage molecular validity of over 99% and success ratio of 82.58%, 68.03%, and\n67.48%, respectively. The model also exhibits adaptability through zero-shot\ntesting, creating molecules that satisfy combinations of properties that have\nnot been encountered. It can comprehend text inputs with various language\nstyles, extending beyond the confines of outlined prompts, as confirmed through\nempirical validation. Additionally, the knowledge distillation feature of TSMMG\ncontributes to the continuous enhancement of small models, while the innovative\napproach to dataset construction effectively addresses the issues of data\nscarcity and quality, which positions TSMMG as a promising tool in the domains\nof drug discovery and materials science.\n","authors":["Peng Zhou","Jianmin Wang","Chunyan Li","Zixu Wang","Yiping Liu","Siqi Sun","Jianxin Lin","Leyi Wei","Xibao Cai","Houtim Lai","Wei Liu","Longyue Wang","Xiangxiang Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.13244v2.pdf","comment":"37 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.07495v1","updated":"2024-07-10T09:27:23Z","published":"2024-07-10T09:27:23Z","title":"Bucket Pre-training is All You Need","summary":"  Large language models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, the conventional\nfixed-length data composition strategy for pretraining, which involves\nconcatenating and splitting documents, can introduce noise and limit the\nmodel's ability to capture long-range dependencies. To address this, we first\nintroduce three metrics for evaluating data composition quality: padding ratio,\ntruncation ratio, and concatenation ratio. We further propose a multi-bucket\ndata composition method that moves beyond the fixed-length paradigm, offering a\nmore flexible and efficient approach to pretraining. Extensive experiments\ndemonstrate that our proposed method could significantly improving both the\nefficiency and efficacy of LLMs pretraining. Our approach not only reduces\nnoise and preserves context but also accelerates training, making it a\npromising solution for LLMs pretraining.\n","authors":["Hongtao Liu","Qiyao Peng","Qing Yang","Kai Liu","Hongyan Xu"],"pdf_url":"https://arxiv.org/pdf/2407.07495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03881v2","updated":"2024-07-10T09:23:11Z","published":"2024-04-05T04:04:23Z","title":"A Bi-consolidating Model for Joint Relational Triple Extraction","summary":"  Current methods to extract relational triples directly make a prediction\nbased on a possible entity pair in a raw sentence without depending on entity\nrecognition. The task suffers from a serious semantic overlapping problem, in\nwhich several relation triples may share one or two entities in a sentence. In\nthis paper, based on a two-dimensional sentence representation, a\nbi-consolidating model is proposed to address this problem by simultaneously\nreinforcing the local and global semantic features relevant to a relation\ntriple. This model consists of a local consolidation component and a global\nconsolidation component. The first component uses a pixel difference\nconvolution to enhance semantic information of a possible triple representation\nfrom adjacent regions and mitigate noise in neighbouring neighbours. The second\ncomponent strengthens the triple representation based a channel attention and a\nspatial attention, which has the advantage to learn remote semantic\ndependencies in a sentence. They are helpful to improve the performance of both\nentity identification and relation type classification in relation triple\nextraction. After evaluated on several publish datasets, the bi-consolidating\nmodel achieves competitive performance. Analytical experiments demonstrate the\neffectiveness of our model for relational triple extraction and give motivation\nfor other natural language processing tasks.\n","authors":["Xiaocheng Luo","Yanping Chen","Ruixue Tang","Caiwei Yang","Ruizhang Huang","Yongbin Qin"],"pdf_url":"https://arxiv.org/pdf/2404.03881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07487v1","updated":"2024-07-10T09:22:19Z","published":"2024-07-10T09:22:19Z","title":"Review-LLM: Harnessing Large Language Models for Personalized Review\n  Generation","summary":"  Product review generation is an important task in recommender systems, which\ncould provide explanation and persuasiveness for the recommendation. Recently,\nLarge Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling\nand generating ability, which could be applied in review generation. However,\ndirectly applying the LLMs for generating reviews might be troubled by the\n``polite'' phenomenon of the LLMs and could not generate personalized reviews\n(e.g., negative reviews). In this paper, we propose Review-LLM that customizes\nLLMs for personalized review generation. Firstly, we construct the prompt input\nby aggregating user historical behaviors, which include corresponding item\ntitles and reviews. This enables the LLMs to capture user interest features and\nreview writing style. Secondly, we incorporate ratings as indicators of\nsatisfaction into the prompt, which could further improve the model's\nunderstanding of user preferences and the sentiment tendency control of\ngenerated reviews. Finally, we feed the prompt text into LLMs, and use\nSupervised Fine-Tuning (SFT) to make the model generate personalized reviews\nfor the given user and target item. Experimental results on the real-world\ndataset show that our fine-tuned model could achieve better review generation\nperformance than existing close-source LLMs.\n","authors":["Qiyao Peng","Hongtao Liu","Hongyan Xu","Qing Yang","Minglai Shao","Wenjun Wang"],"pdf_url":"https://arxiv.org/pdf/2407.07487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12273v2","updated":"2024-07-10T09:07:52Z","published":"2024-01-22T17:11:37Z","title":"The Ethics of Interaction: Mitigating Security Threats in LLMs","summary":"  This paper comprehensively explores the ethical challenges arising from\nsecurity threats to Large Language Models (LLMs). These intricate digital\nrepositories are increasingly integrated into our daily lives, making them\nprime targets for attacks that can compromise their training data and the\nconfidentiality of their data sources. The paper delves into the nuanced\nethical repercussions of such security threats on society and individual\nprivacy. We scrutinize five major threats--prompt injection, jailbreaking,\nPersonal Identifiable Information (PII) exposure, sexually explicit content,\nand hate-based content--going beyond mere identification to assess their\ncritical ethical consequences and the urgency they create for robust defensive\nstrategies. The escalating reliance on LLMs underscores the crucial need for\nensuring these systems operate within the bounds of ethical norms, particularly\nas their misuse can lead to significant societal and individual harm. We\npropose conceptualizing and developing an evaluative tool tailored for LLMs,\nwhich would serve a dual purpose: guiding developers and designers in\npreemptive fortification of backend systems and scrutinizing the ethical\ndimensions of LLM chatbot responses during the testing phase. By comparing LLM\nresponses with those expected from humans in a moral context, we aim to discern\nthe degree to which AI behaviors align with the ethical values held by a\nbroader society. Ultimately, this paper not only underscores the ethical\ntroubles presented by LLMs; it also highlights a path toward cultivating trust\nin these systems.\n","authors":["Ashutosh Kumar","Shiv Vignesh Murthy","Sagarika Singh","Swathy Ragupathy"],"pdf_url":"https://arxiv.org/pdf/2401.12273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05413v2","updated":"2024-07-10T09:01:31Z","published":"2024-07-07T15:37:13Z","title":"SBoRA: Low-Rank Adaptation with Regional Weight Updates","summary":"  This paper introduces Standard Basis LoRA (SBoRA), a novel\nparameter-efficient fine-tuning approach for Large Language Models that builds\nupon the pioneering works of Low-Rank Adaptation (LoRA) and Orthogonal\nAdaptation. SBoRA further reduces the computational and memory requirements of\nLoRA while enhancing learning performance. By leveraging orthogonal standard\nbasis vectors to initialize one of the low-rank matrices, either A or B, SBoRA\nenables regional weight updates and memory-efficient fine-tuning. This approach\ngives rise to two variants, SBoRA-FA and SBoRA-FB, where only one of the\nmatrices is updated, resulting in a sparse update matrix with a majority of\nzero rows or columns. Consequently, the majority of the fine-tuned model's\nweights remain unchanged from the pre-trained weights. This characteristic of\nSBoRA, wherein regional weight updates occur, is reminiscent of the modular\norganization of the human brain, which efficiently adapts to new tasks. Our\nempirical results demonstrate the superiority of SBoRA-FA over LoRA in various\nfine-tuning tasks, including commonsense reasoning and arithmetic reasoning.\nFurthermore, we evaluate the effectiveness of QSBoRA on quantized LLaMA models\nof varying scales, highlighting its potential for efficient adaptation to new\ntasks. Code is available at https://github.com/cityuhkai/SBoRA\n","authors":["Lai-Man Po","Yuyang Liu","Haoxuan Wu","Tianqi Zhang","Wing-Yin Yu","Zeyu Jiang","Kun Li"],"pdf_url":"https://arxiv.org/pdf/2407.05413v2.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2405.18448v2","updated":"2024-07-10T08:47:52Z","published":"2024-05-28T01:15:21Z","title":"Multi-objective Representation for Numbers in Clinical Narratives Using\n  CamemBERT-bio","summary":"  This research aims to classify numerical values extracted from medical\ndocuments across seven distinct physiological categories, employing\nCamemBERT-bio. Previous studies suggested that transformer-based models might\nnot perform as well as traditional NLP models in such tasks. To enhance\nCamemBERT-bio's performances, we introduce two main innovations: integrating\nkeyword embeddings into the model and adopting a number-agnostic strategy by\nexcluding all numerical data from the text. The implementation of label\nembedding techniques refines the attention mechanisms, while the technique of\nusing a `numerical-blind' dataset aims to bolster context-centric learning.\nAnother key component of our research is determining the criticality of\nextracted numerical data. To achieve this, we utilized a simple approach that\ninvolves verifying if the value falls within the established standard ranges.\nOur findings are encouraging, showing substantial improvements in the\neffectiveness of CamemBERT-bio, surpassing conventional methods with an F1\nscore of 0.89. This represents an over 20\\% increase over the 0.73 $F_1$ score\nof traditional approaches and an over 9\\% increase over the 0.82 $F_1$ score of\nstate-of-the-art approaches. All this was achieved despite using small and\nimbalanced training datasets.\n","authors":["Boammani Aser Lompo","Thanh-Dung Le"],"pdf_url":"https://arxiv.org/pdf/2405.18448v2.pdf","comment":"Under the revision. arXiv admin note: substantial text overlap with\n  arXiv:2404.10171"},{"id":"http://arxiv.org/abs/2404.16698v3","updated":"2024-07-10T08:34:06Z","published":"2024-04-25T15:59:16Z","title":"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society\n  of LLM Agents","summary":"  As AI systems pervade human life, ensuring that large language models (LLMs)\nmake safe decisions remains a significant challenge. We introduce the\nGovernance of the Commons Simulation (GovSim), a generative simulation platform\ndesigned to study strategic interactions and cooperative decision-making in\nLLMs. In GovSim, a society of AI agents must collectively balance exploiting a\ncommon resource with sustaining it for future use. This environment enables the\nstudy of how ethical considerations, strategic planning, and negotiation skills\nimpact cooperative outcomes. We develop an LLM-based agent architecture and\ntest it with the leading open and closed LLMs. We find that all but the most\npowerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with\nthe highest survival rate below 54%. Ablations reveal that successful\nmulti-agent communication between agents is critical for achieving cooperation\nin these cases. Furthermore, our analyses show that the failure to achieve\nsustainable cooperation in most LLMs stems from their inability to formulate\nand analyze hypotheses about the long-term effects of their actions on the\nequilibrium of the group. Finally, we show that agents that leverage\n\"Universalization\"-based reasoning, a theory of moral thinking, are able to\nachieve significantly better sustainability. Taken together, GovSim enables us\nto study the mechanisms that underlie sustainable self-government with\nspecificity and scale. We open source the full suite of our research results,\nincluding the simulation environment, agent prompts, and a comprehensive web\ninterface.\n","authors":["Giorgio Piatti","Zhijing Jin","Max Kleiman-Weiner","Bernhard SchÃ¶lkopf","Mrinmaya Sachan","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2404.16698v3.pdf","comment":"Revised version"},{"id":"http://arxiv.org/abs/2406.10882v4","updated":"2024-07-10T08:22:10Z","published":"2024-06-16T10:10:37Z","title":"SCAR: Efficient Instruction-Tuning for Large Language Models via Style\n  Consistency-Aware Response Ranking","summary":"  Recent studies have shown that maintaining a consistent response style by\nhuman experts and enhancing data quality in training sets can significantly\nimprove the performance of fine-tuned Large Language Models (LLMs) while\nreducing the number of training examples needed. However, the precise\ndefinition of style and the relationship between style, data quality, and LLM\nperformance remains unclear. This research decomposes response style into\npresentation and composition styles and finds that, among training data of\nsimilar quality, those with higher style consistency lead to better LLM\nperformance. Inspired by this, we introduce Style Consistency-Aware Response\nRanking (SCAR), which automatically prioritizes instruction-response pairs in\nthe training set based on their response stylistic consistency. By selecting\nthe most style-consistent examples, ranging from the top 25% to 0.7% of the\nfull dataset, the fine-tuned LLMs can match or even surpass the performance of\nmodels trained on the entire dataset in coding and open-ended\nquestion-answering benchmarks. Code and data are available at\nhttps://github.com/zhuang-li/SCAR .\n","authors":["Zhuang Li","Yuncheng Hua","Thuy-Trang Vu","Haolan Zhan","Lizhen Qu","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.10882v4.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2403.09974v2","updated":"2024-07-10T08:20:56Z","published":"2024-03-15T02:40:13Z","title":"Unlocking the Multi-modal Potential of CLIP for Generalized Category\n  Discovery","summary":"  Given unlabelled datasets containing both old and new categories, generalized\ncategory discovery (GCD) aims to accurately discover new classes while\ncorrectly classifying old classes, leveraging the class concepts learned from\nlabeled samples. Current GCD methods only use a single visual modality of\ninformation, resulting in poor classification of visually similar classes. As a\ndifferent modality, text information can provide complementary discriminative\ninformation, which motivates us to introduce it into the GCD task. However, the\nlack of class names for unlabelled data makes it impractical to utilize text\ninformation. To tackle this challenging problem, in this paper, we propose a\nText Embedding Synthesizer (TES) to generate pseudo text embeddings for\nunlabelled samples. Specifically, our TES leverages the property that CLIP can\ngenerate aligned vision-language features, converting visual embeddings into\ntokens of the CLIP's text encoder to generate pseudo text embeddings. Besides,\nwe employ a dual-branch framework, through the joint learning and instance\nconsistency of different modality branches, visual and semantic information\nmutually enhance each other, promoting the interaction and fusion of visual and\ntext knowledge. Our method unlocks the multi-modal potentials of CLIP and\noutperforms the baseline methods by a large margin on all GCD benchmarks,\nachieving new state-of-the-art. The code will be released at\nhttps://github.com/enguangW/GET .\n","authors":["Enguang Wang","Zhimao Peng","Zhengyuan Xie","Fei Yang","Xialei Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.09974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07457v1","updated":"2024-07-10T08:20:47Z","published":"2024-07-10T08:20:47Z","title":"GLBench: A Comprehensive Benchmark for Graph with Large Language Models","summary":"  The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench.\n","authors":["Yuhan Li","Peisong Wang","Xiao Zhu","Aochuan Chen","Haiyun Jiang","Deng Cai","Victor Wai Kin Chan","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2407.07457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08698v2","updated":"2024-07-10T07:38:32Z","published":"2024-04-10T16:11:09Z","title":"Lossless Acceleration of Large Language Model via Adaptive N-gram\n  Parallel Decoding","summary":"  While Large Language Models (LLMs) have shown remarkable abilities, they are\nhindered by significant resource consumption and considerable latency due to\nautoregressive processing. In this study, we introduce Adaptive N-gram Parallel\nDecoding (ANPD), an innovative and lossless approach that accelerates inference\nby allowing the simultaneous generation of multiple tokens. ANPD incorporates a\ntwo-stage approach: it begins with a rapid drafting phase that employs an\nN-gram module, which adapts based on the current interactive context, followed\nby a verification phase, during which the original LLM assesses and confirms\nthe proposed tokens. Consequently, ANPD preserves the integrity of the LLM's\noriginal output while enhancing processing speed. We further leverage a\nmulti-level architecture for the N-gram module to enhance the precision of the\ninitial draft, consequently reducing inference latency. ANPD eliminates the\nneed for retraining or extra GPU memory, making it an efficient and\nplug-and-play enhancement. In our experiments, models such as LLaMA and its\nfine-tuned variants have shown speed improvements up to 3.67x, validating the\neffectiveness of our proposed ANPD.\n","authors":["Jie Ou","Yueming Chen","Wenhong Tian"],"pdf_url":"https://arxiv.org/pdf/2404.08698v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07425v1","updated":"2024-07-10T07:27:38Z","published":"2024-07-10T07:27:38Z","title":"Out-of-distribution generalisation in spoken language understanding","summary":"  Test data is said to be out-of-distribution (OOD) when it unexpectedly\ndiffers from the training data, a common challenge in real-world use cases of\nmachine learning. Although OOD generalisation has gained interest in recent\nyears, few works have focused on OOD generalisation in spoken language\nunderstanding (SLU) tasks. To facilitate research on this topic, we introduce a\nmodified version of the popular SLU dataset SLURP, featuring data splits for\ntesting OOD generalisation in the SLU task. We call our modified dataset SLURP\nFor OOD generalisation, or SLURPFOOD. Utilising our OOD data splits, we find\nend-to-end SLU models to have limited capacity for generalisation. Furthermore,\nby employing model interpretability techniques, we shed light on the factors\ncontributing to the generalisation difficulties of the models. To improve the\ngeneralisation, we experiment with two techniques, which improve the results on\nsome, but not all the splits, emphasising the need for new techniques.\n","authors":["Dejan Porjazovski","Anssi Moisio","Mikko Kurimo"],"pdf_url":"https://arxiv.org/pdf/2407.07425v1.pdf","comment":"Accepted for INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.07413v1","updated":"2024-07-10T07:14:51Z","published":"2024-07-10T07:14:51Z","title":"KpopMT: Translation Dataset with Terminology for Kpop Fandom","summary":"  While machines learn from existing corpora, humans have the unique capability\nto establish and accept new language systems. This makes human form unique\nlanguage systems within social groups. Aligning with this, we focus on a gap\nremaining in addressing translation challenges within social groups, where\nin-group members utilize unique terminologies. We propose KpopMT dataset, which\naims to fill this gap by enabling precise terminology translation, choosing\nKpop fandom as an initiative for social groups given its global popularity.\nExpert translators provide 1k English translations for Korean posts and\ncomments, each annotated with specific terminology within social groups'\nlanguage systems. We evaluate existing translation systems including GPT models\non KpopMT to identify their failure cases. Results show overall low scores,\nunderscoring the challenges of reflecting group-specific terminologies and\nstyles in translation. We make KpopMT publicly available.\n","authors":["JiWoo Kim","Yunsu Kim","JinYeong Bak"],"pdf_url":"https://arxiv.org/pdf/2407.07413v1.pdf","comment":"accepted to LoresMT 2024"},{"id":"http://arxiv.org/abs/2407.06567v2","updated":"2024-07-10T06:59:18Z","published":"2024-07-09T05:52:26Z","title":"FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal\n  Reinforcement for Enhanced Financial Decision Making","summary":"  Large language models (LLMs) have demonstrated notable potential in\nconducting complex tasks and are increasingly utilized in various financial\napplications. However, high-quality sequential financial investment\ndecision-making remains challenging. These tasks require multiple interactions\nwith a volatile environment for every decision, demanding sufficient\nintelligence to maximize returns and manage risks. Although LLMs have been used\nto develop agent systems that surpass human teams and yield impressive\ninvestment returns, opportunities to enhance multi-sourced information\nsynthesis and optimize decision-making outcomes through timely experience\nrefinement remain unexplored. Here, we introduce the FinCon, an LLM-based\nmulti-agent framework with CONceptual verbal reinforcement tailored for diverse\nFINancial tasks. Inspired by effective real-world investment firm\norganizational structures, FinCon utilizes a manager-analyst communication\nhierarchy. This structure allows for synchronized cross-functional agent\ncollaboration towards unified goals through natural language interactions and\nequips each agent with greater memory capacity than humans. Additionally, a\nrisk-control component in FinCon enhances decision quality by episodically\ninitiating a self-critiquing mechanism to update systematic investment beliefs.\nThe conceptualized beliefs serve as verbal reinforcement for the future agent's\nbehavior and can be selectively propagated to the appropriate node that\nrequires knowledge updates. This feature significantly improves performance\nwhile reducing unnecessary peer-to-peer communication costs. Moreover, FinCon\ndemonstrates strong generalization capabilities in various financial tasks,\nincluding single stock trading and portfolio management.\n","authors":["Yangyang Yu","Zhiyuan Yao","Haohang Li","Zhiyang Deng","Yupeng Cao","Zhi Chen","Jordan W. Suchow","Rong Liu","Zhenyu Cui","Denghui Zhang","Koduvayur Subbalakshmi","Guojun Xiong","Yueru He","Jimin Huang","Dong Li","Qianqian Xie"],"pdf_url":"https://arxiv.org/pdf/2407.06567v2.pdf","comment":"LLM Applications, LLM Agents, Financial Technology, Quantitative\n  Finance, Algorithmic Trading, Cognitive Science"},{"id":"http://arxiv.org/abs/2403.04325v3","updated":"2024-07-10T06:49:40Z","published":"2024-03-07T08:44:42Z","title":"Measuring Meaning Composition in the Human Brain with Composition Scores\n  from Large Language Models","summary":"  The process of meaning composition, wherein smaller units like morphemes or\nwords combine to form the meaning of phrases and sentences, is essential for\nhuman sentence comprehension. Despite extensive neurolinguistic research into\nthe brain regions involved in meaning composition, a computational metric to\nquantify the extent of composition is still lacking. Drawing on the key-value\nmemory interpretation of transformer feed-forward network blocks, we introduce\nthe Composition Score, a novel model-based metric designed to quantify the\ndegree of meaning composition during sentence comprehension. Experimental\nfindings show that this metric correlates with brain clusters associated with\nword frequency, structural processing, and general sensitivity to words,\nsuggesting the multifaceted nature of meaning composition during human sentence\ncomprehension.\n","authors":["Changjiang Gao","Jixing Li","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2403.04325v3.pdf","comment":"Accepted by ACL 2024 (main conference, long paper)"},{"id":"http://arxiv.org/abs/2403.18624v2","updated":"2024-07-10T05:26:17Z","published":"2024-03-27T14:34:29Z","title":"Vulnerability Detection with Code Language Models: How Far Are We?","summary":"  In the context of the rising interest in code language models (code LMs) and\nvulnerability detection, we study the effectiveness of code LMs for detecting\nvulnerabilities. Our analysis reveals significant shortcomings in existing\nvulnerability datasets, including poor data quality, low label accuracy, and\nhigh duplication rates, leading to unreliable model performance in realistic\nvulnerability detection scenarios. Additionally, the evaluation methods used\nwith these datasets are not representative of real-world vulnerability\ndetection.\n  To address these challenges, we introduce PrimeVul, a new dataset for\ntraining and evaluating code LMs for vulnerability detection. PrimeVul\nincorporates a novel set of data labeling techniques that achieve comparable\nlabel accuracy to human-verified benchmarks while significantly expanding the\ndataset. It also implements a rigorous data de-duplication and chronological\ndata splitting strategy to mitigate data leakage issues, alongside introducing\nmore realistic evaluation metrics and settings. This comprehensive approach\naims to provide a more accurate assessment of code LMs' performance in\nreal-world conditions.\n  Evaluating code LMs on PrimeVul reveals that existing benchmarks\nsignificantly overestimate the performance of these models. For instance, a\nstate-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on\nPrimeVul. Attempts to improve performance through advanced training techniques\nand larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin\nto random guessing in the most stringent settings. These findings underscore\nthe considerable gap between current capabilities and the practical\nrequirements for deploying code LMs in security roles, highlighting the need\nfor more innovative research in this domain.\n","authors":["Yangruibo Ding","Yanjun Fu","Omniyyah Ibrahim","Chawin Sitawarin","Xinyun Chen","Basel Alomair","David Wagner","Baishakhi Ray","Yizheng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18624v2.pdf","comment":"Accepted for the 47th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2025); Camera-ready Work in Progress"},{"id":"http://arxiv.org/abs/2407.07373v1","updated":"2024-07-10T05:17:55Z","published":"2024-07-10T05:17:55Z","title":"Automatic Extraction of Disease Risk Factors from Medical Publications","summary":"  We present a novel approach to automating the identification of risk factors\nfor diseases from medical literature, leveraging pre-trained models in the\nbio-medical domain, while tuning them for the specific task. Faced with the\nchallenges of the diverse and unstructured nature of medical articles, our\nstudy introduces a multi-step system to first identify relevant articles, then\nclassify them based on the presence of risk factor discussions and, finally,\nextract specific risk factor information for a disease through a\nquestion-answering model.\n  Our contributions include the development of a comprehensive pipeline for the\nautomated extraction of risk factors and the compilation of several datasets,\nwhich can serve as valuable resources for further research in this area. These\ndatasets encompass a wide range of diseases, as well as their associated risk\nfactors, meticulously identified and validated through a fine-grained\nevaluation scheme. We conducted both automatic and thorough manual evaluation,\ndemonstrating encouraging results. We also highlight the importance of\nimproving models and expanding dataset comprehensiveness to keep pace with the\nrapidly evolving field of medical research.\n","authors":["Maxim Rubchinsky","Ella Rabinovich","Adi Shraibman","Netanel Golan","Tali Sahar","Dorit Shweiki"],"pdf_url":"https://arxiv.org/pdf/2407.07373v1.pdf","comment":"BioNLP@ACL2024, 12 pages"},{"id":"http://arxiv.org/abs/2407.07370v1","updated":"2024-07-10T05:05:47Z","published":"2024-07-10T05:05:47Z","title":"LokiLM: Technical Report","summary":"  In this work, we introduce LokiLM, a 1.4B parameter large language model\ntrained on 500B tokens. Our model performs strongly in natural language\nreasoning tasks and achieves state-of-the-art performance among models with\n1.5B parameters or less. LokiLM is trained using multi-teacher knowledge\ndistillation and high-quality training data to achieve benchmark results\ncompetitive with larger models trained on significantly more tokens. We support\nthese findings by introducing steps to avoid benchmark contamination and\noverfitting throughout our development process. Despite its promising\nperformance, LokiLM exhibits a concerning amount of hallucinations and scores\npoorly on the TruthfulQA benchmark, so we do not release the model publicly.\n","authors":["Justin Kiefel","Shrey Shah"],"pdf_url":"https://arxiv.org/pdf/2407.07370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09806v2","updated":"2024-07-10T04:04:06Z","published":"2024-05-16T04:28:44Z","title":"MediSyn: Text-Guided Diffusion Models for Broad Medical 2D and 3D Image\n  Synthesis","summary":"  Diffusion models have recently gained significant traction due to their\nability to generate high-fidelity and diverse images and videos conditioned on\ntext prompts. In medicine, this application promises to address the critical\nchallenge of data scarcity, a consequence of barriers in data sharing,\nstringent patient privacy regulations, and disparities in patient population\nand demographics. By generating realistic and varying medical 2D and 3D images,\nthese models offer a rich, privacy-respecting resource for algorithmic training\nand research. To this end, we introduce MediSyn, a pair of instruction-tuned\ntext-guided latent diffusion models with the ability to generate high-fidelity\nand diverse medical 2D and 3D images across specialties and modalities. Through\nestablished metrics, we show significant improvement in broad medical image and\nvideo synthesis guided by text prompts.\n","authors":["Joseph Cho","Cyril Zakka","Dhamanpreet Kaur","Rohan Shad","Ross Wightman","Akshay Chaudhari","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2405.09806v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07342v1","updated":"2024-07-10T03:26:15Z","published":"2024-07-10T03:26:15Z","title":"Multilingual Blending: LLM Safety Alignment Evaluation with Language\n  Mixture","summary":"  As safety remains a crucial concern throughout the development lifecycle of\nLarge Language Models (LLMs), researchers and industrial practitioners have\nincreasingly focused on safeguarding and aligning LLM behaviors with human\npreferences and ethical standards. LLMs, trained on extensive multilingual\ncorpora, exhibit powerful generalization abilities across diverse languages and\ndomains. However, current safety alignment practices predominantly focus on\nsingle-language scenarios, which leaves their effectiveness in complex\nmultilingual contexts, especially for those complex mixed-language formats,\nlargely unexplored. In this study, we introduce Multilingual Blending, a\nmixed-language query-response scheme designed to evaluate the safety alignment\nof various state-of-the-art LLMs (e.g., GPT-4o, GPT-3.5, Llama3) under\nsophisticated, multilingual conditions. We further investigate language\npatterns such as language availability, morphology, and language family that\ncould impact the effectiveness of Multilingual Blending in compromising the\nsafeguards of LLMs. Our experimental results show that, without meticulously\ncrafted prompt templates, Multilingual Blending significantly amplifies the\ndetriment of malicious queries, leading to dramatically increased bypass rates\nin LLM safety alignment (67.23% on GPT-3.5 and 40.34% on GPT-4o), far exceeding\nthose of single-language baselines. Moreover, the performance of Multilingual\nBlending varies notably based on intrinsic linguistic properties, with\nlanguages of different morphology and from diverse families being more prone to\nevading safety alignments. These findings underscore the necessity of\nevaluating LLMs and developing corresponding safety alignment strategies in a\ncomplex, multilingual context to align with their superior cross-language\ngeneralization capabilities.\n","authors":["Jiayang Song","Yuheng Huang","Zhehua Zhou","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2407.07342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07341v1","updated":"2024-07-10T03:25:47Z","published":"2024-07-10T03:25:47Z","title":"MixSumm: Topic-based Data Augmentation using LLMs for Low-resource\n  Extractive Text Summarization","summary":"  Low-resource extractive text summarization is a vital but heavily\nunderexplored area of research. Prior literature either focuses on abstractive\ntext summarization or prompts a large language model (LLM) like GPT-3 directly\nto generate summaries. In this work, we propose MixSumm for low-resource\nextractive text summarization. Specifically, MixSumm prompts an open-source\nLLM, LLaMA-3-70b, to generate documents that mix information from multiple\ntopics as opposed to generating documents without mixup, and then trains a\nsummarization model on the generated dataset. We use ROUGE scores and L-Eval, a\nreference-free LLaMA-3-based evaluation method to measure the quality of\ngenerated summaries. We conduct extensive experiments on a challenging text\nsummarization benchmark comprising the TweetSumm, WikiHow, and ArXiv/PubMed\ndatasets and show that our LLM-based data augmentation framework outperforms\nrecent prompt-based approaches for low-resource extractive summarization.\nAdditionally, our results also demonstrate effective knowledge distillation\nfrom LLaMA-3-70b to a small BERT-based extractive summarizer.\n","authors":["Gaurav Sahu","Issam H. Laradji"],"pdf_url":"https://arxiv.org/pdf/2407.07341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07330v1","updated":"2024-07-10T02:58:37Z","published":"2024-07-10T02:58:37Z","title":"Interpretable Differential Diagnosis with Dual-Inference Large Language\n  Models","summary":"  Methodological advancements to automate the generation of differential\ndiagnosis (DDx) to predict a list of potential diseases as differentials given\npatients' symptom descriptions are critical to clinical reasoning and\napplications such as decision support. However, providing reasoning or\ninterpretation for these differential diagnoses is more meaningful.\nFortunately, large language models (LLMs) possess powerful language processing\nabilities and have been proven effective in various related tasks. Motivated by\nthis potential, we investigate the use of LLMs for interpretable DDx. First, we\ndevelop a new DDx dataset with expert-derived interpretation on 570 public\nclinical notes. Second, we propose a novel framework, named Dual-Inf, that\nenables LLMs to conduct bidirectional inference for interpretation. Both human\nand automated evaluation demonstrate the effectiveness of Dual-Inf in\npredicting differentials and diagnosis explanations. Specifically, the\nperformance improvement of Dual-Inf over the baseline methods exceeds 32%\nw.r.t. BERTScore in DDx interpretation. Furthermore, experiments verify that\nDual-Inf (1) makes fewer errors in interpretation, (2) has great\ngeneralizability, (3) is promising for rare disease diagnosis and explanation.\n","authors":["Shuang Zhou","Sirui Ding","Jiashuo Wang","Mingquan Lin","Genevieve B. Melton","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.07330v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2407.07329v1","updated":"2024-07-10T02:56:55Z","published":"2024-07-10T02:56:55Z","title":"Probability of Differentiation Reveals Brittleness of Homogeneity Bias\n  in Large Language Models","summary":"  Homogeneity bias in Large Language Models (LLMs) refers to their tendency to\nhomogenize the representations of some groups compared to others. Previous\nstudies documenting this bias have predominantly used encoder models, which may\nhave inadvertently introduced biases. To address this limitation, we prompted\nGPT-4 to generate single word/expression completions associated with 18\nsituation cues - specific, measurable elements of environments that influence\nhow individuals perceive situations and compared the variability of these\ncompletions using probability of differentiation. This approach directly\nassessed homogeneity bias from the model's outputs, bypassing encoder models.\nAcross five studies, we find that homogeneity bias is highly volatile across\nsituation cues and writing prompts, suggesting that the bias observed in past\nwork may reflect those within encoder models rather than LLMs. Furthermore,\nthese results suggest that homogeneity bias in LLMs is brittle, as even minor\nand arbitrary changes in prompts can significantly alter the expression of\nbiases. Future work should further explore how variations in syntactic features\nand topic choices in longer text generations influence homogeneity bias in\nLLMs.\n","authors":["Messi H. J. Lee","Calvin K. Lai"],"pdf_url":"https://arxiv.org/pdf/2407.07329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03138v2","updated":"2024-07-10T02:54:23Z","published":"2024-05-06T03:21:55Z","title":"CRAFT: Extracting and Tuning Cultural Instructions from the Wild","summary":"  Large language models (LLMs) have rapidly evolved as the foundation of\nvarious natural language processing (NLP) applications. Despite their wide use\ncases, their understanding of culturally-related concepts and reasoning remains\nlimited. Meantime, there is a significant need to enhance these models'\ncultural reasoning capabilities, especially concerning underrepresented\nregions. This paper introduces a novel pipeline for extracting high-quality,\nculturally-related instruction tuning datasets from vast unstructured corpora.\nWe utilize a self-instruction generation pipeline to identify cultural concepts\nand trigger instruction. By integrating with a general-purpose instruction\ntuning dataset, our model demonstrates enhanced capabilities in recognizing and\nunderstanding regional cultural nuances, thereby enhancing its reasoning\ncapabilities. We conduct experiments across three regions: Singapore, the\nPhilippines, and the United States, achieving performance improvement of up to\n6%. Our research opens new avenues for extracting cultural instruction tuning\nsets directly from unstructured data, setting a precedent for future\ninnovations in the field.\n","authors":["Bin Wang","Geyu Lin","Zhengyuan Liu","Chengwei Wei","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2405.03138v2.pdf","comment":"Aceepted to ACL 2024 Workshop - C3NLP (Workshop on Cross-Cultural\n  Considerations in NLP)"},{"id":"http://arxiv.org/abs/2407.07325v1","updated":"2024-07-10T02:43:18Z","published":"2024-07-10T02:43:18Z","title":"HiLight: Technical Report on the Motern AI Video Language Model","summary":"  This technical report presents the implementation of a state-of-the-art video\nencoder for video-text modal alignment and a video conversation framework\ncalled HiLight, which features dual visual towers. The work is divided into two\nmain parts: 1.alignment of video and text modalities; 2.convenient and\nefficient way to interact with users. Our goal is to address the task of video\ncomprehension in the context of billiards. The report includes a discussion of\nthe concepts and the final solution developed during the task's implementation.\n","authors":["Zhiting Wang","Qiangong Zhou","Kangjie Yang","Zongyang Liu. Xin Mao"],"pdf_url":"https://arxiv.org/pdf/2407.07325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07321v1","updated":"2024-07-10T02:33:09Z","published":"2024-07-10T02:33:09Z","title":"RAG vs. Long Context: Examining Frontier Large Language Models for\n  Environmental Review Document Comprehension","summary":"  Large Language Models (LLMs) have been applied to many research problems\nacross various domains. One of the applications of LLMs is providing\nquestion-answering systems that cater to users from different fields. The\neffectiveness of LLM-based question-answering systems has already been\nestablished at an acceptable level for users posing questions in popular and\npublic domains such as trivia and literature. However, it has not often been\nestablished in niche domains that traditionally require specialized expertise.\nTo this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performance\nof three frontier LLMs -- Claude Sonnet, Gemini, and GPT-4 -- when answering\nquestions originating from Environmental Impact Statements prepared by U.S.\nfederal government agencies in accordance with the National Environmental\nEnvironmental Act (NEPA). We specifically measure the ability of LLMs to\nunderstand the nuances of legal, technical, and compliance-related information\npresent in NEPA documents in different contextual scenarios. For example, we\ntest the LLMs' internal prior NEPA knowledge by providing questions without any\ncontext, as well as assess how LLMs synthesize the contextual information\npresent in long NEPA documents to facilitate the question/answering task. We\ncompare the performance of the long context LLMs and RAG powered models in\nhandling different types of questions (e.g., problem-solving, divergent). Our\nresults suggest that RAG powered models significantly outperform the long\ncontext models in the answer accuracy regardless of the choice of the frontier\nLLM. Our further analysis reveals that many models perform better answering\nclosed questions than divergent and problem-solving questions.\n","authors":["Hung Phan","Anurag Acharya","Sarthak Chaturvedi","Shivam Sharma","Mike Parker","Dan Nally","Ali Jannesari","Karl Pazdernik","Mahantesh Halappanavar","Sai Munikoti","Sameera Horawalavithana"],"pdf_url":"https://arxiv.org/pdf/2407.07321v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2407.07313v1","updated":"2024-07-10T02:20:19Z","published":"2024-07-10T02:20:19Z","title":"ESM+: Modern Insights into Perspective on Text-to-SQL Evaluation in the\n  Age of Large Language Models","summary":"  The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. Despite several challenges, recent models\nhave made remarkable advancements in this task using large language models\n(LLMs). Interestingly, we find that LLM-based models without fine-tuning\nexhibit distinct natures compared to their fine-tuned counterparts, leading to\ninadequacies in current evaluation metrics to accurately convey their\nperformance. Thus, we analyze the two primary metrics, Test Suite Execution\nAccuracy (EXE) and Exact Set Matching Accuracy (ESM), to examine their\nrobustness for this task and address shortcomings. We compare the performance\nof 9 LLM-based models using EXE, the original ESM, and our improved ESM (called\nESM+). Our results show that EXE and ESM have high false positive and negative\nrates of 11.3% and 13.9%, while ESM+ gives those of 0.1% and 2.6% respectively,\nproviding a significantly more stable evaluation. We release the ESM+ script as\nopen-source for the community to contribute, while enjoying a more reliable\nassessment of Text-to-SQL.\n","authors":["Benjamin Ascoli","Ram Kandikonda","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2407.07313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06645v2","updated":"2024-07-10T01:55:29Z","published":"2024-07-09T08:14:29Z","title":"Entropy Law: The Story Behind Data Compression and LLM Performance","summary":"  Data is the cornerstone of large language models (LLMs), but not all data is\nuseful for model learning. Carefully selected data can better elicit the\ncapabilities of LLMs with much less computational overhead. Most methods\nconcentrate on evaluating the quality of individual samples in data selection,\nwhile the combinatorial effects among samples are neglected. Even if each\nsample is of perfect quality, their combinations may be suboptimal in teaching\nLLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim\nto uncover the underlying relationships between LLM performance and data\nselection. Inspired by the information compression nature of LLMs, we uncover\nan ``entropy law'' that connects LLM performance with data compression ratio\nand first-epoch training loss, which reflect the information redundancy of a\ndataset and the mastery of inherent knowledge encoded in this dataset,\nrespectively. Through both theoretical deduction and empirical evaluation, we\nfind that model performance is negatively correlated to the compression ratio\nof training data, which usually yields a lower training loss. Based on the\nfindings of the entropy law, we propose a quite efficient and universal data\nselection method named \\textbf{ZIP} for training LLMs, which aim to prioritize\ndata subsets exhibiting a low compression ratio. Based on a multi-stage\nalgorithm that selects diverse data in a greedy manner, we can obtain a good\ndata subset with satisfactory diversity. Extensive experiments have been\nconducted to validate the entropy law and the superiority of ZIP across\ndifferent LLM backbones and alignment stages. We also present an interesting\napplication of entropy law that can detect potential performance risks at the\nbeginning of model training.\n","authors":["Mingjia Yin","Chuhan Wu","Yufei Wang","Hao Wang","Wei Guo","Yasheng Wang","Yong Liu","Ruiming Tang","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.06645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09939v2","updated":"2024-07-10T01:25:50Z","published":"2024-05-16T09:42:37Z","title":"SciQAG: A Framework for Auto-Generated Science Question Answering\n  Dataset with Fine-grained Evaluation","summary":"  We introduce SciQAG, a novel framework for automatically generating\nhigh-quality science question-answer pairs from a large corpus of scientific\nliterature based on large language models (LLMs). SciQAG consists of a QA\ngenerator and a QA evaluator, which work together to extract diverse and\nresearch-level questions and answers from scientific papers. Utilizing this\nframework, we construct a large-scale, high-quality, open-ended science QA\ndataset containing 188,042 QA pairs extracted from 22,743 scientific papers\nacross 24 scientific domains. We also introduce SciQAG-24D, a new benchmark\ntask designed to evaluate the science question-answering ability of LLMs.\nExtensive experiments demonstrate that fine-tuning LLMs on the SciQAG dataset\nsignificantly improves their performance on both open-ended question answering\nand scientific tasks. To foster research and collaboration, we make the\ndatasets, models, and evaluation codes publicly available, contributing to the\nadvancement of science question answering and developing more interpretable and\nreasoning-capable AI systems.\n","authors":["Yuwei Wan","Yixuan Liu","Aswathy Ajith","Clara Grazian","Bram Hoex","Wenjie Zhang","Chunyu Kit","Tong Xie","Ian Foster"],"pdf_url":"https://arxiv.org/pdf/2405.09939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12426v2","updated":"2024-07-10T00:35:17Z","published":"2023-09-21T18:48:02Z","title":"Can LLMs Augment Low-Resource Reading Comprehension Datasets?\n  Opportunities and Challenges","summary":"  Large Language Models (LLMs) have demonstrated impressive zero shot\nperformance on a wide range of NLP tasks, demonstrating the ability to reason\nand apply commonsense. A relevant application is to use them for creating high\nquality synthetic datasets for downstream tasks. In this work, we probe whether\nGPT-4 can be used to augment existing extractive reading comprehension\ndatasets. Automating data annotation processes has the potential to save large\namounts of time, money and effort that goes into manually labelling datasets.\nIn this paper, we evaluate the performance of GPT-4 as a replacement for human\nannotators for low resource reading comprehension tasks, by comparing\nperformance after fine tuning, and the cost associated with annotation. This\nwork serves to be the first analysis of LLMs as synthetic data augmenters for\nQA systems, highlighting the unique opportunities and challenges. Additionally,\nwe release augmented versions of low resource datasets, that will allow the\nresearch community to create further benchmarks for evaluation of generated\ndatasets.\n","authors":["Vinay Samuel","Houda Aynaou","Arijit Ghosh Chowdhury","Karthik Venkat Ramanan","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2309.12426v2.pdf","comment":"ACL 2024 SRW"},{"id":"http://arxiv.org/abs/2407.06866v2","updated":"2024-07-10T18:47:55Z","published":"2024-07-09T13:53:38Z","title":"ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context","summary":"  While the biases of language models in production are extensively documented,\nthe biases of their guardrails have been neglected. This paper studies how\ncontextual information about the user influences the likelihood of an LLM to\nrefuse to execute a request. By generating user biographies that offer\nideological and demographic information, we find a number of biases in\nguardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas\nare more likely to trigger a refusal guardrail when requesting censored or\nillegal information. Guardrails are also sycophantic, refusing to comply with\nrequests for a political position the user is likely to disagree with. We find\nthat certain identity groups and seemingly innocuous information, e.g., sports\nfandom, can elicit changes in guardrail sensitivity similar to direct\nstatements of political ideology. For each demographic category and even for\nAmerican football team fandom, we find that ChatGPT appears to infer a likely\npolitical ideology and modify guardrail behavior accordingly.\n","authors":["Victoria R. Li","Yida Chen","Naomi Saphra"],"pdf_url":"https://arxiv.org/pdf/2407.06866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08095v1","updated":"2024-07-10T23:50:08Z","published":"2024-07-10T23:50:08Z","title":"Virtual Agents for Alcohol Use Counseling: Exploring LLM-Powered\n  Motivational Interviewing","summary":"  We introduce a novel application of large language models (LLMs) in\ndeveloping a virtual counselor capable of conducting motivational interviewing\n(MI) for alcohol use counseling. Access to effective counseling remains\nlimited, particularly for substance abuse, and virtual agents offer a promising\nsolution by leveraging LLM capabilities to simulate nuanced communication\ntechniques inherent in MI. Our approach combines prompt engineering and\nintegration into a user-friendly virtual platform to facilitate realistic,\nempathetic interactions. We evaluate the effectiveness of our virtual agent\nthrough a series of studies focusing on replicating MI techniques and human\ncounselor dialog. Initial findings suggest that our LLM-powered virtual agent\nmatches human counselors' empathetic and adaptive conversational skills,\npresenting a significant step forward in virtual health counseling and\nproviding insights into the design and implementation of LLM-based therapeutic\ninteractions.\n","authors":["Ian Steenstra","Farnaz Nouraei","Mehdi Arjmand","Timothy W. Bickmore"],"pdf_url":"https://arxiv.org/pdf/2407.08095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14558v5","updated":"2024-07-10T23:46:06Z","published":"2023-10-23T04:22:50Z","title":"AlpaCare:Instruction-tuned Large Language Models for Medical Application","summary":"  Instruction-finetuning (IFT) has become crucial in aligning Large Language\nModels (LLMs) with diverse human needs and has shown great potential in medical\napplications. However, previous studies mainly fine-tune LLMs on biomedical\ndatasets with limited diversity, which often rely on benchmarks or narrow task\nscopes, and hence significantly limit the effectiveness on their medical\ninstruction-following ability and generalizability. To bridge this gap, we\npropose creating a diverse, machine-generated medical IFT dataset,\nMedInstruct-52k, using GPT-4 and ChatGPT with a high-quality expert-curated\nseed set. We then fine-tune LLaMA-series models on the dataset to develop\nAlpaCare. Despite using a smaller domain-specific dataset than previous medical\nLLMs, AlpaCare not only demonstrates superior performance on medical\napplications, with up to 38.1% absolute gain over best baselines in medical\nfree-form instruction evaluations, but also achieves 6.7% absolute gains\naveraged over multiple general domain benchmarks. Human evaluation further\nshows that AlpaCare consistently outperforms best baselines in terms of both\ncorrectness and helpfulness. We offer public access to our data, model, and\ncodebase in https://github.com/XZhang97666/AlpaCare.\n","authors":["Xinlu Zhang","Chenxin Tian","Xianjun Yang","Lichang Chen","Zekun Li","Linda Ruth Petzold"],"pdf_url":"https://arxiv.org/pdf/2310.14558v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03716v2","updated":"2024-07-10T23:15:49Z","published":"2023-10-05T17:38:28Z","title":"A Long Way to Go: Investigating Length Correlations in RLHF","summary":"  Great success has been reported using Reinforcement Learning from Human\nFeedback (RLHF) to align large language models, with open preference datasets\nenabling wider experimentation, particularly for \"helpfulness\" in tasks like\ndialogue and web question answering. Alongside these improvements, however,\nRLHF also often drives models to produce longer outputs. This paper\ndemonstrates, on three diverse settings, that optimizing for response length\nis, much more than previously thought, a significant factor behind RLHF.\nStudying the strategies RL optimization uses to maximize reward, we find\nimprovements in reward to largely be driven by increasing response length,\ninstead of other features. Indeed, we find that even a purely length-based\nreward reproduces most downstream RLHF improvements over supervised fine-tuned\nmodels. Testing a comprehensive set of length-countering interventions, we\nidentify the dominant source of these biases to be reward models, which, by\nstudying training dynamics, we find are non-robust and easily influenced by\nlength biases in preference data.\n","authors":["Prasann Singhal","Tanya Goyal","Jiacheng Xu","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2310.03716v2.pdf","comment":"21 pages, 13 figures, Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2311.05112v6","updated":"2024-07-10T22:10:32Z","published":"2023-11-09T02:55:58Z","title":"A Survey of Large Language Models in Medicine: Progress, Application,\n  and Challenge","summary":"  Large language models (LLMs), such as ChatGPT, have received substantial\nattention due to their capabilities for understanding and generating human\nlanguage. While there has been a burgeoning trend in research focusing on the\nemployment of LLMs in supporting different medical tasks (e.g., enhancing\nclinical diagnostics and providing medical education), a review of these\nefforts, particularly their development, practical applications, and outcomes\nin medicine, remains scarce. Therefore, this review aims to provide a detailed\noverview of the development and deployment of LLMs in medicine, including the\nchallenges and opportunities they face. In terms of development, we provide a\ndetailed introduction to the principles of existing medical LLMs, including\ntheir basic model structures, number of parameters, and sources and scales of\ndata used for model development. It serves as a guide for practitioners in\ndeveloping medical LLMs tailored to their specific needs. In terms of\ndeployment, we offer a comparison of the performance of different LLMs across\nvarious medical tasks, and further compare them with state-of-the-art\nlightweight models, aiming to provide an understanding of the advantages and\nlimitations of LLMs in medicine. Overall, in this review, we address the\nfollowing questions: 1) What are the practices for developing medical LLMs 2)\nHow to measure the medical task performance of LLMs in a medical setting? 3)\nHow have medical LLMs been employed in real-world practice? 4) What challenges\narise from the use of medical LLMs? and 5) How to more effectively develop and\ndeploy medical LLMs? By answering these questions, this review aims to provide\ninsights into the opportunities for LLMs in medicine and serve as a practical\nresource. We also maintain a regularly updated list of practical guides on\nmedical LLMs at https://github.com/AI-in-Health/MedLLMsPracticalGuide\n","authors":["Hongjian Zhou","Fenglin Liu","Boyang Gu","Xinyu Zou","Jinfa Huang","Jinge Wu","Yiru Li","Sam S. Chen","Peilin Zhou","Junling Liu","Yining Hua","Chengfeng Mao","Chenyu You","Xian Wu","Yefeng Zheng","Lei Clifton","Zheng Li","Jiebo Luo","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2311.05112v6.pdf","comment":"Preprint. Version 6. Update Figures 1-5; Tables 2-3; 31 pages"},{"id":"http://arxiv.org/abs/2407.04965v2","updated":"2024-07-10T21:31:11Z","published":"2024-07-06T05:56:22Z","title":"Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM\n  Compression","summary":"  Large language models (LLMs) are increasingly deployed in real-world\nscenarios with the help of recent model compression techniques. Such momentum\ntowards local deployment means the use of compressed LLMs will widely impact a\nlarge population. However, prior analysis works often prioritize on preserving\nperplexity which is a direct analogy to training loss. The impact of\ncompression method on other critical aspects of model behavior, particularly\nsafety, still calls for a systematic assessment. To this end, we investigate\nthe impact of model compression on four dimensions: (1) degeneration harm,\ni.e., bias and toxicity in generation; (2) representational harm, i.e., biases\nin discriminative tasks; (3) dialect bias; (4) language modeling and downstream\ntask performance. We cover a wide spectrum of LLM compression techniques,\nincluding unstructured pruning, semi-structured pruning and quantization. Our\nanalysis reveals that compression can lead to unexpected consequences. Although\ncompression may unintentionally remedy LLMs' degeneration harm, it can still\nexacerbate on the representational harm axis. Although compression may\nunintentionally remedy LLMs' degeneration harm, it can still exacerbate on the\nrepresentational harm axis. Moreover, there is a divergent impact on different\nprotected groups as the compression rate grows. Finally, different compression\nmethods have drastically different safety impacts, e.g., quantization mostly\npreserves bias while pruning degrades quickly. Our findings underscore the\nimportance of integrating safety assessments into the development of compressed\nLLMs to ensure their reliability across real-world applications. Our full\nresults are available here:\n\\url{https://github.com/zhichaoxu-shufe/Beyond-Perplexity-Compression-Safety-Eval}\n","authors":["Zhichao Xu","Ashim Gupta","Tao Li","Oliver Bentham","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2407.04965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05893v3","updated":"2024-07-10T21:06:48Z","published":"2024-04-08T22:29:53Z","title":"Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models","summary":"  Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base\n","authors":["Sowmya S. Sundaram","Benjamin Solomon","Avani Khatri","Anisha Laumas","Purvesh Khatri","Mark A. Musen"],"pdf_url":"https://arxiv.org/pdf/2404.05893v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08044v1","updated":"2024-07-10T20:52:18Z","published":"2024-07-10T20:52:18Z","title":"RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective\n  Weight-Activation Quantization","summary":"  Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient\nFine-Tuning (PEFT)method, significantly enhances the training efficiency by\nupdating only a small portion of the weights in Large Language Models (LLMs).\nRecently, weight-only quantization techniques have also been applied to LoRA\nmethods to reduce the memory footprint of fine-tuning. However, applying\nweight-activation quantization to the LoRA pipeline is under-explored, and we\nobserve substantial performance degradation primarily due to the presence of\nactivation outliers. In this work, we propose RoLoRA, the first LoRA-based\nscheme for effective weight-activation quantization. RoLoRA utilizes rotation\nfor outlier elimination and proposes rotation-aware fine-tuning to preserve the\noutlier-free characteristics in rotated LLMs. Experimental results show RoLoRA\nconsistently improves low-bit LoRA convergence and post-training quantization\nrobustness in weight-activation settings. We evaluate RoLoRA across\nLLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain\nof 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks\ncompared to LoRA baseline. We further demonstrate its effectiveness on Large\nMultimodal Models (LLaVA-1.5-7B). Codes are available at\nhttps://github.com/HuangOwen/RoLoRA\n","authors":["Xijie Huang","Zechun Liu","Shih-Yang Liu","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.08044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08039v1","updated":"2024-07-10T20:37:42Z","published":"2024-07-10T20:37:42Z","title":"Knowledge Overshadowing Causes Amalgamated Hallucination in Large\n  Language Models","summary":"  Hallucination is often regarded as a major impediment for using large\nlanguage models (LLMs), especially for knowledge-intensive tasks. Even when the\ntraining corpus consists solely of true statements, language models still\ngenerate hallucinations in the form of amalgamations of multiple facts. We coin\nthis phenomenon as ``knowledge overshadowing'': when we query knowledge from a\nlanguage model with multiple conditions, some conditions overshadow others,\nleading to hallucinated outputs. This phenomenon partially stems from training\ndata imbalance, which we verify on both pretrained models and fine-tuned\nmodels, over a wide range of LM model families and sizes.From a theoretical\npoint of view, knowledge overshadowing can be interpreted as\nover-generalization of the dominant conditions (patterns). We show that the\nhallucination rate grows with both the imbalance ratio (between the popular and\nunpopular condition) and the length of dominant condition description,\nconsistent with our derived generalization bound. Finally, we propose to\nutilize overshadowing conditions as a signal to catch hallucination before it\nis produced, along with a training-free self-contrastive decoding method to\nalleviate hallucination during inference. Our proposed approach showcases up to\n82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control,\nwith different models and datasets.\n","authors":["Yuji Zhang","Sha Li","Jiateng Liu","Pengfei Yu","Yi R. Fung","Jing Li","Manling Li","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2407.08039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08035v1","updated":"2024-07-10T20:32:50Z","published":"2024-07-10T20:32:50Z","title":"FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in\n  Domain-specific Scenarios","summary":"  Large Language Models (LLMs) have provided a new pathway for Named Entity\nRecognition (NER) tasks. Compared with fine-tuning, LLM-powered prompting\nmethods avoid the need for training, conserve substantial computational\nresources, and rely on minimal annotated data. Previous studies have achieved\ncomparable performance to fully supervised BERT-based fine-tuning approaches on\ngeneral NER benchmarks. However, none of the previous approaches has\ninvestigated the efficiency of LLM-based few-shot learning in domain-specific\nscenarios. To address this gap, we introduce FsPONER, a novel approach for\noptimizing few-shot prompts, and evaluate its performance on domain-specific\nNER datasets, with a focus on industrial manufacturing and maintenance, while\nusing multiple LLMs -- GPT-4-32K, GPT-3.5-Turbo, LLaMA 2-chat, and Vicuna.\nFsPONER consists of three few-shot selection methods based on random sampling,\nTF-IDF vectors, and a combination of both. We compare these methods with a\ngeneral-purpose GPT-NER method as the number of few-shot examples increases and\nevaluate their optimal NER performance against fine-tuned BERT and LLaMA\n2-chat. In the considered real-world scenarios with data scarcity, FsPONER with\nTF-IDF surpasses fine-tuned models by approximately 10% in F1 score.\n","authors":["Yongjian Tang","Rakebul Hasan","Thomas Runkler"],"pdf_url":"https://arxiv.org/pdf/2407.08035v1.pdf","comment":"accepted for publication at the 27th European Conference on\n  Artificial Intelligence (ECAI-2024)"},{"id":"http://arxiv.org/abs/2407.08029v1","updated":"2024-07-10T20:11:51Z","published":"2024-07-10T20:11:51Z","title":"A Critical Review of Causal Reasoning Benchmarks for Large Language\n  Models","summary":"  Numerous benchmarks aim to evaluate the capabilities of Large Language Models\n(LLMs) for causal inference and reasoning. However, many of them can likely be\nsolved through the retrieval of domain knowledge, questioning whether they\nachieve their purpose. In this review, we present a comprehensive overview of\nLLM benchmarks for causality. We highlight how recent benchmarks move towards a\nmore thorough definition of causal reasoning by incorporating interventional or\ncounterfactual reasoning. We derive a set of criteria that a useful benchmark\nor set of benchmarks should aim to satisfy. We hope this work will pave the way\ntowards a general framework for the assessment of causal understanding in LLMs\nand the design of novel benchmarks.\n","authors":["Linying Yang","Vik Shirvaikar","Oscar Clivio","Fabian Falck"],"pdf_url":"https://arxiv.org/pdf/2407.08029v1.pdf","comment":"AAAI 2024 Workshop on ''Are Large Language Models Simply Causal\n  Parrots?''"},{"id":"http://arxiv.org/abs/2407.08008v1","updated":"2024-07-10T19:30:16Z","published":"2024-07-10T19:30:16Z","title":"DS@GT eRisk 2024: Sentence Transformers for Social Media Risk Assessment","summary":"  We present working notes for DS@GT team in the eRisk 2024 for Tasks 1 and 3.\nWe propose a ranking system for Task 1 that predicts symptoms of depression\nbased on the Beck Depression Inventory (BDI-II) questionnaire using binary\nclassifiers trained on question relevancy as a proxy for ranking. We find that\nbinary classifiers are not well calibrated for ranking, and perform poorly\nduring evaluation. For Task 3, we use embeddings from BERT to predict the\nseverity of eating disorder symptoms based on user post history. We find that\nclassical machine learning models perform well on the task, and end up\ncompetitive with the baseline models. Representation of text data is crucial in\nboth tasks, and we find that sentence transformers are a powerful tool for\ndownstream modeling. Source code and models are available at\n\\url{https://github.com/dsgt-kaggle-clef/erisk-2024}.\n","authors":["David Guecha","Aaryan Potdar","Anthony Miyaguchi"],"pdf_url":"https://arxiv.org/pdf/2407.08008v1.pdf","comment":"Paper Submitted to CLEF 2024 CEUR-WS"},{"id":"http://arxiv.org/abs/2403.02178v2","updated":"2024-07-10T19:15:24Z","published":"2024-03-04T16:21:54Z","title":"Masked Thought: Simply Masking Partial Reasoning Steps Can Improve\n  Mathematical Reasoning Learning of Language Models","summary":"  In reasoning tasks, even a minor error can cascade into inaccurate results,\nleading to suboptimal performance of large language models in such domains.\nEarlier fine-tuning approaches sought to mitigate this by leveraging more\nprecise supervisory signals from human labeling, larger models, or\nself-sampling, although at a high cost. Conversely, we develop a method that\navoids external resources, relying instead on introducing perturbations to the\ninput. Our training approach randomly masks certain tokens within the chain of\nthought, a technique we found to be particularly effective for reasoning tasks.\nWhen applied to fine-tuning with GSM8K on Llama-2-7B, this method achieved a\n5\\% improvement in GSM8K accuracy and a 10\\% improvement in GSM-IC accuracy\nover standard supervised fine-tuning with a few codes modified. Furthermore, it\nis complementary to existing methods. When integrated with related explicit\ndata augmentation methods, it leads to improvements across five datasets of\nvarious augmentation methods, as well as two different base models. We further\ninvestigate the mechanisms behind this improvement through case studies and\nquantitative analysis, suggesting that our approach may provide superior\nsupport for the model in capturing long-distance dependencies, especially those\nrelated to questions. This enhancement could deepen understanding of the\npremises in questions and prior steps. Our code is available at Github.\n","authors":["Changyu Chen","Xiting Wang","Ting-En Lin","Ang Lv","Yuchuan Wu","Xin Gao","Ji-Rong Wen","Rui Yan","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.02178v2.pdf","comment":"Accepted by ACL 2024"},{"id":"http://arxiv.org/abs/2407.08001v1","updated":"2024-07-10T19:13:37Z","published":"2024-07-10T19:13:37Z","title":"Automated Neural Patent Landscaping in the Small Data Regime","summary":"  Patent landscaping is the process of identifying all patents related to a\nparticular technological area, and is important for assessing various aspects\nof the intellectual property context. Traditionally, constructing patent\nlandscapes is intensely laborious and expensive, and the rapid expansion of\npatenting activity in recent decades has driven an increasing need for\nefficient and effective automated patent landscaping approaches. In particular,\nit is critical that we be able to construct patent landscapes using a minimal\nnumber of labeled examples, as labeling patents for a narrow technology area\nrequires highly specialized (and hence expensive) technical knowledge. We\npresent an automated neural patent landscaping system that demonstrates\nsignificantly improved performance on difficult examples (0.69 $F_1$ on 'hard'\nexamples, versus 0.6 for previously reported systems), and also significant\nimprovements with much less training data (overall 0.75 $F_1$ on as few as 24\nexamples). Furthermore, in evaluating such automated landscaping systems,\nacquiring good data is challenge; we demonstrate a higher-quality training data\ngeneration procedure by merging Abood and Feltenberger's (2018)\n\"seed/anti-seed\" approach with active learning to collect difficult labeled\nexamples near the decision boundary. Using this procedure we created a new\ndataset of labeled AI patents for training and testing. As in prior work we\ncompare our approach with a number of baseline systems, and we release our code\nand data for others to build upon.\n","authors":["Tisa Islam Erana","Mark A. Finlayson"],"pdf_url":"https://arxiv.org/pdf/2407.08001v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.09403v2","updated":"2024-07-10T18:09:56Z","published":"2024-06-13T17:59:31Z","title":"Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal\n  Language Models","summary":"  Humans draw to facilitate reasoning: we draw auxiliary lines when solving\ngeometry problems; we mark and circle when reasoning on maps; we use sketches\nto amplify our ideas and relieve our limited-capacity working memory. However,\nsuch actions are missing in current multimodal language models (LMs). Current\nchain-of-thought and tool-use paradigms only use text as intermediate reasoning\nsteps. In this work, we introduce Sketchpad, a framework that gives multimodal\nLMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts\nplanning and reasoning according to the visual artifacts it has drawn.\nDifferent from prior work, which uses text-to-image models to enable LMs to\ndraw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is\ncloser to human sketching and better facilitates reasoning. Sketchpad can also\nuse specialist vision models during the sketching process (e.g., draw bounding\nboxes with object detection models, draw masks with segmentation models), to\nfurther enhance visual perception and reasoning. We experiment with a wide\nrange of math tasks (including geometry, functions, graphs, and chess) and\ncomplex visual reasoning tasks. Sketchpad substantially improves performance on\nall tasks over strong base models with no sketching, yielding an average gain\nof 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a\nnew state of the art on all tasks, including V*Bench (80.3%), BLINK spatial\nreasoning (83.9%), and visual correspondence (80.8%). All codes and data are in\nhttps://visualsketchpad.github.io/.\n","authors":["Yushi Hu","Weijia Shi","Xingyu Fu","Dan Roth","Mari Ostendorf","Luke Zettlemoyer","Noah A Smith","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2406.09403v2.pdf","comment":"Project and codes url: https://visualsketchpad.github.io/"},{"id":"http://arxiv.org/abs/2407.07950v1","updated":"2024-07-10T18:00:05Z","published":"2024-07-10T18:00:05Z","title":"Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM\n  Reliance","summary":"  The reconfiguration of human-LM interactions from simple sentence completions\nto complex, multi-domain, humanlike engagements necessitates new methodologies\nto understand how humans choose to rely on LMs. In our work, we contend that\nreliance is influenced by numerous factors within the interactional context of\na generation, a departure from prior work that used verbalized confidence\n(e.g., \"I'm certain the answer is...\") as the key determinant of reliance.\nHere, we introduce Rel-A.I., an in situ, system-level evaluation approach to\nmeasure human reliance on LM-generated epistemic markers (e.g., \"I think\nit's..\", \"Undoubtedly it's...\"). Using this methodology, we measure reliance\nrates in three emergent human-LM interaction settings: long-term interactions,\nanthropomorphic generations, and variable subject matter. Our findings reveal\nthat reliance is not solely based on verbalized confidence but is significantly\naffected by other features of the interaction context. Prior interactions,\nanthropomorphic cues, and subject domain all contribute to reliance\nvariability. An expression such as, \"I'm pretty sure it's...\", can vary up to\n20% in reliance frequency depending on its interactional context. Our work\nunderscores the importance of context in understanding human reliance and\noffers future designers and researchers with a methodology to conduct such\nmeasurements.\n","authors":["Kaitlyn Zhou","Jena D. Hwang","Xiang Ren","Nouha Dziri","Dan Jurafsky","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2407.07950v1.pdf","comment":"Preprint"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.07895v1","updated":"2024-07-10T17:59:43Z","published":"2024-07-10T17:59:43Z","title":"LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large\n  Multimodal Models","summary":"  Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT\n","authors":["Feng Li","Renrui Zhang","Hao Zhang","Yuanhan Zhang","Bo Li","Wei Li","Zejun Ma","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2407.07895v1.pdf","comment":"Project Page:\n  https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/"},{"id":"http://arxiv.org/abs/2406.06609v2","updated":"2024-07-10T17:58:14Z","published":"2024-06-06T18:52:28Z","title":"Mitigating Bias in Dataset Distillation","summary":"  Dataset Distillation has emerged as a technique for compressing large\ndatasets into smaller synthetic counterparts, facilitating downstream training\ntasks. In this paper, we study the impact of bias inside the original dataset\non the performance of dataset distillation. With a comprehensive empirical\nevaluation on canonical datasets with color, corruption and background biases,\nwe found that color and background biases in the original dataset will be\namplified through the distillation process, resulting in a notable decline in\nthe performance of models trained on the distilled dataset, while corruption\nbias is suppressed through the distillation process. To reduce bias\namplification in dataset distillation, we introduce a simple yet highly\neffective approach based on a sample reweighting scheme utilizing kernel\ndensity estimation. Empirical results on multiple real-world and synthetic\ndatasets demonstrate the effectiveness of the proposed method. Notably, on\nCMNIST with 5% bias-conflict ratio and IPC 50, our method achieves 91.5% test\naccuracy compared to 23.8% from vanilla DM, boosting the performance by 67.7%,\nwhereas applying state-of-the-art debiasing method on the same dataset only\nachieves 53.7% accuracy. Our findings highlight the importance of addressing\nbiases in dataset distillation and provide a promising avenue to address bias\namplification in the process.\n","authors":["Justin Cui","Ruochen Wang","Yuanhao Xiong","Cho-Jui Hsieh"],"pdf_url":"https://arxiv.org/pdf/2406.06609v2.pdf","comment":"ICML"},{"id":"http://arxiv.org/abs/2407.07889v1","updated":"2024-07-10T17:57:04Z","published":"2024-07-10T17:57:04Z","title":"AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic\n  Manipulation","summary":"  Predictive models are a crucial component of many robotic systems. Yet,\nconstructing accurate predictive models for a variety of deformable objects,\nespecially those with unknown physical properties, remains a significant\nchallenge. This paper introduces AdaptiGraph, a learning-based dynamics\nmodeling approach that enables robots to predict, adapt to, and control a wide\narray of challenging deformable materials with unknown physical properties.\nAdaptiGraph leverages the highly flexible graph-based neural dynamics (GBND)\nframework, which represents material bits as particles and employs a graph\nneural network (GNN) to predict particle motion. Its key innovation is a\nunified physical property-conditioned GBND model capable of predicting the\nmotions of diverse materials with varying physical properties without\nretraining. Upon encountering new materials during online deployment,\nAdaptiGraph utilizes a physical property optimization process for a few-shot\nadaptation of the model, enhancing its fit to the observed interaction data.\nThe adapted models can precisely simulate the dynamics and predict the motion\nof various deformable materials, such as ropes, granular media, rigid boxes,\nand cloth, while adapting to different physical properties, including\nstiffness, granular size, and center of pressure. On prediction and\nmanipulation tasks involving a diverse set of real-world deformable objects,\nour method exhibits superior prediction accuracy and task proficiency over\nnon-material-conditioned and non-adaptive models. The project page is available\nat https://robopil.github.io/adaptigraph/ .\n","authors":["Kaifeng Zhang","Baoyu Li","Kris Hauser","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2407.07889v1.pdf","comment":"Project page: https://robopil.github.io/adaptigraph/"},{"id":"http://arxiv.org/abs/2407.07875v1","updated":"2024-07-10T17:41:10Z","published":"2024-07-10T17:41:10Z","title":"Generative Image as Action Models","summary":"  Image-generation diffusion models have been fine-tuned to unlock new\ncapabilities such as image-editing and novel view synthesis. Can we similarly\nunlock image-generation models for visuomotor control? We present GENIMA, a\nbehavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'\nas targets on RGB images. These images are fed into a controller that maps the\nvisual targets into a sequence of joint-positions. We study GENIMA on 25\nRLBench and 9 real-world manipulation tasks. We find that, by lifting actions\ninto image-space, internet pre-trained diffusion models can generate policies\nthat outperform state-of-the-art visuomotor approaches, especially in\nrobustness to scene perturbations and generalizing to novel objects. Our method\nis also competitive with 3D agents, despite lacking priors such as depth,\nkeypoints, or motion-planners.\n","authors":["Mohit Shridhar","Yat Long Lo","Stephen James"],"pdf_url":"https://arxiv.org/pdf/2407.07875v1.pdf","comment":"Project website, code, checkpoints: https://genima-robot.github.io/"},{"id":"http://arxiv.org/abs/2407.06508v2","updated":"2024-07-10T17:38:45Z","published":"2024-07-09T02:33:13Z","title":"A Clinical Benchmark of Public Self-Supervised Pathology Foundation\n  Models","summary":"  The use of self-supervised learning (SSL) to train pathology foundation\nmodels has increased substantially in the past few years. Notably, several\nmodels trained on large quantities of clinical data have been made publicly\navailable in recent months. This will significantly enhance scientific research\nin computational pathology and help bridge the gap between research and\nclinical deployment. With the increase in availability of public foundation\nmodels of different sizes, trained using different algorithms on different\ndatasets, it becomes important to establish a benchmark to compare the\nperformance of such models on a variety of clinically relevant tasks spanning\nmultiple organs and diseases. In this work, we present a collection of\npathology datasets comprising clinical slides associated with clinically\nrelevant endpoints including cancer diagnoses and a variety of biomarkers\ngenerated during standard hospital operation from two medical centers. We\nleverage these datasets to systematically assess the performance of public\npathology foundation models and provide insights into best practices for\ntraining new foundation models and selecting appropriate pretrained models.\n","authors":["Gabriele Campanella","Shengjia Chen","Ruchika Verma","Jennifer Zeng","Aryeh Stock","Matt Croken","Brandon Veremis","Abdulkadir Elmas","Kuan-lin Huang","Ricky Kwan","Jane Houldsworth","Adam J. Schoenfeld","Chad Vanderbilt"],"pdf_url":"https://arxiv.org/pdf/2407.06508v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2310.07033"},{"id":"http://arxiv.org/abs/2310.05615v2","updated":"2024-07-10T17:37:48Z","published":"2023-10-09T11:08:34Z","title":"Adaptive Multi-head Contrastive Learning","summary":"  In contrastive learning, two views of an original image, generated by\ndifferent augmentations, are considered a positive pair, and their similarity\nis required to be high. Similarly, two views of distinct images form a negative\npair, with encouraged low similarity. Typically, a single similarity measure,\nprovided by a lone projection head, evaluates positive and negative sample\npairs. However, due to diverse augmentation strategies and varying intra-sample\nsimilarity, views from the same image may not always be similar. Additionally,\nowing to inter-sample similarity, views from different images may be more akin\nthan those from the same image. Consequently, enforcing high similarity for\npositive pairs and low similarity for negative pairs may be unattainable, and\nin some cases, such enforcement could detrimentally impact performance. To\naddress this challenge, we propose using multiple projection heads, each\nproducing a distinct set of features. Our pre-training loss function emerges\nfrom a solution to the maximum likelihood estimation over head-wise posterior\ndistributions of positive samples given observations. This loss incorporates\nthe similarity measure over positive and negative pairs, each re-weighted by an\nindividual adaptive temperature, regulated to prevent ill solutions. Our\napproach, Adaptive Multi-Head Contrastive Learning (AMCL), can be applied to\nand experimentally enhances several popular contrastive learning methods such\nas SimCLR, MoCo, and Barlow Twins. The improvement remains consistent across\nvarious backbones and linear probing epochs, and becomes more significant when\nemploying multiple augmentation methods.\n","authors":["Lei Wang","Piotr Koniusz","Tom Gedeon","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.05615v2.pdf","comment":"Accepted at the 18th European Conference on Computer Vision (ECCV\n  2024)"},{"id":"http://arxiv.org/abs/2404.09349v2","updated":"2024-07-10T17:32:29Z","published":"2024-04-14T20:14:38Z","title":"Adversarial Robustness Limits via Scaling-Law and Human-Alignment\n  Studies","summary":"  This paper revisits the simple, long-studied, yet still unsolved problem of\nmaking image classifiers robust to imperceptible perturbations. Taking CIFAR10\nas an example, SOTA clean accuracy is about $100$%, but SOTA robustness to\n$\\ell_{\\infty}$-norm bounded perturbations barely exceeds $70$%. To understand\nthis gap, we analyze how model size, dataset size, and synthetic data quality\naffect robustness by developing the first scaling laws for adversarial\ntraining. Our scaling laws reveal inefficiencies in prior art and provide\nactionable feedback to advance the field. For instance, we discovered that SOTA\nmethods diverge notably from compute-optimal setups, using excess compute for\ntheir level of robustness. Leveraging a compute-efficient setup, we surpass the\nprior SOTA with $20$% ($70$%) fewer training (inference) FLOPs. We trained\nvarious compute-efficient models, with our best achieving $74$% AutoAttack\naccuracy ($+3$% gain). However, our scaling laws also predict robustness slowly\ngrows then plateaus at $90$%: dwarfing our new SOTA by scaling is impractical,\nand perfect robustness is impossible. To better understand this predicted\nlimit, we carry out a small-scale human evaluation on the AutoAttack data that\nfools our top-performing model. Concerningly, we estimate that human\nperformance also plateaus near $90$%, which we show to be attributable to\n$\\ell_{\\infty}$-constrained attacks' generation of invalid images not\nconsistent with their original labels. Having characterized limiting\nroadblocks, we outline promising paths for future research.\n","authors":["Brian R. Bartoldson","James Diffenderfer","Konstantinos Parasyris","Bhavya Kailkhura"],"pdf_url":"https://arxiv.org/pdf/2404.09349v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2407.07868v1","updated":"2024-07-10T17:32:05Z","published":"2024-07-10T17:32:05Z","title":"Green Screen Augmentation Enables Scene Generalisation in Robotic\n  Manipulation","summary":"  Generalising vision-based manipulation policies to novel environments remains\na challenging area with limited exploration. Current practices involve\ncollecting data in one location, training imitation learning or reinforcement\nlearning policies with this data, and deploying the policy in the same\nlocation. However, this approach lacks scalability as it necessitates data\ncollection in multiple locations for each task. This paper proposes a novel\napproach where data is collected in a location predominantly featuring green\nscreens. We introduce Green-screen Augmentation (GreenAug), employing a chroma\nkey algorithm to overlay background textures onto a green screen. Through\nextensive real-world empirical studies with over 850 training demonstrations\nand 8.2k evaluation episodes, we demonstrate that GreenAug surpasses no\naugmentation, standard computer vision augmentation, and prior generative\naugmentation methods in performance. While no algorithmic novelties are\nclaimed, our paper advocates for a fundamental shift in data collection\npractices. We propose that real-world demonstrations in future research should\nutilise green screens, followed by the application of GreenAug. We believe\nGreenAug unlocks policy generalisation to visually distinct novel locations,\naddressing the current scene generalisation limitations in robot learning.\n","authors":["Eugene Teoh","Sumit Patidar","Xiao Ma","Stephen James"],"pdf_url":"https://arxiv.org/pdf/2407.07868v1.pdf","comment":"Project website: https://greenaug.github.io/"},{"id":"http://arxiv.org/abs/2312.02249v2","updated":"2024-07-10T17:26:21Z","published":"2023-12-04T17:27:24Z","title":"Recursive Visual Programming","summary":"  Visual Programming (VP) has emerged as a powerful framework for Visual\nQuestion Answering (VQA). By generating and executing bespoke code for each\nquestion, these methods demonstrate impressive compositional and reasoning\ncapabilities, especially in few-shot and zero-shot scenarios. However, existing\nVP methods generate all code in a single function, resulting in code that is\nsuboptimal in terms of both accuracy and interpretability. Inspired by human\ncoding practices, we propose Recursive Visual Programming (RVP), which\nsimplifies generated routines, provides more efficient problem solving, and can\nmanage more complex data structures. RVP is inspired by human coding practices\nand approaches VQA tasks with an iterative recursive code generation approach,\nallowing decomposition of complicated problems into smaller parts. Notably, RVP\nis capable of dynamic type assignment, i.e., as the system recursively\ngenerates a new piece of code, it autonomously determines the appropriate\nreturn type and crafts the requisite code to generate that output. We show\nRVP's efficacy through extensive experiments on benchmarks including VSR, COVR,\nGQA, and NextQA, underscoring the value of adopting human-like recursive and\nmodular programming techniques for solving VQA tasks through coding.\n","authors":["Jiaxin Ge","Sanjay Subramanian","Baifeng Shi","Roei Herzig","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2312.02249v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07860v1","updated":"2024-07-10T17:23:33Z","published":"2024-07-10T17:23:33Z","title":"Controlling Space and Time with Diffusion Models","summary":"  We present 4DiM, a cascaded diffusion model for 4D novel view synthesis\n(NVS), conditioned on one or more images of a general scene, and a set of\ncamera poses and timestamps. To overcome challenges due to limited availability\nof 4D training data, we advocate joint training on 3D (with camera pose), 4D\n(pose+time) and video (time but no pose) data and propose a new architecture\nthat enables the same. We further advocate the calibration of SfM posed data\nusing monocular metric depth estimators for metric scale camera control. For\nmodel evaluation, we introduce new metrics to enrich and overcome shortcomings\nof current evaluation schemes, demonstrating state-of-the-art results in both\nfidelity and pose control compared to existing diffusion models for 3D NVS,\nwhile at the same time adding the ability to handle temporal dynamics. 4DiM is\nalso used for improved panorama stitching, pose-conditioned video to video\ntranslation, and several other tasks. For an overview see\nhttps://4d-diffusion.github.io\n","authors":["Daniel Watson","Saurabh Saxena","Lala Li","Andrea Tagliasacchi","David J. Fleet"],"pdf_url":"https://arxiv.org/pdf/2407.07860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07053v2","updated":"2024-07-10T17:17:15Z","published":"2024-07-09T17:18:27Z","title":"Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning\n  Instruction Using Language Model","summary":"  Although most current large multimodal models (LMMs) can already understand\nphotos of natural scenes and portraits, their understanding of abstract images,\ne.g., charts, maps, or layouts, and visual reasoning capabilities remains quite\nrudimentary. They often struggle with simple daily tasks, such as reading time\nfrom a clock, understanding a flowchart, or planning a route using a road map.\nIn light of this, we design a multi-modal self-instruct, utilizing large\nlanguage models and their code capabilities to synthesize massive abstract\nimages and visual reasoning instructions across daily scenarios. Our strategy\neffortlessly creates a multimodal benchmark with 11,193 instructions for eight\nvisual scenarios: charts, tables, simulated maps, dashboards, flowcharts,\nrelation graphs, floor plans, and visual puzzles. \\textbf{This benchmark,\nconstructed with simple lines and geometric elements, exposes the shortcomings\nof most advanced LMMs} like Claude-3.5-Sonnet and GPT-4o in abstract image\nunderstanding, spatial relations reasoning, and visual element induction.\nBesides, to verify the quality of our synthetic data, we fine-tune an LMM using\n62,476 synthetic chart, table and road map instructions. The results\ndemonstrate improved chart understanding and map navigation performance, and\nalso demonstrate potential benefits for other visual reasoning tasks. Our code\nis available at: \\url{https://github.com/zwq2018/Multi-modal-Self-instruct}.\n","authors":["Wenqi Zhang","Zhenglin Cheng","Yuanyu He","Mengna Wang","Yongliang Shen","Zeqi Tan","Guiyang Hou","Mingqian He","Yanna Ma","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2407.07053v2.pdf","comment":"code: https://github.com/zwq2018/Multi-modal-Self-instruct dataset:\n  https://huggingface.co/datasets/zwq2018/Multi-modal-Self-instruct\n  Leaderboard: https://multi-modal-self-instruct.github.io/"},{"id":"http://arxiv.org/abs/2407.07853v1","updated":"2024-07-10T17:14:54Z","published":"2024-07-10T17:14:54Z","title":"Progressive Growing of Patch Size: Resource-Efficient Curriculum\n  Learning for Dense Prediction Tasks","summary":"  In this work, we introduce Progressive Growing of Patch Size, a\nresource-efficient implicit curriculum learning approach for dense prediction\ntasks. Our curriculum approach is defined by growing the patch size during\nmodel training, which gradually increases the task's difficulty. We integrated\nour curriculum into the nnU-Net framework and evaluated the methodology on all\n10 tasks of the Medical Segmentation Decathlon. With our approach, we are able\nto substantially reduce runtime, computational costs, and CO$_{2}$ emissions of\nnetwork training compared to classical constant patch size training. In our\nexperiments, the curriculum approach resulted in improved convergence. We are\nable to outperform standard nnU-Net training, which is trained with constant\npatch size, in terms of Dice Score on 7 out of 10 MSD tasks while only spending\nroughly 50\\% of the original training runtime. To the best of our knowledge,\nour Progressive Growing of Patch Size is the first successful employment of a\nsample-length curriculum in the form of patch size in the field of computer\nvision. Our code is publicly available at \\url{https://github.com}.\n","authors":["Stefan M. Fischer","Lina Felsner","Richard Osuala","Johannes Kiechle","Daniel M. Lang","Jan C. Peeken","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2407.07853v1.pdf","comment":"Accepted at MICCAI2024"},{"id":"http://arxiv.org/abs/2407.07844v1","updated":"2024-07-10T17:05:49Z","published":"2024-07-10T17:05:49Z","title":"OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective\n  Fusion","summary":"  Open-vocabulary detection is a challenging task due to the requirement of\ndetecting objects based on class names, including those not encountered during\ntraining. Existing methods have shown strong zero-shot detection capabilities\nthrough pre-training on diverse large-scale datasets. However, these approaches\nstill face two primary challenges: (i) how to universally integrate diverse\ndata sources for end-to-end training, and (ii) how to effectively leverage the\nlanguage-aware capability for region-level cross-modality understanding. To\naddress these challenges, we propose a novel unified open-vocabulary detection\nmethod called OV-DINO, which pre-trains on diverse large-scale datasets with\nlanguage-aware selective fusion in a unified framework. Specifically, we\nintroduce a Unified Data Integration (UniDI) pipeline to enable end-to-end\ntraining and eliminate noise from pseudo-label generation by unifying different\ndata sources into detection-centric data. In addition, we propose a\nLanguage-Aware Selective Fusion (LASF) module to enable the language-aware\nability of the model through a language-aware query selection and fusion\nprocess. We evaluate the performance of the proposed OV-DINO on popular\nopen-vocabulary detection benchmark datasets, achieving state-of-the-art\nresults with an AP of 50.6\\% on the COCO dataset and 40.0\\% on the LVIS dataset\nin a zero-shot manner, demonstrating its strong generalization ability.\nFurthermore, the fine-tuned OV-DINO on COCO achieves 58.4\\% AP, outperforming\nmany existing methods with the same backbone. The code for OV-DINO will be\navailable at\n\\href{https://github.com/wanghao9610/OV-DINO}{https://github.com/wanghao9610/OV-DINO}.\n","authors":["Hao Wang","Pengzhen Ren","Zequn Jie","Xiao Dong","Chengjian Feng","Yinlong Qian","Lin Ma","Dongmei Jiang","Yaowei Wang","Xiangyuan Lan","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2407.07844v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2402.11354v2","updated":"2024-07-10T17:05:43Z","published":"2024-02-17T18:08:37Z","title":"Probabilistic Routing for Graph-Based Approximate Nearest Neighbor\n  Search","summary":"  Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a\npivotal challenge in the field of machine learning. In recent years,\ngraph-based methods have emerged as the superior approach to ANNS, establishing\na new state of the art. Although various optimizations for graph-based ANNS\nhave been introduced, they predominantly rely on heuristic methods that lack\nformal theoretical backing. This paper aims to enhance routing within\ngraph-based ANNS by introducing a method that offers a probabilistic guarantee\nwhen exploring a node's neighbors in the graph. We formulate the problem as\nprobabilistic routing and develop two baseline strategies by incorporating\nlocality-sensitive techniques. Subsequently, we introduce PEOs, a novel\napproach that efficiently identifies which neighbors in the graph should be\nconsidered for exact distance calculation, thus significantly improving\nefficiency in practice. Our experiments demonstrate that equipping PEOs can\nincrease throughput on commonly utilized graph indexes (HNSW and NSSG) by a\nfactor of 1.6 to 2.5, and its efficiency consistently outperforms the\nleading-edge routing technique by 1.1 to 1.4 times.\n","authors":["Kejing Lu","Chuan Xiao","Yoshiharu Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2402.11354v2.pdf","comment":"Source code is available at https://github.com/ICML2024-code/PEOs"},{"id":"http://arxiv.org/abs/2407.07842v1","updated":"2024-07-10T17:02:42Z","published":"2024-07-10T17:02:42Z","title":"Study on Aspect Ratio Variability toward Robustness of Vision\n  Transformer-based Vehicle Re-identification","summary":"  Vision Transformers (ViTs) have excelled in vehicle re-identification (ReID)\ntasks. However, non-square aspect ratios of image or video input might\nsignificantly affect the re-identification performance. To address this issue,\nwe propose a novel ViT-based ReID framework in this paper, which fuses models\ntrained on a variety of aspect ratios. Our main contributions are threefold:\n(i) We analyze aspect ratio performance on VeRi-776 and VehicleID datasets,\nguiding input settings based on aspect ratios of original images. (ii) We\nintroduce patch-wise mixup intra-image during ViT patchification (guided by\nspatial attention scores) and implement uneven stride for better object aspect\nratio matching. (iii) We propose a dynamic feature fusing ReID network,\nenhancing model robustness. Our ReID method achieves a significantly improved\nmean Average Precision (mAP) of 91.0\\% compared to the the closest\nstate-of-the-art (CAL) result of 80.9\\% on VehicleID dataset.\n","authors":["Mei Qiu","Lauren Christopher","Lingxi Li"],"pdf_url":"https://arxiv.org/pdf/2407.07842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07841v1","updated":"2024-07-10T17:00:57Z","published":"2024-07-10T17:00:57Z","title":"Benchmarking Embedding Aggregation Methods in Computational Pathology: A\n  Clinical Data Perspective","summary":"  Recent advances in artificial intelligence (AI), in particular\nself-supervised learning of foundation models (FMs), are revolutionizing\nmedical imaging and computational pathology (CPath). A constant challenge in\nthe analysis of digital Whole Slide Images (WSIs) is the problem of aggregating\ntens of thousands of tile-level image embeddings to a slide-level\nrepresentation. Due to the prevalent use of datasets created for genomic\nresearch, such as TCGA, for method development, the performance of these\ntechniques on diagnostic slides from clinical practice has been inadequately\nexplored. This study conducts a thorough benchmarking analysis of ten\nslide-level aggregation techniques across nine clinically relevant tasks,\nincluding diagnostic assessment, biomarker classification, and outcome\nprediction. The results yield following key insights: (1) Embeddings derived\nfrom domain-specific (histological images) FMs outperform those from generic\nImageNet-based models across aggregation methods. (2) Spatial-aware aggregators\nenhance the performance significantly when using ImageNet pre-trained models\nbut not when using FMs. (3) No single model excels in all tasks and\nspatially-aware models do not show general superiority as it would be expected.\nThese findings underscore the need for more adaptable and universally\napplicable aggregation techniques, guiding future research towards tools that\nbetter meet the evolving needs of clinical-AI in pathology. The code used in\nthis work is available at\n\\url{https://github.com/fuchs-lab-public/CPath_SABenchmark}.\n","authors":["Shengjia Chen","Gabriele Campanella","Abdulkadir Elmas","Aryeh Stock","Jennifer Zeng","Alexandros D. Polydorides","Adam J. Schoenfeld","Kuan-lin Huang","Jane Houldsworth","Chad Vanderbilt","Thomas J. Fuchs"],"pdf_url":"https://arxiv.org/pdf/2407.07841v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.07840v1","updated":"2024-07-10T17:00:29Z","published":"2024-07-10T17:00:29Z","title":"Decompose and Compare Consistency: Measuring VLMs' Answer Reliability\n  via Task-Decomposition Consistency Comparison","summary":"  Despite tremendous advancements, current state-of-the-art Vision-Language\nModels (VLMs) are still far from perfect. They tend to hallucinate and may\ngenerate biased responses. In such circumstances, having a way to assess the\nreliability of a given response generated by a VLM is quite useful. Existing\nmethods, such as estimating uncertainty using answer likelihoods or\nprompt-based confidence generation, often suffer from overconfidence. Other\nmethods use self-consistency comparison but are affected by confirmation\nbiases. To alleviate these, we propose \\textbf{De}compose and \\textbf{C}ompare\n\\textbf{C}onsistency (\\texttt{DeCC}) for reliability measurement. By comparing\nthe consistency between the direct answer generated using the VLM's internal\nreasoning process, and the indirect answers obtained by decomposing the\nquestion into sub-questions and reasoning over the sub-answers produced by the\nVLM, \\texttt{DeCC} measures the reliability of VLM's direct answer. Experiments\nacross six vision-language tasks with three VLMs show \\texttt{DeCC}'s\nreliability estimation achieves better correlation with task accuracy compared\nto the existing methods.\n","authors":["Qian Yang","Weixiang Yan","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.07840v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.07835v1","updated":"2024-07-10T16:55:01Z","published":"2024-07-10T16:55:01Z","title":"RoBus: A Multimodal Dataset for Controllable Road Networks and Building\n  Layouts Generation","summary":"  Automated 3D city generation, focusing on road networks and building layouts,\nis in high demand for applications in urban design, multimedia games and\nautonomous driving simulations. The surge of generative AI facilitates\ndesigning city layouts based on deep learning models. However, the lack of\nhigh-quality datasets and benchmarks hinders the progress of these data-driven\nmethods in generating road networks and building layouts. Furthermore, few\nstudies consider urban characteristics, which generally take graphics as\nanalysis objects and are crucial for practical applications, to control the\ngenerative process. To alleviate these problems, we introduce a multimodal\ndataset with accompanying evaluation metrics for controllable generation of\nRoad networks and Building layouts (RoBus), which is the first and largest\nopen-source dataset in city generation so far. RoBus dataset is formatted as\nimages, graphics and texts, with $72,400$ paired samples that cover around\n$80,000km^2$ globally. We analyze the RoBus dataset statistically and validate\nthe effectiveness against existing road networks and building layouts\ngeneration methods. Additionally, we design new baselines that incorporate\nurban characteristics, such as road orientation and building density, in the\nprocess of generating road networks and building layouts using the RoBus\ndataset, enhancing the practicality of automated urban design. The RoBus\ndataset and related codes are published at\nhttps://github.com/tourlics/RoBus_Dataset.\n","authors":["Tao Li","Ruihang Li","Huangnan Zheng","Shanding Ye","Shijian Li","Zhijie Pan"],"pdf_url":"https://arxiv.org/pdf/2407.07835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07829v1","updated":"2024-07-10T16:51:32Z","published":"2024-07-10T16:51:32Z","title":"Disentangled Representation Learning through Geometry Preservation with\n  the Gromov-Monge Gap","summary":"  Learning disentangled representations in an unsupervised manner is a\nfundamental challenge in machine learning. Solving it may unlock other\nproblems, such as generalization, interpretability, or fairness. While\nremarkably difficult to solve in general, recent works have shown that\ndisentanglement is provably achievable under additional assumptions that can\nleverage geometrical constraints, such as local isometry. To use these\ninsights, we propose a novel perspective on disentangled representation\nlearning built on quadratic optimal transport. Specifically, we formulate the\nproblem in the Gromov-Monge setting, which seeks isometric mappings between\ndistributions supported on different spaces. We propose the Gromov-Monge-Gap\n(GMG), a regularizer that quantifies the geometry-preservation of an arbitrary\npush-forward map between two distributions supported on different spaces. We\ndemonstrate the effectiveness of GMG regularization for disentanglement on four\nstandard benchmarks. Moreover, we show that geometry preservation can even\nencourage unsupervised disentanglement without the standard reconstruction\nobjective - making the underlying model decoder-free, and promising a more\npractically viable and scalable perspective on unsupervised disentanglement.\n","authors":["ThÃ©o Uscidda","Luca Eyring","Karsten Roth","Fabian Theis","Zeynep Akata","Marco Cuturi"],"pdf_url":"https://arxiv.org/pdf/2407.07829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07825v1","updated":"2024-07-10T16:49:23Z","published":"2024-07-10T16:49:23Z","title":"RT-LA-VocE: Real-Time Low-SNR Audio-Visual Speech Enhancement","summary":"  In this paper, we aim to generate clean speech frame by frame from a live\nvideo stream and a noisy audio stream without relying on future inputs. To this\nend, we propose RT-LA-VocE, which completely re-designs every component of\nLA-VocE, a state-of-the-art non-causal audio-visual speech enhancement model,\nto perform causal real-time inference with a 40ms input frame. We do so by\ndevising new visual and audio encoders that rely solely on past frames,\nreplacing the Transformer encoder with the Emformer, and designing a new causal\nneural vocoder C-HiFi-GAN. On the popular AVSpeech dataset, we show that our\nalgorithm achieves state-of-the-art results in all real-time scenarios. More\nimportantly, each component is carefully tuned to minimize the algorithm\nlatency to the theoretical minimum (40ms) while maintaining a low end-to-end\nprocessing latency of 28.15ms per frame, enabling real-time frame-by-frame\nenhancement with minimal delay.\n","authors":["Honglie Chen","Rodrigo Mira","Stavros Petridis","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2407.07825v1.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2407.07816v1","updated":"2024-07-10T16:40:44Z","published":"2024-07-10T16:40:44Z","title":"A Survey on Deep Stereo Matching in the Twenties","summary":"  Stereo matching is close to hitting a half-century of history, yet witnessed\na rapid evolution in the last decade thanks to deep learning. While previous\nsurveys in the late 2010s covered the first stage of this revolution, the last\nfive years of research brought further ground-breaking advancements to the\nfield. This paper aims to fill this gap in a two-fold manner: first, we offer\nan in-depth examination of the latest developments in deep stereo matching,\nfocusing on the pioneering architectural designs and groundbreaking paradigms\nthat have redefined the field in the 2020s; second, we present a thorough\nanalysis of the critical challenges that have emerged alongside these advances,\nproviding a comprehensive taxonomy of these issues and exploring the\nstate-of-the-art techniques proposed to address them. By reviewing both the\narchitectural innovations and the key challenges, we offer a holistic view of\ndeep stereo matching and highlight the specific areas that require further\ninvestigation. To accompany this survey, we maintain a regularly updated\nproject page that catalogs papers on deep stereo matching in our\nAwesome-Deep-Stereo-Matching\n(https://github.com/fabiotosi92/Awesome-Deep-Stereo-Matching) repository.\n","authors":["Fabio Tosi","Luca Bartolomei","Matteo Poggi"],"pdf_url":"https://arxiv.org/pdf/2407.07816v1.pdf","comment":"Extended version of CVPR 2024 Tutorial \"Deep Stereo Matching in the\n  Twenties\" (https://sites.google.com/view/stereo-twenties)"},{"id":"http://arxiv.org/abs/2407.07090v2","updated":"2024-07-10T16:38:35Z","published":"2024-07-09T17:59:30Z","title":"3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes","summary":"  Particle-based representations of radiance fields such as 3D Gaussian\nSplatting have found great success for reconstructing and re-rendering of\ncomplex scenes. Most existing methods render particles via rasterization,\nprojecting them to screen space tiles for processing in a sorted order. This\nwork instead considers ray tracing the particles, building a bounding volume\nhierarchy and casting a ray for each pixel using high-performance GPU ray\ntracing hardware. To efficiently handle large numbers of semi-transparent\nparticles, we describe a specialized rendering algorithm which encapsulates\nparticles with bounding meshes to leverage fast ray-triangle intersections, and\nshades batches of intersections in depth-order. The benefits of ray tracing are\nwell-known in computer graphics: processing incoherent rays for secondary\nlighting effects such as shadows and reflections, rendering from\nhighly-distorted cameras common in robotics, stochastically sampling rays, and\nmore. With our renderer, this flexibility comes at little cost compared to\nrasterization. Experiments demonstrate the speed and accuracy of our approach,\nas well as several applications in computer graphics and vision. We further\npropose related improvements to the basic Gaussian representation, including a\nsimple use of generalized kernel functions which significantly reduces particle\nhit counts.\n","authors":["Nicolas Moenne-Loccoz","Ashkan Mirzaei","Or Perel","Riccardo de Lutio","Janick Martinez Esturo","Gavriel State","Sanja Fidler","Nicholas Sharp","Zan Gojcic"],"pdf_url":"https://arxiv.org/pdf/2407.07090v2.pdf","comment":"Project page: https://gaussiantracer.github.io/"},{"id":"http://arxiv.org/abs/2407.07805v1","updated":"2024-07-10T16:25:26Z","published":"2024-07-10T16:25:26Z","title":"SUMix: Mixup with Semantic and Uncertain Information","summary":"  Mixup data augmentation approaches have been applied for various tasks of\ndeep learning to improve the generalization ability of deep neural networks.\nSome existing approaches CutMix, SaliencyMix, etc. randomly replace a patch in\none image with patches from another to generate the mixed image. Similarly, the\ncorresponding labels are linearly combined by a fixed ratio $\\lambda$ by l. The\nobjects in two images may be overlapped during the mixing process, so some\nsemantic information is corrupted in the mixed samples. In this case, the mixed\nimage does not match the mixed label information. Besides, such a label may\nmislead the deep learning model training, which results in poor performance. To\nsolve this problem, we proposed a novel approach named SUMix to learn the\nmixing ratio as well as the uncertainty for the mixed samples during the\ntraining process. First, we design a learnable similarity function to compute\nan accurate mix ratio. Second, an approach is investigated as a regularized\nterm to model the uncertainty of the mixed samples. We conduct experiments on\nfive image benchmarks, and extensive experimental results imply that our method\nis capable of improving the performance of classifiers with different\ncutting-based mixup approaches. The source code is available at\nhttps://github.com/JinXins/SUMix.\n","authors":["Huafeng Qin","Xin Jin","Hongyu Zhu","Hongchao Liao","MounÃ®m A. El-Yacoubi","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2407.07805v1.pdf","comment":"Accepted by ECCV2024 [Camera Ready] (16 pages, 5 figures) with the\n  source code at https://github.com/JinXins/SUMix"},{"id":"http://arxiv.org/abs/2211.15597v2","updated":"2024-07-10T16:16:46Z","published":"2022-11-28T17:50:19Z","title":"Lightning Fast Video Anomaly Detection via Adversarial Knowledge\n  Distillation","summary":"  We propose a very fast frame-level model for anomaly detection in video,\nwhich learns to detect anomalies by distilling knowledge from multiple highly\naccurate object-level teacher models. To improve the fidelity of our student,\nwe distill the low-resolution anomaly maps of the teachers by jointly applying\nstandard and adversarial distillation, introducing an adversarial discriminator\nfor each teacher to distinguish between target and generated anomaly maps. We\nconduct experiments on three benchmarks (Avenue, ShanghaiTech, UCSD Ped2),\nshowing that our method is over 7 times faster than the fastest competing\nmethod, and between 28 and 62 times faster than object-centric models, while\nobtaining comparable results to recent methods. Our evaluation also indicates\nthat our model achieves the best trade-off between speed and accuracy, due to\nits previously unheard-of speed of 1480 FPS. In addition, we carry out a\ncomprehensive ablation study to justify our architectural design choices. Our\ncode is freely available at: https://github.com/ristea/fast-aed.\n","authors":["Florinel-Alin Croitoru","Nicolae-Catalin Ristea","Dana Dascalescu","Radu Tudor Ionescu","Fahad Shahbaz Khan","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2211.15597v2.pdf","comment":"Accepted in Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2401.06122v2","updated":"2024-07-10T16:08:08Z","published":"2024-01-11T18:57:17Z","title":"Manipulating Feature Visualizations with Gradient Slingshots","summary":"  Deep Neural Networks (DNNs) are capable of learning complex and versatile\nrepresentations, however, the semantic nature of the learned concepts remains\nunknown. A common method used to explain the concepts learned by DNNs is\nFeature Visualization (FV), which generates a synthetic input signal that\nmaximally activates a particular neuron in the network. In this paper, we\ninvestigate the vulnerability of this approach to adversarial model\nmanipulations and introduce a novel method for manipulating FV without\nsignificantly impacting the model's decision-making process. The key\ndistinction of our proposed approach is that it does not alter the model\narchitecture. We evaluate the effectiveness of our method on several neural\nnetwork models and demonstrate its capabilities to hide the functionality of\narbitrarily chosen neurons by masking the original explanations of neurons with\nchosen target explanations during model auditing.\n","authors":["Dilyara Bareeva","Marina M. -C. HÃ¶hne","Alexander Warnecke","Lukas Pirch","Klaus-Robert MÃ¼ller","Konrad Rieck","Kirill Bykov"],"pdf_url":"https://arxiv.org/pdf/2401.06122v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07789v1","updated":"2024-07-10T16:06:32Z","published":"2024-07-10T16:06:32Z","title":"Raising the Ceiling: Conflict-Free Local Feature Matching with Dynamic\n  View Switching","summary":"  Current feature matching methods prioritize improving modeling capabilities\nto better align outputs with ground-truth matches, which are the theoretical\nupper bound on matching results, metaphorically depicted as the \"ceiling\".\nHowever, these enhancements fail to address the underlying issues that directly\nhinder ground-truth matches, including the scarcity of matchable points in\nsmall scale images, matching conflicts in dense methods, and the\nkeypoint-repeatability reliance in sparse methods. We propose a novel feature\nmatching method named RCM, which Raises the Ceiling of Matching from three\naspects. 1) RCM introduces a dynamic view switching mechanism to address the\nscarcity of matchable points in source images by strategically switching image\npairs. 2) RCM proposes a conflict-free coarse matching module, addressing\nmatching conflicts in the target image through a many-to-one matching strategy.\n3) By integrating the semi-sparse paradigm and the coarse-to-fine architecture,\nRCM preserves the benefits of both high efficiency and global search,\nmitigating the reliance on keypoint repeatability. As a result, RCM enables\nmore matchable points in the source image to be matched in an exhaustive and\nconflict-free manner in the target image, leading to a substantial 260%\nincrease in ground-truth matches. Comprehensive experiments show that RCM\nexhibits remarkable performance and efficiency in comparison to\nstate-of-the-art methods.\n","authors":["Xiaoyong Lu","Songlin Du"],"pdf_url":"https://arxiv.org/pdf/2407.07789v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07788v1","updated":"2024-07-10T16:04:18Z","published":"2024-07-10T16:04:18Z","title":"BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark","summary":"  We introduce BiGym, a new benchmark and learning environment for mobile\nbi-manual demo-driven robotic manipulation. BiGym features 40 diverse tasks set\nin home environments, ranging from simple target reaching to complex kitchen\ncleaning. To capture the real-world performance accurately, we provide\nhuman-collected demonstrations for each task, reflecting the diverse modalities\nfound in real-world robot trajectories. BiGym supports a variety of\nobservations, including proprioceptive data and visual inputs such as RGB, and\ndepth from 3 camera views. To validate the usability of BiGym, we thoroughly\nbenchmark the state-of-the-art imitation learning algorithms and demo-driven\nreinforcement learning algorithms within the environment and discuss the future\nopportunities.\n","authors":["Nikita Chernyadev","Nicholas Backshall","Xiao Ma","Yunfan Lu","Younggyo Seo","Stephen James"],"pdf_url":"https://arxiv.org/pdf/2407.07788v1.pdf","comment":"Project webpage: https://chernyadev.github.io/bigym/"},{"id":"http://arxiv.org/abs/2407.07787v1","updated":"2024-07-10T16:04:08Z","published":"2024-07-10T16:04:08Z","title":"Continuous Control with Coarse-to-fine Reinforcement Learning","summary":"  Despite recent advances in improving the sample-efficiency of reinforcement\nlearning (RL) algorithms, designing an RL algorithm that can be practically\ndeployed in real-world environments remains a challenge. In this paper, we\npresent Coarse-to-fine Reinforcement Learning (CRL), a framework that trains RL\nagents to zoom-into a continuous action space in a coarse-to-fine manner,\nenabling the use of stable, sample-efficient value-based RL algorithms for\nfine-grained continuous control tasks. Our key idea is to train agents that\noutput actions by iterating the procedure of (i) discretizing the continuous\naction space into multiple intervals and (ii) selecting the interval with the\nhighest Q-value to further discretize at the next level. We then introduce a\nconcrete, value-based algorithm within the CRL framework called Coarse-to-fine\nQ-Network (CQN). Our experiments demonstrate that CQN significantly outperforms\nRL and behavior cloning baselines on 20 sparsely-rewarded RLBench manipulation\ntasks with a modest number of environment interactions and expert\ndemonstrations. We also show that CQN robustly learns to solve real-world\nmanipulation tasks within a few minutes of online training.\n","authors":["Younggyo Seo","Jafar UruÃ§","Stephen James"],"pdf_url":"https://arxiv.org/pdf/2407.07787v1.pdf","comment":"Project webpage: https://younggyo.me/cqn/"},{"id":"http://arxiv.org/abs/2407.07780v1","updated":"2024-07-10T15:56:24Z","published":"2024-07-10T15:56:24Z","title":"Cross Domain Object Detection via Multi-Granularity Confidence Alignment\n  based Mean Teacher","summary":"  Cross domain object detection learns an object detector for an unlabeled\ntarget domain by transferring knowledge from an annotated source domain.\nPromising results have been achieved via Mean Teacher, however, pseudo labeling\nwhich is the bottleneck of mutual learning remains to be further explored. In\nthis study, we find that confidence misalignment of the predictions, including\ncategory-level overconfidence, instance-level task confidence inconsistency,\nand image-level confidence misfocusing, leading to the injection of noisy\npseudo label in the training process, will bring suboptimal performance on the\ntarget domain. To tackle this issue, we present a novel general framework\ntermed Multi-Granularity Confidence Alignment Mean Teacher (MGCAMT) for cross\ndomain object detection, which alleviates confidence misalignment across\ncategory-, instance-, and image-levels simultaneously to obtain high quality\npseudo supervision for better teacher-student learning. Specifically, to align\nconfidence with accuracy at category level, we propose Classification\nConfidence Alignment (CCA) to model category uncertainty based on Evidential\nDeep Learning (EDL) and filter out the category incorrect labels via an\nuncertainty-aware selection strategy. Furthermore, to mitigate the\ninstance-level misalignment between classification and localization, we design\nTask Confidence Alignment (TCA) to enhance the interaction between the two task\nbranches and allow each classification feature to adaptively locate the optimal\nfeature for the regression. Finally, we develop imagery Focusing Confidence\nAlignment (FCA) adopting another way of pseudo label learning, i.e., we use the\noriginal outputs from the Mean Teacher network for supervised learning without\nlabel assignment to concentrate on holistic information in the target image.\nThese three procedures benefit from each other from a cooperative learning\nperspective.\n","authors":["Jiangming Chen","Li Liu","Wanxia Deng","Zhen Liu","Yu Liu","Yingmei Wei","Yongxiang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.07780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14428v3","updated":"2024-07-10T15:54:11Z","published":"2023-05-23T18:00:22Z","title":"Prompting Language-Informed Distribution for Compositional Zero-Shot\n  Learning","summary":"  Compositional zero-shot learning (CZSL) task aims to recognize unseen\ncompositional visual concepts, e.g., sliced tomatoes, where the model is\nlearned only from the seen compositions, e.g., sliced potatoes and red\ntomatoes. Thanks to the prompt tuning on large pre-trained visual language\nmodels such as CLIP, recent literature shows impressively better CZSL\nperformance than traditional vision-based methods. However, the key aspects\nthat impact the generalization to unseen compositions, including the diversity\nand informativeness of class context, and the entanglement between visual\nprimitives, i.e., state and object, are not properly addressed in existing\nCLIP-based CZSL literature. In this paper, we propose a model by prompting the\nlanguage-informed distribution, aka., PLID, for the CZSL task. Specifically,\nthe PLID leverages pre-trained large language models (LLM) to (i) formulate the\nlanguage-informed class distributions which are diverse and informative, and\n(ii) enhance the compositionality of the class embedding. Moreover, a\nvisual-language primitive decomposition (VLPD) module is proposed to\ndynamically fuse the classification decisions from the compositional and the\nprimitive space. Orthogonal to the existing literature of soft, hard, or\ndistributional prompts, our method advocates prompting the LLM-supported class\ndistributions, leading to a better zero-shot generalization. Experimental\nresults on MIT-States, UT-Zappos, and C-GQA datasets show the superior\nperformance of the PLID to the prior arts. Our code and models are released:\nhttps://github.com/Cogito2012/PLID.\n","authors":["Wentao Bao","Lichang Chen","Heng Huang","Yu Kong"],"pdf_url":"https://arxiv.org/pdf/2305.14428v3.pdf","comment":"ECCV 2024 Accepted"},{"id":"http://arxiv.org/abs/2312.05286v3","updated":"2024-07-10T15:49:37Z","published":"2023-12-08T15:10:55Z","title":"Bridging Synthetic and Real Worlds for Pre-training Scene Text Detectors","summary":"  Existing scene text detection methods typically rely on extensive real data\nfor training. Due to the lack of annotated real images, recent works have\nattempted to exploit large-scale labeled synthetic data (LSD) for pre-training\ntext detectors. However, a synth-to-real domain gap emerges, further limiting\nthe performance of text detectors. Differently, in this work, we propose\nFreeReal, a real-domain-aligned pre-training paradigm that enables the\ncomplementary strengths of both LSD and unlabeled real data (URD).\nSpecifically, to bridge real and synthetic worlds for pre-training, a\nglyph-based mixing mechanism (GlyphMix) is tailored for text images.GlyphMix\ndelineates the character structures of synthetic images and embeds them as\ngraffiti-like units onto real images. Without introducing real domain drift,\nGlyphMix freely yields real-world images with annotations derived from\nsynthetic labels. Furthermore, when given free fine-grained synthetic labels,\nGlyphMix can effectively bridge the linguistic domain gap stemming from\nEnglish-dominated LSD to URD in various languages. Without bells and whistles,\nFreeReal achieves average gains of 1.97%, 3.90%, 3.85%, and 4.56% in improving\nthe performance of FCENet, PSENet, PANet, and DBNet methods, respectively,\nconsistently outperforming previous pre-training methods by a substantial\nmargin across four public datasets. Code is available at\nhttps://github.com/SJTU-DeepVisionLab/FreeReal.\n","authors":["Tongkun Guan","Wei Shen","Xue Yang","Xuehui Wang","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2312.05286v3.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.07771v1","updated":"2024-07-10T15:46:32Z","published":"2024-07-10T15:46:32Z","title":"Multi-task Prompt Words Learning for Social Media Content Generation","summary":"  The rapid development of the Internet has profoundly changed human life.\nHumans are increasingly expressing themselves and interacting with others on\nsocial media platforms. However, although artificial intelligence technology\nhas been widely used in many aspects of life, its application in social media\ncontent creation is still blank. To solve this problem, we propose a new prompt\nword generation framework based on multi-modal information fusion, which\ncombines multiple tasks including topic classification, sentiment analysis,\nscene recognition and keyword extraction to generate more comprehensive prompt\nwords. Subsequently, we use a template containing a set of prompt words to\nguide ChatGPT to generate high-quality tweets. Furthermore, in the absence of\neffective and objective evaluation criteria in the field of content generation,\nwe use the ChatGPT tool to evaluate the results generated by the algorithm,\nmaking large-scale evaluation of content generation algorithms possible.\nEvaluation results on extensive content generation demonstrate that our cue\nword generation framework generates higher quality content compared to manual\nmethods and other cueing techniques, while topic classification, sentiment\nanalysis, and scene recognition significantly enhance content clarity and its\nconsistency with the image.\n","authors":["Haochen Xue","Chong Zhang","Chengzhi Liu","Fangyu Wu","Xiaobo Jin"],"pdf_url":"https://arxiv.org/pdf/2407.07771v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.07764v1","updated":"2024-07-10T15:42:58Z","published":"2024-07-10T15:42:58Z","title":"PosFormer: Recognizing Complex Handwritten Mathematical Expression with\n  Position Forest Transformer","summary":"  Handwritten Mathematical Expression Recognition (HMER) has wide applications\nin human-machine interaction scenarios, such as digitized education and\nautomated offices. Recently, sequence-based models with encoder-decoder\narchitectures have been commonly adopted to address this task by directly\npredicting LaTeX sequences of expression images. However, these methods only\nimplicitly learn the syntax rules provided by LaTeX, which may fail to describe\nthe position and hierarchical relationship between symbols due to complex\nstructural relations and diverse handwriting styles. To overcome this\nchallenge, we propose a position forest transformer (PosFormer) for HMER, which\njointly optimizes two tasks: expression recognition and position recognition,\nto explicitly enable position-aware symbol feature representation learning.\nSpecifically, we first design a position forest that models the mathematical\nexpression as a forest structure and parses the relative position relationships\nbetween symbols. Without requiring extra annotations, each symbol is assigned a\nposition identifier in the forest to denote its relative spatial position.\nSecond, we propose an implicit attention correction module to accurately\ncapture attention for HMER in the sequence-based decoder architecture.\nExtensive experiments validate the superiority of PosFormer, which consistently\noutperforms the state-of-the-art methods 2.03%/1.22%/2.00%, 1.83%, and 4.62%\ngains on the single-line CROHME 2014/2016/2019, multi-line M2E, and complex MNE\ndatasets, respectively, with no additional latency or computational cost. Code\nis available at https://github.com/SJTU-DeepVisionLab/PosFormer.\n","authors":["Tongkun Guan","Chengyu Lin","Wei Shen","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2407.07764v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.07763v1","updated":"2024-07-10T15:39:47Z","published":"2024-07-10T15:39:47Z","title":"S&D Messenger: Exchanging Semantic and Domain Knowledge for Generic\n  Semi-Supervised Medical Image Segmentation","summary":"  Semi-supervised medical image segmentation (SSMIS) has emerged as a promising\nsolution to tackle the challenges of time-consuming manual labeling in the\nmedical field. However, in practical scenarios, there are often domain\nvariations within the datasets, leading to derivative scenarios like\nsemi-supervised medical domain generalization (Semi-MDG) and unsupervised\nmedical domain adaptation (UMDA). In this paper, we aim to develop a generic\nframework that masters all three tasks. We notice a critical shared challenge\nacross three scenarios: the explicit semantic knowledge for segmentation\nperformance and rich domain knowledge for generalizability exclusively exist in\nthe labeled set and unlabeled set respectively. Such discrepancy hinders\nexisting methods from effectively comprehending both types of knowledge under\nsemi-supervised settings. To tackle this challenge, we develop a Semantic &\nDomain Knowledge Messenger (S&D Messenger) which facilitates direct knowledge\ndelivery between the labeled and unlabeled set, and thus allowing the model to\ncomprehend both of them in each individual learning flow. Equipped with our S&D\nMessenger, a naive pseudo-labeling method can achieve huge improvement on six\nbenchmark datasets for SSMIS (+7.5%), UMDA (+5.6%), and Semi-MDG tasks\n(+1.14%), compared with state-of-the-art methods designed for specific tasks.\n","authors":["Qixiang Zhang","Haonan Wang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2407.07763v1.pdf","comment":"10 pages, under review of IEEE Transcations on Medical Imaging"},{"id":"http://arxiv.org/abs/2407.07760v1","updated":"2024-07-10T15:36:00Z","published":"2024-07-10T15:36:00Z","title":"Learning Spatial-Semantic Features for Robust Video Object Segmentation","summary":"  Tracking and segmenting multiple similar objects with complex or separate\nparts in long-term videos is inherently challenging due to the ambiguity of\ntarget parts and identity confusion caused by occlusion, background clutter,\nand long-term variations. In this paper, we propose a robust video object\nsegmentation framework equipped with spatial-semantic features and\ndiscriminative object queries to address the above issues. Specifically, we\nconstruct a spatial-semantic network comprising a semantic embedding block and\nspatial dependencies modeling block to associate the pretrained ViT features\nwith global semantic features and local spatial features, providing a\ncomprehensive target representation. In addition, we develop a masked\ncross-attention module to generate object queries that focus on the most\ndiscriminative parts of target objects during query propagation, alleviating\nnoise accumulation and ensuring effective long-term query propagation. The\nexperimental results show that the proposed method set a new state-of-the-art\nperformance on multiple datasets, including the DAVIS2017 test (89.1%),\nYoutubeVOS 2019 (88.5%), MOSE (75.1%), LVOS test (73.0%), and LVOS val (75.1%),\nwhich demonstrate the effectiveness and generalization capacity of the proposed\nmethod. We will make all source code and trained models publicly available.\n","authors":["Xin Li","Deshui Miao","Zhenyu He","Yaowei Wang","Huchuan Lu","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2407.07760v1.pdf","comment":"Winner solution of the VOTS2024 Challenge"},{"id":"http://arxiv.org/abs/2407.01523v2","updated":"2024-07-10T15:31:09Z","published":"2024-07-01T17:59:26Z","title":"MMLongBench-Doc: Benchmarking Long-context Document Understanding with\n  Visualizations","summary":"  Understanding documents with rich layouts and multi-modal components is a\nlong-standing and practical task. Recent Large Vision-Language Models (LVLMs)\nhave made remarkable strides in various tasks, particularly in single-page\ndocument understanding (DU). However, their abilities on long-context DU remain\nan open problem. This work presents MMLongBench-Doc, a long-context,\nmulti-modal benchmark comprising 1,062 expert-annotated questions. Distinct\nfrom previous datasets, it is constructed upon 130 lengthy PDF-formatted\ndocuments with an average of 49.4 pages and 20,971 textual tokens. Towards\ncomprehensive evaluation, answers to these questions rely on pieces of evidence\nfrom (1) different sources (text, image, chart, table, and layout structure)\nand (2) various locations (i.e. page number). Moreover, 33.2% of the questions\nare cross-page questions requiring evidence across multiple pages. 22.8% of the\nquestions are designed to be unanswerable for detecting potential\nhallucinations. Experiments on 14 LVLMs demonstrate that long-context DU\ngreatly challenges current models. Notably, the best-performing model, GPT-4o,\nachieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores\n31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse\nperformance than their LLM counterparts which are fed with lossy-parsed OCR\ndocuments. These results validate the necessity of future research toward more\ncapable long-context LVLMs. Project Page:\nhttps://mayubo2333.github.io/MMLongBench-Doc\n","authors":["Yubo Ma","Yuhang Zang","Liangyu Chen","Meiqi Chen","Yizhu Jiao","Xinze Li","Xinyuan Lu","Ziyu Liu","Yan Ma","Xiaoyi Dong","Pan Zhang","Liangming Pan","Yu-Gang Jiang","Jiaqi Wang","Yixin Cao","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2407.01523v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07755v1","updated":"2024-07-10T15:28:02Z","published":"2024-07-10T15:28:02Z","title":"Neural Geometry Processing via Spherical Neural Surfaces","summary":"  Neural surfaces (e.g., neural map encoding, deep implicits and neural\nradiance fields) have recently gained popularity because of their generic\nstructure (e.g., multi-layer perceptron) and easy integration with modern\nlearning-based setups. Traditionally, we have a rich toolbox of geometry\nprocessing algorithms designed for polygonal meshes to analyze and operate on\nsurface geometry. However, neural representations are typically discretized and\nconverted into a mesh, before applying any geometry processing algorithm. This\nis unsatisfactory and, as we demonstrate, unnecessary. In this work, we propose\na spherical neural surface representation (a spherical parametrization) for\ngenus-0 surfaces and demonstrate how to compute core geometric operators\ndirectly on this representation. Namely, we show how to construct the normals\nand the first and second fundamental forms of the surface, and how to compute\nthe surface gradient, surface divergence and Laplace Beltrami operator on\nscalar/vector fields defined on the surface. These operators, in turn, enable\nus to create geometry processing tools that act directly on the neural\nrepresentations without any unnecessary meshing. We demonstrate illustrative\napplications in (neural) spectral analysis, heat flow and mean curvature flow,\nand our method shows robustness to isometric shape variations. We both propose\ntheoretical formulations and validate their numerical estimates. By\nsystematically linking neural surface representations with classical geometry\nprocessing algorithms, we believe this work can become a key ingredient in\nenabling neural geometry processing.\n","authors":["Romy Williamson","Niloy J. Mitra"],"pdf_url":"https://arxiv.org/pdf/2407.07755v1.pdf","comment":"10 pages, 12 figures"},{"id":"http://arxiv.org/abs/2406.03002v2","updated":"2024-07-10T15:17:42Z","published":"2024-06-05T07:09:19Z","title":"Phy-Diff: Physics-guided Hourglass Diffusion Model for Diffusion MRI\n  Synthesis","summary":"  Diffusion MRI (dMRI) is an important neuroimaging technique with high\nacquisition costs. Deep learning approaches have been used to enhance dMRI and\npredict diffusion biomarkers through undersampled dMRI. To generate more\ncomprehensive raw dMRI, generative adversarial network based methods are\nproposed to include b-values and b-vectors as conditions, but they are limited\nby unstable training and less desirable diversity. The emerging diffusion model\n(DM) promises to improve generative performance. However, it remains\nchallenging to include essential information in conditioning DM for more\nrelevant generation, i.e., the physical principles of dMRI and white matter\ntract structures. In this study, we propose a physics-guided diffusion model to\ngenerate high-quality dMRI. Our model introduces the physical principles of\ndMRI in the noise evolution in the diffusion process and introduce a\nquery-based conditional mapping within the difussion model. In addition, to\nenhance the anatomical fine detials of the generation, we introduce the XTRACT\natlas as prior of white matter tracts by adopting an adapter technique. Our\nexperiment results show that our method outperforms other state-of-the-art\nmethods and has the potential to advance dMRI enhancement.\n","authors":["Juanhua Zhang","Ruodan Yan","Alessandro Perelli","Xi Chen","Chao Li"],"pdf_url":"https://arxiv.org/pdf/2406.03002v2.pdf","comment":"Accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.07740v1","updated":"2024-07-10T15:11:37Z","published":"2024-07-10T15:11:37Z","title":"LSM: A Comprehensive Metric for Assessing the Safety of Lane Detection\n  Systems in Autonomous Driving","summary":"  Comprehensive perception of the vehicle's environment and correct\ninterpretation of the environment are crucial for the safe operation of\nautonomous vehicles. The perception of surrounding objects is the main\ncomponent for further tasks such as trajectory planning. However, safe\ntrajectory planning requires not only object detection, but also the detection\nof drivable areas and lane corridors. While first approaches consider an\nadvanced safety evaluation of object detection, the evaluation of lane\ndetection still lacks sufficient safety metrics. Similar to the safety metrics\nfor object detection, additional factors such as the semantics of the scene\nwith road type and road width, the detection range as well as the potential\ncauses of missing detections, incorporated by vehicle speed, should be\nconsidered for the evaluation of lane detection. Therefore, we propose the Lane\nSafety Metric (LSM), which takes these factors into account and allows to\nevaluate the safety of lane detection systems by determining an easily\ninterpretable safety score. We evaluate our offline safety metric on various\nvirtual scenarios using different lane detection approaches and compare it with\nstate-of-the-art performance metrics.\n","authors":["JÃ¶rg Gamerdinger","Sven Teufel","Stephan Amann","Georg Volk","Oliver Bringmann"],"pdf_url":"https://arxiv.org/pdf/2407.07740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07735v1","updated":"2024-07-10T15:06:52Z","published":"2024-07-10T15:06:52Z","title":"Protecting NeRFs' Copyright via Plug-And-Play Watermarking Base Model","summary":"  Neural Radiance Fields (NeRFs) have become a key method for 3D scene\nrepresentation. With the rising prominence and influence of NeRF, safeguarding\nits intellectual property has become increasingly important. In this paper, we\npropose \\textbf{NeRFProtector}, which adopts a plug-and-play strategy to\nprotect NeRF's copyright during its creation. NeRFProtector utilizes a\npre-trained watermarking base model, enabling NeRF creators to embed binary\nmessages directly while creating their NeRF. Our plug-and-play property ensures\nNeRF creators can flexibly choose NeRF variants without excessive\nmodifications. Leveraging our newly designed progressive distillation, we\ndemonstrate performance on par with several leading-edge neural rendering\nmethods. Our project is available at:\n\\url{https://qsong2001.github.io/NeRFProtector}.\n","authors":["Qi Song","Ziyuan Luo","Ka Chun Cheung","Simon See","Renjie Wan"],"pdf_url":"https://arxiv.org/pdf/2407.07735v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2403.12445v2","updated":"2024-07-10T15:04:43Z","published":"2024-03-19T05:10:10Z","title":"Boosting Transferability in Vision-Language Attacks via Diversification\n  along the Intersection Region of Adversarial Trajectory","summary":"  Vision-language pre-training (VLP) models exhibit remarkable capabilities in\ncomprehending both images and text, yet they remain susceptible to multimodal\nadversarial examples (AEs).Strengthening attacks and uncovering\nvulnerabilities, especially common issues in VLP models (e.g., high\ntransferable AEs), can advance reliable and practical VLP models. A recent work\n(i.e., Set-level guidance attack) indicates that augmenting image-text pairs to\nincrease AE diversity along the optimization path enhances the transferability\nof adversarial examples significantly. However, this approach predominantly\nemphasizes diversity around the online adversarial examples (i.e., AEs in the\noptimization period), leading to the risk of overfitting the victim model and\naffecting the transferability.In this study, we posit that the diversity of\nadversarial examples towards the clean input and online AEs are both pivotal\nfor enhancing transferability across VLP models. Consequently, we propose using\ndiversification along the intersection region of adversarial trajectory to\nexpand the diversity of AEs.To fully leverage the interaction between\nmodalities, we introduce text-guided adversarial example selection during\noptimization. Furthermore, to further mitigate the potential overfitting, we\ndirect the adversarial text deviating from the last intersection region along\nthe optimization path, rather than adversarial images as in existing\nmethods.Extensive experiments affirm the effectiveness of our method in\nimproving transferability across various VLP models and downstream\nvision-and-language tasks.\n","authors":["Sensen Gao","Xiaojun Jia","Xuhong Ren","Ivor Tsang","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2403.12445v2.pdf","comment":"ECCV2024. Code is available at\n  https://github.com/SensenGao/VLPTransferAttack"},{"id":"http://arxiv.org/abs/2407.07726v1","updated":"2024-07-10T14:57:46Z","published":"2024-07-10T14:57:46Z","title":"PaliGemma: A versatile 3B VLM for transfer","summary":"  PaliGemma is an open Vision-Language Model (VLM) that is based on the\nSigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to\nbe a versatile and broadly knowledgeable base model that is effective to\ntransfer. It achieves strong performance on a wide variety of open-world tasks.\nWe evaluate PaliGemma on almost 40 diverse tasks including standard VLM\nbenchmarks, but also more specialized tasks such as remote-sensing and\nsegmentation.\n","authors":["Lucas Beyer","Andreas Steiner","AndrÃ© Susano Pinto","Alexander Kolesnikov","Xiao Wang","Daniel Salz","Maxim Neumann","Ibrahim Alabdulmohsin","Michael Tschannen","Emanuele Bugliarello","Thomas Unterthiner","Daniel Keysers","Skanda Koppula","Fangyu Liu","Adam Grycner","Alexey Gritsenko","Neil Houlsby","Manoj Kumar","Keran Rong","Julian Eisenschlos","Rishabh Kabra","Matthias Bauer","Matko BoÅ¡njak","Xi Chen","Matthias Minderer","Paul Voigtlaender","Ioana Bica","Ivana Balazevic","Joan Puigcerver","Pinelopi Papalampidi","Olivier Henaff","Xi Xiong","Radu Soricut","Jeremiah Harmsen","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2407.07726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07720v1","updated":"2024-07-10T14:53:37Z","published":"2024-07-10T14:53:37Z","title":"SvANet: A Scale-variant Attention-based Network for Small Medical Object\n  Segmentation","summary":"  Early detection and accurate diagnosis can predict the risk of malignant\ndisease transformation, thereby increasing the probability of effective\ntreatment. A mild syndrome with small infected regions is an ominous warning\nand is foremost in the early diagnosis of diseases. Deep learning algorithms,\nsuch as convolutional neural networks (CNNs), have been used to segment natural\nor medical objects, showing promising results. However, analyzing medical\nobjects of small areas in images remains a challenge due to information losses\nand compression defects caused by convolution and pooling operations in CNNs.\nThese losses and defects become increasingly significant as the network\ndeepens, particularly for small medical objects. To address these challenges,\nwe propose a novel scale-variant attention-based network (SvANet) for accurate\nsmall-scale object segmentation in medical images. The SvANet consists of Monte\nCarlo attention, scale-variant attention, and vision transformer, which\nincorporates cross-scale features and alleviates compression artifacts for\nenhancing the discrimination of small medical objects. Quantitative\nexperimental results demonstrate the superior performance of SvANet, achieving\n96.12%, 96.11%, 89.79%, 84.15%, 80.25%, 73.05%, and 72.58% in mean Dice\ncoefficient for segmenting kidney tumors, skin lesions, hepatic tumors, polyps,\nsurgical excision cells, retinal vasculatures, and sperms, which occupy less\nthan 1% of the image areas in KiTS23, ISIC 2018, ATLAS, PolypGen, TissueNet,\nFIVES, and SpermHealth datasets, respectively.\n","authors":["Wei Dai"],"pdf_url":"https://arxiv.org/pdf/2407.07720v1.pdf","comment":"14 pages, 9 figures, under review"},{"id":"http://arxiv.org/abs/2405.19201v2","updated":"2024-07-10T14:42:18Z","published":"2024-05-29T15:41:53Z","title":"Going beyond Compositions, DDPMs Can Produce Zero-Shot Interpolations","summary":"  Denoising Diffusion Probabilistic Models (DDPMs) exhibit remarkable\ncapabilities in image generation, with studies suggesting that they can\ngeneralize by composing latent factors learned from the training data. In this\nwork, we go further and study DDPMs trained on strictly separate subsets of the\ndata distribution with large gaps on the support of the latent factors. We show\nthat such a model can effectively generate images in the unexplored,\nintermediate regions of the distribution. For instance, when trained on clearly\nsmiling and non-smiling faces, we demonstrate a sampling procedure which can\ngenerate slightly smiling faces without reference images (zero-shot\ninterpolation). We replicate these findings for other attributes as well as\nother datasets. Our code is available at\nhttps://github.com/jdeschena/ddpm-zero-shot-interpolation.\n","authors":["Justin Deschenaux","Igor Krawczuk","Grigorios Chrysos","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2405.19201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17214v3","updated":"2024-07-10T14:25:08Z","published":"2024-02-27T05:10:59Z","title":"CharacterGen: Efficient 3D Character Generation from Single Images with\n  Multi-View Pose Canonicalization","summary":"  In the field of digital content creation, generating high-quality 3D\ncharacters from single images is challenging, especially given the complexities\nof various body poses and the issues of self-occlusion and pose ambiguity. In\nthis paper, we present CharacterGen, a framework developed to efficiently\ngenerate 3D characters. CharacterGen introduces a streamlined generation\npipeline along with an image-conditioned multi-view diffusion model. This model\neffectively calibrates input poses to a canonical form while retaining key\nattributes of the input image, thereby addressing the challenges posed by\ndiverse poses. A transformer-based, generalizable sparse-view reconstruction\nmodel is the other core component of our approach, facilitating the creation of\ndetailed 3D models from multi-view images. We also adopt a\ntexture-back-projection strategy to produce high-quality texture maps.\nAdditionally, we have curated a dataset of anime characters, rendered in\nmultiple poses and views, to train and evaluate our model. Our approach has\nbeen thoroughly evaluated through quantitative and qualitative experiments,\nshowing its proficiency in generating 3D characters with high-quality shapes\nand textures, ready for downstream applications such as rigging and animation.\n","authors":["Hao-Yang Peng","Jia-Peng Zhang","Meng-Hao Guo","Yan-Pei Cao","Shi-Min Hu"],"pdf_url":"https://arxiv.org/pdf/2402.17214v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13616v2","updated":"2024-07-10T14:06:16Z","published":"2023-11-22T06:49:44Z","title":"Online Video Quality Enhancement with Spatial-Temporal Look-up Tables","summary":"  Low latency rates are crucial for online video-based applications, such as\nvideo conferencing and cloud gaming, which make improving video quality in\nonline scenarios increasingly important. However, existing quality enhancement\nmethods are limited by slow inference speed and the requirement for temporal\ninformation contained in future frames, making it challenging to deploy them\ndirectly in online tasks. In this paper, we propose a novel method, STLVQE,\nspecifically designed to address the rarely studied online video quality\nenhancement (Online-VQE) problem. Our STLVQE designs a new VQE framework which\ncontains a Module-Agnostic Feature Extractor that greatly reduces the redundant\ncomputations and redesign the propagation, alignment, and enhancement module of\nthe network. A Spatial-Temporal Look-up Tables (STL) is proposed, which\nextracts spatial-temporal information in videos while saving substantial\ninference time. To the best of our knowledge, we are the first to exploit the\nLUT structure to extract temporal information in video tasks. Extensive\nexperiments on the MFQE 2.0 dataset demonstrate that our STLVQE achieves a\nsatisfactory performance-speed trade-off.\n","authors":["Zefan Qu","Xinyang Jiang","Yifan Yang","Dongsheng Li","Cairong Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.13616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07673v1","updated":"2024-07-10T14:00:19Z","published":"2024-07-10T14:00:19Z","title":"Towards Adaptive Pseudo-label Learning for Semi-Supervised Temporal\n  Action Localization","summary":"  Alleviating noisy pseudo labels remains a key challenge in Semi-Supervised\nTemporal Action Localization (SS-TAL). Existing methods often filter pseudo\nlabels based on strict conditions, but they typically assess classification and\nlocalization quality separately, leading to suboptimal pseudo-label ranking and\nselection. In particular, there might be inaccurate pseudo labels within\nselected positives, alongside reliable counterparts erroneously assigned to\nnegatives. To tackle these problems, we propose a novel Adaptive Pseudo-label\nLearning (APL) framework to facilitate better pseudo-label selection.\nSpecifically, to improve the ranking quality, Adaptive Label Quality Assessment\n(ALQA) is proposed to jointly learn classification confidence and localization\nreliability, followed by dynamically selecting pseudo labels based on the joint\nscore. Additionally, we propose an Instance-level Consistency Discriminator\n(ICD) for eliminating ambiguous positives and mining potential positives\nsimultaneously based on inter-instance intrinsic consistency, thereby leading\nto a more precise selection. We further introduce a general unsupervised\nAction-aware Contrastive Pre-training (ACP) to enhance the discrimination both\nwithin actions and between actions and backgrounds, which benefits SS-TAL.\nExtensive experiments on THUMOS14 and ActivityNet v1.3 demonstrate that our\nmethod achieves state-of-the-art performance under various semi-supervised\nsettings.\n","authors":["Feixiang Zhou","Bryan Williams","Hossein Rahmani"],"pdf_url":"https://arxiv.org/pdf/2407.07673v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07667v1","updated":"2024-07-10T13:46:08Z","published":"2024-07-10T13:46:08Z","title":"VEnhancer: Generative Space-Time Enhancement for Video Generation","summary":"  We present VEnhancer, a generative space-time enhancement framework that\nimproves the existing text-to-video results by adding more details in spatial\ndomain and synthetic detailed motion in temporal domain. Given a generated\nlow-quality video, our approach can increase its spatial and temporal\nresolution simultaneously with arbitrary up-sampling space and time scales\nthrough a unified video diffusion model. Furthermore, VEnhancer effectively\nremoves generated spatial artifacts and temporal flickering of generated\nvideos. To achieve this, basing on a pretrained video diffusion model, we train\na video ControlNet and inject it to the diffusion model as a condition on low\nframe-rate and low-resolution videos. To effectively train this video\nControlNet, we design space-time data augmentation as well as video-aware\nconditioning. Benefiting from the above designs, VEnhancer yields to be stable\nduring training and shares an elegant end-to-end training manner. Extensive\nexperiments show that VEnhancer surpasses existing state-of-the-art video\nsuper-resolution and space-time super-resolution methods in enhancing\nAI-generated videos. Moreover, with VEnhancer, exisiting open-source\nstate-of-the-art text-to-video method, VideoCrafter-2, reaches the top one in\nvideo generation benchmark -- VBench.\n","authors":["Jingwen He","Tianfan Xue","Dongyang Liu","Xinqi Lin","Peng Gao","Dahua Lin","Yu Qiao","Wanli Ouyang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.07667v1.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2311.09999v2","updated":"2024-07-10T13:44:42Z","published":"2023-11-16T16:23:11Z","title":"TransFusion -- A Transparency-Based Diffusion Model for Anomaly\n  Detection","summary":"  Surface anomaly detection is a vital component in manufacturing inspection.\nCurrent discriminative methods follow a two-stage architecture composed of a\nreconstructive network followed by a discriminative network that relies on the\nreconstruction output. Currently used reconstructive networks often produce\npoor reconstructions that either still contain anomalies or lack details in\nanomaly-free regions. Discriminative methods are robust to some reconstructive\nnetwork failures, suggesting that the discriminative network learns a strong\nnormal appearance signal that the reconstructive networks miss. We reformulate\nthe two-stage architecture into a single-stage iterative process that allows\nthe exchange of information between the reconstruction and localization. We\npropose a novel transparency-based diffusion process where the transparency of\nanomalous regions is progressively increased, restoring their normal appearance\naccurately while maintaining the appearance of anomaly-free regions using\nlocalization cues of previous steps. We implement the proposed process as\nTRANSparency DifFUSION (TransFusion), a novel discriminative anomaly detection\nmethod that achieves state-of-the-art performance on both the VisA and the\nMVTec AD datasets, with an image-level AUROC of 98.5% and 99.2%, respectively.\nCode: https://github.com/MaticFuc/ECCV_TransFusion\n","authors":["Matic FuÄka","Vitjan Zavrtanik","Danijel SkoÄaj"],"pdf_url":"https://arxiv.org/pdf/2311.09999v2.pdf","comment":"Accepted to ECCV2024"},{"id":"http://arxiv.org/abs/2407.07664v1","updated":"2024-07-10T13:44:19Z","published":"2024-07-10T13:44:19Z","title":"A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning\n  Geometry","summary":"  Hyperspherical Prototypical Learning (HPL) is a supervised approach to\nrepresentation learning that designs class prototypes on the unit hypersphere.\nThe prototypes bias the representations to class separation in a scale\ninvariant and known geometry. Previous approaches to HPL have either of the\nfollowing shortcomings: (i) they follow an unprincipled optimisation procedure;\nor (ii) they are theoretically sound, but are constrained to only one possible\nlatent dimension. In this paper, we address both shortcomings. To address (i),\nwe present a principled optimisation procedure whose solution we show is\noptimal. To address (ii), we construct well-separated prototypes in a wide\nrange of dimensions using linear block codes. Additionally, we give a full\ncharacterisation of the optimal prototype placement in terms of achievable and\nconverse bounds, showing that our proposed methods are near-optimal.\n","authors":["Martin LindstrÃ¶m","Borja RodrÃ­guez-GÃ¡lvez","Ragnar Thobaben","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2407.07664v1.pdf","comment":"14 pages: 9 of the main paper, 2 of references, and 3 of appendices.\n  To appear in the Proceedings of the Geometry-grounded Representation Learning\n  and Generative Modeling at the 41st International Conference on Machine\n  Learning, Vienna, Austria. Code is available at:\n  https://github.com/martinlindstrom/coding_theoretic_hpl"},{"id":"http://arxiv.org/abs/2407.07662v1","updated":"2024-07-10T13:43:47Z","published":"2024-07-10T13:43:47Z","title":"Mitigating Backdoor Attacks using Activation-Guided Model Editing","summary":"  Backdoor attacks compromise the integrity and reliability of machine learning\nmodels by embedding a hidden trigger during the training process, which can\nlater be activated to cause unintended misbehavior. We propose a novel backdoor\nmitigation approach via machine unlearning to counter such backdoor attacks.\nThe proposed method utilizes model activation of domain-equivalent unseen data\nto guide the editing of the model's weights. Unlike the previous\nunlearning-based mitigation methods, ours is computationally inexpensive and\nachieves state-of-the-art performance while only requiring a handful of unseen\nsamples for unlearning. In addition, we also point out that unlearning the\nbackdoor may cause the whole targeted class to be unlearned, thus introducing\nan additional repair step to preserve the model's utility after editing the\nmodel. Experiment results show that the proposed method is effective in\nunlearning the backdoor on different datasets and trigger patterns.\n","authors":["Felix Hsieh","Huy H. Nguyen","AprilPyone MaungMaung","Dmitrii Usynin","Isao Echizen"],"pdf_url":"https://arxiv.org/pdf/2407.07662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07660v1","updated":"2024-07-10T13:41:26Z","published":"2024-07-10T13:41:26Z","title":"Boosting Medical Image Synthesis via Registration-guided Consistency and\n  Disentanglement Learning","summary":"  Medical image synthesis remains challenging due to misalignment noise during\ntraining. Existing methods have attempted to address this challenge by\nincorporating a registration-guided module. However, these methods tend to\noverlook the task-specific constraints on the synthetic and registration\nmodules, which may cause the synthetic module to still generate spatially\naligned images with misaligned target images during training, regardless of the\nregistration module's function. Therefore, this paper proposes\nregistration-guided consistency and incorporates disentanglement learning for\nmedical image synthesis. The proposed registration-guided consistency\narchitecture fosters task-specificity within the synthetic and registration\nmodules by applying identical deformation fields before and after synthesis,\nwhile enforcing output consistency through an alignment loss. Moreover, the\nsynthetic module is designed to possess the capability of disentangling\nanatomical structures and specific styles across various modalities. An anatomy\nconsistency loss is introduced to further compel the synthetic module to\npreserve geometrical integrity within latent spaces. Experiments conducted on\nboth an in-house abdominal CECT-CT dataset and a publicly available pelvic\nMR-CT dataset have demonstrated the superiority of the proposed method.\n","authors":["Chuanpu Li","Zeli Chen","Yiwen Zhang","Liming Zhong","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2407.07660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03482v2","updated":"2024-07-10T13:27:20Z","published":"2024-07-03T20:10:55Z","title":"Domain-Aware Fine-Tuning of Foundation Models","summary":"  Foundation models (FMs) have revolutionized computer vision, enabling\neffective learning across different domains. However, their performance under\ndomain shift is yet underexplored. This paper investigates the zero-shot domain\nadaptation potential of FMs by comparing different backbone architectures and\nintroducing novel domain-aware components that leverage domain related textual\nembeddings. We propose domain adaptive normalization, termed as Domino, which\nexplicitly leverages domain embeddings during fine-tuning, thus making the\nmodel domain aware. Ultimately, Domino enables more robust computer vision\nmodels that can adapt effectively to various unseen domains.\n","authors":["Ugur Ali Kaplan","Margret Keuper","Anna Khoreva","Dan Zhang","Yumeng Li"],"pdf_url":"https://arxiv.org/pdf/2407.03482v2.pdf","comment":"Accepted at ICML 2024 Workshop on Foundation Models in the Wild"},{"id":"http://arxiv.org/abs/2403.11150v2","updated":"2024-07-10T13:26:48Z","published":"2024-03-17T09:01:02Z","title":"Training A Small Emotional Vision Language Model for Visual Art\n  Comprehension","summary":"  This paper develops small vision language models to understand visual art,\nwhich, given an art work, aims to identify its emotion category and explain\nthis prediction with natural language. While small models are computationally\nefficient, their capacity is much limited compared with large models. To break\nthis trade-off, this paper builds a small emotional vision language model\n(SEVLM) by emotion modeling and input-output feature alignment. On the one\nhand, based on valence-arousal-dominance (VAD) knowledge annotated by\npsychology experts, we introduce and fuse emotional features derived through\nVAD dictionary and a VAD head to align VAD vectors of predicted emotion\nexplanation and the ground truth. This allows the vision language model to\nbetter understand and generate emotional texts, compared with using traditional\ntext embeddings alone. On the other hand, we design a contrastive head to pull\nclose embeddings of the image, its emotion class, and explanation, which aligns\nmodel outputs and inputs. On two public affective explanation datasets, we show\nthat the proposed techniques consistently improve the visual art understanding\nperformance of baseline SEVLMs. Importantly, the proposed model can be trained\nand evaluated on a single RTX 2080 Ti while exhibiting very strong performance:\nit not only outperforms the state-of-the-art small models but is also\ncompetitive compared with LLaVA 7B after fine-tuning and GPT4(V). The code is\navailable at https://github.com/BetterZH/SEVLM-code.\n","authors":["Jing Zhang","Liang Zheng","Meng Wang","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2403.11150v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2407.07638v1","updated":"2024-07-10T13:19:31Z","published":"2024-07-10T13:19:31Z","title":"Tuning Vision-Language Models with Candidate Labels by Prompt Alignment","summary":"  Vision-language models (VLMs) can learn high-quality representations from a\nlarge-scale training dataset of image-text pairs. Prompt learning is a popular\napproach to fine-tuning VLM to adapt them to downstream tasks. Despite the\nsatisfying performance, a major limitation of prompt learning is the demand for\nlabelled data. In real-world scenarios, we may only obtain candidate labels\n(where the true label is included) instead of the true labels due to data\nprivacy or sensitivity issues. In this paper, we provide the first study on\nprompt learning with candidate labels for VLMs. We empirically demonstrate that\nprompt learning is more advantageous than other fine-tuning methods, for\nhandling candidate labels. Nonetheless, its performance drops when the label\nambiguity increases. In order to improve its robustness, we propose a simple\nyet effective framework that better leverages the prior knowledge of VLMs to\nguide the learning process with candidate labels. Specifically, our framework\ndisambiguates candidate labels by aligning the model output with the mixed\nclass posterior jointly predicted by both the learnable and the handcrafted\nprompt. Besides, our framework can be equipped with various off-the-shelf\ntraining objectives for learning with candidate labels to further improve their\nperformance. Extensive experiments demonstrate the effectiveness of our\nproposed framework.\n","authors":["Zhifang Zhang","Beibei Li"],"pdf_url":"https://arxiv.org/pdf/2407.07638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07633v1","updated":"2024-07-10T13:11:58Z","published":"2024-07-10T13:11:58Z","title":"Few-Shot Domain Adaptive Object Detection for Microscopic Images","summary":"  In recent years, numerous domain adaptive strategies have been proposed to\nhelp deep learning models overcome the challenges posed by domain shift.\nHowever, even unsupervised domain adaptive strategies still require a large\namount of target data. Medical imaging datasets are often characterized by\nclass imbalance and scarcity of labeled and unlabeled data. Few-shot domain\nadaptive object detection (FSDAOD) addresses the challenge of adapting object\ndetectors to target domains with limited labeled data. Existing works struggle\nwith randomly selected target domain images that may not accurately represent\nthe real population, resulting in overfitting to small validation sets and poor\ngeneralization to larger test sets. Medical datasets exhibit high class\nimbalance and background similarity, leading to increased false positives and\nlower mean Average Precision (map) in target domains. To overcome these\nchallenges, we propose a novel FSDAOD strategy for microscopic imaging. Our\ncontributions include a domain adaptive class balancing strategy for few-shot\nscenarios, multi-layer instance-level inter and intra-domain alignment to\nenhance similarity between class instances regardless of domain, and an\ninstance-level classification loss applied in the middle layers of the object\ndetector to enforce feature retention necessary for correct classification\nacross domains. Extensive experimental results with competitive baselines\ndemonstrate the effectiveness of our approach, achieving state-of-the-art\nresults on two public microscopic datasets. Code available at\nhttps://github.co/intelligentMachinesLab/few-shot-domain-adaptive-microscopy\n","authors":["Sumayya Inayat","Nimra Dilawar","Waqas Sultani","Mohsen Ali"],"pdf_url":"https://arxiv.org/pdf/2407.07633v1.pdf","comment":"Accepted to MICCAI 2024 main conference"},{"id":"http://arxiv.org/abs/2407.07627v1","updated":"2024-07-10T13:07:39Z","published":"2024-07-10T13:07:39Z","title":"Synthetic to Authentic: Transferring Realism to 3D Face Renderings for\n  Boosting Face Recognition","summary":"  In this paper, we investigate the potential of image-to-image translation\n(I2I) techniques for transferring realism to 3D-rendered facial images in the\ncontext of Face Recognition (FR) systems. The primary motivation for using\n3D-rendered facial images lies in their ability to circumvent the challenges\nassociated with collecting large real face datasets for training FR systems.\nThese images are generated entirely by 3D rendering engines, facilitating the\ngeneration of synthetic identities. However, it has been observed that FR\nsystems trained on such synthetic datasets underperform when compared to those\ntrained on real datasets, on various FR benchmarks. In this work, we\ndemonstrate that by transferring the realism to 3D-rendered images (i.e.,\nmaking the 3D-rendered images look more real), we can boost the performance of\nFR systems trained on these more photorealistic images. This improvement is\nevident when these systems are evaluated against FR benchmarks utilizing\nreal-world data, thereby paving new pathways for employing synthetic data in\nreal-world applications.\n","authors":["Parsa Rahimi","Behrooz Razeghi","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2407.07627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05546v3","updated":"2024-07-10T13:00:34Z","published":"2023-05-09T15:32:50Z","title":"ColonMapper: topological mapping and localization for colonoscopy","summary":"  We propose a topological mapping and localization system able to operate on\nreal human colonoscopies, despite significant shape and illumination changes.\nThe map is a graph where each node codes a colon location by a set of real\nimages, while edges represent traversability between nodes. For close-in-time\nimages, where scene changes are minor, place recognition can be successfully\nmanaged with the recent transformers-based local feature matching algorithms.\nHowever, under long-term changes -- such as different colonoscopies of the same\npatient -- feature-based matching fails. To address this, we train on real\ncolonoscopies a deep global descriptor achieving high recall with significant\nchanges in the scene. The addition of a Bayesian filter boosts the accuracy of\nlong-term place recognition, enabling relocalization in a previously built map.\nOur experiments show that ColonMapper is able to autonomously build a map and\nlocalize against it in two important use cases: localization within the same\ncolonoscopy or within different colonoscopies of the same patient. Code:\nhttps://github.com/jmorlana/ColonMapper.\n","authors":["Javier Morlana","Juan D. TardÃ³s","J. M. M. Montiel"],"pdf_url":"https://arxiv.org/pdf/2305.05546v3.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2407.07616v1","updated":"2024-07-10T12:54:51Z","published":"2024-07-10T12:54:51Z","title":"Satellite Image Time Series Semantic Change Detection: Novel\n  Architecture and Analysis of Domain Shift","summary":"  Satellite imagery plays a crucial role in monitoring changes happening on\nEarth's surface and aiding in climate analysis, ecosystem assessment, and\ndisaster response. In this paper, we tackle semantic change detection with\nsatellite image time series (SITS-SCD) which encompasses both change detection\nand semantic segmentation tasks. We propose a new architecture that improves\nover the state of the art, scales better with the number of parameters, and\nleverages long-term temporal information. However, for practical use cases,\nmodels need to adapt to spatial and temporal shifts, which remains a challenge.\nWe investigate the impact of temporal and spatial shifts separately on global,\nmulti-year SITS datasets using DynamicEarthNet and MUDS. We show that the\nspatial domain shift represents the most complex setting and that the impact of\ntemporal shift on performance is more pronounced on change detection than on\nsemantic segmentation, highlighting that it is a specific issue deserving\nfurther attention.\n","authors":["Elliot Vincent","Jean Ponce","Mathieu Aubry"],"pdf_url":"https://arxiv.org/pdf/2407.07616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07614v1","updated":"2024-07-10T12:52:49Z","published":"2024-07-10T12:52:49Z","title":"MARS: Mixture of Auto-Regressive Models for Fine-grained Text-to-image\n  Synthesis","summary":"  Auto-regressive models have made significant progress in the realm of\nlanguage generation, yet they do not perform on par with diffusion models in\nthe domain of image synthesis. In this work, we introduce MARS, a novel\nframework for T2I generation that incorporates a specially designed Semantic\nVision-Language Integration Expert (SemVIE). This innovative component\nintegrates pre-trained LLMs by independently processing linguistic and visual\ninformation, freezing the textual component while fine-tuning the visual\ncomponent. This methodology preserves the NLP capabilities of LLMs while\nimbuing them with exceptional visual understanding. Building upon the powerful\nbase of the pre-trained Qwen-7B, MARS stands out with its bilingual generative\ncapabilities corresponding to both English and Chinese language prompts and the\ncapacity for joint image and text generation. The flexibility of this framework\nlends itself to migration towards any-to-any task adaptability. Furthermore,\nMARS employs a multi-stage training strategy that first establishes robust\nimage-text alignment through complementary bidirectional tasks and subsequently\nconcentrates on refining the T2I generation process, significantly augmenting\ntext-image synchrony and the granularity of image details. Notably, MARS\nrequires only 9% of the GPU days needed by SD1.5, yet it achieves remarkable\nresults across a variety of benchmarks, illustrating the training efficiency\nand the potential for swift deployment in various applications.\n","authors":["Wanggui He","Siming Fu","Mushui Liu","Xierui Wang","Wenyi Xiao","Fangxun Shu","Yi Wang","Lei Zhang","Zhelun Yu","Haoyuan Li","Ziwei Huang","LeiLei Gan","Hao Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.07614v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2112.09532v2","updated":"2024-07-10T12:49:41Z","published":"2021-12-17T14:31:40Z","title":"Pixel Distillation: A New Knowledge Distillation Scheme for\n  Low-Resolution Image Recognition","summary":"  Previous knowledge distillation (KD) methods mostly focus on compressing\nnetwork architectures, which is not thorough enough in deployment as some costs\nlike transmission bandwidth and imaging equipment are related to the image\nsize. Therefore, we propose Pixel Distillation that extends knowledge\ndistillation into the input level while simultaneously breaking architecture\nconstraints. Such a scheme can achieve flexible cost control for deployment, as\nit allows the system to adjust both network architecture and image quality\naccording to the overall requirement of resources. Specifically, we first\npropose an input spatial representation distillation (ISRD) mechanism to\ntransfer spatial knowledge from large images to student's input module, which\ncan facilitate stable knowledge transfer between CNN and ViT. Then, a\nTeacher-Assistant-Student (TAS) framework is further established to disentangle\npixel distillation into the model compression stage and input compression\nstage, which significantly reduces the overall complexity of pixel distillation\nand the difficulty of distilling intermediate knowledge. Finally, we adapt\npixel distillation to object detection via an aligned feature for preservation\n(AFP) strategy for TAS, which aligns output dimensions of detectors at each\nstage by manipulating features and anchors of the assistant. Comprehensive\nexperiments on image classification and object detection demonstrate the\neffectiveness of our method. Code is available at\nhttps://github.com/gyguo/PixelDistillation.\n","authors":["Guangyu Guo","Dingwen Zhang","Longfei Han","Nian Liu","Ming-Ming Cheng","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2112.09532v2.pdf","comment":"TPAMI 2024"},{"id":"http://arxiv.org/abs/2407.07605v1","updated":"2024-07-10T12:44:22Z","published":"2024-07-10T12:44:22Z","title":"Early Explorations of Lightweight Models for Wound Segmentation on\n  Mobile Devices","summary":"  The aging population poses numerous challenges to healthcare, including the\nincrease in chronic wounds in the elderly. The current approach to wound\nassessment by therapists based on photographic documentation is subjective,\nhighlighting the need for computer-aided wound recognition from smartphone\nphotos. This offers objective and convenient therapy monitoring, while being\naccessible to patients from their home at any time. However, despite research\nin mobile image segmentation, there is a lack of focus on mobile wound\nsegmentation. To address this gap, we conduct initial research on three\nlightweight architectures to investigate their suitability for smartphone-based\nwound segmentation. Using public datasets and UNet as a baseline, our results\nare promising, with both ENet and TopFormer, as well as the larger UNeXt\nvariant, showing comparable performance to UNet. Furthermore, we deploy the\nmodels into a smartphone app for visual assessment of live segmentation, where\nresults demonstrate the effectiveness of TopFormer in distinguishing wounds\nfrom wound-coloured objects. While our study highlights the potential of\ntransformer models for mobile wound segmentation, future work should aim to\nfurther improve the mask contours.\n","authors":["Vanessa Borst","Timo Dittus","Konstantin MÃ¼ller","Samuel Kounev"],"pdf_url":"https://arxiv.org/pdf/2407.07605v1.pdf","comment":"To be published in the \"47th German Conference on Artificial\n  Intelligence (KI 2024)\""},{"id":"http://arxiv.org/abs/2407.07604v1","updated":"2024-07-10T12:42:39Z","published":"2024-07-10T12:42:39Z","title":"H-FCBFormer Hierarchical Fully Convolutional Branch Transformer for\n  Occlusal Contact Segmentation with Articulating Paper","summary":"  Occlusal contacts are the locations at which the occluding surfaces of the\nmaxilla and the mandible posterior teeth meet. Occlusal contact detection is a\nvital tool for restoring the loss of masticatory function and is a mandatory\nassessment in the field of dentistry, with particular importance in\nprosthodontics and restorative dentistry. The most common method for occlusal\ncontact detection is articulating paper. However, this method can indicate\nsignificant medically false positive and medically false negative contact\nareas, leaving the identification of true occlusal indications to clinicians.\nTo address this, we propose a multiclass Vision Transformer and Fully\nConvolutional Network ensemble semantic segmentation model with a combination\nhierarchical loss function, which we name as Hierarchical Fully Convolutional\nBranch Transformer (H-FCBFormer). We also propose a method of generating\nmedically true positive semantic segmentation masks derived from expert\nannotated articulating paper masks and gold standard masks. The proposed model\noutperforms other machine learning methods evaluated at detecting medically\ntrue positive contacts and performs better than dentists in terms of accurately\nidentifying object-wise occlusal contact areas while taking significantly less\ntime to identify them. Code is available at\nhttps://github.com/Banksylel/H-FCBFormer.\n","authors":["Ryan Banks","Bernat Rovira-Lastra","Jordi Martinez-Gomis","Akhilanand Chaurasia","Yunpeng Li"],"pdf_url":"https://arxiv.org/pdf/2407.07604v1.pdf","comment":"15 pages, 5 figures, 2 tables, 5 equations, peer reviewed and\n  accepted to Medical Imaging Understanding and Analysis (MIUA 2024)"},{"id":"http://arxiv.org/abs/2407.07603v1","updated":"2024-07-10T12:39:02Z","published":"2024-07-10T12:39:02Z","title":"iiANET: Inception Inspired Attention Hybrid Network for efficient\n  Long-Range Dependency","summary":"  The recent emergence of hybrid models has introduced another transformative\napproach to solving computer vision tasks, slowly shifting away from\nconventional CNN (Convolutional Neural Network) and ViT (Vision Transformer).\nHowever, not enough effort has been made to efficiently combine these two\napproaches to improve capturing long-range dependencies prevalent in complex\nimages. In this paper, we introduce iiANET (Inception Inspired Attention\nNetwork), an efficient hybrid model designed to capture long-range dependencies\nin complex images. The fundamental building block, iiABlock, integrates global\n2D-MHSA (Multi-Head Self-Attention) with Registers, MBConv2 (MobileNetV2-based\nconvolution), and dilated convolution in parallel, enabling the model to\nadeptly leverage self-attention for capturing long-range dependencies while\nutilizing MBConv2 for effective local-detail extraction and dilated convolution\nfor efficiently expanding the kernel receptive field to capture more contextual\ninformation. Lastly, we serially integrate an ECANET (Efficient Channel\nAttention Network) at the end of each iiABlock to calibrate channel-wise\nattention for enhanced model performance. Extensive qualitative and\nquantitative comparative evaluation on various benchmarks demonstrates improved\nperformance over some state-of-the-art models.\n","authors":["Haruna Yunusa","Qin Shiyin","Abdulrahman Hamman Adama Chukkol","Isah Bello","Adamu Lawan"],"pdf_url":"https://arxiv.org/pdf/2407.07603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07587v1","updated":"2024-07-10T12:20:11Z","published":"2024-07-10T12:20:11Z","title":"Let Occ Flow: Self-Supervised 3D Occupancy Flow Prediction","summary":"  Accurate perception of the dynamic environment is a fundamental task for\nautonomous driving and robot systems. This paper introduces Let Occ Flow, the\nfirst self-supervised work for joint 3D occupancy and occupancy flow prediction\nusing only camera inputs, eliminating the need for 3D annotations. Utilizing\nTPV for unified scene representation and deformable attention layers for\nfeature aggregation, our approach incorporates a backward-forward temporal\nattention module to capture dynamic object dependencies, followed by a 3D\nrefine module for fine-gained volumetric representation. Besides, our method\nextends differentiable rendering to 3D volumetric flow fields, leveraging\nzero-shot 2D segmentation and optical flow cues for dynamic decomposition and\nmotion optimization. Extensive experiments on nuScenes and KITTI datasets\ndemonstrate the competitive performance of our approach over prior\nstate-of-the-art methods.\n","authors":["Yili Liu","Linzhan Mou","Xuan Yu","Chenrui Han","Sitong Mao","Rong Xiong","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2407.07587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07586v1","updated":"2024-07-10T12:18:38Z","published":"2024-07-10T12:18:38Z","title":"Simplifying Source-Free Domain Adaptation for Object Detection:\n  Effective Self-Training Strategies and Performance Insights","summary":"  This paper focuses on source-free domain adaptation for object detection in\ncomputer vision. This task is challenging and of great practical interest, due\nto the cost of obtaining annotated data sets for every new domain. Recent\nresearch has proposed various solutions for Source-Free Object Detection\n(SFOD), most being variations of teacher-student architectures with diverse\nfeature alignment, regularization and pseudo-label selection strategies. Our\nwork investigates simpler approaches and their performance compared to more\ncomplex SFOD methods in several adaptation scenarios. We highlight the\nimportance of batch normalization layers in the detector backbone, and show\nthat adapting only the batch statistics is a strong baseline for SFOD. We\npropose a simple extension of a Mean Teacher with strong-weak augmentation in\nthe source-free setting, Source-Free Unbiased Teacher (SF-UT), and show that it\nactually outperforms most of the previous SFOD methods. Additionally, we\nshowcase that an even simpler strategy consisting in training on a fixed set of\npseudo-labels can achieve similar performance to the more complex\nteacher-student mutual learning, while being computationally efficient and\nmitigating the major issue of teacher-student collapse. We conduct experiments\non several adaptation tasks using benchmark driving datasets including\n(Foggy)Cityscapes, Sim10k and KITTI, and achieve a notable improvement of 4.7\\%\nAP50 on Cityscapes$\\rightarrow$Foggy-Cityscapes compared with the latest\nstate-of-the-art in SFOD. Source code is available at\nhttps://github.com/EPFL-IMOS/simple-SFOD.\n","authors":["Yan Hao","Florent Forest","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2407.07586v1.pdf","comment":"Accepted at ECCV 2024. 19 pages"},{"id":"http://arxiv.org/abs/2407.07582v1","updated":"2024-07-10T12:16:15Z","published":"2024-07-10T12:16:15Z","title":"TIP: Tabular-Image Pre-training for Multimodal Classification with\n  Incomplete Data","summary":"  Images and structured tables are essential parts of real-world databases.\nThough tabular-image representation learning is promising to create new\ninsights, it remains a challenging task, as tabular data is typically\nheterogeneous and incomplete, presenting significant modality disparities with\nimages. Earlier works have mainly focused on simple modality fusion strategies\nin complete data scenarios, without considering the missing data issue, and\nthus are limited in practice. In this paper, we propose TIP, a novel\ntabular-image pre-training framework for learning multimodal representations\nrobust to incomplete tabular data. Specifically, TIP investigates a novel\nself-supervised learning (SSL) strategy, including a masked tabular\nreconstruction task for tackling data missingness, and image-tabular matching\nand contrastive learning objectives to capture multimodal information.\nMoreover, TIP proposes a versatile tabular encoder tailored for incomplete,\nheterogeneous tabular data and a multimodal interaction module for\ninter-modality representation learning. Experiments are performed on downstream\nmultimodal classification tasks using both natural and medical image datasets.\nThe results show that TIP outperforms state-of-the-art supervised/SSL\nimage/multimodal algorithms in both complete and incomplete data scenarios. Our\ncode is available at https://github.com/siyi-wind/TIP.\n","authors":["Siyi Du","Shaoming Zheng","Yinsong Wang","Wenjia Bai","Declan P. O'Regan","Chen Qin"],"pdf_url":"https://arxiv.org/pdf/2407.07582v1.pdf","comment":"28 pages (including 9 pages of supplementary materials), accepted by\n  ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07580v1","updated":"2024-07-10T12:13:39Z","published":"2024-07-10T12:13:39Z","title":"InstructLayout: Instruction-Driven 2D and 3D Layout Synthesis with\n  Semantic Graph Prior","summary":"  Comprehending natural language instructions is a charming property for both\n2D and 3D layout synthesis systems. Existing methods implicitly model object\njoint distributions and express object relations, hindering generation's\ncontrollability. We introduce InstructLayout, a novel generative framework that\nintegrates a semantic graph prior and a layout decoder to improve\ncontrollability and fidelity for 2D and 3D layout synthesis. The proposed\nsemantic graph prior learns layout appearances and object distributions\nsimultaneously, demonstrating versatility across various downstream tasks in a\nzero-shot manner. To facilitate the benchmarking for text-driven 2D and 3D\nscene synthesis, we respectively curate two high-quality datasets of\nlayout-instruction pairs from public Internet resources with large language and\nmultimodal models. Extensive experimental results reveal that the proposed\nmethod outperforms existing state-of-the-art approaches by a large margin in\nboth 2D and 3D layout synthesis tasks. Thorough ablation studies confirm the\nefficacy of crucial design components.\n","authors":["Chenguo Lin","Yuchen Lin","Yadong Mu"],"pdf_url":"https://arxiv.org/pdf/2407.07580v1.pdf","comment":"25 pages, 16 figures"},{"id":"http://arxiv.org/abs/2407.07577v1","updated":"2024-07-10T12:11:59Z","published":"2024-07-10T12:11:59Z","title":"IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language\n  Model","summary":"  The rapid advancement of Large Vision-Language models (LVLMs) has\ndemonstrated a spectrum of emergent capabilities. Nevertheless, current models\nonly focus on the visual content of a single scenario, while their ability to\nassociate instances across different scenes has not yet been explored, which is\nessential for understanding complex visual content, such as movies with\nmultiple characters and intricate plots. Towards movie understanding, a\ncritical initial step for LVLMs is to unleash the potential of character\nidentities memory and recognition across multiple visual scenarios. To achieve\nthe goal, we propose visual instruction tuning with ID reference and develop an\nID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our research\nintroduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory and\nrecognition across four dimensions: matching, location, question-answering, and\ncaptioning. Our findings highlight the limitations of existing LVLMs in\nrecognizing and associating instance identities with ID reference. This paper\npaves the way for future artificial intelligence systems to possess\nmulti-identity visual inputs, thereby facilitating the comprehension of complex\nvisual narratives like movies.\n","authors":["Yatai Ji","Shilong Zhang","Jie Wu","Peize Sun","Weifeng Chen","Xuefeng Xiao","Sidi Yang","Yujiu Yang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2407.07577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07564v1","updated":"2024-07-10T11:49:29Z","published":"2024-07-10T11:49:29Z","title":"Trainable Highly-expressive Activation Functions","summary":"  Nonlinear activation functions are pivotal to the success of deep neural\nnets, and choosing the appropriate activation function can significantly affect\ntheir performance. Most networks use fixed activation functions (e.g., ReLU,\nGELU, etc.), and this choice might limit their expressiveness. Furthermore,\ndifferent layers may benefit from diverse activation functions. Consequently,\nthere has been a growing interest in trainable activation functions. In this\npaper, we introduce DiTAC, a trainable highly-expressive activation function\nbased on an efficient diffeomorphic transformation (called CPAB). Despite\nintroducing only a negligible number of trainable parameters, DiTAC enhances\nmodel expressiveness and performance, often yielding substantial improvements.\nIt also outperforms existing activation functions (regardless whether the\nlatter are fixed or trainable) in tasks such as semantic segmentation, image\ngeneration, regression problems, and image classification. Our code is\navailable at https://github.com/BGU-CS-VIL/DiTAC.\n","authors":["Irit Chelly","Shahaf E. Finder","Shira Ifergane","Oren Freifeld"],"pdf_url":"https://arxiv.org/pdf/2407.07564v1.pdf","comment":"Accepted to ECCV 2024 on July 1st, 2024"},{"id":"http://arxiv.org/abs/2407.05363v2","updated":"2024-07-10T11:31:50Z","published":"2024-07-07T13:27:14Z","title":"Multi-branch Collaborative Learning Network for 3D Visual Grounding","summary":"  3D referring expression comprehension (3DREC) and segmentation (3DRES) have\noverlapping objectives, indicating their potential for collaboration. However,\nexisting collaborative approaches predominantly depend on the results of one\ntask to make predictions for the other, limiting effective collaboration. We\nargue that employing separate branches for 3DREC and 3DRES tasks enhances the\nmodel's capacity to learn specific information for each task, enabling them to\nacquire complementary knowledge. Thus, we propose the MCLN framework, which\nincludes independent branches for 3DREC and 3DRES tasks. This enables dedicated\nexploration of each task and effective coordination between the branches.\nFurthermore, to facilitate mutual reinforcement between these branches, we\nintroduce a Relative Superpoint Aggregation (RSA) module and an Adaptive Soft\nAlignment (ASA) module. These modules significantly contribute to the precise\nalignment of prediction results from the two branches, directing the module to\nallocate increased attention to key positions. Comprehensive experimental\nevaluation demonstrates that our proposed method achieves state-of-the-art\nperformance on both the 3DREC and 3DRES tasks, with an increase of 2.05% in\nAcc@0.5 for 3DREC and 3.96% in mIoU for 3DRES.\n","authors":["Zhipeng Qian","Yiwei Ma","Zhekai Lin","Jiayi Ji","Xiawu Zheng","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.05363v2.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2407.07557v1","updated":"2024-07-10T11:30:50Z","published":"2024-07-10T11:30:50Z","title":"Federated Foundation Model for Cardiac CT Imaging","summary":"  Federated learning (FL) is a renowned technique for utilizing decentralized\ndata while preserving privacy. However, real-world applications often involve\ninherent challenges such as partially labeled datasets, where not all clients\npossess expert annotations of all labels of interest, leaving large portions of\nunlabeled data unused. In this study, we conduct the largest federated cardiac\nCT imaging analysis to date, focusing on partially labeled datasets ($n=8,124$)\nof Transcatheter Aortic Valve Implantation (TAVI) patients over eight hospital\nclients. Transformer architectures, which are the major building blocks of\ncurrent foundation models, have shown superior performance when trained on\nlarger cohorts than traditional CNNs. However, when trained on small\ntask-specific labeled sample sizes, it is currently not feasible to exploit\ntheir underlying attention mechanism for improved performance. Therefore, we\ndeveloped a two-stage semi-supervised learning strategy that distills knowledge\nfrom several task-specific CNNs (landmark detection and segmentation of\ncalcification) into a single transformer model by utilizing large amounts of\nunlabeled data typically residing unused in hospitals to mitigate these issues.\nThis method not only improves the predictive accuracy and generalizability of\ntransformer-based architectures but also facilitates the simultaneous learning\nof all partial labels within a single transformer model across the federation.\nAdditionally, we show that our transformer-based model extracts more meaningful\nfeatures for further downstream tasks than the UNet-based one by only training\nthe last layer to also solve segmentation of coronary arteries. We make the\ncode and weights of the final model openly available, which can serve as a\nfoundation model for further research in cardiac CT imaging.\n","authors":["Malte TÃ¶lle","Philipp Garthe","Clemens Scherer","Jan Moritz Seliger","Andreas Leha","Nina KrÃ¼ger","Stefan Simm","Simon Martin","Sebastian Eble","Halvar Kelm","Moritz Bednorz","Florian AndrÃ©","Peter Bannas","Gerhard Diller","Norbert Frey","Stefan GroÃ","Anja Hennemuth","Lars Kaderali","Alexander Meyer","Eike Nagel","Stefan Orwat","Moritz Seiffert","Tim Friede","Tim Seidler","Sandy Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2407.07557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07056v2","updated":"2024-07-10T11:25:26Z","published":"2024-07-09T17:25:04Z","title":"CAPformer: Compression-Aware Pre-trained Transformer for Low-Light Image\n  Enhancement","summary":"  Low-Light Image Enhancement (LLIE) has advanced with the surge in phone\nphotography demand, yet many existing methods neglect compression, a crucial\nconcern for resource-constrained phone photography. Most LLIE methods overlook\nthis, hindering their effectiveness. In this study, we investigate the effects\nof JPEG compression on low-light images and reveal substantial information loss\ncaused by JPEG due to widespread low pixel values in dark areas. Hence, we\npropose the Compression-Aware Pre-trained Transformer (CAPformer), employing a\nnovel pre-training strategy to learn lossless information from uncompressed\nlow-light images. Additionally, the proposed Brightness-Guided Self-Attention\n(BGSA) mechanism enhances rational information gathering. Experiments\ndemonstrate the superiority of our approach in mitigating compression effects\non LLIE, showcasing its potential for improving LLIE in resource-constrained\nscenarios.\n","authors":["Wei Wang","Zhi Jin"],"pdf_url":"https://arxiv.org/pdf/2407.07056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07544v1","updated":"2024-07-10T11:11:36Z","published":"2024-07-10T11:11:36Z","title":"Disentangling Masked Autoencoders for Unsupervised Domain Generalization","summary":"  Domain Generalization (DG), designed to enhance out-of-distribution (OOD)\ngeneralization, is all about learning invariance against domain shifts\nutilizing sufficient supervision signals. Yet, the scarcity of such labeled\ndata has led to the rise of unsupervised domain generalization (UDG) - a more\nimportant yet challenging task in that models are trained across diverse\ndomains in an unsupervised manner and eventually tested on unseen domains. UDG\nis fast gaining attention but is still far from well-studied. To close the\nresearch gap, we propose a novel learning framework designed for UDG, termed\nthe Disentangled Masked Auto Encoder (DisMAE), aiming to discover the\ndisentangled representations that faithfully reveal the intrinsic features and\nsuperficial variations without access to the class label. At its core is the\ndistillation of domain-invariant semantic features, which cannot be\ndistinguished by domain classifier, while filtering out the domain-specific\nvariations (for example, color schemes and texture patterns) that are unstable\nand redundant. Notably, DisMAE co-trains the asymmetric dual-branch\narchitecture with semantic and lightweight variation encoders, offering dynamic\ndata manipulation and representation level augmentation capabilities. Extensive\nexperiments on four benchmark datasets (i.e., DomainNet, PACS, VLCS, Colored\nMNIST) with both DG and UDG tasks demonstrate that DisMAE can achieve\ncompetitive OOD performance compared with the state-of-the-art DG and UDG\nbaselines, which shed light on potential research line in improving the\ngeneralization ability with large-scale unlabeled data.\n","authors":["An Zhang","Han Wang","Xiang Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2407.07544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05329v2","updated":"2024-07-10T11:08:14Z","published":"2024-03-08T14:07:37Z","title":"OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy\n  Prediction","summary":"  3D occupancy prediction based on multi-sensor fusion,crucial for a reliable\nautonomous driving system, enables fine-grained understanding of 3D scenes.\nPrevious fusion-based 3D occupancy predictions relied on depth estimation for\nprocessing 2D image features. However, depth estimation is an ill-posed\nproblem, hindering the accuracy and robustness of these methods. Furthermore,\nfine-grained occupancy prediction demands extensive computational resources. To\naddress these issues, we propose OccFusion, a depth estimation free multi-modal\nfusion framework. Additionally, we introduce a generalizable active training\nmethod and an active decoder that can be applied to any occupancy prediction\nmodel, with the potential to enhance their performance. Experiments conducted\non nuScenes-Occupancy and nuScenes-Occ3D demonstrate our framework's superior\nperformance. Detailed ablation studies highlight the effectiveness of each\nproposed method.\n","authors":["Ji Zhang","Yiran Ding","Zixin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07541v1","updated":"2024-07-10T11:05:02Z","published":"2024-07-10T11:05:02Z","title":"Swiss DINO: Efficient and Versatile Vision Framework for On-device\n  Personal Object Search","summary":"  In this paper, we address a recent trend in robotic home appliances to\ninclude vision systems on personal devices, capable of personalizing the\nappliances on the fly. In particular, we formulate and address an important\ntechnical task of personal object search, which involves localization and\nidentification of personal items of interest on images captured by robotic\nappliances, with each item referenced only by a few annotated images. The task\nis crucial for robotic home appliances and mobile systems, which need to\nprocess personal visual scenes or to operate with particular personal objects\n(e.g., for grasping or navigation). In practice, personal object search\npresents two main technical challenges. First, a robot vision system needs to\nbe able to distinguish between many fine-grained classes, in the presence of\nocclusions and clutter. Second, the strict resource requirements for the\non-device system restrict the usage of most state-of-the-art methods for\nfew-shot learning and often prevent on-device adaptation. In this work, we\npropose Swiss DINO: a simple yet effective framework for one-shot personal\nobject search based on the recent DINOv2 transformer model, which was shown to\nhave strong zero-shot generalization properties. Swiss DINO handles challenging\non-device personalized scene understanding requirements and does not require\nany adaptation training. We show significant improvement (up to 55%) in\nsegmentation and recognition accuracy compared to the common lightweight\nsolutions, and significant footprint reduction of backbone inference time (up\nto 100x) and GPU consumption (up to 10x) compared to the heavy\ntransformer-based solutions.\n","authors":["Kirill Paramonov","Jia-Xing Zhong","Umberto Michieli","Jijoong Moon","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2407.07541v1.pdf","comment":"8 pages, 2 figures, accepted to IROS2024"},{"id":"http://arxiv.org/abs/2407.07539v1","updated":"2024-07-10T10:59:28Z","published":"2024-07-10T10:59:28Z","title":"Machine Unlearning for Medical Imaging","summary":"  Machine unlearning is the process of removing the impact of a particular set\nof training samples from a pretrained model. It aims to fulfill the \"right to\nbe forgotten\", which grants the individuals such as patients the right to\nreconsider their contribution in models including medical imaging models. In\nthis study, we evaluate the effectiveness (performance) and computational\nefficiency of different unlearning algorithms in medical imaging domain. Our\nevaluations demonstrate that the considered unlearning algorithms perform well\non the retain set (samples whose influence on the model is allowed to be\nretained) and forget set (samples whose contribution to the model should be\neliminated), and show no bias against male or female samples. They, however,\nadversely impact the generalization of the model, especially for larger forget\nset sizes. Moreover, they might be biased against easy or hard samples, and\nneed additional computational overhead for hyper-parameter tuning. In\nconclusion, machine unlearning seems promising for medical imaging, but the\nexisting unlearning algorithms still needs further improvements to become more\npractical for medical applications.\n","authors":["Reza Nasirigerdeh","Nader Razmi","Julia A. Schnabel","Daniel Rueckert","Georgios Kaissis"],"pdf_url":"https://arxiv.org/pdf/2407.07539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07536v1","updated":"2024-07-10T10:51:34Z","published":"2024-07-10T10:51:34Z","title":"KaiRacters: Character-level-based Writer Retrieval for Greek Papyri","summary":"  This paper presents a character-based approach for enhancing writer retrieval\nperformance in the context of Greek papyri. Our contribution lies in\nintroducing character-level annotations for frequently used characters, in our\ncase the trigram kai and four additional letters (epsilon, kappa, mu, omega),\nin Greek texts. We use a state-of-the-art writer retrieval approach based on\nNetVLAD and compare a character-level-based feature aggregation method against\nthe current default baseline of using small patches located at SIFT keypoint\nlocations for building the page descriptors. We demonstrate that by using only\nabout 15 characters per page, we are able to boost the performance up to 4% mAP\n(a relative improvement of 11%) on the GRK-120 dataset. Additionally, our\nqualitative analysis offers insights into the similarity scores of SIFT patches\nand specific characters. We publish the dataset with character-level\nannotations, including a quality label and our binarized images for further\nresearch.\n","authors":["Marco Peer","Robert Sablatnig","Olga Serbaeva","Isabelle Marthot-Santaniello"],"pdf_url":"https://arxiv.org/pdf/2407.07536v1.pdf","comment":"submitted to ICPR2024"},{"id":"http://arxiv.org/abs/2407.07532v1","updated":"2024-07-10T10:44:18Z","published":"2024-07-10T10:44:18Z","title":"Neural Localizer Fields for Continuous 3D Human Pose and Shape\n  Estimation","summary":"  With the explosive growth of available training data, single-image 3D human\nmodeling is ahead of a transition to a data-centric paradigm. A key to\nsuccessfully exploiting data scale is to design flexible models that can be\nsupervised from various heterogeneous data sources produced by different\nresearchers or vendors. To this end, we propose a simple yet powerful paradigm\nfor seamlessly unifying different human pose and shape-related tasks and\ndatasets. Our formulation is centered on the ability - both at training and\ntest time - to query any arbitrary point of the human volume, and obtain its\nestimated location in 3D. We achieve this by learning a continuous neural field\nof body point localizer functions, each of which is a differently parameterized\n3D heatmap-based convolutional point localizer (detector). For generating\nparametric output, we propose an efficient post-processing step for fitting\nSMPL-family body models to nonparametric joint and vertex predictions. With\nthis approach, we can naturally exploit differently annotated data sources\nincluding mesh, 2D/3D skeleton and dense pose, without having to convert\nbetween them, and thereby train large-scale 3D human mesh and skeleton\nestimation models that outperform the state-of-the-art on several public\nbenchmarks including 3DPW, EMDB and SSP-3D by a considerable margin.\n","authors":["IstvÃ¡n SÃ¡rÃ¡ndi","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2407.07532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07530v1","updated":"2024-07-10T10:36:11Z","published":"2024-07-10T10:36:11Z","title":"How Aligned are Different Alignment Metrics?","summary":"  In recent years, various methods and benchmarks have been proposed to\nempirically evaluate the alignment of artificial neural networks to human\nneural and behavioral data. But how aligned are different alignment metrics? To\nanswer this question, we analyze visual data from Brain-Score (Schrimpf et al.,\n2018), including metrics from the model-vs-human toolbox (Geirhos et al.,\n2021), together with human feature alignment (Linsley et al., 2018; Fel et al.,\n2022) and human similarity judgements (Muttenthaler et al., 2022). We find that\npairwise correlations between neural scores and behavioral scores are quite low\nand sometimes even negative. For instance, the average correlation between\nthose 80 models on Brain-Score that were fully evaluated on all 69 alignment\nmetrics we considered is only 0.198. Assuming that all of the employed metrics\nare sound, this implies that alignment with human perception may best be\nthought of as a multidimensional concept, with different methods measuring\nfundamentally different aspects. Our results underline the importance of\nintegrative benchmarking, but also raise questions about how to correctly\ncombine and aggregate individual metrics. Aggregating by taking the arithmetic\naverage, as done in Brain-Score, leads to the overall performance currently\nbeing dominated by behavior (95.25% explained variance) while the neural\npredictivity plays a less important role (only 33.33% explained variance). As a\nfirst step towards making sure that different alignment metrics all contribute\nfairly towards an integrative benchmark score, we therefore conclude by\ncomparing three different aggregation options.\n","authors":["Jannis Ahlert","Thomas Klein","Felix Wichmann","Robert Geirhos"],"pdf_url":"https://arxiv.org/pdf/2407.07530v1.pdf","comment":"submitted to the ICLR 2024 Workshop on Representational Alignment\n  (Re-Align)"},{"id":"http://arxiv.org/abs/2407.07525v1","updated":"2024-07-10T10:24:28Z","published":"2024-07-10T10:24:28Z","title":"Incremental Multiview Point Cloud Registration with Two-stage Candidate\n  Retrieval","summary":"  Multiview point cloud registration serves as a cornerstone of various\ncomputer vision tasks. Previous approaches typically adhere to a global\nparadigm, where a pose graph is initially constructed followed by motion\nsynchronization to determine the absolute pose. However, this separated\napproach may not fully leverage the characteristics of multiview registration\nand might struggle with low-overlap scenarios. In this paper, we propose an\nincremental multiview point cloud registration method that progressively\nregisters all scans to a growing meta-shape. To determine the incremental\nordering, we employ a two-stage coarse-to-fine strategy for point cloud\ncandidate retrieval. The first stage involves the coarse selection of scans\nbased on neighbor fusion-enhanced global aggregation features, while the second\nstage further reranks candidates through geometric-based matching.\nAdditionally, we apply a transformation averaging technique to mitigate\naccumulated errors during the registration process. Finally, we utilize a\nReservoir sampling-based technique to address density variance issues while\nreducing computational load. Comprehensive experimental results across various\nbenchmarks validate the effectiveness and generalization of our approach.\n","authors":["Shiqi Li","Jihua Zhu","Yifan Xie","Mingchen Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.07525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07523v1","updated":"2024-07-10T10:22:35Z","published":"2024-07-10T10:22:35Z","title":"SHERL: Synthesizing High Accuracy and Efficient Memory for\n  Resource-Limited Transfer Learning","summary":"  Parameter-efficient transfer learning (PETL) has emerged as a flourishing\nresearch field for adapting large pre-trained models to downstream tasks,\ngreatly reducing trainable parameters while grappling with memory challenges\nduring fine-tuning. To address it, memory-efficient series (METL) avoid\nbackpropagating gradients through the large backbone. However, they compromise\nby exclusively relying on frozen intermediate outputs and limiting the\nexhaustive exploration of prior knowledge from pre-trained models. Moreover,\nthe dependency and redundancy between cross-layer features are frequently\noverlooked, thereby submerging more discriminative representations and causing\nan inherent performance gap (vs. conventional PETL methods). Hence, we propose\nan innovative METL strategy called SHERL for resource-limited scenarios to\ndecouple the entire adaptation into two successive and complementary processes.\nIn the early route, intermediate outputs are consolidated via an\nanti-redundancy operation, enhancing their compatibility for subsequent\ninteractions; thereby in the late route, utilizing minimal late pre-trained\nlayers could alleviate the peak demand on memory overhead and regulate these\nfairly flexible features into more adaptive and powerful representations for\nnew domains. Extensive ablations on vision-and-language and language-only tasks\nshow that SHERL combines the strengths of both parameter and memory-efficient\ntechniques, performing on-par or better across diverse architectures with lower\nmemory during fine-tuning. Our code is publicly available at:\nhttps://github.com/Paranioar/SHERL.\n","authors":["Haiwen Diao","Bo Wan","Xu Jia","Yunzhi Zhuge","Ying Zhang","Huchuan Lu","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2407.07523v1.pdf","comment":"23 pages, 11 figures, Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.07520v1","updated":"2024-07-10T10:17:57Z","published":"2024-07-10T10:17:57Z","title":"IRSAM: Advancing Segment Anything Model for Infrared Small Target\n  Detection","summary":"  The recent Segment Anything Model (SAM) is a significant advancement in\nnatural image segmentation, exhibiting potent zero-shot performance suitable\nfor various downstream image segmentation tasks. However, directly utilizing\nthe pretrained SAM for Infrared Small Target Detection (IRSTD) task falls short\nin achieving satisfying performance due to a notable domain gap between natural\nand infrared images. Unlike a visible light camera, a thermal imager reveals an\nobject's temperature distribution by capturing infrared radiation. Small\ntargets often show a subtle temperature transition at the object's boundaries.\nTo address this issue, we propose the IRSAM model for IRSTD, which improves\nSAM's encoder-decoder architecture to learn better feature representation of\ninfrared small objects. Specifically, we design a Perona-Malik diffusion\n(PMD)-based block and incorporate it into multiple levels of SAM's encoder to\nhelp it capture essential structural features while suppressing noise.\nAdditionally, we devise a Granularity-Aware Decoder (GAD) to fuse the\nmulti-granularity feature from the encoder to capture structural information\nthat may be lost in long-distance modeling. Extensive experiments on the public\ndatasets, including NUAA-SIRST, NUDT-SIRST, and IRSTD-1K, validate the design\nchoice of IRSAM and its significant superiority over representative\nstate-of-the-art methods. The source code are available at:\ngithub.com/IPIC-Lab/IRSAM.\n","authors":["Mingjin Zhang","Yuchun Wang","Jie Guo","Yunsong Li","Xinbo Gao","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.07520v1.pdf","comment":"18 pages, 8 figures, to be published in ECCV2024"},{"id":"http://arxiv.org/abs/2407.07518v1","updated":"2024-07-10T10:13:11Z","published":"2024-07-10T10:13:11Z","title":"Multi-modal Crowd Counting via a Broker Modality","summary":"  Multi-modal crowd counting involves estimating crowd density from both visual\nand thermal/depth images. This task is challenging due to the significant gap\nbetween these distinct modalities. In this paper, we propose a novel approach\nby introducing an auxiliary broker modality and on this basis frame the task as\na triple-modal learning problem. We devise a fusion-based method to generate\nthis broker modality, leveraging a non-diffusion, lightweight counterpart of\nmodern denoising diffusion-based fusion models. Additionally, we identify and\naddress the ghosting effect caused by direct cross-modal image fusion in\nmulti-modal crowd counting. Through extensive experimental evaluations on\npopular multi-modal crowd-counting datasets, we demonstrate the effectiveness\nof our method, which introduces only 4 million additional parameters, yet\nachieves promising results. The code is available at\nhttps://github.com/HenryCilence/Broker-Modality-Crowd-Counting.\n","authors":["Haoliang Meng","Xiaopeng Hong","Chenhao Wang","Miao Shang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2407.07518v1.pdf","comment":"This is the preprint version of the paper and supplemental material\n  to appear in ECCV 2024. Please cite the final published version. Code is\n  available at https://github.com/HenryCilence/Broker-Modality-Crowd-Counting"},{"id":"http://arxiv.org/abs/2407.07517v1","updated":"2024-07-10T10:12:26Z","published":"2024-07-10T10:12:26Z","title":"Parameter Efficient Fine Tuning for Multi-scanner PET to PET\n  Reconstruction","summary":"  Reducing scan time in Positron Emission Tomography (PET) imaging while\nmaintaining high-quality images is crucial for minimizing patient discomfort\nand radiation exposure. Due to the limited size of datasets and distribution\ndiscrepancy across scanners in medical imaging, fine-tuning in a\nparameter-efficient and effective manner is on the rise. Motivated by the\npotential of Parameter-Efficient Fine-Tuning (PEFT), we aim to address these\nissues by effectively leveraging PEFT to improve limited data and GPU resource\nissues in multi-scanner setups. In this paper, we introduce PETITE,\nParameter-Efficient Fine-Tuning for MultI-scanner PET to PET REconstruction\nthat uses fewer than 1% of the parameters. To the best of our knowledge, this\nstudy is the first to systematically explore the efficacy of diverse PEFT\ntechniques in medical imaging reconstruction tasks via prevalent\nencoder-decoder-type deep models. This investigation, in particular, brings\nintriguing insights into PETITE as we show further improvements by treating\nencoder and decoder separately and mixing different PEFT methods, namely,\nMix-PEFT. Using multi-scanner PET datasets comprised of five different\nscanners, we extensively test the cross-scanner PET scan time reduction\nperformances (i.e., a model pre-trained on one scanner is fine-tuned on a\ndifferent scanner) of 21 feasible Mix-PEFT combinations to derive optimal\nPETITE. We show that training with less than 1% parameters using PETITE\nperforms on par with full fine-tuning (i.e., 100% parameter)\n","authors":["Yumin Kim","Gayoon Choi","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2407.07517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07516v1","updated":"2024-07-10T10:09:12Z","published":"2024-07-10T10:09:12Z","title":"HDKD: Hybrid Data-Efficient Knowledge Distillation Network for Medical\n  Image Classification","summary":"  Vision Transformers (ViTs) have achieved significant advancement in computer\nvision tasks due to their powerful modeling capacity. However, their\nperformance notably degrades when trained with insufficient data due to lack of\ninherent inductive biases. Distilling knowledge and inductive biases from a\nConvolutional Neural Network (CNN) teacher has emerged as an effective strategy\nfor enhancing the generalization of ViTs on limited datasets. Previous\napproaches to Knowledge Distillation (KD) have pursued two primary paths: some\nfocused solely on distilling the logit distribution from CNN teacher to ViT\nstudent, neglecting the rich semantic information present in intermediate\nfeatures due to the structural differences between them. Others integrated\nfeature distillation along with logit distillation, yet this introduced\nalignment operations that limits the amount of knowledge transferred due to\nmismatched architectures and increased the computational overhead. To this end,\nthis paper presents Hybrid Data-efficient Knowledge Distillation (HDKD)\nparadigm which employs a CNN teacher and a hybrid student. The choice of hybrid\nstudent serves two main aspects. First, it leverages the strengths of both\nconvolutions and transformers while sharing the convolutional structure with\nthe teacher model. Second, this shared structure enables the direct application\nof feature distillation without any information loss or additional\ncomputational overhead. Additionally, we propose an efficient light-weight\nconvolutional block named Mobile Channel-Spatial Attention (MBCSA), which\nserves as the primary convolutional block in both teacher and student models.\nExtensive experiments on two medical public datasets showcase the superiority\nof HDKD over other state-of-the-art models and its computational efficiency.\nSource code at: https://github.com/omarsherif200/HDKD\n","authors":["Omar S. EL-Assiouti","Ghada Hamed","Dina Khattab","Hala M. Ebied"],"pdf_url":"https://arxiv.org/pdf/2407.07516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14022v2","updated":"2024-07-10T10:07:44Z","published":"2024-05-22T21:55:58Z","title":"I2I-Mamba: Multi-modal medical image synthesis via selective state space\n  modeling","summary":"  In recent years, deep learning models comprising transformer components have\npushed the performance envelope in medical image synthesis tasks. Contrary to\nconvolutional neural networks (CNNs) that use static, local filters,\ntransformers use self-attention mechanisms to permit adaptive, non-local\nfiltering to sensitively capture long-range context. However, this sensitivity\ncomes at the expense of substantial model complexity, which can compromise\nlearning efficacy particularly on relatively modest-sized imaging datasets.\nHere, we propose a novel adversarial model for multi-modal medical image\nsynthesis, I2I-Mamba, that leverages selective state space modeling (SSM) to\nefficiently capture long-range context while maintaining local precision. To do\nthis, I2I-Mamba injects channel-mixed Mamba (cmMamba) blocks in the bottleneck\nof a convolutional backbone. In cmMamba blocks, SSM layers are used to learn\ncontext across the spatial dimension and channel-mixing layers are used to\nlearn context across the channel dimension of feature maps. Comprehensive\ndemonstrations are reported for imputing missing images in multi-contrast MRI\nand MRI-CT protocols. Our results indicate that I2I-Mamba offers superior\nperformance against state-of-the-art CNN- and transformer-based methods in\nsynthesizing target-modality images.\n","authors":["Omer F. Atli","Bilal Kabas","Fuat Arslan","Mahmut Yurt","Onat Dalmaz","Tolga Ãukur"],"pdf_url":"https://arxiv.org/pdf/2405.14022v2.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.07514v1","updated":"2024-07-10T10:05:22Z","published":"2024-07-10T10:05:22Z","title":"Swin SMT: Global Sequential Modeling in 3D Medical Image Segmentation","summary":"  Recent advances in Vision Transformers (ViTs) have significantly enhanced\nmedical image segmentation by facilitating the learning of global\nrelationships. However, these methods face a notable challenge in capturing\ndiverse local and global long-range sequential feature representations,\nparticularly evident in whole-body CT (WBCT) scans. To overcome this\nlimitation, we introduce Swin Soft Mixture Transformer (Swin SMT), a novel\narchitecture based on Swin UNETR. This model incorporates a Soft\nMixture-of-Experts (Soft MoE) to effectively handle complex and diverse\nlong-range dependencies. The use of Soft MoE allows for scaling up model\nparameters maintaining a balance between computational complexity and\nsegmentation performance in both training and inference modes. We evaluate Swin\nSMT on the publicly available TotalSegmentator-V2 dataset, which includes 117\nmajor anatomical structures in WBCT images. Comprehensive experimental results\ndemonstrate that Swin SMT outperforms several state-of-the-art methods in 3D\nanatomical structure segmentation, achieving an average Dice Similarity\nCoefficient of 85.09%. The code and pre-trained weights of Swin SMT are\npublicly available at https://github.com/MI2DataLab/SwinSMT.\n","authors":["Szymon PÅotka","Maciej Chrabaszcz","Przemyslaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2407.07514v1.pdf","comment":"Accepted to MICCAI 2024 (early accept)"},{"id":"http://arxiv.org/abs/2407.07510v1","updated":"2024-07-10T09:55:31Z","published":"2024-07-10T09:55:31Z","title":"Invisible Optical Adversarial Stripes on Traffic Sign against Autonomous\n  Vehicles","summary":"  Camera-based computer vision is essential to autonomous vehicle's perception.\nThis paper presents an attack that uses light-emitting diodes and exploits the\ncamera's rolling shutter effect to create adversarial stripes in the captured\nimages to mislead traffic sign recognition. The attack is stealthy because the\nstripes on the traffic sign are invisible to human. For the attack to be\nthreatening, the recognition results need to be stable over consecutive image\nframes. To achieve this, we design and implement GhostStripe, an attack system\nthat controls the timing of the modulated light emission to adapt to camera\noperations and victim vehicle movements. Evaluated on real testbeds,\nGhostStripe can stably spoof the traffic sign recognition results for up to\n94\\% of frames to a wrong class when the victim vehicle passes the road\nsection. In reality, such attack effect may fool victim vehicles into\nlife-threatening incidents. We discuss the countermeasures at the levels of\ncamera sensor, perception model, and autonomous driving system.\n","authors":["Dongfang Guo","Yuting Wu","Yimin Dai","Pengfei Zhou","Xin Lou","Rui Tan"],"pdf_url":"https://arxiv.org/pdf/2407.07510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05892v2","updated":"2024-07-10T09:44:17Z","published":"2024-07-08T12:59:28Z","title":"An efficient method to automate tooth identification and 3D bounding box\n  extraction from Cone Beam CT Images","summary":"  Accurate identification, localization, and segregation of teeth from Cone\nBeam Computed Tomography (CBCT) images are essential for analyzing dental\npathologies. Modeling an individual tooth can be challenging and intricate to\naccomplish, especially when fillings and other restorations introduce\nartifacts. This paper proposes a method for automatically detecting,\nidentifying, and extracting teeth from CBCT images. Our approach involves\ndividing the three-dimensional images into axial slices for image detection.\nTeeth are pinpointed and labeled using a single-stage object detector.\nSubsequently, bounding boxes are delineated and identified to create\nthree-dimensional representations of each tooth. The proposed solution has been\nsuccessfully integrated into the dental analysis tool Dentomo.\n","authors":["Ignacio Garrido Botella","Ignacio Arranz Ãgueda","Juan Carlos Armenteros Carmona","Oleg Vorontsov","Fernando BayÃ³n Robledo","Evgeny Solovykh","Obrubov Aleksandr Andreevich","AdriÃ¡n Alonso Barriuso"],"pdf_url":"https://arxiv.org/pdf/2407.05892v2.pdf","comment":"7 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2407.07504v1","updated":"2024-07-10T09:42:41Z","published":"2024-07-10T09:42:41Z","title":"Pan-cancer Histopathology WSI Pre-training with Position-aware Masked\n  Autoencoder","summary":"  Large-scale pre-training models have promoted the development of\nhistopathology image analysis. However, existing self-supervised methods for\nhistopathology images focus on learning patch features, while there is still a\nlack of available pre-training models for WSI-level feature learning. In this\npaper, we propose a novel self-supervised learning framework for pan-cancer\nWSI-level representation pre-training with the designed position-aware masked\nautoencoder (PAMA). Meanwhile, we propose the position-aware cross-attention\n(PACA) module with a kernel reorientation (KRO) strategy and an anchor dropout\n(AD) mechanism. The KRO strategy can capture the complete semantic structure\nand eliminate ambiguity in WSIs, and the AD contributes to enhancing the\nrobustness and generalization of the model. We evaluated our method on 6\nlarge-scale datasets from multiple organs for pan-cancer classification tasks.\nThe results have demonstrated the effectiveness of PAMA in generalized and\ndiscriminative WSI representation learning and pan-cancer WSI pre-training. The\nproposed method was also compared with \\R{7} WSI analysis methods. The\nexperimental results have indicated that our proposed PAMA is superior to the\nstate-of-the-art methods.The code and checkpoints are available at\nhttps://github.com/WkEEn/PAMA.\n","authors":["Kun Wu","Zhiguo Jiang","Kunming Tang","Jun Shi","Fengying Xie","Wei Wang","Haibo Wu","Yushan Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.07504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07503v1","updated":"2024-07-10T09:41:36Z","published":"2024-07-10T09:41:36Z","title":"Metasurface-based Snapshot Shortwave-Infrared Hyperspectral Image\n  Reconstruction with Inter and Intra Prior Learning Network","summary":"  Shortwave-infrared(SWIR) spectral information,ranging from 1 {\\mu}m to\n2.5{\\mu}m, breaks the limitations of traditional color cameras in acquiring\nscene information and has been used in many fields. However, conventional SWIR\nhyperspectral imaging systems face challenges due to their bulky setups and low\nacquisition speed. In this work, we introduce a snapshot SWIR hyperspectral\nimaging system based on a metasurface filter and a corresponding filter\nselection method to achieve the lowest correlation coefficient among these\nfilters.This systemhas the advantages of small size and snapshot imaging. We\npropose a novel inter and intra prior learning unfolding framework proposed to\nachieve high-quality SWIR hyperspectral image reconstruction, which bridges the\ngap between prior learning and cross-stage information interaction. We also\ndesign an adaptive feature transfer mechanism to adaptively the transfer\ncontextual correlation of multi-scale encoder features to prevent detailed\ninformation loss in the decoder. Experiment results demonstrate that our method\ncan reconstruct HSI with high speed and superior performance over existing\nmethods.\n","authors":["Linqiang Li","Pan Liu","Haofang Yan","Ziqin Zhang","Jinglei Hao","Seong G. Kong","Yongqiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.07503v1.pdf","comment":"10 pages,5 figures"},{"id":"http://arxiv.org/abs/2402.05655v3","updated":"2024-07-10T09:28:44Z","published":"2024-02-08T13:12:50Z","title":"Real-time Holistic Robot Pose Estimation with Unknown States","summary":"  Estimating robot pose from RGB images is a crucial problem in computer vision\nand robotics. While previous methods have achieved promising performance, most\nof them presume full knowledge of robot internal states, e.g. ground-truth\nrobot joint angles. However, this assumption is not always valid in practical\nsituations. In real-world applications such as multi-robot collaboration or\nhuman-robot interaction, the robot joint states might not be shared or could be\nunreliable. On the other hand, existing approaches that estimate robot pose\nwithout joint state priors suffer from heavy computation burdens and thus\ncannot support real-time applications. This work introduces an efficient\nframework for real-time robot pose estimation from RGB images without requiring\nknown robot states. Our method estimates camera-to-robot rotation, robot state\nparameters, keypoint locations, and root depth, employing a neural network\nmodule for each task to facilitate learning and sim-to-real transfer. Notably,\nit achieves inference in a single feed-forward pass without iterative\noptimization. Our approach offers a 12-time speed increase with\nstate-of-the-art accuracy, enabling real-time holistic robot pose estimation\nfor the first time. Code and models are available at\nhttps://github.com/Oliverbansk/Holistic-Robot-Pose-Estimation.\n","authors":["Shikun Ban","Juling Fan","Xiaoxuan Ma","Wentao Zhu","Yu Qiao","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2402.05655v3.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07494v1","updated":"2024-07-10T09:26:26Z","published":"2024-07-10T09:26:26Z","title":"Panoptic Segmentation of Galactic Structures in LSB Images","summary":"  We explore the use of deep learning to localise galactic structures in low\nsurface brightness (LSB) images. LSB imaging reveals many interesting\nstructures, though these are frequently confused with galactic dust\ncontamination, due to a strong local visual similarity. We propose a novel\nunified approach to multi-class segmentation of galactic structures and of\nextended amorphous image contaminants. Our panoptic segmentation model combines\nMask R-CNN with a contaminant specialised network and utilises an adaptive\npreprocessing layer to better capture the subtle features of LSB images.\nFurther, a human-in-the-loop training scheme is employed to augment ground\ntruth labels. These different approaches are evaluated in turn, and together\ngreatly improve the detection of both galactic structures and contaminants in\nLSB images.\n","authors":["Felix Richards","Adeline Paiement","Xianghua Xie","Elisabeth Sola","Pierre-Alain Duc"],"pdf_url":"https://arxiv.org/pdf/2407.07494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07493v1","updated":"2024-07-10T09:24:53Z","published":"2024-07-10T09:24:53Z","title":"Deformable-Heatmap-Segmentation for Automobile Visual Perception","summary":"  Semantic segmentation of road elements in 2D images is a crucial task in the\nrecognition of some static objects such as lane lines and free space. In this\npaper, we propose DHSNet,which extracts the objects features with a end-to-end\narchitecture along with a heatmap proposal. Deformable convolutions are also\nutilized in the proposed network. The DHSNet finely combines low-level feature\nmaps with high-level ones by using upsampling operators as well as downsampling\noperators in a U-shape manner. Besides, DHSNet also aims to capture static\nobjects of various shapes and scales. We also predict a proposal heatmap to\ndetect the proposal points for more accurate target aiming in the network.\n","authors":["Hongyu Jin"],"pdf_url":"https://arxiv.org/pdf/2407.07493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07492v1","updated":"2024-07-10T09:24:50Z","published":"2024-07-10T09:24:50Z","title":"Fine-Grained Classification for Poisonous Fungi Identification with\n  Transfer Learning","summary":"  FungiCLEF 2024 addresses the fine-grained visual categorization (FGVC) of\nfungi species, with a focus on identifying poisonous species. This task is\nchallenging due to the size and class imbalance of the dataset, subtle\ninter-class variations, and significant intra-class variability amongst\nsamples. In this paper, we document our approach in tackling this challenge\nthrough the use of ensemble classifier heads on pre-computed image embeddings.\nOur team (DS@GT) demonstrate that state-of-the-art self-supervised vision\nmodels can be utilized as robust feature extractors for downstream application\nof computer vision tasks without the need for task-specific fine-tuning on the\nvision backbone. Our approach achieved the best Track 3 score (0.345), accuracy\n(78.4%) and macro-F1 (0.577) on the private test set in post competition\nevaluation. Our code is available at\nhttps://github.com/dsgt-kaggle-clef/fungiclef-2024.\n","authors":["Christopher Chiu","Maximilian Heil","Teresa Kim","Anthony Miyaguchi"],"pdf_url":"https://arxiv.org/pdf/2407.07492v1.pdf","comment":"Submitted and accepted into CLEF 2024 CEUR-WS proceedings"},{"id":"http://arxiv.org/abs/2406.18586v2","updated":"2024-07-10T09:24:19Z","published":"2024-06-06T09:06:42Z","title":"Cut-and-Paste with Precision: a Content and Perspective-aware Data\n  Augmentation for Road Damage Detection","summary":"  Damage to road pavement can develop into cracks, potholes, spallings, and\nother issues posing significant challenges to the integrity, safety, and\ndurability of the road structure. Detecting and monitoring the evolution of\nthese damages is crucial for maintaining the condition and structural health of\nroad infrastructure. In recent years, researchers have explored various\ndata-driven methods for image-based damage detection in road monitoring\napplications. The field gained attention with the introduction of the Road\nDamage Detection Challenge (RDDC2018), encouraging competition in developing\nobject detectors on street-view images from various countries. Leading teams\nhave demonstrated the effectiveness of ensemble models, mostly based on the\nYOLO and Faster R-CNN series. Data augmentations have also shown benefits in\nobject detection within the computer vision field, including transformations\nsuch as random flipping, cropping, cutting out patches, as well as\ncut-and-pasting object instances. Applying cut-and-paste augmentation to road\ndamages appears to be a promising approach to increase data diversity. However,\nthe standard cut-and-paste technique, which involves sampling an object\ninstance from a random image and pasting it at a random location onto the\ntarget image, has demonstrated limited effectiveness for road damage detection.\nThis method overlooks the location of the road and disregards the difference in\nperspective between the sampled damage and the target image, resulting in\nunrealistic augmented images. In this work, we propose an improved\nCut-and-Paste augmentation technique that is both content-aware (i.e. considers\nthe true location of the road in the image) and perspective-aware (i.e. takes\ninto account the difference in perspective between the injected damage and the\ntarget image).\n","authors":["Punnawat Siripathitti","Florent Forest","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2406.18586v2.pdf","comment":"Extended abstract accepted at ESREL 2024. 2 pages"},{"id":"http://arxiv.org/abs/2407.07488v1","updated":"2024-07-10T09:23:55Z","published":"2024-07-10T09:23:55Z","title":"FUNAvg: Federated Uncertainty Weighted Averaging for Datasets with\n  Diverse Labels","summary":"  Federated learning is one popular paradigm to train a joint model in a\ndistributed, privacy-preserving environment. But partial annotations pose an\nobstacle meaning that categories of labels are heterogeneous over clients. We\npropose to learn a joint backbone in a federated manner, while each site\nreceives its own multi-label segmentation head. By using Bayesian techniques we\nobserve that the different segmentation heads although only trained on the\nindividual client's labels also learn information about the other labels not\npresent at the respective site. This information is encoded in their predictive\nuncertainty. To obtain a final prediction we leverage this uncertainty and\nperform a weighted averaging of the ensemble of distributed segmentation heads,\nwhich allows us to segment \"locally unknown\" structures. With our method, which\nwe refer to as FUNAvg, we are even on-par with the models trained and tested on\nthe same dataset on average. The code is publicly available at\nhttps://github.com/Cardio-AI/FUNAvg.\n","authors":["Malte TÃ¶lle","Fernando Navarro","Sebastian Eble","Ivo Wolf","Bjoern Menze","Sandy Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2407.07488v1.pdf","comment":"Accepted at MICCAI24"},{"id":"http://arxiv.org/abs/2407.02165v2","updated":"2024-07-10T09:20:39Z","published":"2024-07-02T11:17:48Z","title":"WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation","summary":"  Existing human datasets for avatar creation are typically limited to\nlaboratory environments, wherein high-quality annotations (e.g., SMPL\nestimation from 3D scans or multi-view images) can be ideally provided.\nHowever, their annotating requirements are impractical for real-world images or\nvideos, posing challenges toward real-world applications on current avatar\ncreation methods. To this end, we propose the WildAvatar dataset, a web-scale\nin-the-wild human avatar creation dataset extracted from YouTube, with\n$10,000+$ different human subjects and scenes. WildAvatar is at least\n$10\\times$ richer than previous datasets for 3D human avatar creation. We\nevaluate several state-of-the-art avatar creation methods on our dataset,\nhighlighting the unexplored challenges in real-world applications on avatar\ncreation. We also demonstrate the potential for generalizability of avatar\ncreation methods, when provided with data at scale. We publicly release our\ndata source links and annotations, to push forward 3D human avatar creation and\nother related fields for real-world applications.\n","authors":["Zihao Huang","Shoukang Hu","Guangcong Wang","Tianqi Liu","Yuhang Zang","Zhiguo Cao","Wei Li","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.02165v2.pdf","comment":"Project page: https://wildavatar.github.io/"},{"id":"http://arxiv.org/abs/2407.01230v2","updated":"2024-07-10T09:19:44Z","published":"2024-07-01T12:22:16Z","title":"DaBiT: Depth and Blur informed Transformer for Joint Refocusing and\n  Super-Resolution","summary":"  In many real-world scenarios, recorded videos suffer from accidental focus\nblur, and while video deblurring methods exist, most specifically target motion\nblur. This paper introduces a framework optimised for the joint task of focal\ndeblurring (refocusing) and video super-resolution (VSR). The proposed method\nemploys novel map guided transformers, in addition to image propagation, to\neffectively leverage the continuous spatial variance of focal blur and restore\nthe footage. We also introduce a flow re-focusing module to efficiently align\nrelevant features between the blurry and sharp domains. Additionally, we\npropose a novel technique for generating synthetic focal blur data, broadening\nthe model's learning capabilities to include a wider array of content. We have\nmade a new benchmark dataset, DAVIS-Blur, available. This dataset, a modified\nextension of the popular DAVIS video segmentation set, provides realistic\nout-of-focus blur degradations as well as the corresponding blur maps.\nComprehensive experiments on DAVIS-Blur demonstrate the superiority of our\napproach. We achieve state-of-the-art results with an average PSNR performance\nover 1.9dB greater than comparable existing video restoration methods. Our\nsource code will be made available at https://github.com/crispianm/DaBiT\n","authors":["Crispian Morris","Nantheera Anantrasirichai","Fan Zhang","David Bull"],"pdf_url":"https://arxiv.org/pdf/2407.01230v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07485v1","updated":"2024-07-10T09:16:14Z","published":"2024-07-10T09:16:14Z","title":"Zero-Shot Class Unlearning in CLIP with Synthetic Samples","summary":"  Machine unlearning is a crucial area of research. It is driven by the need to\nremove sensitive information from models to safeguard individuals' right to be\nforgotten under rigorous regulations such as GDPR. In this work, we focus on\nunlearning within CLIP, a dual vision-language encoder model trained on a\nmassive dataset of image-text pairs using contrastive loss. To achieve\nforgetting we expand the application of Lipschitz regularization to the\nmultimodal context of CLIP. Specifically, we ensure the smoothing of both\nvisual and textual embeddings associated with the class intended to be\nforgotten relative to the perturbation introduced to the samples from that\nclass. Additionally, importantly, we remove the necessity for real forgetting\ndata by generating synthetic samples through gradient ascent maximizing the\ntarget class. Our forgetting procedure is iterative, where we track accuracy on\na synthetic forget set and stop when accuracy falls below a chosen threshold.\nWe employ a selective layers update strategy based on their average absolute\ngradient value to mitigate over-forgetting. We validate our approach on several\nstandard datasets and provide thorough ablation analysis and comparisons with\nprevious work.\n","authors":["A. Kravets","V. Namboodiri"],"pdf_url":"https://arxiv.org/pdf/2407.07485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07479v1","updated":"2024-07-10T09:10:01Z","published":"2024-07-10T09:10:01Z","title":"How to Make Cross Encoder a Good Teacher for Efficient Image-Text\n  Retrieval?","summary":"  Dominant dual-encoder models enable efficient image-text retrieval but suffer\nfrom limited accuracy while the cross-encoder models offer higher accuracy at\nthe expense of efficiency. Distilling cross-modality matching knowledge from\ncross-encoder to dual-encoder provides a natural approach to harness their\nstrengths. Thus we investigate the following valuable question: how to make\ncross-encoder a good teacher for dual-encoder? Our findings are threefold:(1)\nCross-modal similarity score distribution of cross-encoder is more concentrated\nwhile the result of dual-encoder is nearly normal making vanilla logit\ndistillation less effective. However ranking distillation remains practical as\nit is not affected by the score distribution.(2) Only the relative order\nbetween hard negatives conveys valid knowledge while the order information\nbetween easy negatives has little significance.(3) Maintaining the coordination\nbetween distillation loss and dual-encoder training loss is beneficial for\nknowledge transfer. Based on these findings we propose a novel Contrastive\nPartial Ranking Distillation (CPRD) method which implements the objective of\nmimicking relative order between hard negative samples with contrastive\nlearning. This approach coordinates with the training of the dual-encoder\neffectively transferring valid knowledge from the cross-encoder to the\ndual-encoder. Extensive experiments on image-text retrieval and ranking tasks\nshow that our method surpasses other distillation methods and significantly\nimproves the accuracy of dual-encoder.\n","authors":["Yuxin Chen","Zongyang Ma","Ziqi Zhang","Zhongang Qi","Chunfeng Yuan","Bing Li","Junfu Pu","Ying Shan","Xiaojuan Qi","Weiming Hu"],"pdf_url":"https://arxiv.org/pdf/2407.07479v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2407.07478v1","updated":"2024-07-10T09:09:58Z","published":"2024-07-10T09:09:58Z","title":"EA-VTR: Event-Aware Video-Text Retrieval","summary":"  Understanding the content of events occurring in the video and their inherent\ntemporal logic is crucial for video-text retrieval. However, web-crawled\npre-training datasets often lack sufficient event information, and the widely\nadopted video-level cross-modal contrastive learning also struggles to capture\ndetailed and complex video-text event alignment. To address these challenges,\nwe make improvements from both data and model perspectives. In terms of\npre-training data, we focus on supplementing the missing specific event content\nand event temporal transitions with the proposed event augmentation strategies.\nBased on the event-augmented data, we construct a novel Event-Aware Video-Text\nRetrieval model, ie, EA-VTR, which achieves powerful video-text retrieval\nability through superior video event awareness. EA-VTR can efficiently encode\nframe-level and video-level visual representations simultaneously, enabling\ndetailed event content and complex event temporal cross-modal alignment,\nultimately enhancing the comprehensive understanding of video events. Our\nmethod not only significantly outperforms existing approaches on multiple\ndatasets for Text-to-Video Retrieval and Video Action Recognition tasks, but\nalso demonstrates superior event content perceive ability on Multi-event\nVideo-Text Retrieval and Video Moment Retrieval tasks, as well as outstanding\nevent temporal logic understanding ability on Test of Time task.\n","authors":["Zongyang Ma","Ziqi Zhang","Yuxin Chen","Zhongang Qi","Chunfeng Yuan","Bing Li","Yingmin Luo","Xu Li","Xiaojuan Qi","Ying Shan","Weiming Hu"],"pdf_url":"https://arxiv.org/pdf/2407.07478v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2403.06378v2","updated":"2024-07-10T09:05:54Z","published":"2024-03-11T02:05:31Z","title":"Eliminating Warping Shakes for Unsupervised Online Video Stitching","summary":"  In this paper, we retarget video stitching to an emerging issue, named\nwarping shake, when extending image stitching to video stitching. It unveils\nthe temporal instability of warped content in non-overlapping regions, despite\nimage stitching having endeavored to preserve the natural structures.\nTherefore, in most cases, even if the input videos to be stitched are stable,\nthe stitched video will inevitably cause undesired warping shakes and affect\nthe visual experience. To eliminate the shakes, we propose StabStitch to\nsimultaneously realize video stitching and video stabilization in a unified\nunsupervised learning framework. Starting from the camera paths in video\nstabilization, we first derive the expression of stitching trajectories in\nvideo stitching by elaborately integrating spatial and temporal warps. Then a\nwarp smoothing model is presented to optimize them with a comprehensive\nconsideration regarding content alignment, trajectory smoothness, spatial\nconsistency, and online collaboration. To establish an evaluation benchmark and\ntrain the learning framework, we build a video stitching dataset with a rich\ndiversity in camera motions and scenes. Compared with existing stitching\nsolutions, StabStitch exhibits significant superiority in scene robustness and\ninference speed in addition to stitching and stabilization performance,\ncontributing to a robust and real-time online video stitching system. The code\nand dataset are available at https://github.com/nie-lang/StabStitch.\n","authors":["Lang Nie","Chunyu Lin","Kang Liao","Yun Zhang","Shuaicheng Liu","Rui Ai","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.06378v2.pdf","comment":"Aceepted to ECCV2024"},{"id":"http://arxiv.org/abs/2405.14224v2","updated":"2024-07-10T09:02:11Z","published":"2024-05-23T06:53:18Z","title":"DiM: Diffusion Mamba for Efficient High-Resolution Image Synthesis","summary":"  Diffusion models have achieved great success in image generation, with the\nbackbone evolving from U-Net to Vision Transformers. However, the computational\ncost of Transformers is quadratic to the number of tokens, leading to\nsignificant challenges when dealing with high-resolution images. In this work,\nwe propose Diffusion Mamba (DiM), which combines the efficiency of Mamba, a\nsequence model based on State Space Models (SSM), with the expressive power of\ndiffusion models for efficient high-resolution image synthesis. To address the\nchallenge that Mamba cannot generalize to 2D signals, we make several\narchitecture designs including multi-directional scans, learnable padding\ntokens at the end of each row and column, and lightweight local feature\nenhancement. Our DiM architecture achieves inference-time efficiency for\nhigh-resolution images. In addition, to further improve training efficiency for\nhigh-resolution image generation with DiM, we investigate \"weak-to-strong\"\ntraining strategy that pretrains DiM on low-resolution images ($256\\times 256$)\nand then finetune it on high-resolution images ($512 \\times 512$). We further\nexplore training-free upsampling strategies to enable the model to generate\nhigher-resolution images (e.g., $1024\\times 1024$ and $1536\\times 1536$)\nwithout further fine-tuning. Experiments demonstrate the effectiveness and\nefficiency of our DiM. The code of our work is available here:\n{\\url{https://github.com/tyshiwo1/DiM-DiffusionMamba/}}.\n","authors":["Yao Teng","Yue Wu","Han Shi","Xuefei Ning","Guohao Dai","Yu Wang","Zhenguo Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2405.14224v2.pdf","comment":"The code of our work is available here:\n  {\\url{https://github.com/tyshiwo1/DiM-DiffusionMamba/}}"},{"id":"http://arxiv.org/abs/2403.10854v2","updated":"2024-07-10T08:55:10Z","published":"2024-03-16T08:30:45Z","title":"A Comprehensive Study of Multimodal Large Language Models for Image\n  Quality Assessment","summary":"  While Multimodal Large Language Models (MLLMs) have experienced significant\nadvancement in visual understanding and reasoning, their potential to serve as\npowerful, flexible, interpretable, and text-driven models for Image Quality\nAssessment (IQA) remains largely unexplored. In this paper, we conduct a\ncomprehensive and systematic study of prompting MLLMs for IQA. We first\ninvestigate nine prompting systems for MLLMs as the combinations of three\nstandardized testing procedures in psychophysics (i.e., the single-stimulus,\ndouble-stimulus, and multiple-stimulus methods) and three popular prompting\nstrategies in natural language processing (i.e., the standard, in-context, and\nchain-of-thought prompting). We then present a difficult sample selection\nprocedure, taking into account sample diversity and uncertainty, to further\nchallenge MLLMs equipped with the respective optimal prompting systems. We\nassess three open-source and one closed-source MLLMs on several visual\nattributes of image quality (e.g., structural and textural distortions,\ngeometric transformations, and color differences) in both full-reference and\nno-reference scenarios. Experimental results show that only the closed-source\nGPT-4V provides a reasonable account for human perception of image quality, but\nis weak at discriminating fine-grained quality variations (e.g., color\ndifferences) and at comparing visual quality of multiple images, tasks humans\ncan perform effortlessly.\n","authors":["Tianhe Wu","Kede Ma","Jie Liang","Yujiu Yang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.10854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07468v1","updated":"2024-07-10T08:52:56Z","published":"2024-07-10T08:52:56Z","title":"Rethinking Few-shot Class-incremental Learning: Learning from Yourself","summary":"  Few-shot class-incremental learning (FSCIL) aims to learn sequential classes\nwith limited samples in a few-shot fashion. Inherited from the classical\nclass-incremental learning setting, the popular benchmark of FSCIL uses\naveraged accuracy (aAcc) and last-task averaged accuracy (lAcc) as the\nevaluation metrics. However, we reveal that such evaluation metrics may not\nprovide adequate emphasis on the novel class performance, and the continual\nlearning ability of FSCIL methods could be ignored under this benchmark. In\nthis work, as a complement to existing metrics, we offer a new metric called\ngeneralized average accuracy (gAcc) which is designed to provide an extra\nequitable evaluation by incorporating different perspectives of the performance\nunder the guidance of a parameter $\\alpha$. We also present an overall metric\nin the form of the area under the curve (AUC) along the $\\alpha$. Under the\nguidance of gAcc, we release the potential of intermediate features of the\nvision transformers to boost the novel-class performance. Taking information\nfrom intermediate layers which are less class-specific and more generalizable,\nwe manage to rectify the final features, leading to a more generalizable\ntransformer-based FSCIL framework. Without complex network designs or\ncumbersome training procedures, our method outperforms existing FSCIL methods\nat aAcc and gAcc on three datasets. See codes at\nhttps://github.com/iSEE-Laboratory/Revisting_FSCIL\n","authors":["Yu-Ming Tang","Yi-Xing Peng","Jingke Meng","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.07468v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07465v1","updated":"2024-07-10T08:46:29Z","published":"2024-07-10T08:46:29Z","title":"Exploring the Untouched Sweeps for Conflict-Aware 3D Segmentation\n  Pretraining","summary":"  LiDAR-camera 3D representation pretraining has shown significant promise for\n3D perception tasks and related applications. However, two issues widely exist\nin this framework: 1) Solely keyframes are used for training. For example, in\nnuScenes, a substantial quantity of unpaired LiDAR and camera frames remain\nunutilized, limiting the representation capabilities of the pretrained network.\n2) The contrastive loss erroneously distances points and image regions with\nidentical semantics but from different frames, disturbing the semantic\nconsistency of the learned presentations. In this paper, we propose a novel\nVision-Foundation-Model-driven sample exploring module to meticulously select\nLiDAR-Image pairs from unexplored frames, enriching the original training set.\nWe utilized timestamps and the semantic priors from VFMs to identify\nwell-synchronized training pairs and to discover samples with diverse content.\nMoreover, we design a cross- and intra-modal conflict-aware contrastive loss\nusing the semantic mask labels of VFMs to avoid contrasting semantically\nsimilar points and image regions. Our method consistently outperforms existing\nstate-of-the-art pretraining frameworks across three major public autonomous\ndriving datasets: nuScenes, SemanticKITTI, and Waymo on 3D semantic\nsegmentation by +3.0\\%, +3.0\\%, and +3.3\\% in mIoU, respectively. Furthermore,\nour approach exhibits adaptable generalization to different 3D backbones and\ntypical semantic masks generated by non-VFM models.\n","authors":["Tianfang Sun","Zhizhong Zhang","Xin Tan","Yanyun Qu","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2407.07465v1.pdf","comment":"preprint, version 1"},{"id":"http://arxiv.org/abs/2312.03341v2","updated":"2024-07-10T08:46:19Z","published":"2023-12-06T08:26:26Z","title":"Online Vectorized HD Map Construction using Geometry","summary":"  The construction of online vectorized High-Definition (HD) maps is critical\nfor downstream prediction and planning. Recent efforts have built strong\nbaselines for this task, however, shapes and relations of instances in urban\nroad systems are still under-explored, such as parallelism, perpendicular, or\nrectangle-shape. In our work, we propose GeMap ($\\textbf{Ge}$ometry\n$\\textbf{Map}$), which end-to-end learns Euclidean shapes and relations of map\ninstances beyond basic perception. Specifically, we design a geometric loss\nbased on angle and distance clues, which is robust to rigid transformations. We\nalso decouple self-attention to independently handle Euclidean shapes and\nrelations. Our method achieves new state-of-the-art performance on the NuScenes\nand Argoverse 2 datasets. Remarkably, it reaches a 71.8% mAP on the large-scale\nArgoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP\nthreshold for the first time. Code is available at\nhttps://github.com/cnzzx/GeMap.\n","authors":["Zhixin Zhang","Yiyuan Zhang","Xiaohan Ding","Fusheng Jin","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2312.03341v2.pdf","comment":"ECCV 2024, Project page: https://invictus717.github.io/GeMap/"},{"id":"http://arxiv.org/abs/2407.07464v1","updated":"2024-07-10T08:40:39Z","published":"2024-07-10T08:40:39Z","title":"Video-to-Audio Generation with Hidden Alignment","summary":"  Generating semantically and temporally aligned audio content in accordance\nwith video input has become a focal point for researchers, particularly\nfollowing the remarkable breakthrough in text-to-video generation. In this\nwork, we aim to offer insights into the video-to-audio generation paradigm,\nfocusing on three crucial aspects: vision encoders, auxiliary embeddings, and\ndata augmentation techniques. Beginning with a foundational model VTA-LDM built\non a simple yet surprisingly effective intuition, we explore various vision\nencoders and auxiliary embeddings through ablation studies. Employing a\ncomprehensive evaluation pipeline that emphasizes generation quality and\nvideo-audio synchronization alignment, we demonstrate that our model exhibits\nstate-of-the-art video-to-audio generation capabilities. Furthermore, we\nprovide critical insights into the impact of different data augmentation\nmethods on enhancing the generation framework's overall capacity. We showcase\npossibilities to advance the challenge of generating synchronized audio from\nsemantic and temporal perspectives. We hope these insights will serve as a\nstepping stone toward developing more realistic and accurate audio-visual\ngeneration models.\n","authors":["Manjie Xu","Chenxing Li","Yong Ren","Rilin Chen","Yu Gu","Wei Liang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2407.07464v1.pdf","comment":"https://sites.google.com/view/vta-ldm"},{"id":"http://arxiv.org/abs/2312.02364v2","updated":"2024-07-10T08:39:59Z","published":"2023-12-04T21:46:21Z","title":"Class-Discriminative Attention Maps for Vision Transformers","summary":"  Importance estimators are explainability methods that quantify feature\nimportance for deep neural networks (DNN). In vision transformers (ViT), the\nself-attention mechanism naturally leads to attention maps, which are sometimes\nused as importance scores for which input features ViT models are focusing on.\nHowever, attention maps do not account for signals from downstream tasks. To\ngenerate explanations that are sensitive to downstream tasks, we have developed\nclass-discriminative attention maps (CDAM), a gradient-based extension that\nestimates feature importance with respect to a known class or a latent concept.\nCDAM scales attention scores by how relevant the corresponding tokens are for\nthe predictions of a classifier head. In addition to targeting the supervised\nclassifier, CDAM can explain an arbitrary concept shared by selected samples by\nmeasuring similarity in the latent space of ViT. Additionally, we introduce\nSmooth CDAM and Integrated CDAM, which average a series of CDAMs with slightly\naltered tokens. Our quantitative benchmarks include correctness, compactness,\nand class sensitivity, in comparison to six other importance estimators.\nVanilla, Smooth, and Integrated CDAM excel across all three benchmarks. In\nparticular, our results suggest that existing importance estimators may not\nprovide sufficient class-sensitivity. We demonstrate the utility of CDAM in\nmedical images by training and explaining malignancy and biomarker prediction\nmodels based on lung Computed Tomography (CT) scans. Overall, CDAM is shown to\nbe highly class-discriminative and semantically relevant, while providing\ncompact explanations.\n","authors":["Lennart Brocki","Jakub Binda","Neo Christopher Chung"],"pdf_url":"https://arxiv.org/pdf/2312.02364v2.pdf","comment":"Earlier version at 2024 International Joint Conference on Artificial\n  Intelligence (IJCAI) Workshop on Explainable Artificial Intelligence (XAI)"},{"id":"http://arxiv.org/abs/2407.07462v1","updated":"2024-07-10T08:32:26Z","published":"2024-07-10T08:32:26Z","title":"MAN TruckScenes: A multimodal dataset for autonomous trucking in diverse\n  conditions","summary":"  Autonomous trucking is a promising technology that can greatly impact modern\nlogistics and the environment. Ensuring its safety on public roads is one of\nthe main duties that requires an accurate perception of the environment. To\nachieve this, machine learning methods rely on large datasets, but to this day,\nno such datasets are available for autonomous trucks. In this work, we present\nMAN TruckScenes, the first multimodal dataset for autonomous trucking. MAN\nTruckScenes allows the research community to come into contact with\ntruck-specific challenges, such as trailer occlusions, novel sensor\nperspectives, and terminal environments for the first time. It comprises more\nthan 740 scenes of 20 s each within a multitude of different environmental\nconditions. The sensor set includes 4 cameras, 6 lidar, 6 radar sensors, 2\nIMUs, and a high-precision GNSS. The dataset's 3D bounding boxes were manually\nannotated and carefully reviewed to achieve a high quality standard. Bounding\nboxes are available for 27 object classes, 15 attributes, and a range of more\nthan 230 m. The scenes are tagged according to 34 distinct scene tags, and all\nobjects are tracked throughout the scene to promote a wide range of\napplications. Additionally, MAN TruckScenes is the first dataset to provide 4D\nradar data with 360{\\deg} coverage and is thereby the largest radar dataset\nwith annotated 3D bounding boxes. Finally, we provide extensive dataset\nanalysis and baseline results. The dataset, development kit and more are\navailable online.\n","authors":["Felix Fent","Fabian Kuttenreich","Florian Ruch","Farija Rizwin","Stefan Juergens","Lorenz Lechermann","Christian Nissler","Andrea Perl","Ulrich Voll","Min Yan","Markus Lienkamp"],"pdf_url":"https://arxiv.org/pdf/2407.07462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07461v1","updated":"2024-07-10T08:32:13Z","published":"2024-07-10T08:32:13Z","title":"Drantal-NeRF: Diffusion-Based Restoration for Anti-aliasing Neural\n  Radiance Field","summary":"  Aliasing artifacts in renderings produced by Neural Radiance Field (NeRF) is\na long-standing but complex issue in the field of 3D implicit representation,\nwhich arises from a multitude of intricate causes and was mitigated by\ndesigning more advanced but complex scene parameterization methods before. In\nthis paper, we present a Diffusion-based restoration method for anti-aliasing\nNeural Radiance Field (Drantal-NeRF). We consider the anti-aliasing issue from\na low-level restoration perspective by viewing aliasing artifacts as a kind of\ndegradation model added to clean ground truths. By leveraging the powerful\nprior knowledge encapsulated in diffusion model, we could restore the\nhigh-realism anti-aliasing renderings conditioned on aliased low-quality\ncounterparts. We further employ a feature-wrapping operation to ensure\nmulti-view restoration consistency and finetune the VAE decoder to better adapt\nto the scene-specific data distribution. Our proposed method is easy to\nimplement and agnostic to various NeRF backbones. We conduct extensive\nexperiments on challenging large-scale urban scenes as well as unbounded\n360-degree scenes and achieve substantial qualitative and quantitative\nimprovements.\n","authors":["Ganlin Yang","Kaidong Zhang","Jingjing Fu","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2407.07461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07223v2","updated":"2024-07-10T08:23:20Z","published":"2023-03-13T15:58:00Z","title":"PromptFusion: Decoupling Stability and Plasticity for Continual Learning","summary":"  Current research on continual learning mainly focuses on relieving\ncatastrophic forgetting, and most of their success is at the cost of limiting\nthe performance of newly incoming tasks. Such a trade-off is referred to as the\nstability-plasticity dilemma and is a more general and challenging problem for\ncontinual learning. However, the inherent conflict between these two concepts\nmakes it seemingly impossible to devise a satisfactory solution to both of them\nsimultaneously. Therefore, we ask, \"is it possible to divide them into two\nseparate problems to conquer them independently?\". To this end, we propose a\nprompt-tuning-based method termed PromptFusion to enable the decoupling of\nstability and plasticity. Specifically, PromptFusion consists of a carefully\ndesigned \\stab module that deals with catastrophic forgetting and a \\boo module\nto learn new knowledge concurrently. Furthermore, to address the computational\noverhead brought by the additional architecture, we propose PromptFusion-Lite\nwhich improves PromptFusion by dynamically determining whether to activate both\nmodules for each input image. Extensive experiments show that both PromptFusion\nand PromptFusion-Lite achieve promising results on popular continual learning\ndatasets for class-incremental and domain-incremental settings. Especially on\nSplit-Imagenet-R, one of the most challenging datasets for class-incremental\nlearning, our method can exceed state-of-the-art prompt-based methods by more\nthan 5\\% in accuracy, with PromptFusion-Lite using 14.8\\% less computational\nresources than PromptFusion.\n","authors":["Haoran Chen","Zuxuan Wu","Xintong Han","Menglin Jia","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.07223v2.pdf","comment":"ECCV 2024 camera-ready version"},{"id":"http://arxiv.org/abs/2403.09974v2","updated":"2024-07-10T08:20:56Z","published":"2024-03-15T02:40:13Z","title":"Unlocking the Multi-modal Potential of CLIP for Generalized Category\n  Discovery","summary":"  Given unlabelled datasets containing both old and new categories, generalized\ncategory discovery (GCD) aims to accurately discover new classes while\ncorrectly classifying old classes, leveraging the class concepts learned from\nlabeled samples. Current GCD methods only use a single visual modality of\ninformation, resulting in poor classification of visually similar classes. As a\ndifferent modality, text information can provide complementary discriminative\ninformation, which motivates us to introduce it into the GCD task. However, the\nlack of class names for unlabelled data makes it impractical to utilize text\ninformation. To tackle this challenging problem, in this paper, we propose a\nText Embedding Synthesizer (TES) to generate pseudo text embeddings for\nunlabelled samples. Specifically, our TES leverages the property that CLIP can\ngenerate aligned vision-language features, converting visual embeddings into\ntokens of the CLIP's text encoder to generate pseudo text embeddings. Besides,\nwe employ a dual-branch framework, through the joint learning and instance\nconsistency of different modality branches, visual and semantic information\nmutually enhance each other, promoting the interaction and fusion of visual and\ntext knowledge. Our method unlocks the multi-modal potentials of CLIP and\noutperforms the baseline methods by a large margin on all GCD benchmarks,\nachieving new state-of-the-art. The code will be released at\nhttps://github.com/enguangW/GET .\n","authors":["Enguang Wang","Zhimao Peng","Zhengyuan Xie","Fei Yang","Xialei Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.09974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07441v1","updated":"2024-07-10T07:53:24Z","published":"2024-07-10T07:53:24Z","title":"HAFormer: Unleashing the Power of Hierarchy-Aware Features for\n  Lightweight Semantic Segmentation","summary":"  Both Convolutional Neural Networks (CNNs) and Transformers have shown great\nsuccess in semantic segmentation tasks. Efforts have been made to integrate\nCNNs with Transformer models to capture both local and global context\ninteractions. However, there is still room for enhancement, particularly when\nconsidering constraints on computational resources. In this paper, we introduce\nHAFormer, a model that combines the hierarchical features extraction ability of\nCNNs with the global dependency modeling capability of Transformers to tackle\nlightweight semantic segmentation challenges. Specifically, we design a\nHierarchy-Aware Pixel-Excitation (HAPE) module for adaptive multi-scale local\nfeature extraction. During the global perception modeling, we devise an\nEfficient Transformer (ET) module streamlining the quadratic calculations\nassociated with traditional Transformers. Moreover, a correlation-weighted\nFusion (cwF) module selectively merges diverse feature representations,\nsignificantly enhancing predictive accuracy. HAFormer achieves high performance\nwith minimal computational overhead and compact model size, achieving 74.2\\%\nmIoU on Cityscapes and 71.1\\% mIoU on CamVid test datasets, with frame rates of\n105FPS and 118FPS on a single 2080Ti GPU. The source codes are available at\n\\textit{https://github.com/XU-GITHUB-curry/HAFormer}.\n","authors":["Guoan Xu","Wenjing Jia","Tao Wu","Ligeng Chen","Guangwei Gao"],"pdf_url":"https://arxiv.org/pdf/2407.07441v1.pdf","comment":"13 pages, 10 figures, 8 tables, accepted by IEEE Transactions on\n  Image Processing"},{"id":"http://arxiv.org/abs/2403.10153v2","updated":"2024-07-10T07:45:54Z","published":"2024-03-15T09:54:04Z","title":"Improving Medical Multi-modal Contrastive Learning with Expert\n  Annotations","summary":"  We introduce eCLIP, an enhanced version of the CLIP model that integrates\nexpert annotations in the form of radiologist eye-gaze heatmaps. It tackles key\nchallenges in contrastive multi-modal medical imaging analysis, notably data\nscarcity and the \"modality gap\" -- a significant disparity between image and\ntext embeddings that diminishes the quality of representations and hampers\ncross-modal interoperability. eCLIP integrates a heatmap processor and\nleverages mixup augmentation to efficiently utilize the scarce expert\nannotations, thus boosting the model's learning effectiveness. eCLIP is\ndesigned to be generally applicable to any variant of CLIP without requiring\nany modifications of the core architecture. Through detailed evaluations across\nseveral tasks, including zero-shot inference, linear probing, cross-modal\nretrieval, and Retrieval Augmented Generation (RAG) of radiology reports using\na frozen Large Language Model, eCLIP showcases consistent improvements in\nembedding quality. The outcomes reveal enhanced alignment and uniformity,\naffirming eCLIP's capability to harness high-quality annotations for enriched\nmulti-modal analysis in the medical imaging domain.\n","authors":["Yogesh Kumar","Pekka Marttinen"],"pdf_url":"https://arxiv.org/pdf/2403.10153v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2403.09176v2","updated":"2024-07-10T07:39:08Z","published":"2024-03-14T08:43:43Z","title":"Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse\n  Mixture-of-Experts","summary":"  Diffusion models have achieved remarkable success across a range of\ngenerative tasks. Recent efforts to enhance diffusion model architectures have\nreimagined them as a form of multi-task learning, where each task corresponds\nto a denoising task at a specific noise level. While these efforts have focused\non parameter isolation and task routing, they fall short of capturing detailed\ninter-task relationships and risk losing semantic information, respectively. In\nresponse, we introduce Switch Diffusion Transformer (Switch-DiT), which\nestablishes inter-task relationships between conflicting tasks without\ncompromising semantic information. To achieve this, we employ a sparse\nmixture-of-experts within each transformer block to utilize semantic\ninformation and facilitate handling conflicts in tasks through parameter\nisolation. Additionally, we propose a diffusion prior loss, encouraging similar\ntasks to share their denoising paths while isolating conflicting ones. Through\nthese, each transformer block contains a shared expert across all tasks, where\nthe common and task-specific denoising paths enable the diffusion model to\nconstruct its beneficial way of synergizing denoising tasks. Extensive\nexperiments validate the effectiveness of our approach in improving both image\nquality and convergence rate, and further analysis demonstrates that Switch-DiT\nconstructs tailored denoising paths across various generation scenarios.\n","authors":["Byeongjun Park","Hyojun Go","Jin-Young Kim","Sangmin Woo","Seokil Ham","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2403.09176v2.pdf","comment":"Project Page: https://byeongjun-park.github.io/Switch-DiT/"},{"id":"http://arxiv.org/abs/2407.07433v1","updated":"2024-07-10T07:37:20Z","published":"2024-07-10T07:37:20Z","title":"Controllable Navigation Instruction Generation with Chain of Thought\n  Prompting","summary":"  Instruction generation is a vital and multidisciplinary research area with\nbroad applications. Existing instruction generation models are limited to\ngenerating instructions in a single style from a particular dataset, and the\nstyle and content of generated instructions cannot be controlled. Moreover,\nmost existing instruction generation methods also disregard the spatial\nmodeling of the navigation environment. Leveraging the capabilities of Large\nLanguage Models (LLMs), we propose C-Instructor, which utilizes the\nchain-of-thought-style prompt for style-controllable and content-controllable\ninstruction generation. Firstly, we propose a Chain of Thought with Landmarks\n(CoTL) mechanism, which guides the LLM to identify key landmarks and then\ngenerate complete instructions. CoTL renders generated instructions more\naccessible to follow and offers greater controllability over the manipulation\nof landmark objects. Furthermore, we present a Spatial Topology Modeling Task\nto facilitate the understanding of the spatial structure of the environment.\nFinally, we introduce a Style-Mixed Training policy, harnessing the prior\nknowledge of LLMs to enable style control for instruction generation based on\ndifferent prompts within a single model instance. Extensive experiments\ndemonstrate that instructions generated by C-Instructor outperform those\ngenerated by previous methods in text metrics, navigation guidance evaluation,\nand user studies.\n","authors":["Xianghao Kong","Jinyu Chen","Wenguan Wang","Hang Su","Xiaolin Hu","Yi Yang","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2407.07433v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.01295v3","updated":"2024-07-10T07:33:58Z","published":"2024-07-01T13:47:54Z","title":"Formal Verification of Object Detection","summary":"  Deep Neural Networks (DNNs) are ubiquitous in real-world applications, yet\nthey remain vulnerable to errors and adversarial attacks. This work tackles the\nchallenge of applying formal verification to ensure the safety of computer\nvision models, extending verification beyond image classification to object\ndetection. We propose a general formulation for certifying the robustness of\nobject detection models using formal verification and outline implementation\nstrategies compatible with state-of-the-art verification tools. Our approach\nenables the application of these tools, originally designed for verifying\nclassification models, to object detection. We define various attacks for\nobject detection, illustrating the diverse ways adversarial inputs can\ncompromise neural network outputs. Our experiments, conducted on several common\ndatasets and networks, reveal potential errors in object detection models,\nhighlighting system vulnerabilities and emphasizing the need for expanding\nformal verification to these new domains. This work paves the way for further\nresearch in integrating formal verification across a broader range of computer\nvision applications.\n","authors":["Avraham Raviv","Yizhak Y. Elboher","Michelle Aluf-Medina","Yael Leibovich Weiss","Omer Cohen","Roy Assa","Guy Katz","Hillel Kugler"],"pdf_url":"https://arxiv.org/pdf/2407.01295v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07427v1","updated":"2024-07-10T07:30:51Z","published":"2024-07-10T07:30:51Z","title":"Unified Embedding Alignment for Open-Vocabulary Video Instance\n  Segmentation","summary":"  Open-Vocabulary Video Instance Segmentation (VIS) is attracting increasing\nattention due to its ability to segment and track arbitrary objects. However,\nthe recent Open-Vocabulary VIS attempts obtained unsatisfactory results,\nespecially in terms of generalization ability of novel categories. We discover\nthat the domain gap between the VLM features (e.g., CLIP) and the instance\nqueries and the underutilization of temporal consistency are two central\ncauses. To mitigate these issues, we design and train a novel Open-Vocabulary\nVIS baseline called OVFormer. OVFormer utilizes a lightweight module for\nunified embedding alignment between query embeddings and CLIP image embeddings\nto remedy the domain gap. Unlike previous image-based training methods, we\nconduct video-based model training and deploy a semi-online inference scheme to\nfully mine the temporal consistency in the video. Without bells and whistles,\nOVFormer achieves 21.9 mAP with a ResNet-50 backbone on LV-VIS, exceeding the\nprevious state-of-the-art performance by 7.7. Extensive experiments on some\nClose-Vocabulary VIS datasets also demonstrate the strong zero-shot\ngeneralization ability of OVFormer (+ 7.6 mAP on YouTube-VIS 2019, + 3.9 mAP on\nOVIS). Code is available at https://github.com/fanghaook/OVFormer.\n","authors":["Hao Fang","Peng Wu","Yawei Li","Xinxin Zhang","Xiankai Lu"],"pdf_url":"https://arxiv.org/pdf/2407.07427v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07412v1","updated":"2024-07-10T07:14:48Z","published":"2024-07-10T07:14:48Z","title":"Pseudo-RIS: Distinctive Pseudo-supervision Generation for Referring\n  Image Segmentation","summary":"  We propose a new framework that automatically generates high-quality\nsegmentation masks with their referring expressions as pseudo supervisions for\nreferring image segmentation (RIS). These pseudo supervisions allow the\ntraining of any supervised RIS methods without the cost of manual labeling. To\nachieve this, we incorporate existing segmentation and image captioning\nfoundation models, leveraging their broad generalization capabilities. However,\nthe naive incorporation of these models may generate non-distinctive\nexpressions that do not distinctively refer to the target masks. To address\nthis challenge, we propose two-fold strategies that generate distinctive\ncaptions: 1) 'distinctive caption sampling', a new decoding method for the\ncaptioning model, to generate multiple expression candidates with detailed\nwords focusing on the target. 2) 'distinctiveness-based text filtering' to\nfurther validate the candidates and filter out those with a low level of\ndistinctiveness. These two strategies ensure that the generated text\nsupervisions can distinguish the target from other objects, making them\nappropriate for the RIS annotations. Our method significantly outperforms both\nweakly and zero-shot SoTA methods on the RIS benchmark datasets. It also\nsurpasses fully supervised methods in unseen domains, proving its capability to\ntackle the open-world challenge within RIS. Furthermore, integrating our method\nwith human annotations yields further improvements, highlighting its potential\nin semi-supervised learning applications.\n","authors":["Seonghoon Yu","Paul Hongsuck Seo","Jeany Son"],"pdf_url":"https://arxiv.org/pdf/2407.07412v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.07410v1","updated":"2024-07-10T07:12:50Z","published":"2024-07-10T07:12:50Z","title":"Mutual Information calculation on different appearances","summary":"  Mutual information has many applications in image alignment and matching,\nmainly due to its ability to measure the statistical dependence between two\nimages, even if the two images are from different modalities (e.g., CT and\nMRI). It considers not only the pixel intensities of the images but also the\nspatial relationships between the pixels. In this project, we apply the mutual\ninformation formula to image matching, where image A is the moving object and\nimage B is the target object and calculate the mutual information between them\nto evaluate the similarity between the images. For comparison, we also used\nentropy and information-gain methods to test the dependency of the images. We\nalso investigated the effect of different environments on the mutual\ninformation of the same image and used experiments and plots to demonstrate.\n","authors":["Jiecheng Liao","Junhao Lu","Jeff Ji","Jiacheng He"],"pdf_url":"https://arxiv.org/pdf/2407.07410v1.pdf","comment":"demo for the work: elucidator.cn/demo-mi/"},{"id":"http://arxiv.org/abs/2407.07406v1","updated":"2024-07-10T07:07:58Z","published":"2024-07-10T07:07:58Z","title":"Weakly-supervised Medical Image Segmentation with Gaze Annotations","summary":"  Eye gaze that reveals human observational patterns has increasingly been\nincorporated into solutions for vision tasks. Despite recent explorations on\nleveraging gaze to aid deep networks, few studies exploit gaze as an efficient\nannotation approach for medical image segmentation which typically entails\nheavy annotating costs. In this paper, we propose to collect dense weak\nsupervision for medical image segmentation with a gaze annotation scheme. To\ntrain with gaze, we propose a multi-level framework that trains multiple\nnetworks from discriminative human attention, simulated with a set of\npseudo-masks derived by applying hierarchical thresholds on gaze heatmaps.\nFurthermore, to mitigate gaze noise, a cross-level consistency is exploited to\nregularize overfitting noisy labels, steering models toward clean patterns\nlearned by peer networks. The proposed method is validated on two public\nmedical datasets of polyp and prostate segmentation tasks. We contribute a\nhigh-quality gaze dataset entitled GazeMedSeg as an extension to the popular\nmedical segmentation datasets. To the best of our knowledge, this is the first\ngaze dataset for medical image segmentation. Our experiments demonstrate that\ngaze annotation outperforms previous label-efficient annotation schemes in\nterms of both performance and annotation time. Our collected gaze data and code\nare available at: https://github.com/med-air/GazeMedSeg.\n","authors":["Yuan Zhong","Chenhui Tang","Yumeng Yang","Ruoxi Qi","Kang Zhou","Yuqi Gong","Pheng Ann Heng","Janet H. Hsiao","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2407.07406v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2401.02717v2","updated":"2024-07-10T07:07:11Z","published":"2024-01-05T09:21:45Z","title":"Complementary Information Mutual Learning for Multimodality Medical\n  Image Segmentation","summary":"  Radiologists must utilize multiple modal images for tumor segmentation and\ndiagnosis due to the limitations of medical imaging and the diversity of tumor\nsignals. This leads to the development of multimodal learning in segmentation.\nHowever, the redundancy among modalities creates challenges for existing\nsubtraction-based joint learning methods, such as misjudging the importance of\nmodalities, ignoring specific modal information, and increasing cognitive load.\nThese thorny issues ultimately decrease segmentation accuracy and increase the\nrisk of overfitting. This paper presents the complementary information mutual\nlearning (CIML) framework, which can mathematically model and address the\nnegative impact of inter-modal redundant information. CIML adopts the idea of\naddition and removes inter-modal redundant information through inductive\nbias-driven task decomposition and message passing-based redundancy filtering.\nCIML first decomposes the multimodal segmentation task into multiple subtasks\nbased on expert prior knowledge, minimizing the information dependence between\nmodalities. Furthermore, CIML introduces a scheme in which each modality can\nextract information from other modalities additively through message passing.\nTo achieve non-redundancy of extracted information, the redundant filtering is\ntransformed into complementary information learning inspired by the variational\ninformation bottleneck. The complementary information learning procedure can be\nefficiently solved by variational inference and cross-modal spatial attention.\nNumerical results from the verification task and standard benchmarks indicate\nthat CIML efficiently removes redundant information between modalities,\noutperforming SOTA methods regarding validation accuracy and segmentation\neffect.\n","authors":["Chuyun Shen","Wenhao Li","Haoqing Chen","Xiaoling Wang","Fengping Zhu","Yuxin Li","Xiangfeng Wang","Bo Jin"],"pdf_url":"https://arxiv.org/pdf/2401.02717v2.pdf","comment":"35 pages, 18 figures"},{"id":"http://arxiv.org/abs/2403.14121v2","updated":"2024-07-10T07:01:39Z","published":"2024-03-21T04:24:49Z","title":"External Knowledge Enhanced 3D Scene Generation from Sketch","summary":"  Generating realistic 3D scenes is challenging due to the complexity of room\nlayouts and object geometries.We propose a sketch based knowledge enhanced\ndiffusion architecture (SEK) for generating customized, diverse, and plausible\n3D scenes. SEK conditions the denoising process with a hand-drawn sketch of the\ntarget scene and cues from an object relationship knowledge base. We first\nconstruct an external knowledge base containing object relationships and then\nleverage knowledge enhanced graph reasoning to assist our model in\nunderstanding hand-drawn sketches. A scene is represented as a combination of\n3D objects and their relationships, and then incrementally diffused to reach a\nGaussian distribution.We propose a 3D denoising scene transformer that learns\nto reverse the diffusion process, conditioned by a hand-drawn sketch along with\nknowledge cues, to regressively generate the scene including the 3D object\ninstances as well as their layout. Experiments on the 3D-FRONT dataset show\nthat our model improves FID, CKL by 17.41%, 37.18% in 3D scene generation and\nFID, KID by 19.12%, 20.06% in 3D scene completion compared to the nearest\ncompetitor DiffuScene.\n","authors":["Zijie Wu","Mingtao Feng","Yaonan Wang","He Xie","Weisheng Dong","Bo Miao","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2403.14121v2.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.07403v1","updated":"2024-07-10T06:57:58Z","published":"2024-07-10T06:57:58Z","title":"A Survey of Attacks on Large Vision-Language Models: Resources,\n  Advances, and Future Trends","summary":"  With the significant development of large models in recent years, Large\nVision-Language Models (LVLMs) have demonstrated remarkable capabilities across\na wide range of multimodal understanding and reasoning tasks. Compared to\ntraditional Large Language Models (LLMs), LVLMs present great potential and\nchallenges due to its closer proximity to the multi-resource real-world\napplications and the complexity of multi-modal processing. However, the\nvulnerability of LVLMs is relatively underexplored, posing potential security\nrisks in daily usage. In this paper, we provide a comprehensive review of the\nvarious forms of existing LVLM attacks. Specifically, we first introduce the\nbackground of attacks targeting LVLMs, including the attack preliminary, attack\nchallenges, and attack resources. Then, we systematically review the\ndevelopment of LVLM attack methods, such as adversarial attacks that manipulate\nmodel outputs, jailbreak attacks that exploit model vulnerabilities for\nunauthorized actions, prompt injection attacks that engineer the prompt type\nand pattern, and data poisoning that affects model training. Finally, we\ndiscuss promising research directions in the future. We believe that our survey\nprovides insights into the current landscape of LVLM vulnerabilities, inspiring\nmore researchers to explore and mitigate potential safety issues in LVLM\ndevelopments. The latest papers on LVLM attacks are continuously collected in\nhttps://github.com/liudaizong/Awesome-LVLM-Attack.\n","authors":["Daizong Liu","Mingyu Yang","Xiaoye Qu","Pan Zhou","Wei Hu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.07403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07402v1","updated":"2024-07-10T06:57:04Z","published":"2024-07-10T06:57:04Z","title":"ActionVOS: Actions as Prompts for Video Object Segmentation","summary":"  Delving into the realm of egocentric vision, the advancement of referring\nvideo object segmentation (RVOS) stands as pivotal in understanding human\nactivities. However, existing RVOS task primarily relies on static attributes\nsuch as object names to segment target objects, posing challenges in\ndistinguishing target objects from background objects and in identifying\nobjects undergoing state changes. To address these problems, this work proposes\na novel action-aware RVOS setting called ActionVOS, aiming at segmenting only\nactive objects in egocentric videos using human actions as a key language\nprompt. This is because human actions precisely describe the behavior of\nhumans, thereby helping to identify the objects truly involved in the\ninteraction and to understand possible state changes. We also build a method\ntailored to work under this specific setting. Specifically, we develop an\naction-aware labeling module with an efficient action-guided focal loss. Such\ndesigns enable ActionVOS model to prioritize active objects with existing\nreadily-available annotations. Experimental results on VISOR dataset reveal\nthat ActionVOS significantly reduces the mis-segmentation of inactive objects,\nconfirming that actions help the ActionVOS model understand objects'\ninvolvement. Further evaluations on VOST and VSCOS datasets show that the novel\nActionVOS setting enhances segmentation performance when encountering\nchallenging circumstances involving object state changes. We will make our\nimplementation available at https://github.com/ut-vision/ActionVOS.\n","authors":["Liangyang Ouyang","Ruicong Liu","Yifei Huang","Ryosuke Furuta","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2407.07402v1.pdf","comment":"This paper is accepted by ECCV2024. Code will be released at\n  https://github.com/ut-vision/ActionVOS"},{"id":"http://arxiv.org/abs/2403.06104v3","updated":"2024-07-10T06:49:28Z","published":"2024-03-10T06:15:42Z","title":"Debiased Noise Editing for Fair Medical Image Classification","summary":"  In the era of Foundation Models' (FMs) rising prominence in AI, our study\naddresses the challenge of biases in medical images while the model operates in\nblack-box (e.g., using FM API), particularly spurious correlations between\npixels and sensitive attributes. Traditional methods for bias mitigation face\nlimitations due to the restricted access to web-hosted FMs and difficulties in\naddressing the underlying bias encoded within the FM API. We propose a\nD(ebiased) N(oise) E(diting) strategy, termed DNE, which generates DNE noise to\nmask such spurious correlation. DNE is capable of mitigating bias both within\nthe FM API embedding and the images themselves. Furthermore, DNE is suitable\nfor both white-box and black-box FM APIs, where we introduced G(reedy)\n(Z)eroth-O(rder) (GeZO) optimization for it when the gradient is inaccessible\nin black-box APIs. Our whole pipeline enables fairness-aware image editing that\ncan be applied across various medical contexts without requiring direct model\nmanipulation or significant computational resources. Our empirical results\ndemonstrate the method's effectiveness in maintaining fairness and utility\nacross different patient groups and diseases. In the era of AI-driven medicine,\nthis work contributes to making healthcare diagnostics more equitable,\nshowcasing a practical solution for bias mitigation in pre-trained image FMs.\nOur code is provided at\nhttps://github.com/ubc-tea/DNE-foundation-model-fairness.\n","authors":["Ruinan Jin","Wenlong Deng","Minghui Chen","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2403.06104v3.pdf","comment":"13 pages, 3 figures. Accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2405.08786v2","updated":"2024-07-10T06:44:03Z","published":"2024-05-14T17:35:27Z","title":"Incorporating Clinical Guidelines through Adapting Multi-modal Large\n  Language Model for Prostate Cancer PI-RADS Scoring","summary":"  The Prostate Imaging Reporting and Data System (PI-RADS) is pivotal in the\ndiagnosis of clinically significant prostate cancer through MRI imaging.\nCurrent deep learning-based PI-RADS scoring methods often lack the\nincorporation of common PI-RADS clinical guideline~(PICG) utilized by\nradiologists, potentially compromising scoring accuracy. This paper introduces\na novel approach that adapts a multi-modal large language model (MLLM) to\nincorporate PICG into PI-RADS scoring model without additional annotations and\nnetwork parameters. We present a designed two-stage fine-tuning process aiming\nat adapting a MLLM originally trained on natural images to the MRI images while\neffectively integrating the PICG. Specifically, in the first stage, we develop\na domain adapter layer tailored for processing 3D MRI inputs and instruct the\nMLLM to differentiate MRI sequences. In the second stage, we translate PICG for\nguiding instructions from the model to generate PICG-guided image features.\nThrough such a feature distillation step, we align the scoring network's\nfeatures with the PICG-guided image features, which enables the model to\neffectively incorporate the PICG information. We develop our model on a public\ndataset and evaluate it on an in-house dataset. Experimental results\ndemonstrate that our approach effectively improves the performance of current\nscoring networks. Code is available at: https://github.com/med-air/PICG2scoring\n","authors":["Tiantian Zhang","Manxi Lin","Hongda Guo","Xiaofan Zhang","Ka Fung Peter Chiu","Aasa Feragen","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2405.08786v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04237v2","updated":"2024-07-10T06:41:49Z","published":"2024-07-05T03:43:08Z","title":"GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction","summary":"  We present GSD, a diffusion model approach based on Gaussian Splatting (GS)\nrepresentation for 3D object reconstruction from a single view. Prior works\nsuffer from inconsistent 3D geometry or mediocre rendering quality due to\nimproper representations. We take a step towards resolving these shortcomings\nby utilizing the recent state-of-the-art 3D explicit representation, Gaussian\nSplatting, and an unconditional diffusion model. This model learns to generate\n3D objects represented by sets of GS ellipsoids. With these strong generative\n3D priors, though learning unconditionally, the diffusion model is ready for\nview-guided reconstruction without further model fine-tuning. This is achieved\nby propagating fine-grained 2D features through the efficient yet flexible\nsplatting function and the guided denoising sampling process. In addition, a 2D\ndiffusion model is further employed to enhance rendering fidelity, and improve\nreconstructed GS quality by polishing and re-using the rendered images. The\nfinal reconstructed objects explicitly come with high-quality 3D structure and\ntexture, and can be efficiently rendered in arbitrary views. Experiments on the\nchallenging real-world CO3D dataset demonstrate the superiority of our\napproach. Project page: $\\href{https://yxmu.foo/GSD/}{\\text{this https URL}}$\n","authors":["Yuxuan Mu","Xinxin Zuo","Chuan Guo","Yilin Wang","Juwei Lu","Xiaofeng Wu","Songcen Xu","Peng Dai","Youliang Yan","Li Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.04237v2.pdf","comment":"Accepted for ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07395v1","updated":"2024-07-10T06:36:45Z","published":"2024-07-10T06:36:45Z","title":"Standard compliant video coding using low complexity, switchable neural\n  wrappers","summary":"  The proliferation of high resolution videos posts great storage and bandwidth\npressure on cloud video services, driving the development of next-generation\nvideo codecs. Despite great progress made in neural video coding, existing\napproaches are still far from economical deployment considering the complexity\nand rate-distortion performance tradeoff. To clear the roadblocks for neural\nvideo coding, in this paper we propose a new framework featuring standard\ncompatibility, high performance, and low decoding complexity. We employ a set\nof jointly optimized neural pre- and post-processors, wrapping a standard video\ncodec, to encode videos at different resolutions. The rate-distorion optimal\ndownsampling ratio is signaled to the decoder at the per-sequence level for\neach target rate. We design a low complexity neural post-processor architecture\nthat can handle different upsampling ratios. The change of resolution exploits\nthe spatial redundancy in high-resolution videos, while the neural wrapper\nfurther achieves rate-distortion performance improvement through end-to-end\noptimization with a codec proxy. Our light-weight post-processor architecture\nhas a complexity of 516 MACs / pixel, and achieves 9.3% BD-Rate reduction over\nVVC on the UVG dataset, and 6.4% on AOM CTC Class A1. Our approach has the\npotential to further advance the performance of the latest video coding\nstandards using neural processing with minimal added complexity.\n","authors":["Yueyu Hu","Chenhao Zhang","Onur G. Guleryuz","Debargha Mukherjee","Yao Wang"],"pdf_url":"https://arxiv.org/pdf/2407.07395v1.pdf","comment":"Accepted by IEEE ICIP 2024"},{"id":"http://arxiv.org/abs/2407.07392v1","updated":"2024-07-10T06:32:58Z","published":"2024-07-10T06:32:58Z","title":"Malicious Path Manipulations via Exploitation of Representation\n  Vulnerabilities of Vision-Language Navigation Systems","summary":"  Building on the unprecedented capabilities of large language models for\ncommand understanding and zero-shot recognition of multi-modal vision-language\ntransformers, visual language navigation (VLN) has emerged as an effective way\nto address multiple fundamental challenges toward a natural language interface\nto robot navigation. However, such vision-language models are inherently\nvulnerable due to the lack of semantic meaning of the underlying embedding\nspace. Using a recently developed gradient based optimization procedure, we\ndemonstrate that images can be modified imperceptibly to match the\nrepresentation of totally different images and unrelated texts for a\nvision-language model. Building on this, we develop algorithms that can\nadversarially modify a minimal number of images so that the robot will follow a\nroute of choice for commands that require a number of landmarks. We demonstrate\nthat experimentally using a recently proposed VLN system; for a given\nnavigation command, a robot can be made to follow drastically different routes.\nWe also develop an efficient algorithm to detect such malicious modifications\nreliably based on the fact that the adversarially modified images have much\nhigher sensitivity to added Gaussian noise than the original images.\n","authors":["Chashi Mahiul Islam","Shaeke Salman","Montasir Shams","Xiuwen Liu","Piyush Kumar"],"pdf_url":"https://arxiv.org/pdf/2407.07392v1.pdf","comment":"8 pages, 5 figures. This paper has been accepted for publication at\n  the IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2024"},{"id":"http://arxiv.org/abs/2407.07389v1","updated":"2024-07-10T06:28:25Z","published":"2024-07-10T06:28:25Z","title":"Greit-HRNet: Grouped Lightweight High-Resolution Network for Human Pose\n  Estimation","summary":"  As multi-scale features are necessary for human pose estimation tasks,\nhigh-resolution networks are widely applied.\n  To improve efficiency, lightweight modules are proposed to replace costly\npoint-wise convolutions in high-resolution networks, including channel\nweighting and spatial weighting methods.\n  However, they fail to maintain the consistency of weights and capture global\nspatial information.\n  To address these problems, we present a Grouped lightweight High-Resolution\nNetwork (Greit-HRNet), in which we propose a Greit block including a group\nmethod Grouped Channel Weighting (GCW) and a spatial weighting method Global\nSpatial Weighting (GSW).\n  GCW modules group conditional channel weighting to make weights stable and\nmaintain the high-resolution features with the deepening of the network, while\nGSW modules effectively extract global spatial information and exchange\ninformation across channels.\n  In addition, we apply the Large Kernel Attention (LKA) method to improve the\nwhole efficiency of our Greit-HRNet.\n  Our experiments on both MS-COCO and MPII human pose estimation datasets\ndemonstrate the superior performance of our Greit-HRNet, outperforming other\nstate-of-the-art lightweight networks.\n","authors":["Junjia Han"],"pdf_url":"https://arxiv.org/pdf/2407.07389v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.02202v2","updated":"2024-07-10T06:27:00Z","published":"2023-12-02T20:45:18Z","title":"Volumetric Rendering with Baked Quadrature Fields","summary":"  We propose a novel Neural Radiance Field (NeRF) representation for non-opaque\nscenes that enables fast inference by utilizing textured polygons. Despite the\nhigh-quality novel view rendering that NeRF provides, a critical limitation is\nthat it relies on volume rendering that can be computationally expensive and\ndoes not utilize the advancements in modern graphics hardware. Many existing\nmethods fall short when it comes to modelling volumetric effects as they rely\npurely on surface rendering. We thus propose to model the scene with polygons,\nwhich can then be used to obtain the quadrature points required to model\nvolumetric effects, and also their opacity and colour from the texture. To\nobtain such polygonal mesh, we train a specialized field whose zero-crossings\nwould correspond to the quadrature points when volume rendering, and perform\nmarching cubes on this field. We then perform ray-tracing and utilize the\nray-tracing shader to obtain the final colour image. Our method allows an easy\nintegration with existing graphics frameworks allowing rendering speed of over\n100 frames-per-second for a $1920\\times1080$ image, while still being able to\nrepresent non-opaque objects.\n","authors":["Gopal Sharma","Daniel Rebain","Kwang Moo Yi","Andrea Tagliasacchi"],"pdf_url":"https://arxiv.org/pdf/2312.02202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05393v3","updated":"2024-07-10T06:26:35Z","published":"2024-04-08T10:52:29Z","title":"PAT: Pixel-wise Adaptive Training for Long-tailed Segmentation","summary":"  Beyond class frequency, we recognize the impact of class-wise relationships\namong various class-specific predictions and the imbalance in label masks on\nlong-tailed segmentation learning. To address these challenges, we propose an\ninnovative Pixel-wise Adaptive Training (PAT) technique tailored for\nlong-tailed segmentation. PAT has two key features: 1) class-wise gradient\nmagnitude homogenization, and 2) pixel-wise class-specific loss adaptation\n(PCLA). First, the class-wise gradient magnitude homogenization helps alleviate\nthe imbalance among label masks by ensuring equal consideration of the\nclass-wise impact on model updates. Second, PCLA tackles the detrimental impact\nof both rare classes within the long-tailed distribution and inaccurate\npredictions from previous training stages by encouraging learning classes with\nlow prediction confidence and guarding against forgetting classes with high\nconfidence. This combined approach fosters robust learning while preventing the\nmodel from forgetting previously learned knowledge. PAT exhibits significant\nperformance improvements, surpassing the current state-of-the-art by 2.2% in\nthe NyU dataset. Moreover, it enhances overall pixel-wise accuracy by 2.85% and\nintersection over union value by 2.07%, with a particularly notable declination\nof 0.39% in detecting rare classes compared to Balance Logits Variation, as\ndemonstrated on the three popular datasets, i.e., OxfordPetIII, CityScape, and\nNYU.\n","authors":["Khoi Do","Duong Nguyen","Nguyen H. Tran","Viet Dung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.05393v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18459v3","updated":"2024-07-10T06:18:13Z","published":"2024-06-26T16:10:31Z","title":"DiffuseHigh: Training-free Progressive High-Resolution Image Synthesis\n  through Structure Guidance","summary":"  Recent surge in large-scale generative models has spurred the development of\nvast fields in computer vision. In particular, text-to-image diffusion models\nhave garnered widespread adoption across diverse domain due to their potential\nfor high-fidelity image generation. Nonetheless, existing large-scale diffusion\nmodels are confined to generate images of up to 1K resolution, which is far\nfrom meeting the demands of contemporary commercial applications. Directly\nsampling higher-resolution images often yields results marred by artifacts such\nas object repetition and distorted shapes. Addressing the aforementioned issues\ntypically necessitates training or fine-tuning models on higher resolution\ndatasets. However, this undertaking poses a formidable challenge due to the\ndifficulty in collecting large-scale high-resolution contents and substantial\ncomputational resources. While several preceding works have proposed\nalternatives, they often fail to produce convincing results. In this work, we\nprobe the generative ability of diffusion models at higher resolution beyond\nits original capability and propose a novel progressive approach that fully\nutilizes generated low-resolution image to guide the generation of higher\nresolution image. Our method obviates the need for additional training or\nfine-tuning which significantly lowers the burden of computational costs.\nExtensive experiments and results validate the efficiency and efficacy of our\nmethod. Project page: https://yhyun225.github.io/DiffusHigh/\n","authors":["Younghyun Kim","Geunmin Hwang","Junyu Zhang","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2406.18459v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04326v2","updated":"2024-07-10T05:52:25Z","published":"2024-07-05T07:55:06Z","title":"LMSeg: A deep graph message-passing network for efficient and accurate\n  semantic segmentation of large-scale 3D landscape meshes","summary":"  Semantic segmentation of large-scale 3D landscape meshes is pivotal for\nvarious geospatial applications, including spatial analysis, automatic mapping\nand localization of target objects, and urban planning and development. This\nrequires an efficient and accurate 3D perception system to understand and\nanalyze real-world environments. However, traditional mesh segmentation methods\nface challenges in accurately segmenting small objects and maintaining\ncomputational efficiency due to the complexity and large size of 3D landscape\nmesh datasets. This paper presents an end-to-end deep graph message-passing\nnetwork, LMSeg, designed to efficiently and accurately perform semantic\nsegmentation on large-scale 3D landscape meshes. The proposed approach takes\nthe barycentric dual graph of meshes as inputs and applies deep message-passing\nneural networks to hierarchically capture the geometric and spatial features\nfrom the barycentric graph structures and learn intricate semantic information\nfrom textured meshes. The hierarchical and local pooling of the barycentric\ngraph, along with the effective geometry aggregation modules of LMSeg, enable\nfast inference and accurate segmentation of small-sized and irregular mesh\nobjects in various complex landscapes. Extensive experiments on two benchmark\ndatasets (natural and urban landscapes) demonstrate that LMSeg significantly\noutperforms existing learning-based segmentation methods in terms of object\nsegmentation accuracy and computational efficiency. Furthermore, our method\nexhibits strong generalization capabilities across diverse landscapes and\ndemonstrates robust resilience against varying mesh densities and landscape\ntopologies.\n","authors":["Zexian Huang","Kourosh Khoshelham","Gunditj Mirring Traditional Owners Corporation","Martin Tomko"],"pdf_url":"https://arxiv.org/pdf/2407.04326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04621v4","updated":"2024-07-10T05:35:48Z","published":"2024-07-05T16:27:00Z","title":"OneRestore: A Universal Restoration Framework for Composite Degradation","summary":"  In real-world scenarios, image impairments often manifest as composite\ndegradations, presenting a complex interplay of elements such as low light,\nhaze, rain, and snow. Despite this reality, existing restoration methods\ntypically target isolated degradation types, thereby falling short in\nenvironments where multiple degrading factors coexist. To bridge this gap, our\nstudy proposes a versatile imaging model that consolidates four physical\ncorruption paradigms to accurately represent complex, composite degradation\nscenarios. In this context, we propose OneRestore, a novel transformer-based\nframework designed for adaptive, controllable scene restoration. The proposed\nframework leverages a unique cross-attention mechanism, merging degraded scene\ndescriptors with image features, allowing for nuanced restoration. Our model\nallows versatile input scene descriptors, ranging from manual text embeddings\nto automatic extractions based on visual attributes. Our methodology is further\nenhanced through a composite degradation restoration loss, using extra degraded\nimages as negative samples to fortify model constraints. Comparative results on\nsynthetic and real-world datasets demonstrate OneRestore as a superior\nsolution, significantly advancing the state-of-the-art in addressing complex,\ncomposite degradations.\n","authors":["Yu Guo","Yuan Gao","Yuxu Lu","Huilin Zhu","Ryan Wen Liu","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2407.04621v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07284v2","updated":"2024-07-10T05:20:23Z","published":"2024-03-12T03:34:03Z","title":"SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object\n  Detection","summary":"  Sparse 3D detectors have received significant attention since the query-based\nparadigm embraces low latency without explicit dense BEV feature construction.\nHowever, these detectors achieve worse performance than their dense\ncounterparts. In this paper, we find the key to bridging the performance gap is\nto enhance the awareness of rich representations in two modalities. Here, we\npresent a high-performance fully sparse detector for end-to-end multi-modality\n3D object detection. The detector, termed SparseLIF, contains three key\ndesigns, which are (1) Perspective-Aware Query Generation (PAQG) to generate\nhigh-quality 3D queries with perspective priors, (2) RoI-Aware Sampling (RIAS)\nto further refine prior queries by sampling RoI features from each modality,\n(3) Uncertainty-Aware Fusion (UAF) to precisely quantify the uncertainty of\neach sensor modality and adaptively conduct final multi-modality fusion, thus\nachieving great robustness against sensor noises. By the time of paper\nsubmission, SparseLIF achieves state-of-the-art performance on the nuScenes\ndataset, ranking 1st on both validation set and test benchmark, outperforming\nall state-of-the-art 3D object detectors by a notable margin.\n","authors":["Hongcheng Zhang","Liu Liang","Pengxin Zeng","Xiao Song","Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.07284v2.pdf","comment":"The 18th European Conference on Computer Vision ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07374v1","updated":"2024-07-10T05:19:40Z","published":"2024-07-10T05:19:40Z","title":"DuInNet: Dual-Modality Feature Interaction for Point Cloud Completion","summary":"  To further promote the development of multimodal point cloud completion, we\ncontribute a large-scale multimodal point cloud completion benchmark\nModelNet-MPC with richer shape categories and more diverse test data, which\ncontains nearly 400,000 pairs of high-quality point clouds and rendered images\nof 40 categories. Besides the fully supervised point cloud completion task, two\nadditional tasks including denoising completion and zero-shot learning\ncompletion are proposed in ModelNet-MPC, to simulate real-world scenarios and\nverify the robustness to noise and the transfer ability across categories of\ncurrent methods. Meanwhile, considering that existing multimodal completion\npipelines usually adopt a unidirectional fusion mechanism and ignore the shape\nprior contained in the image modality, we propose a Dual-Modality Feature\nInteraction Network (DuInNet) in this paper. DuInNet iteratively interacts\nfeatures between point clouds and images to learn both geometric and texture\ncharacteristics of shapes with the dual feature interactor. To adapt to\nspecific tasks such as fully supervised, denoising, and zero-shot learning\npoint cloud completions, an adaptive point generator is proposed to generate\ncomplete point clouds in blocks with different weights for these two\nmodalities. Extensive experiments on the ShapeNet-ViPC and ModelNet-MPC\nbenchmarks demonstrate that DuInNet exhibits superiority, robustness and\ntransfer ability in all completion tasks over state-of-the-art methods. The\ncode and dataset will be available soon.\n","authors":["Xinpu Liu","Baolin Hou","Hanyun Wang","Ke Xu","Jianwei Wan","Yulan Guo"],"pdf_url":"https://arxiv.org/pdf/2407.07374v1.pdf","comment":"Under Review, 13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.07372v1","updated":"2024-07-10T05:17:01Z","published":"2024-07-10T05:17:01Z","title":"Trustworthy Contrast-enhanced Brain MRI Synthesis","summary":"  Contrast-enhanced brain MRI (CE-MRI) is a valuable diagnostic technique but\nmay pose health risks and incur high costs. To create safer alternatives,\nmulti-modality medical image translation aims to synthesize CE-MRI images from\nother available modalities. Although existing methods can generate promising\npredictions, they still face two challenges, i.e., exhibiting over-confidence\nand lacking interpretability on predictions. To address the above challenges,\nthis paper introduces TrustI2I, a novel trustworthy method that reformulates\nmulti-to-one medical image translation problem as a multimodal regression\nproblem, aiming to build an uncertainty-aware and reliable system.\nSpecifically, our method leverages deep evidential regression to estimate\nprediction uncertainties and employs an explicit intermediate and late fusion\nstrategy based on the Mixture of Normal Inverse Gamma (MoNIG) distribution,\nenhancing both synthesis quality and interpretability. Additionally, we\nincorporate uncertainty calibration to improve the reliability of uncertainty.\nValidation on the BraTS2018 dataset demonstrates that our approach surpasses\ncurrent methods, producing higher-quality images with rational uncertainty\nestimation.\n","authors":["Jiyao Liu","Yuxin Li","Shangqi Gao","Yuncheng Zhou","Xin Gao","Ningsheng Xu","Xiao-Yong Zhang","Xiahai Zhuang"],"pdf_url":"https://arxiv.org/pdf/2407.07372v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.07365v1","updated":"2024-07-10T04:54:03Z","published":"2024-07-10T04:54:03Z","title":"High-Resolution Cloud Detection Network","summary":"  The complexity of clouds, particularly in terms of texture detail at high\nresolutions, has not been well explored by most existing cloud detection\nnetworks. This paper introduces the High-Resolution Cloud Detection Network\n(HR-cloud-Net), which utilizes a hierarchical high-resolution integration\napproach. HR-cloud-Net integrates a high-resolution representation module,\nlayer-wise cascaded feature fusion module, and multi-resolution pyramid pooling\nmodule to effectively capture complex cloud features. This architecture\npreserves detailed cloud texture information while facilitating feature\nexchange across different resolutions, thereby enhancing overall performance in\ncloud detection. Additionally, a novel approach is introduced wherein a student\nview, trained on noisy augmented images, is supervised by a teacher view\nprocessing normal images. This setup enables the student to learn from cleaner\nsupervisions provided by the teacher, leading to improved performance.\nExtensive evaluations on three optical satellite image cloud detection datasets\nvalidate the superior performance of HR-cloud-Net compared to existing\nmethods.The source code is available at\n\\url{https://github.com/kunzhan/HR-cloud-Net}.\n","authors":["Jingsheng Li","Tianxiang Xue","Jiayi Zhao","Jingmin Ge","Yufang Min","Wei Su","Kun Zhan"],"pdf_url":"https://arxiv.org/pdf/2407.07365v1.pdf","comment":"Journal of Electronic Imaging"},{"id":"http://arxiv.org/abs/2406.03865v2","updated":"2024-07-10T04:34:13Z","published":"2024-06-06T08:51:26Z","title":"Semantic Similarity Score for Measuring Visual Similarity at Semantic\n  Level","summary":"  Semantic communication, as a revolutionary communication architecture, is\nconsidered a promising novel communication paradigm. Unlike traditional\nsymbol-based error-free communication systems, semantic-based visual\ncommunication systems extract, compress, transmit, and reconstruct images at\nthe semantic level. However, widely used image similarity evaluation metrics,\nwhether pixel-based MSE or PSNR or structure-based MS-SSIM, struggle to\naccurately measure the loss of semantic-level information of the source during\nsystem transmission. This presents challenges in evaluating the performance of\nvisual semantic communication systems, especially when comparing them with\ntraditional communication systems. To address this, we propose a semantic\nevaluation metric -- SeSS (Semantic Similarity Score), based on Scene Graph\nGeneration and graph matching, which shifts the similarity scores between\nimages into semantic-level graph matching scores. Meanwhile, semantic\nsimilarity scores for tens of thousands of image pairs are manually annotated\nto fine-tune the hyperparameters in the graph matching algorithm, aligning the\nmetric more closely with human semantic perception. The performance of the SeSS\nis tested on different datasets, including (1)images transmitted by traditional\nand semantic communication systems at different compression rates, (2)images\ntransmitted by traditional and semantic communication systems at different\nsignal-to-noise ratios, (3)images generated by large-scale model with different\nnoise levels introduced, and (4)cases of images subjected to certain special\ntransformations. The experiments demonstrate the effectiveness of SeSS,\nindicating that the metric can measure the semantic-level differences in\nsemantic-level information of images and can be used for evaluation in visual\nsemantic communication systems.\n","authors":["Senran Fan","Zhicheng Bao","Chen Dong","Haotai Liang","Xiaodong Xu","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.03865v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07360v1","updated":"2024-07-10T04:33:43Z","published":"2024-07-10T04:33:43Z","title":"Towards a text-based quantitative and explainable histopathology image\n  analysis","summary":"  Recently, vision-language pre-trained models have emerged in computational\npathology. Previous works generally focused on the alignment of image-text\npairs via the contrastive pre-training paradigm. Such pre-trained models have\nbeen applied to pathology image classification in zero-shot learning or\ntransfer learning fashion. Herein, we hypothesize that the pre-trained\nvision-language models can be utilized for quantitative histopathology image\nanalysis through a simple image-to-text retrieval. To this end, we propose a\nText-based Quantitative and Explainable histopathology image analysis, which we\ncall TQx. Given a set of histopathology images, we adopt a pre-trained\nvision-language model to retrieve a word-of-interest pool. The retrieved words\nare then used to quantify the histopathology images and generate understandable\nfeature embeddings due to the direct mapping to the text description. To\nevaluate the proposed method, the text-based embeddings of four histopathology\nimage datasets are utilized to perform clustering and classification tasks. The\nresults demonstrate that TQx is able to quantify and analyze histopathology\nimages that are comparable to the prevalent visual models in computational\npathology.\n","authors":["Anh Tien Nguyen","Trinh Thi Le Vuong","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.07360v1.pdf","comment":"MICCAI 2024 - Early acceptance (Top 11%)"},{"id":"http://arxiv.org/abs/2407.07356v1","updated":"2024-07-10T04:27:06Z","published":"2024-07-10T04:27:06Z","title":"Video In-context Learning","summary":"  In-context learning for vision data has been underexplored compared with that\nin natural language. Previous works studied image in-context learning, urging\nmodels to generate a single image guided by demonstrations. In this paper, we\npropose and study video in-context learning, where the model starts from an\nexisting video clip and generates diverse potential future sequences, each\nsemantically guided by the prompted video demonstrations. To achieve this, we\nprovide a clear definition of the task, and train an autoregressive Transformer\non video datasets. We thoroughly analyze the effect of different datasets and\nrepresent frames as discrete tokens, and then model them by next token\npredictions. We design various evaluation metrics, including both objective and\nsubjective measures, to demonstrate the visual quality and semantic accuracy of\ngeneration results. Our model follows the scaling law and generates\nhigh-quality video clips that accurately align with the semantic guidance\nprovided by in-context examples.\n","authors":["Wentao Zhang","Junliang Guo","Tianyu He","Li Zhao","Linli Xu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2407.07356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07351v1","updated":"2024-07-10T04:06:39Z","published":"2024-07-10T04:06:39Z","title":"Unity in Diversity: Multi-expert Knowledge Confrontation and\n  Collaboration for Generalizable Vehicle Re-identification","summary":"  Generalizable vehicle re-identification (ReID) aims to enable the\nwell-trained model in diverse source domains to broadly adapt to unknown target\ndomains without additional fine-tuning or retraining. However, it still faces\nthe challenges of domain shift problem and has difficulty accurately\ngeneralizing to unknown target domains. This limitation occurs because the\nmodel relies heavily on primary domain-invariant features in the training data\nand pays less attention to potentially valuable secondary features. To solve\nthis complex and common problem, this paper proposes the two-stage Multi-expert\nKnowledge Confrontation and Collaboration (MiKeCoCo) method, which incorporates\nmultiple experts with unique perspectives into Contrastive Language-Image\nPretraining (CLIP) and fully leverages high-level semantic knowledge for\ncomprehensive feature representation. Specifically, we propose to construct the\nlearnable prompt set of all specific-perspective experts by adversarial\nlearning in the latent space of visual features during the first stage of\ntraining. The learned prompt set with high-level semantics is then utilized to\nguide representation learning of the multi-level features for final knowledge\nfusion in the next stage. In this process of knowledge fusion, although\nmultiple experts employ different assessment ways to examine the same vehicle,\ntheir common goal is to confirm the vehicle's true identity. Their collective\ndecision can ensure the accuracy and consistency of the evaluation results.\nFurthermore, we design different image inputs for two-stage training, which\ninclude image component separation and diversity enhancement in order to\nextract the ID-related prompt representation and to obtain feature\nrepresentation highlighted by all experts, respectively. Extensive experimental\nresults demonstrate that our method achieves state-of-the-art recognition\nperformance.\n","authors":["Zhenyu Kuang","Hongyang Zhang","Lidong Cheng","Yinhao Liu","Yue Huang","Xinghao Ding"],"pdf_url":"https://arxiv.org/pdf/2407.07351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09806v2","updated":"2024-07-10T04:04:06Z","published":"2024-05-16T04:28:44Z","title":"MediSyn: Text-Guided Diffusion Models for Broad Medical 2D and 3D Image\n  Synthesis","summary":"  Diffusion models have recently gained significant traction due to their\nability to generate high-fidelity and diverse images and videos conditioned on\ntext prompts. In medicine, this application promises to address the critical\nchallenge of data scarcity, a consequence of barriers in data sharing,\nstringent patient privacy regulations, and disparities in patient population\nand demographics. By generating realistic and varying medical 2D and 3D images,\nthese models offer a rich, privacy-respecting resource for algorithmic training\nand research. To this end, we introduce MediSyn, a pair of instruction-tuned\ntext-guided latent diffusion models with the ability to generate high-fidelity\nand diverse medical 2D and 3D images across specialties and modalities. Through\nestablished metrics, we show significant improvement in broad medical image and\nvideo synthesis guided by text prompts.\n","authors":["Joseph Cho","Cyril Zakka","Dhamanpreet Kaur","Rohan Shad","Ross Wightman","Akshay Chaudhari","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2405.09806v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07347v1","updated":"2024-07-10T03:57:29Z","published":"2024-07-10T03:57:29Z","title":"MNeRV: A Multilayer Neural Representation for Videos","summary":"  As a novel video representation method, Neural Representations for Videos\n(NeRV) has shown great potential in the fields of video compression, video\nrestoration, and video interpolation. In the process of representing videos\nusing NeRV, each frame corresponds to an embedding, which is then reconstructed\ninto a video frame sequence after passing through a small number of decoding\nlayers (E-NeRV, HNeRV, etc.). However, this small number of decoding layers can\neasily lead to the problem of redundant model parameters due to the large\nproportion of parameters in a single decoding layer, which greatly restricts\nthe video regression ability of neural network models. In this paper, we\npropose a multilayer neural representation for videos (MNeRV) and design a new\ndecoder M-Decoder and its matching encoder M-Encoder. MNeRV has more encoding\nand decoding layers, which effectively alleviates the problem of redundant\nmodel parameters caused by too few layers. In addition, we design MNeRV blocks\nto perform more uniform and effective parameter allocation between decoding\nlayers. In the field of video regression reconstruction, we achieve better\nreconstruction quality (+4.06 PSNR) with fewer parameters. Finally, we showcase\nMNeRV performance in downstream tasks such as video restoration and video\ninterpolation. The source code of MNeRV is available at\nhttps://github.com/Aaronbtb/MNeRV.\n","authors":["Qingling Chang","Haohui Yu","Shuxuan Fu","Zhiqiang Zeng","Chuangquan Chen"],"pdf_url":"https://arxiv.org/pdf/2407.07347v1.pdf","comment":"14 pages, 12 figures, 8 table"},{"id":"http://arxiv.org/abs/2407.04396v2","updated":"2024-07-10T03:54:23Z","published":"2024-07-05T10:06:55Z","title":"Graph-Guided Test-Time Adaptation for Glaucoma Diagnosis using Fundus\n  Photography","summary":"  Glaucoma is a leading cause of irreversible blindness worldwide. While deep\nlearning approaches using fundus images have largely improved early diagnosis\nof glaucoma, variations in images from different devices and locations (known\nas domain shifts) challenge the use of pre-trained models in real-world\nsettings. To address this, we propose a novel Graph-guided Test-Time Adaptation\n(GTTA) framework to generalize glaucoma diagnosis models to unseen test\nenvironments. GTTA integrates the topological information of fundus images into\nthe model training, enhancing the model's transferability and reducing the risk\nof learning spurious correlation. During inference, GTTA introduces a novel\ntest-time training objective to make the source-trained classifier\nprogressively adapt to target patterns with reliable class conditional\nestimation and consistency regularization. Experiments on cross-domain glaucoma\ndiagnosis benchmarks demonstrate the superiority of the overall framework and\nindividual components under different backbone networks.\n","authors":["Qian Zeng","Le Zhang","Yipeng Liu","Ce Zhu","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.04396v2.pdf","comment":"11 pages, 3 figures, 3 tables, submitted to MICCAI"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.07871v1","updated":"2024-07-10T17:37:15Z","published":"2024-07-10T17:37:15Z","title":"Enhancing HNSW Index for Real-Time Updates: Addressing Unreachable\n  Points and Performance Degradation","summary":"  The approximate nearest neighbor search (ANNS) is a fundamental and essential\ncomponent in information retrieval, with graph-based methodologies\ndemonstrating superior performance compared to alternative approaches.\nExtensive research efforts have been dedicated to improving search efficiency\nby developing various graph-based indices, such as HNSW (Hierarchical Navigable\nSmall World). However, the performance of HNSW and most graph-based indices\nbecomes unacceptable when faced with a large number of real-time deletions,\ninsertions, and updates. Furthermore, during update operations, HNSW can result\nin some data points becoming unreachable, a situation we refer to as the\n`unreachable points phenomenon'. This phenomenon could significantly affect the\nsearch accuracy of the graph in certain situations.\n  To address these issues, we present efficient measures to overcome the\nshortcomings of HNSW, specifically addressing poor performance over long\nperiods of delete and update operations and resolving the issues caused by the\nunreachable points phenomenon. Our proposed MN-RU algorithm effectively\nimproves update efficiency and suppresses the growth rate of unreachable\npoints, ensuring better overall performance and maintaining the integrity of\nthe graph. Our results demonstrate that our methods outperform existing\napproaches. Furthermore, since our methods are based on HNSW, they can be\neasily integrated with existing indices widely used in the industrial field,\nmaking them practical for future real-world applications. Code is available at\nhttps://github.com/xwt1/ICPADS-MN-RU.git\n","authors":["Wentao Xiao","Yueyang Zhan","Rui Xi","Mengshu Hou","Jianming Liao"],"pdf_url":"https://arxiv.org/pdf/2407.07871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07790v1","updated":"2024-07-10T16:07:51Z","published":"2024-07-10T16:07:51Z","title":"Systematic Evaluation of Neural Retrieval Models on the TouchÃ© 2020\n  Argument Retrieval Subset of BEIR","summary":"  The zero-shot effectiveness of neural retrieval models is often evaluated on\nthe BEIR benchmark -- a combination of different IR evaluation datasets.\nInterestingly, previous studies found that particularly on the BEIR subset\nTouch\\'e 2020, an argument retrieval task, neural retrieval models are\nconsiderably less effective than BM25. Still, so far, no further investigation\nhas been conducted on what makes argument retrieval so \"special\". To more\ndeeply analyze the respective potential limits of neural retrieval models, we\nrun a reproducibility study on the Touch\\'e 2020 data. In our study, we focus\non two experiments: (i) a black-box evaluation (i.e., no model retraining),\nincorporating a theoretical exploration using retrieval axioms, and (ii) a data\ndenoising evaluation involving post-hoc relevance judgments. Our black-box\nevaluation reveals an inherent bias of neural models towards retrieving short\npassages from the Touch\\'e 2020 data, and we also find that quite a few of the\nneural models' results are unjudged in the Touch\\'e 2020 data. As many of the\nshort Touch\\'e passages are not argumentative and thus non-relevant per se, and\nas the missing judgments complicate fair comparison, we denoise the Touch\\'e\n2020 data by excluding very short passages (less than 20 words) and by\naugmenting the unjudged data with post-hoc judgments following the Touch\\'e\nguidelines. On the denoised data, the effectiveness of the neural models\nimproves by up to 0.52 in nDCG@10, but BM25 is still more effective. Our code\nand the augmented Touch\\'e 2020 dataset are available at\n\\url{https://github.com/castorini/touche-error-analysis}.\n","authors":["Nandan Thakur","Luiz Bonifacio","Maik FrÃ¶be","Alexander Bondarenko","Ehsan Kamalloo","Martin Potthast","Matthias Hagen","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2407.07790v1.pdf","comment":"SIGIR 2024 (Resource & Reproducibility Track)"},{"id":"http://arxiv.org/abs/2404.01616v3","updated":"2024-07-10T15:20:19Z","published":"2024-04-02T03:42:28Z","title":"Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems","summary":"  Large language models (LLMs) are trained on text-only data that go far beyond\nthe languages with paired speech and text data. At the same time, Dual Encoder\n(DE) based retrieval systems project queries and documents into the same\nembedding space and have demonstrated their success in retrieval and bi-text\nmining. To match speech and text in many languages, we propose using LLMs to\ninitialize multi-modal DE retrieval systems. Unlike traditional methods, our\nsystem doesn't require speech data during LLM pre-training and can exploit\nLLM's multilingual text understanding capabilities to match speech and text in\nlanguages unseen during retrieval training. Our multi-modal LLM-based retrieval\nsystem is capable of matching speech and text in 102 languages despite only\ntraining on 21 languages. Our system outperforms previous systems trained\nexplicitly on all 102 languages. We achieve a 10% absolute improvement in\nRecall@1 averaged across these languages. Additionally, our model demonstrates\ncross-lingual speech and text matching, which is further enhanced by readily\navailable machine translation data.\n","authors":["Frank Palma Gomez","Ramon Sanabria","Yun-hsuan Sung","Daniel Cer","Siddharth Dalmia","Gustavo Hernandez Abrego"],"pdf_url":"https://arxiv.org/pdf/2404.01616v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07550v1","updated":"2024-07-10T11:19:15Z","published":"2024-07-10T11:19:15Z","title":"Evaluating the method reproducibility of deep learning models in the\n  biodiversity domain","summary":"  Artificial Intelligence (AI) is revolutionizing biodiversity research by\nenabling advanced data analysis, species identification, and habitats\nmonitoring, thereby enhancing conservation efforts. Ensuring reproducibility in\nAI-driven biodiversity research is crucial for fostering transparency,\nverifying results, and promoting the credibility of ecological findings.This\nstudy investigates the reproducibility of deep learning (DL) methods within the\nbiodiversity domain. We design a methodology for evaluating the reproducibility\nof biodiversity-related publications that employ DL techniques across three\nstages. We define ten variables essential for method reproducibility, divided\ninto four categories: resource requirements, methodological information,\nuncontrolled randomness, and statistical considerations. These categories\nsubsequently serve as the basis for defining different levels of\nreproducibility. We manually extract the availability of these variables from a\ncurated dataset comprising 61 publications identified using the keywords\nprovided by biodiversity experts. Our study shows that the dataset is shared in\n47% of the publications; however, a significant number of the publications lack\ncomprehensive information on deep learning methods, including details regarding\nrandomness.\n","authors":["Waqas Ahmed","Vamsi Krishna Kommineni","Birgitta KÃ¶nig-Ries","Jitendra Gaikwad","Luiz Gadelha","Sheeba Samuel"],"pdf_url":"https://arxiv.org/pdf/2407.07550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07503v1","updated":"2024-07-10T09:41:36Z","published":"2024-07-10T09:41:36Z","title":"Metasurface-based Snapshot Shortwave-Infrared Hyperspectral Image\n  Reconstruction with Inter and Intra Prior Learning Network","summary":"  Shortwave-infrared(SWIR) spectral information,ranging from 1 {\\mu}m to\n2.5{\\mu}m, breaks the limitations of traditional color cameras in acquiring\nscene information and has been used in many fields. However, conventional SWIR\nhyperspectral imaging systems face challenges due to their bulky setups and low\nacquisition speed. In this work, we introduce a snapshot SWIR hyperspectral\nimaging system based on a metasurface filter and a corresponding filter\nselection method to achieve the lowest correlation coefficient among these\nfilters.This systemhas the advantages of small size and snapshot imaging. We\npropose a novel inter and intra prior learning unfolding framework proposed to\nachieve high-quality SWIR hyperspectral image reconstruction, which bridges the\ngap between prior learning and cross-stage information interaction. We also\ndesign an adaptive feature transfer mechanism to adaptively the transfer\ncontextual correlation of multi-scale encoder features to prevent detailed\ninformation loss in the decoder. Experiment results demonstrate that our method\ncan reconstruct HSI with high speed and superior performance over existing\nmethods.\n","authors":["Linqiang Li","Pan Liu","Haofang Yan","Ziqin Zhang","Jinglei Hao","Seong G. Kong","Yongqiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.07503v1.pdf","comment":"10 pages,5 figures"},{"id":"http://arxiv.org/abs/2403.16424v3","updated":"2024-07-10T07:14:32Z","published":"2024-03-25T05:04:52Z","title":"An Experiment with the Use of ChatGPT for LCSH Subject Assignment on\n  Electronic Theses and Dissertations","summary":"  This study delves into the potential use of large language models (LLMs) for\ngenerating Library of Congress Subject Headings (LCSH). The authors employed\nChatGPT to generate subject headings for electronic theses and dissertations\n(ETDs) based on their titles and abstracts. The results suggests that LLMs such\nas ChatGPT have the potential to reduce cataloging time needed for assigning\nLCSH subject terms for ETDs as well as to improve the discovery of this type of\nresource in academic libraries. Nonetheless, human catalogers remain essential\nfor verifying and enhancing the validity, exhaustivity, and specificity of LCSH\ngenerated by LLMs.\n","authors":["Eric H. C. Chow","TJ Kao","Xiaoli Li"],"pdf_url":"https://arxiv.org/pdf/2403.16424v3.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2404.05893v3","updated":"2024-07-10T21:06:48Z","published":"2024-04-08T22:29:53Z","title":"Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models","summary":"  Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base\n","authors":["Sowmya S. Sundaram","Benjamin Solomon","Avani Khatri","Anisha Laumas","Purvesh Khatri","Mark A. Musen"],"pdf_url":"https://arxiv.org/pdf/2404.05893v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08035v1","updated":"2024-07-10T20:32:50Z","published":"2024-07-10T20:32:50Z","title":"FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in\n  Domain-specific Scenarios","summary":"  Large Language Models (LLMs) have provided a new pathway for Named Entity\nRecognition (NER) tasks. Compared with fine-tuning, LLM-powered prompting\nmethods avoid the need for training, conserve substantial computational\nresources, and rely on minimal annotated data. Previous studies have achieved\ncomparable performance to fully supervised BERT-based fine-tuning approaches on\ngeneral NER benchmarks. However, none of the previous approaches has\ninvestigated the efficiency of LLM-based few-shot learning in domain-specific\nscenarios. To address this gap, we introduce FsPONER, a novel approach for\noptimizing few-shot prompts, and evaluate its performance on domain-specific\nNER datasets, with a focus on industrial manufacturing and maintenance, while\nusing multiple LLMs -- GPT-4-32K, GPT-3.5-Turbo, LLaMA 2-chat, and Vicuna.\nFsPONER consists of three few-shot selection methods based on random sampling,\nTF-IDF vectors, and a combination of both. We compare these methods with a\ngeneral-purpose GPT-NER method as the number of few-shot examples increases and\nevaluate their optimal NER performance against fine-tuned BERT and LLaMA\n2-chat. In the considered real-world scenarios with data scarcity, FsPONER with\nTF-IDF surpasses fine-tuned models by approximately 10% in F1 score.\n","authors":["Yongjian Tang","Rakebul Hasan","Thomas Runkler"],"pdf_url":"https://arxiv.org/pdf/2407.08035v1.pdf","comment":"accepted for publication at the 27th European Conference on\n  Artificial Intelligence (ECAI-2024)"},{"id":"http://arxiv.org/abs/2407.08008v1","updated":"2024-07-10T19:30:16Z","published":"2024-07-10T19:30:16Z","title":"DS@GT eRisk 2024: Sentence Transformers for Social Media Risk Assessment","summary":"  We present working notes for DS@GT team in the eRisk 2024 for Tasks 1 and 3.\nWe propose a ranking system for Task 1 that predicts symptoms of depression\nbased on the Beck Depression Inventory (BDI-II) questionnaire using binary\nclassifiers trained on question relevancy as a proxy for ranking. We find that\nbinary classifiers are not well calibrated for ranking, and perform poorly\nduring evaluation. For Task 3, we use embeddings from BERT to predict the\nseverity of eating disorder symptoms based on user post history. We find that\nclassical machine learning models perform well on the task, and end up\ncompetitive with the baseline models. Representation of text data is crucial in\nboth tasks, and we find that sentence transformers are a powerful tool for\ndownstream modeling. Source code and models are available at\n\\url{https://github.com/dsgt-kaggle-clef/erisk-2024}.\n","authors":["David Guecha","Aaryan Potdar","Anthony Miyaguchi"],"pdf_url":"https://arxiv.org/pdf/2407.08008v1.pdf","comment":"Paper Submitted to CLEF 2024 CEUR-WS"},{"id":"http://arxiv.org/abs/2406.01702v2","updated":"2024-07-10T19:21:51Z","published":"2024-06-03T18:02:13Z","title":"Session Context Embedding for Intent Understanding in Product Search","summary":"  It is often noted that single query-item pair relevance training in search\ndoes not capture the customer intent. User intent can be better deduced from a\nseries of engagements (Clicks, ATCs, Orders) in a given search session. We\npropose a novel method for vectorizing session context for capturing and\nutilizing context in retrieval and rerank. In the runtime, session embedding is\nan alternative to query embedding, saved and updated after each request in the\nsession, it can be used for retrieval and ranking. We outline session\nembedding's solution to session-based intent understanding and its\narchitecture, the background to this line of thought in search and\nrecommendation, detail the methodologies implemented, and finally present the\nresults of an implementation of session embedding for query product type\nclassification. We demonstrate improvements over strategies ignoring session\ncontext in the runtime for user intent understanding.\n","authors":["Navid Mehrdad","Vishal Rathi","Sravanthi Rajanala"],"pdf_url":"https://arxiv.org/pdf/2406.01702v2.pdf","comment":"5 pages, 1 Figure, 5 Tables, SIGIR 2024, LLM for Individuals, Groups,\n  and Society"},{"id":"http://arxiv.org/abs/2407.08001v1","updated":"2024-07-10T19:13:37Z","published":"2024-07-10T19:13:37Z","title":"Automated Neural Patent Landscaping in the Small Data Regime","summary":"  Patent landscaping is the process of identifying all patents related to a\nparticular technological area, and is important for assessing various aspects\nof the intellectual property context. Traditionally, constructing patent\nlandscapes is intensely laborious and expensive, and the rapid expansion of\npatenting activity in recent decades has driven an increasing need for\nefficient and effective automated patent landscaping approaches. In particular,\nit is critical that we be able to construct patent landscapes using a minimal\nnumber of labeled examples, as labeling patents for a narrow technology area\nrequires highly specialized (and hence expensive) technical knowledge. We\npresent an automated neural patent landscaping system that demonstrates\nsignificantly improved performance on difficult examples (0.69 $F_1$ on 'hard'\nexamples, versus 0.6 for previously reported systems), and also significant\nimprovements with much less training data (overall 0.75 $F_1$ on as few as 24\nexamples). Furthermore, in evaluating such automated landscaping systems,\nacquiring good data is challenge; we demonstrate a higher-quality training data\ngeneration procedure by merging Abood and Feltenberger's (2018)\n\"seed/anti-seed\" approach with active learning to collect difficult labeled\nexamples near the decision boundary. Using this procedure we created a new\ndataset of labeled AI patents for training and testing. As in prior work we\ncompare our approach with a number of baseline systems, and we release our code\nand data for others to build upon.\n","authors":["Tisa Islam Erana","Mark A. Finlayson"],"pdf_url":"https://arxiv.org/pdf/2407.08001v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.07931v1","updated":"2024-07-10T07:22:30Z","published":"2024-07-10T07:22:30Z","title":"Search, Examine and Early-Termination: Fake News Detection with\n  Annotation-Free Evidences","summary":"  Pioneer researches recognize evidences as crucial elements in fake news\ndetection apart from patterns. Existing evidence-aware methods either require\nlaborious pre-processing procedures to assure relevant and high-quality\nevidence data, or incorporate the entire spectrum of available evidences in all\nnews cases, regardless of the quality and quantity of the retrieved data. In\nthis paper, we propose an approach named \\textbf{SEE} that retrieves useful\ninformation from web-searched annotation-free evidences with an\nearly-termination mechanism. The proposed SEE is constructed by three main\nphases: \\textbf{S}earching online materials using the news as a query and\ndirectly using their titles as evidences without any annotating or filtering\nprocedure, sequentially \\textbf{E}xamining the news alongside with each piece\nof evidence via attention mechanisms to produce new hidden states with\nretrieved information, and allowing \\textbf{E}arly-termination within the\nexamining loop by assessing whether there is adequate confidence for producing\na correct prediction. We have conducted extensive experiments on datasets with\nunprocessed evidences, i.e., Weibo21, GossipCop, and pre-processed evidences,\nnamely Snopes and PolitiFact. The experimental results demonstrate that the\nproposed method outperforms state-of-the-art approaches.\n","authors":["Yuzhou Yang","Yangming Zhou","Qichao Ying","Zhenxing Qian","Xinpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.07931v1.pdf","comment":"ECAI 2024 paper. Fudan University & NVIDIA. To appear"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.07896v1","updated":"2024-07-10T17:59:55Z","published":"2024-07-10T17:59:55Z","title":"Pentagonal Photonic Crystal Mirrors: Scalable Lightsails with Enhanced\n  Acceleration via Neural Topology Optimization","summary":"  The Starshot Breakthrough Initiative aims to send one-gram microchip probes\nto Alpha Centauri within 20 years, using gram-scale lightsails propelled by\nlaser-based radiation pressure, reaching velocities nearing a fifth of light\nspeed. This mission requires lightsail materials that challenge the\nfundamentals of nanotechnology, requiring innovations in optics, material\nscience and structural engineering. Unlike the microchip payload, which must be\nminimized in every dimension, such lightsails need meter-scale dimensions with\nnanoscale thickness and billions of nanoscale holes to enhance reflectivity and\nreduce mass. Our study employs neural topology optimization, revealing a novel\npentagonal lattice-based photonic crystal (PhC) reflector. The optimized\ndesigns shorten acceleration times, therefore lowering launch costs\nsignificantly. Crucially, these designs also enable lightsail material\nfabrication with orders-of-magnitude reduction in costs. We have fabricated a\n60 x 60 mm$^2$, 200nm thick, single-layer reflector perforated with over a\nbillion nanoscale features; the highest aspect-ratio nanophotonic element to\ndate. We achieve this with nearly 9,000 times cost reduction per m$^2$.\nStarshot lightsails will have several stringent requirements but will\nultimately be driven by costs to build at scale. Here we highlight challenges\nand possible solutions in developing lightsail materials - showcasing the\npotential of scaling nanophotonics for cost-effective next-generation space\nexploration.\n","authors":["L. Norder","S. Yin","M. J. de Jong","F. Stallone","H. Aydogmus","P. M. Sberna","M. A. Bessa","R. A. Norte"],"pdf_url":"https://arxiv.org/pdf/2407.07896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07895v1","updated":"2024-07-10T17:59:43Z","published":"2024-07-10T17:59:43Z","title":"LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large\n  Multimodal Models","summary":"  Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT\n","authors":["Feng Li","Renrui Zhang","Hao Zhang","Yuanhan Zhang","Bo Li","Wei Li","Zejun Ma","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2407.07895v1.pdf","comment":"Project Page:\n  https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/"},{"id":"http://arxiv.org/abs/2406.06609v2","updated":"2024-07-10T17:58:14Z","published":"2024-06-06T18:52:28Z","title":"Mitigating Bias in Dataset Distillation","summary":"  Dataset Distillation has emerged as a technique for compressing large\ndatasets into smaller synthetic counterparts, facilitating downstream training\ntasks. In this paper, we study the impact of bias inside the original dataset\non the performance of dataset distillation. With a comprehensive empirical\nevaluation on canonical datasets with color, corruption and background biases,\nwe found that color and background biases in the original dataset will be\namplified through the distillation process, resulting in a notable decline in\nthe performance of models trained on the distilled dataset, while corruption\nbias is suppressed through the distillation process. To reduce bias\namplification in dataset distillation, we introduce a simple yet highly\neffective approach based on a sample reweighting scheme utilizing kernel\ndensity estimation. Empirical results on multiple real-world and synthetic\ndatasets demonstrate the effectiveness of the proposed method. Notably, on\nCMNIST with 5% bias-conflict ratio and IPC 50, our method achieves 91.5% test\naccuracy compared to 23.8% from vanilla DM, boosting the performance by 67.7%,\nwhereas applying state-of-the-art debiasing method on the same dataset only\nachieves 53.7% accuracy. Our findings highlight the importance of addressing\nbiases in dataset distillation and provide a promising avenue to address bias\namplification in the process.\n","authors":["Justin Cui","Ruochen Wang","Yuanhao Xiong","Cho-Jui Hsieh"],"pdf_url":"https://arxiv.org/pdf/2406.06609v2.pdf","comment":"ICML"},{"id":"http://arxiv.org/abs/2407.07890v1","updated":"2024-07-10T17:57:58Z","published":"2024-07-10T17:57:58Z","title":"Training on the Test Task Confounds Evaluation and Emergence","summary":"  We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of techniques to\ninclude task-relevant data in the pretraining stage of a language model. We\ndemonstrate that training on the test task confounds both relative model\nevaluations and claims about emergent capabilities. We argue that the seeming\nsuperiority of one model family over another may be explained by a different\ndegree of training on the test task. To this end, we propose an effective\nmethod to adjust for training on the test task by fine-tuning each model under\ncomparison on the same task-relevant data before evaluation. We then show that\ninstances of emergent behavior largely vanish once we adjust for training on\nthe test task. This also applies to reported instances of emergent behavior\nthat cannot be explained by the choice of evaluation metric. Our work promotes\na new perspective on the evaluation of large language models with broad\nimplications for benchmarking and the study of emergent capabilities.\n","authors":["Ricardo Dominguez-Olmedo","Florian E. Dorner","Moritz Hardt"],"pdf_url":"https://arxiv.org/pdf/2407.07890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07889v1","updated":"2024-07-10T17:57:04Z","published":"2024-07-10T17:57:04Z","title":"AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic\n  Manipulation","summary":"  Predictive models are a crucial component of many robotic systems. Yet,\nconstructing accurate predictive models for a variety of deformable objects,\nespecially those with unknown physical properties, remains a significant\nchallenge. This paper introduces AdaptiGraph, a learning-based dynamics\nmodeling approach that enables robots to predict, adapt to, and control a wide\narray of challenging deformable materials with unknown physical properties.\nAdaptiGraph leverages the highly flexible graph-based neural dynamics (GBND)\nframework, which represents material bits as particles and employs a graph\nneural network (GNN) to predict particle motion. Its key innovation is a\nunified physical property-conditioned GBND model capable of predicting the\nmotions of diverse materials with varying physical properties without\nretraining. Upon encountering new materials during online deployment,\nAdaptiGraph utilizes a physical property optimization process for a few-shot\nadaptation of the model, enhancing its fit to the observed interaction data.\nThe adapted models can precisely simulate the dynamics and predict the motion\nof various deformable materials, such as ropes, granular media, rigid boxes,\nand cloth, while adapting to different physical properties, including\nstiffness, granular size, and center of pressure. On prediction and\nmanipulation tasks involving a diverse set of real-world deformable objects,\nour method exhibits superior prediction accuracy and task proficiency over\nnon-material-conditioned and non-adaptive models. The project page is available\nat https://robopil.github.io/adaptigraph/ .\n","authors":["Kaifeng Zhang","Baoyu Li","Kris Hauser","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2407.07889v1.pdf","comment":"Project page: https://robopil.github.io/adaptigraph/"},{"id":"http://arxiv.org/abs/2407.07885v1","updated":"2024-07-10T17:52:30Z","published":"2024-07-10T17:52:30Z","title":"Learning In-Hand Translation Using Tactile Skin With Shear and Normal\n  Force Sensing","summary":"  Recent progress in reinforcement learning (RL) and tactile sensing has\nsignificantly advanced dexterous manipulation. However, these methods often\nutilize simplified tactile signals due to the gap between tactile simulation\nand the real world. We introduce a sensor model for tactile skin that enables\nzero-shot sim-to-real transfer of ternary shear and binary normal forces. Using\nthis model, we develop an RL policy that leverages sliding contact for\ndexterous in-hand translation. We conduct extensive real-world experiments to\nassess how tactile sensing facilitates policy adaptation to various unseen\nobject properties and robot hand orientations. We demonstrate that our 3-axis\ntactile policies consistently outperform baselines that use only shear forces,\nonly normal forces, or only proprioception. Website:\nhttps://jessicayin.github.io/tactile-skin-rl/\n","authors":["Jessica Yin","Haozhi Qi","Jitendra Malik","James Pikul","Mark Yim","Tess Hellebrekers"],"pdf_url":"https://arxiv.org/pdf/2407.07885v1.pdf","comment":"Website: https://jessicayin.github.io/tactile-skin-rl/"},{"id":"http://arxiv.org/abs/2407.07884v1","updated":"2024-07-10T17:51:33Z","published":"2024-07-10T17:51:33Z","title":"Vegetable Peeling: A Case Study in Constrained Dexterous Manipulation","summary":"  Recent studies have made significant progress in addressing dexterous\nmanipulation problems, particularly in in-hand object reorientation. However,\nthere are few existing works that explore the potential utilization of\ndeveloped dexterous manipulation controllers for downstream tasks. In this\nstudy, we focus on constrained dexterous manipulation for food peeling. Food\npeeling presents various constraints on the reorientation controller, such as\nthe requirement for the hand to securely hold the object after reorientation\nfor peeling. We propose a simple system for learning a reorientation controller\nthat facilitates the subsequent peeling task. Videos are available at:\nhttps://taochenshh.github.io/projects/veg-peeling.\n","authors":["Tao Chen","Eric Cousineau","Naveen Kuppuswamy","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.07884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07880v1","updated":"2024-07-10T17:48:25Z","published":"2024-07-10T17:48:25Z","title":"Towards Robust Alignment of Language Models: Distributionally\n  Robustifying Direct Preference Optimization","summary":"  This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO.\n","authors":["Junkang Wu","Yuexiang Xie","Zhengyi Yang","Jiancan Wu","Jiawei Chen","Jinyang Gao","Bolin Ding","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2407.07880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07875v1","updated":"2024-07-10T17:41:10Z","published":"2024-07-10T17:41:10Z","title":"Generative Image as Action Models","summary":"  Image-generation diffusion models have been fine-tuned to unlock new\ncapabilities such as image-editing and novel view synthesis. Can we similarly\nunlock image-generation models for visuomotor control? We present GENIMA, a\nbehavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'\nas targets on RGB images. These images are fed into a controller that maps the\nvisual targets into a sequence of joint-positions. We study GENIMA on 25\nRLBench and 9 real-world manipulation tasks. We find that, by lifting actions\ninto image-space, internet pre-trained diffusion models can generate policies\nthat outperform state-of-the-art visuomotor approaches, especially in\nrobustness to scene perturbations and generalizing to novel objects. Our method\nis also competitive with 3D agents, despite lacking priors such as depth,\nkeypoints, or motion-planners.\n","authors":["Mohit Shridhar","Yat Long Lo","Stephen James"],"pdf_url":"https://arxiv.org/pdf/2407.07875v1.pdf","comment":"Project website, code, checkpoints: https://genima-robot.github.io/"},{"id":"http://arxiv.org/abs/2407.07874v1","updated":"2024-07-10T17:40:30Z","published":"2024-07-10T17:40:30Z","title":"Toto: Time Series Optimized Transformer for Observability","summary":"  This technical report describes the Time Series Optimized Transformer for\nObservability (Toto), a new state of the art foundation model for time series\nforecasting developed by Datadog. In addition to advancing the state of the art\non generalized time series benchmarks in domains such as electricity and\nweather, this model is the first general-purpose time series forecasting\nfoundation model to be specifically tuned for observability metrics. Toto was\ntrained on a dataset of one trillion time series data points, the largest among\nall currently published time series foundation models. Alongside publicly\navailable time series datasets, 75% of the data used to train Toto consists of\nfully anonymous numerical metric data points from the Datadog platform. In our\nexperiments, Toto outperforms existing time series foundation models on\nobservability data. It does this while also excelling at general-purpose\nforecasting tasks, achieving state-of-the-art zero-shot performance on multiple\nopen benchmark datasets.\n","authors":["Ben Cohen","Emaad Khwaja","Kan Wang","Charles Masson","Elise RamÃ©","Youssef Doubli","Othmane Abou-Amal"],"pdf_url":"https://arxiv.org/pdf/2407.07874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07873v1","updated":"2024-07-10T17:39:50Z","published":"2024-07-10T17:39:50Z","title":"Dynamical Measure Transport and Neural PDE Solvers for Sampling","summary":"  The task of sampling from a probability density can be approached as\ntransporting a tractable density function to the target, known as dynamical\nmeasure transport. In this work, we tackle it through a principled unified\nframework using deterministic or stochastic evolutions described by partial\ndifferential equations (PDEs). This framework incorporates prior\ntrajectory-based sampling methods, such as diffusion models or Schr\\\"odinger\nbridges, without relying on the concept of time-reversals. Moreover, it allows\nus to propose novel numerical methods for solving the transport task and thus\nsampling from complicated targets without the need for the normalization\nconstant or data samples. We employ physics-informed neural networks (PINNs) to\napproximate the respective PDE solutions, implying both conceptional and\ncomputational advantages. In particular, PINNs allow for simulation- and\ndiscretization-free optimization and can be trained very efficiently, leading\nto significantly better mode coverage in the sampling task compared to\nalternative methods. Moreover, they can readily be fine-tuned with Gauss-Newton\nmethods to achieve high accuracy in sampling.\n","authors":["Jingtong Sun","Julius Berner","Lorenz Richter","Marius Zeinhofer","Johannes MÃ¼ller","Kamyar Azizzadenesheli","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2407.07873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05615v2","updated":"2024-07-10T17:37:48Z","published":"2023-10-09T11:08:34Z","title":"Adaptive Multi-head Contrastive Learning","summary":"  In contrastive learning, two views of an original image, generated by\ndifferent augmentations, are considered a positive pair, and their similarity\nis required to be high. Similarly, two views of distinct images form a negative\npair, with encouraged low similarity. Typically, a single similarity measure,\nprovided by a lone projection head, evaluates positive and negative sample\npairs. However, due to diverse augmentation strategies and varying intra-sample\nsimilarity, views from the same image may not always be similar. Additionally,\nowing to inter-sample similarity, views from different images may be more akin\nthan those from the same image. Consequently, enforcing high similarity for\npositive pairs and low similarity for negative pairs may be unattainable, and\nin some cases, such enforcement could detrimentally impact performance. To\naddress this challenge, we propose using multiple projection heads, each\nproducing a distinct set of features. Our pre-training loss function emerges\nfrom a solution to the maximum likelihood estimation over head-wise posterior\ndistributions of positive samples given observations. This loss incorporates\nthe similarity measure over positive and negative pairs, each re-weighted by an\nindividual adaptive temperature, regulated to prevent ill solutions. Our\napproach, Adaptive Multi-Head Contrastive Learning (AMCL), can be applied to\nand experimentally enhances several popular contrastive learning methods such\nas SimCLR, MoCo, and Barlow Twins. The improvement remains consistent across\nvarious backbones and linear probing epochs, and becomes more significant when\nemploying multiple augmentation methods.\n","authors":["Lei Wang","Piotr Koniusz","Tom Gedeon","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.05615v2.pdf","comment":"Accepted at the 18th European Conference on Computer Vision (ECCV\n  2024)"},{"id":"http://arxiv.org/abs/2403.02502v2","updated":"2024-07-10T17:36:25Z","published":"2024-03-04T21:50:29Z","title":"Trial and Error: Exploration-Based Trajectory Optimization for LLM\n  Agents","summary":"  Large Language Models (LLMs) have become integral components in various\nautonomous agent systems. In this study, we present an exploration-based\ntrajectory optimization approach, referred to as ETO. This learning method is\ndesigned to enhance the performance of open LLM agents. Contrary to previous\nstudies that exclusively train on successful expert trajectories, our method\nallows agents to learn from their exploration failures. This leads to improved\nperformance through an iterative optimization framework. During the exploration\nphase, the agent interacts with the environment while completing given tasks,\ngathering failure trajectories to create contrastive trajectory pairs. In the\nsubsequent training phase, the agent utilizes these trajectory preference pairs\nto update its policy using contrastive learning methods like DPO. This\niterative cycle of exploration and training fosters continued improvement in\nthe agents. Our experiments on three complex tasks demonstrate that ETO\nconsistently surpasses baseline performance by a large margin. Furthermore, an\nexamination of task-solving efficiency and potential in scenarios lacking\nexpert trajectory underscores the effectiveness of our approach.\n","authors":["Yifan Song","Da Yin","Xiang Yue","Jie Huang","Sujian Li","Bill Yuchen Lin"],"pdf_url":"https://arxiv.org/pdf/2403.02502v2.pdf","comment":"Accepted to ACL 2024 Main Conference; Camera Ready"},{"id":"http://arxiv.org/abs/2311.05657v3","updated":"2024-07-10T17:36:02Z","published":"2023-11-09T00:30:13Z","title":"Agent Lumos: Unified and Modular Training for Open-Source Language\n  Agents","summary":"  Closed-source agents suffer from several issues such as a lack of\naffordability, transparency, and reproducibility, particularly on complex\ninteractive tasks. This motivates the development of open-source alternatives.\nWe introduce LUMOS, one of the first frameworks for training open-source\nLLM-based agents. LUMOS features a learnable, unified, and modular architecture\nwith a planning module that learns high-level subgoal generation, and a\ngrounding module trained to translate these into actions using various tools in\nthe execution module. The design allows for modular upgrades and wider\napplicability to diverse interactive tasks. To foster generalizable agent\nlearning, we collect large-scale, unified, and high-quality training\nannotations derived from diverse ground-truth reasoning rationales across\nvarious complex interactive tasks. On 9 datasets, LUMOS exhibits several key\nadvantages: (1) LUMOS excels multiple larger open-source agents on the held-out\ndatasets (unused for training) for each task type. LUMOS even surpasses GPT\nagents on QA and web tasks; (2) LUMOS outperforms open-source agents produced\nby chain-of-thoughts and unmodularized integrated training; and (3) LUMOS\neffectively generalizes to unseen tasks, outperforming 33B-scale agents and\ndomain-specific agents.\n","authors":["Da Yin","Faeze Brahman","Abhilasha Ravichander","Khyathi Chandu","Kai-Wei Chang","Yejin Choi","Bill Yuchen Lin"],"pdf_url":"https://arxiv.org/pdf/2311.05657v3.pdf","comment":"Accepted to ACL 2024 Main Conference; Camera Ready. Project website:\n  https://allenai.github.io/lumos/"},{"id":"http://arxiv.org/abs/2404.09349v2","updated":"2024-07-10T17:32:29Z","published":"2024-04-14T20:14:38Z","title":"Adversarial Robustness Limits via Scaling-Law and Human-Alignment\n  Studies","summary":"  This paper revisits the simple, long-studied, yet still unsolved problem of\nmaking image classifiers robust to imperceptible perturbations. Taking CIFAR10\nas an example, SOTA clean accuracy is about $100$%, but SOTA robustness to\n$\\ell_{\\infty}$-norm bounded perturbations barely exceeds $70$%. To understand\nthis gap, we analyze how model size, dataset size, and synthetic data quality\naffect robustness by developing the first scaling laws for adversarial\ntraining. Our scaling laws reveal inefficiencies in prior art and provide\nactionable feedback to advance the field. For instance, we discovered that SOTA\nmethods diverge notably from compute-optimal setups, using excess compute for\ntheir level of robustness. Leveraging a compute-efficient setup, we surpass the\nprior SOTA with $20$% ($70$%) fewer training (inference) FLOPs. We trained\nvarious compute-efficient models, with our best achieving $74$% AutoAttack\naccuracy ($+3$% gain). However, our scaling laws also predict robustness slowly\ngrows then plateaus at $90$%: dwarfing our new SOTA by scaling is impractical,\nand perfect robustness is impossible. To better understand this predicted\nlimit, we carry out a small-scale human evaluation on the AutoAttack data that\nfools our top-performing model. Concerningly, we estimate that human\nperformance also plateaus near $90$%, which we show to be attributable to\n$\\ell_{\\infty}$-constrained attacks' generation of invalid images not\nconsistent with their original labels. Having characterized limiting\nroadblocks, we outline promising paths for future research.\n","authors":["Brian R. Bartoldson","James Diffenderfer","Konstantinos Parasyris","Bhavya Kailkhura"],"pdf_url":"https://arxiv.org/pdf/2404.09349v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2407.07868v1","updated":"2024-07-10T17:32:05Z","published":"2024-07-10T17:32:05Z","title":"Green Screen Augmentation Enables Scene Generalisation in Robotic\n  Manipulation","summary":"  Generalising vision-based manipulation policies to novel environments remains\na challenging area with limited exploration. Current practices involve\ncollecting data in one location, training imitation learning or reinforcement\nlearning policies with this data, and deploying the policy in the same\nlocation. However, this approach lacks scalability as it necessitates data\ncollection in multiple locations for each task. This paper proposes a novel\napproach where data is collected in a location predominantly featuring green\nscreens. We introduce Green-screen Augmentation (GreenAug), employing a chroma\nkey algorithm to overlay background textures onto a green screen. Through\nextensive real-world empirical studies with over 850 training demonstrations\nand 8.2k evaluation episodes, we demonstrate that GreenAug surpasses no\naugmentation, standard computer vision augmentation, and prior generative\naugmentation methods in performance. While no algorithmic novelties are\nclaimed, our paper advocates for a fundamental shift in data collection\npractices. We propose that real-world demonstrations in future research should\nutilise green screens, followed by the application of GreenAug. We believe\nGreenAug unlocks policy generalisation to visually distinct novel locations,\naddressing the current scene generalisation limitations in robot learning.\n","authors":["Eugene Teoh","Sumit Patidar","Xiao Ma","Stephen James"],"pdf_url":"https://arxiv.org/pdf/2407.07868v1.pdf","comment":"Project website: https://greenaug.github.io/"},{"id":"http://arxiv.org/abs/2407.07858v1","updated":"2024-07-10T17:20:59Z","published":"2024-07-10T17:20:59Z","title":"FACTS About Building Retrieval Augmented Generation-based Chatbots","summary":"  Enterprise chatbots, powered by generative AI, are emerging as key\napplications to enhance employee productivity. Retrieval Augmented Generation\n(RAG), Large Language Models (LLMs), and orchestration frameworks like\nLangchain and Llamaindex are crucial for building these chatbots. However,\ncreating effective enterprise chatbots is challenging and requires meticulous\nRAG pipeline engineering. This includes fine-tuning embeddings and LLMs,\nextracting documents from vector databases, rephrasing queries, reranking\nresults, designing prompts, honoring document access controls, providing\nconcise responses, including references, safeguarding personal information, and\nbuilding orchestration agents. We present a framework for building RAG-based\nchatbots based on our experience with three NVIDIA chatbots: for IT/HR\nbenefits, financial earnings, and general content. Our contributions are\nthree-fold: introducing the FACTS framework (Freshness, Architectures, Cost,\nTesting, Security), presenting fifteen RAG pipeline control points, and\nproviding empirical results on accuracy-latency tradeoffs between large and\nsmall LLMs. To the best of our knowledge, this is the first paper of its kind\nthat provides a holistic view of the factors as well as solutions for building\nsecure enterprise-grade chatbots.\"\n","authors":["Rama Akkiraju","Anbang Xu","Deepak Bora","Tan Yu","Lu An","Vishal Seth","Aaditya Shukla","Pritam Gundecha","Hridhay Mehta","Ashwin Jha","Prithvi Raj","Abhinav Balasubramanian","Murali Maram","Guru Muthusamy","Shivakesh Reddy Annepally","Sidney Knowles","Min Du","Nick Burnett","Sean Javiya","Ashok Marannan","Mamta Kumari","Surbhi Jha","Ethan Dereszenski","Anupam Chakraborty","Subhash Ranjan","Amina Terfai","Anoop Surya","Tracey Mercer","Vinodh Kumar Thanigachalam","Tamar Bar","Sanjana Krishnan","Samy Kilaru","Jasmine Jaksic","Nave Algarici","Jacob Liberman","Joey Conway","Sonu Nayyar","Justin Boitano"],"pdf_url":"https://arxiv.org/pdf/2407.07858v1.pdf","comment":"8 pages, 6 figures, 2 tables, Preprint submission to ACM CIKM 2024"},{"id":"http://arxiv.org/abs/2407.07852v1","updated":"2024-07-10T17:13:17Z","published":"2024-07-10T17:13:17Z","title":"OpenDiLoCo: An Open-Source Framework for Globally Distributed\n  Low-Communication Training","summary":"  OpenDiLoCo is an open-source implementation and replication of the\nDistributed Low-Communication (DiLoCo) training method for large language\nmodels. We provide a reproducible implementation of the DiLoCo experiments,\noffering it within a scalable, decentralized training framework using the\nHivemind library. We demonstrate its effectiveness by training a model across\ntwo continents and three countries, while maintaining 90-95% compute\nutilization. Additionally, we conduct ablations studies focusing on the\nalgorithm's compute efficiency, scalability in the number of workers and show\nthat its gradients can be all-reduced using FP16 without any performance\ndegradation. Furthermore, we scale OpenDiLoCo to 3x the size of the original\nwork, demonstrating its effectiveness for billion parameter models.\n","authors":["Sami Jaghouar","Jack Min Ong","Johannes Hagemann"],"pdf_url":"https://arxiv.org/pdf/2407.07852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11366v2","updated":"2024-07-10T17:12:45Z","published":"2023-10-17T16:04:33Z","title":"Lie Group Decompositions for Equivariant Neural Networks","summary":"  Invariance and equivariance to geometrical transformations have proven to be\nvery useful inductive biases when training (convolutional) neural network\nmodels, especially in the low-data regime. Much work has focused on the case\nwhere the symmetry group employed is compact or abelian, or both. Recent work\nhas explored enlarging the class of transformations used to the case of Lie\ngroups, principally through the use of their Lie algebra, as well as the group\nexponential and logarithm maps. The applicability of such methods is limited by\nthe fact that depending on the group of interest $G$, the exponential map may\nnot be surjective. Further limitations are encountered when $G$ is neither\ncompact nor abelian. Using the structure and geometry of Lie groups and their\nhomogeneous spaces, we present a framework by which it is possible to work with\nsuch groups primarily focusing on the groups $G = \\text{GL}^{+}(n, \\mathbb{R})$\nand $G = \\text{SL}(n, \\mathbb{R})$, as well as their representation as affine\ntransformations $\\mathbb{R}^{n} \\rtimes G$. Invariant integration as well as a\nglobal parametrization is realized by a decomposition into subgroups and\nsubmanifolds which can be handled individually. Under this framework, we show\nhow convolution kernels can be parametrized to build models equivariant with\nrespect to affine transformations. We evaluate the robustness and\nout-of-distribution generalisation capability of our model on the benchmark\naffine-invariant classification task, outperforming previous proposals.\n","authors":["Mircea Mironenco","Patrick ForrÃ©"],"pdf_url":"https://arxiv.org/pdf/2310.11366v2.pdf","comment":"Published at ICLR 2024. Code is available at\n  https://github.com/mirceamironenco/rgenn"},{"id":"http://arxiv.org/abs/2407.07848v1","updated":"2024-07-10T17:10:10Z","published":"2024-07-10T17:10:10Z","title":"Uncovering Layer-Dependent Activation Sparsity Patterns in ReLU\n  Transformers","summary":"  Previous work has demonstrated that MLPs within ReLU Transformers exhibit\nhigh levels of sparsity, with many of their activations equal to zero for any\ngiven token. We build on that work to more deeply explore how token-level\nsparsity evolves over the course of training, and how it connects to broader\nsparsity patterns over the course of a sequence or batch, demonstrating that\nthe different layers within small transformers exhibit distinctly\nlayer-specific patterns on both of these fronts. In particular, we demonstrate\nthat the first and last layer of the network have distinctive and in many ways\ninverted relationships to sparsity, and explore implications for the structure\nof feature representations being learned at different depths of the model. We\nadditionally explore the phenomenon of ReLU dimensions \"turning off\", and show\nevidence suggesting that \"neuron death\" is being primarily driven by the\ndynamics of training, rather than simply occurring randomly or accidentally as\na result of outliers.\n","authors":["Cody Wild","Jesper Anderson"],"pdf_url":"https://arxiv.org/pdf/2407.07848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11354v2","updated":"2024-07-10T17:05:43Z","published":"2024-02-17T18:08:37Z","title":"Probabilistic Routing for Graph-Based Approximate Nearest Neighbor\n  Search","summary":"  Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a\npivotal challenge in the field of machine learning. In recent years,\ngraph-based methods have emerged as the superior approach to ANNS, establishing\na new state of the art. Although various optimizations for graph-based ANNS\nhave been introduced, they predominantly rely on heuristic methods that lack\nformal theoretical backing. This paper aims to enhance routing within\ngraph-based ANNS by introducing a method that offers a probabilistic guarantee\nwhen exploring a node's neighbors in the graph. We formulate the problem as\nprobabilistic routing and develop two baseline strategies by incorporating\nlocality-sensitive techniques. Subsequently, we introduce PEOs, a novel\napproach that efficiently identifies which neighbors in the graph should be\nconsidered for exact distance calculation, thus significantly improving\nefficiency in practice. Our experiments demonstrate that equipping PEOs can\nincrease throughput on commonly utilized graph indexes (HNSW and NSSG) by a\nfactor of 1.6 to 2.5, and its efficiency consistently outperforms the\nleading-edge routing technique by 1.1 to 1.4 times.\n","authors":["Kejing Lu","Chuan Xiao","Yoshiharu Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2402.11354v2.pdf","comment":"Source code is available at https://github.com/ICML2024-code/PEOs"},{"id":"http://arxiv.org/abs/2405.19327v4","updated":"2024-07-10T16:55:47Z","published":"2024-05-29T17:57:16Z","title":"MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model\n  Series","summary":"  Large Language Models (LLMs) have made great strides in recent years to\nachieve unprecedented performance across different tasks. However, due to\ncommercial interest, the most competitive models like GPT, Gemini, and Claude\nhave been gated behind proprietary interfaces without disclosing the training\ndetails. Recently, many institutions have open-sourced several strong LLMs like\nLLaMA-3, comparable to existing closed-source LLMs. However, only the model's\nweights are provided with most details (e.g., intermediate checkpoints,\npre-training corpus, and training code, etc.) being undisclosed. To improve the\ntransparency of LLMs, the research community has formed to open-source truly\nopen LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training\ncorpus and training code) are being provided. These models have greatly\nadvanced the scientific study of these large models including their strengths,\nweaknesses, biases and risks. However, we observe that the existing truly open\nLLMs on reasoning, knowledge, and coding tasks are still inferior to existing\nstate-of-the-art LLMs with similar model sizes. To this end, we open-source\nMAP-Neo, a highly capable and transparent bilingual language model with 7B\nparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the\nfirst fully open-sourced bilingual LLM with comparable performance compared to\nexisting state-of-the-art LLMs. Moreover, we open-source all details to\nreproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning\npipeline, checkpoints, and well-optimized training/evaluation framework are\nprovided. Finally, we hope our MAP-Neo will enhance and strengthen the open\nresearch community and inspire more innovations and creativities to facilitate\nthe further improvements of LLMs.\n","authors":["Ge Zhang","Scott Qu","Jiaheng Liu","Chenchen Zhang","Chenghua Lin","Chou Leuang Yu","Danny Pan","Esther Cheng","Jie Liu","Qunshu Lin","Raven Yuan","Tuney Zheng","Wei Pang","Xinrun Du","Yiming Liang","Yinghao Ma","Yizhi Li","Ziyang Ma","Bill Lin","Emmanouil Benetos","Huan Yang","Junting Zhou","Kaijing Ma","Minghao Liu","Morry Niu","Noah Wang","Quehry Que","Ruibo Liu","Sine Liu","Shawn Guo","Soren Gao","Wangchunshu Zhou","Xinyue Zhang","Yizhi Zhou","Yubo Wang","Yuelin Bai","Yuhan Zhang","Yuxiang Zhang","Zenith Wang","Zhenzhu Yang","Zijian Zhao","Jiajun Zhang","Wanli Ouyang","Wenhao Huang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2405.19327v4.pdf","comment":"https://map-neo.github.io/"},{"id":"http://arxiv.org/abs/2407.07829v1","updated":"2024-07-10T16:51:32Z","published":"2024-07-10T16:51:32Z","title":"Disentangled Representation Learning through Geometry Preservation with\n  the Gromov-Monge Gap","summary":"  Learning disentangled representations in an unsupervised manner is a\nfundamental challenge in machine learning. Solving it may unlock other\nproblems, such as generalization, interpretability, or fairness. While\nremarkably difficult to solve in general, recent works have shown that\ndisentanglement is provably achievable under additional assumptions that can\nleverage geometrical constraints, such as local isometry. To use these\ninsights, we propose a novel perspective on disentangled representation\nlearning built on quadratic optimal transport. Specifically, we formulate the\nproblem in the Gromov-Monge setting, which seeks isometric mappings between\ndistributions supported on different spaces. We propose the Gromov-Monge-Gap\n(GMG), a regularizer that quantifies the geometry-preservation of an arbitrary\npush-forward map between two distributions supported on different spaces. We\ndemonstrate the effectiveness of GMG regularization for disentanglement on four\nstandard benchmarks. Moreover, we show that geometry preservation can even\nencourage unsupervised disentanglement without the standard reconstruction\nobjective - making the underlying model decoder-free, and promising a more\npractically viable and scalable perspective on unsupervised disentanglement.\n","authors":["ThÃ©o Uscidda","Luca Eyring","Karsten Roth","Fabian Theis","Zeynep Akata","Marco Cuturi"],"pdf_url":"https://arxiv.org/pdf/2407.07829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07827v1","updated":"2024-07-10T16:50:59Z","published":"2024-07-10T16:50:59Z","title":"Estimating the stability number of a random graph using convolutional\n  neural networks","summary":"  Graph combinatorial optimization problems are widely applicable and\nnotoriously difficult to compute; for example, consider the traveling salesman\nor facility location problems. In this paper, we explore the feasibility of\nusing convolutional neural networks (CNNs) on graph images to predict the\ncardinality of combinatorial properties of random graphs and networks.\nSpecifically, we use image representations of modified adjacency matrices of\nrandom graphs as training samples for a CNN model to predict the stability\nnumber of random graphs; where the stability number is the cardinality of a\nmaximum set of vertices containing no pairwise adjacency. Our approach\ndemonstrates the potential for applying deep learning in combinatorial\noptimization problems.\n","authors":["Randy Davila"],"pdf_url":"https://arxiv.org/pdf/2407.07827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07821v1","updated":"2024-07-10T16:45:52Z","published":"2024-07-10T16:45:52Z","title":"When to Accept Automated Predictions and When to Defer to Human\n  Judgment?","summary":"  Ensuring the reliability and safety of automated decision-making is crucial.\nIt is well-known that data distribution shifts in machine learning can produce\nunreliable outcomes. This paper proposes a new approach for measuring the\nreliability of predictions under distribution shifts. We analyze how the\noutputs of a trained neural network change using clustering to measure\ndistances between outputs and class centroids. We propose this distance as a\nmetric to evaluate the confidence of predictions under distribution shifts. We\nassign each prediction to a cluster with centroid representing the mean softmax\noutput for all correct predictions of a given class. We then define a safety\nthreshold for a class as the smallest distance from an incorrect prediction to\nthe given class centroid. We evaluate the approach on the MNIST and CIFAR-10\ndatasets using a Convolutional Neural Network and a Vision Transformer,\nrespectively. The results show that our approach is consistent across these\ndata sets and network models, and indicate that the proposed metric can offer\nan efficient way of determining when automated predictions are acceptable and\nwhen they should be deferred to human operators given a distribution shift.\n","authors":["Daniel Sikar","Artur Garcez","Tillman Weyde","Robin Bloomfield","Kaleem Peeroo"],"pdf_url":"https://arxiv.org/pdf/2407.07821v1.pdf","comment":"9 pages, 10 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.07818v1","updated":"2024-07-10T16:43:14Z","published":"2024-07-10T16:43:14Z","title":"The Misclassification Likelihood Matrix: Some Classes Are More Likely To\n  Be Misclassified Than Others","summary":"  This study introduces the Misclassification Likelihood Matrix (MLM) as a\nnovel tool for quantifying the reliability of neural network predictions under\ndistribution shifts. The MLM is obtained by leveraging softmax outputs and\nclustering techniques to measure the distances between the predictions of a\ntrained neural network and class centroids. By analyzing these distances, the\nMLM provides a comprehensive view of the model's misclassification tendencies,\nenabling decision-makers to identify the most common and critical sources of\nerrors. The MLM allows for the prioritization of model improvements and the\nestablishment of decision thresholds based on acceptable risk levels. The\napproach is evaluated on the MNIST dataset using a Convolutional Neural Network\n(CNN) and a perturbed version of the dataset to simulate distribution shifts.\nThe results demonstrate the effectiveness of the MLM in assessing the\nreliability of predictions and highlight its potential in enhancing the\ninterpretability and risk mitigation capabilities of neural networks. The\nimplications of this work extend beyond image classification, with ongoing\napplications in autonomous systems, such as self-driving cars, to improve the\nsafety and reliability of decision-making in complex, real-world environments.\n","authors":["Daniel Sikar","Artur Garcez","Robin Bloomfield","Tillman Weyde","Kaleem Peeroo","Naman Singh","Maeve Hutchinson","Mirela Reljan-Delaney"],"pdf_url":"https://arxiv.org/pdf/2407.07818v1.pdf","comment":"8 pages, 7 figures, 1 table"},{"id":"http://arxiv.org/abs/2305.13498v3","updated":"2024-07-10T16:33:34Z","published":"2023-05-22T21:28:57Z","title":"Parameter estimation from an Ornstein-Uhlenbeck process with measurement\n  noise","summary":"  This article aims to investigate the impact of noise on parameter fitting for\nan Ornstein-Uhlenbeck process, focusing on the effects of multiplicative and\nthermal noise on the accuracy of signal separation. To address these issues, we\npropose algorithms and methods that can effectively distinguish between thermal\nand multiplicative noise and improve the precision of parameter estimation for\noptimal data analysis. Specifically, we explore the impact of both\nmultiplicative and thermal noise on the obfuscation of the actual signal and\npropose methods to resolve them. First, we present an algorithm that can\neffectively separate thermal noise with comparable performance to Hamilton\nMonte Carlo (HMC) but with significantly improved speed. We then analyze\nmultiplicative noise and demonstrate that HMC is insufficient for isolating\nthermal and multiplicative noise. However, we show that, with additional\nknowledge of the ratio between thermal and multiplicative noise, we can\naccurately distinguish between the two types of noise when provided with a\nsufficiently large sampling rate or an amplitude of multiplicative noise\nsmaller than thermal noise. Thus, we demonstrate the mechanism underlying an\notherwise counterintuitive phenomenon: when multiplicative noise dominates the\nnoise spectrum, one can successfully estimate the parameters for such systems\nafter adding additional white noise to shift the noise balance.\n","authors":["Simon Carter","Lilianne Mujica-Parodi","Helmut H. Strey"],"pdf_url":"https://arxiv.org/pdf/2305.13498v3.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.07810v1","updated":"2024-07-10T16:30:27Z","published":"2024-07-10T16:30:27Z","title":"Transformer Alignment in Large Language Models","summary":"  Large Language Models (LLMs) have made significant strides in natural\nlanguage processing, and a precise understanding of the internal mechanisms\ndriving their success is essential. We regard LLMs as transforming embeddings\nvia a discrete, coupled, nonlinear, dynamical system in high dimensions. This\nperspective motivates tracing the trajectories of individual tokens as they\npass through transformer blocks, and linearizing the system along these\ntrajectories through their Jacobian matrices. In our analysis of 38 openly\navailable LLMs, we uncover the alignment of top left and right singular vectors\nof Residual Jacobians, as well as the emergence of linearity and layer-wise\nexponential growth. Notably, we discover that increased alignment\n$\\textit{positively correlates}$ with model performance. Metrics evaluated\npost-training show significant improvement in comparison to measurements made\nwith randomly initialized weights, highlighting the significant effects of\ntraining in transformers. These findings reveal a remarkable level of\nregularity that has previously been overlooked, reinforcing the dynamical\ninterpretation and paving the way for deeper understanding and optimization of\nLLM architectures.\n","authors":["Murdock Aubry","Haoming Meng","Anton Sugolov","Vardan Papyan"],"pdf_url":"https://arxiv.org/pdf/2407.07810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07802v1","updated":"2024-07-10T16:20:53Z","published":"2024-07-10T16:20:53Z","title":"ROSA: Random Subspace Adaptation for Efficient Fine-Tuning","summary":"  Model training requires significantly more memory, compared with inference.\nParameter efficient fine-tuning (PEFT) methods provide a means of adapting\nlarge models to downstream tasks using less memory. However, existing methods\nsuch as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce\nlatency overhead at inference time or achieve subpar downstream performance\ncompared with full fine-tuning. In this work we propose Random Subspace\nAdaptation (ROSA), a method that outperforms previous PEFT methods by a\nsignificant margin, while maintaining a zero latency overhead during inference\ntime. In contrast to previous methods, ROSA is able to adapt subspaces of\narbitrarily large dimension, better approximating full-finetuning. We\ndemonstrate both theoretically and experimentally that this makes ROSA strictly\nmore expressive than LoRA, without consuming additional memory during runtime.\nAs PEFT methods are especially useful in the natural language processing\ndomain, where models operate on scales that make full fine-tuning very\nexpensive, we evaluate ROSA in two common NLP scenarios: natural language\ngeneration (NLG) and natural language understanding (NLU) with GPT-2 and\nRoBERTa, respectively. We show that on almost every GLUE task ROSA outperforms\nLoRA by a significant margin, while also outperforming LoRA on NLG tasks. Our\ncode is available at https://github.com/rosa-paper/rosa\n","authors":["Marawan Gamal Abdel Hameed","Aristides Milios","Siva Reddy","Guillaume Rabusseau"],"pdf_url":"https://arxiv.org/pdf/2407.07802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07801v1","updated":"2024-07-10T16:17:49Z","published":"2024-07-10T16:17:49Z","title":"AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning","summary":"  In recent years, advancements in representation learning and language models\nhave propelled Automated Captioning (AC) to new heights, enabling the\ngeneration of human-level descriptions. Leveraging these advancements, we\npropose \\textbf{AVCap}, an \\textbf{A}udio-\\textbf{V}isual \\textbf{Cap}tioning\nframework, a simple yet powerful baseline approach applicable to audio-visual\ncaptioning. AVCap utilizes audio-visual features as text tokens, which has many\nadvantages not only in performance but also in the extensibility and\nscalability of the model. AVCap is designed around three pivotal dimensions:\nthe exploration of optimal audio-visual encoder architectures, the adaptation\nof pre-trained models according to the characteristics of generated text, and\nthe investigation into the efficacy of modality fusion in captioning. Our\nmethod outperforms existing audio-visual captioning methods across all metrics\nand the code is available on https://github.com/JongSuk1/AVCap\n","authors":["Jongsuk Kim","Jiwon Shin","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.07801v1.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2211.15597v2","updated":"2024-07-10T16:16:46Z","published":"2022-11-28T17:50:19Z","title":"Lightning Fast Video Anomaly Detection via Adversarial Knowledge\n  Distillation","summary":"  We propose a very fast frame-level model for anomaly detection in video,\nwhich learns to detect anomalies by distilling knowledge from multiple highly\naccurate object-level teacher models. To improve the fidelity of our student,\nwe distill the low-resolution anomaly maps of the teachers by jointly applying\nstandard and adversarial distillation, introducing an adversarial discriminator\nfor each teacher to distinguish between target and generated anomaly maps. We\nconduct experiments on three benchmarks (Avenue, ShanghaiTech, UCSD Ped2),\nshowing that our method is over 7 times faster than the fastest competing\nmethod, and between 28 and 62 times faster than object-centric models, while\nobtaining comparable results to recent methods. Our evaluation also indicates\nthat our model achieves the best trade-off between speed and accuracy, due to\nits previously unheard-of speed of 1480 FPS. In addition, we carry out a\ncomprehensive ablation study to justify our architectural design choices. Our\ncode is freely available at: https://github.com/ristea/fast-aed.\n","authors":["Florinel-Alin Croitoru","Nicolae-Catalin Ristea","Dana Dascalescu","Radu Tudor Ionescu","Fahad Shahbaz Khan","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2211.15597v2.pdf","comment":"Accepted in Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2407.07796v1","updated":"2024-07-10T16:14:34Z","published":"2024-07-10T16:14:34Z","title":"Evaluating Large Language Models with Grid-Based Game Competitions: An\n  Extensible LLM Benchmark and Leaderboard","summary":"  We introduce a novel and extensible benchmark for large language models\n(LLMs) through grid-based games such as Tic-Tac-Toe, Connect-Four, and Gomoku.\nThe open-source game simulation code, available on GitHub, allows LLMs to\ncompete and generates detailed data files in JSON, CSV, TXT, and PNG formats\nfor leaderboard rankings and further analysis. We present the results of games\namong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by\nAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and\nGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of\nresults from other LLMs. In total, we simulated 2,310 matches (5 sessions for\neach pair among 7 LLMs and a random player) across three types of games, using\nthree distinct prompt types: list, illustration, and image. The results\nrevealed significant variations in LLM performance across different games and\nprompt types, with analysis covering win and disqualification rates, missed\nopportunity analysis, and invalid move analysis. The details of the leaderboard\nand result matrix data are available as open-access data on GitHub. This study\nenhances our understanding of LLMs' capabilities in playing games they were not\nspecifically trained for, helping to assess their rule comprehension and\nstrategic thinking. On the path to Artificial General Intelligence (AGI), this\nstudy lays the groundwork for future exploration into their utility in complex\ndecision-making scenarios, illuminating their strategic thinking abilities and\noffering directions for further inquiry into the limits of LLMs within\ngame-based frameworks.\n","authors":["Oguzhan Topsakal","Colby Jacob Edell","Jackson Bailey Harper"],"pdf_url":"https://arxiv.org/pdf/2407.07796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00411v2","updated":"2024-07-10T16:12:29Z","published":"2024-03-30T16:41:24Z","title":"End-to-end data-driven weather forecasting","summary":"  Weather forecasting is critical for a range of human activities including\ntransportation, agriculture, industry, as well as the safety of the general\npublic. Machine learning models have the potential to transform the complex\nweather prediction pipeline, but current approaches still rely on numerical\nweather prediction (NWP) systems, limiting forecast speed and accuracy. Here we\ndemonstrate that a machine learning model can replace the entire operational\nNWP pipeline. Aardvark Weather, an end-to-end data-driven weather prediction\nsystem, ingests raw observations and outputs global gridded forecasts and local\nstation forecasts. Further, it can be optimised end-to-end to maximise\nperformance over quantities of interest. Global forecasts outperform an\noperational NWP baseline for multiple variables and lead times. Local station\nforecasts are skillful up to ten days lead time and achieve comparable and\noften lower errors than a post-processed global NWP baseline and a\nstate-of-the-art end-to-end forecasting system with input from human\nforecasters. These forecasts are produced with a remarkably simple neural\nprocess model using just 8\\% of the input data and three orders of magnitude\nless compute than existing NWP and hybrid AI-NWP methods. We anticipate that\nAardvark Weather will be the starting point for a new generation of end-to-end\nmachine learning models for medium-range forecasting that will reduce\ncomputational costs by orders of magnitude and enable the rapid and cheap\ncreation of bespoke models for users in a variety of fields, including for the\ndeveloping world where state-of-the-art local models are not currently\navailable.\n","authors":["Anna Vaughan","Stratis Markou","Will Tebbutt","James Requeima","Wessel P. Bruinsma","Tom R. Andersson","Michael Herzog","Nicholas D. Lane","Matthew Chantry","J. Scott Hosking","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2404.00411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07794v1","updated":"2024-07-10T16:12:09Z","published":"2024-07-10T16:12:09Z","title":"Reinforcement Learning of Adaptive Acquisition Policies for Inverse\n  Problems","summary":"  A promising way to mitigate the expensive process of obtaining a\nhigh-dimensional signal is to acquire a limited number of low-dimensional\nmeasurements and solve an under-determined inverse problem by utilizing the\nstructural prior about the signal. In this paper, we focus on adaptive\nacquisition schemes to save further the number of measurements. To this end, we\npropose a reinforcement learning-based approach that sequentially collects\nmeasurements to better recover the underlying signal by acquiring fewer\nmeasurements. Our approach applies to general inverse problems with continuous\naction spaces and jointly learns the recovery algorithm. Using insights\nobtained from theoretical analysis, we also provide a probabilistic design for\nour methods using variational formulation. We evaluate our approach on multiple\ndatasets and with two measurement spaces (Gaussian, Radon). Our results confirm\nthe benefits of adaptive strategies in low-acquisition horizon settings.\n","authors":["Gianluigi Silvestri","Fabio Valerio Massoli","Tribhuvanesh Orekondy","Afshin Abdi","Arash Behboodi"],"pdf_url":"https://arxiv.org/pdf/2407.07794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06122v2","updated":"2024-07-10T16:08:08Z","published":"2024-01-11T18:57:17Z","title":"Manipulating Feature Visualizations with Gradient Slingshots","summary":"  Deep Neural Networks (DNNs) are capable of learning complex and versatile\nrepresentations, however, the semantic nature of the learned concepts remains\nunknown. A common method used to explain the concepts learned by DNNs is\nFeature Visualization (FV), which generates a synthetic input signal that\nmaximally activates a particular neuron in the network. In this paper, we\ninvestigate the vulnerability of this approach to adversarial model\nmanipulations and introduce a novel method for manipulating FV without\nsignificantly impacting the model's decision-making process. The key\ndistinction of our proposed approach is that it does not alter the model\narchitecture. We evaluate the effectiveness of our method on several neural\nnetwork models and demonstrate its capabilities to hide the functionality of\narbitrarily chosen neurons by masking the original explanations of neurons with\nchosen target explanations during model auditing.\n","authors":["Dilyara Bareeva","Marina M. -C. HÃ¶hne","Alexander Warnecke","Lukas Pirch","Klaus-Robert MÃ¼ller","Konrad Rieck","Kirill Bykov"],"pdf_url":"https://arxiv.org/pdf/2401.06122v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07788v1","updated":"2024-07-10T16:04:18Z","published":"2024-07-10T16:04:18Z","title":"BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark","summary":"  We introduce BiGym, a new benchmark and learning environment for mobile\nbi-manual demo-driven robotic manipulation. BiGym features 40 diverse tasks set\nin home environments, ranging from simple target reaching to complex kitchen\ncleaning. To capture the real-world performance accurately, we provide\nhuman-collected demonstrations for each task, reflecting the diverse modalities\nfound in real-world robot trajectories. BiGym supports a variety of\nobservations, including proprioceptive data and visual inputs such as RGB, and\ndepth from 3 camera views. To validate the usability of BiGym, we thoroughly\nbenchmark the state-of-the-art imitation learning algorithms and demo-driven\nreinforcement learning algorithms within the environment and discuss the future\nopportunities.\n","authors":["Nikita Chernyadev","Nicholas Backshall","Xiao Ma","Yunfan Lu","Younggyo Seo","Stephen James"],"pdf_url":"https://arxiv.org/pdf/2407.07788v1.pdf","comment":"Project webpage: https://chernyadev.github.io/bigym/"},{"id":"http://arxiv.org/abs/2407.07787v1","updated":"2024-07-10T16:04:08Z","published":"2024-07-10T16:04:08Z","title":"Continuous Control with Coarse-to-fine Reinforcement Learning","summary":"  Despite recent advances in improving the sample-efficiency of reinforcement\nlearning (RL) algorithms, designing an RL algorithm that can be practically\ndeployed in real-world environments remains a challenge. In this paper, we\npresent Coarse-to-fine Reinforcement Learning (CRL), a framework that trains RL\nagents to zoom-into a continuous action space in a coarse-to-fine manner,\nenabling the use of stable, sample-efficient value-based RL algorithms for\nfine-grained continuous control tasks. Our key idea is to train agents that\noutput actions by iterating the procedure of (i) discretizing the continuous\naction space into multiple intervals and (ii) selecting the interval with the\nhighest Q-value to further discretize at the next level. We then introduce a\nconcrete, value-based algorithm within the CRL framework called Coarse-to-fine\nQ-Network (CQN). Our experiments demonstrate that CQN significantly outperforms\nRL and behavior cloning baselines on 20 sparsely-rewarded RLBench manipulation\ntasks with a modest number of environment interactions and expert\ndemonstrations. We also show that CQN robustly learns to solve real-world\nmanipulation tasks within a few minutes of online training.\n","authors":["Younggyo Seo","Jafar UruÃ§","Stephen James"],"pdf_url":"https://arxiv.org/pdf/2407.07787v1.pdf","comment":"Project webpage: https://younggyo.me/cqn/"},{"id":"http://arxiv.org/abs/2312.08290v2","updated":"2024-07-10T16:04:03Z","published":"2023-12-13T17:06:33Z","title":"PhenDiff: Revealing Subtle Phenotypes with Diffusion Models in Real\n  Images","summary":"  For the past few years, deep generative models have increasingly been used in\nbiological research for a variety of tasks. Recently, they have proven to be\nvaluable for uncovering subtle cell phenotypic differences that are not\ndirectly discernible to the human eye. However, current methods employed to\nachieve this goal mainly rely on Generative Adversarial Networks (GANs). While\neffective, GANs encompass issues such as training instability and mode\ncollapse, and they do not accurately map images back to the model's latent\nspace, which is necessary to synthesize, manipulate, and thus interpret outputs\nbased on real images. In this work, we introduce PhenDiff: a multi-class\nconditional method leveraging Diffusion Models (DMs) designed to identify\nshifts in cellular phenotypes by translating a real image from one condition to\nanother. We qualitatively and quantitatively validate this method on cases\nwhere the phenotypic changes are visible or invisible, such as in low\nconcentrations of drug treatments. Overall, PhenDiff represents a valuable tool\nfor identifying cellular variations in real microscopy images. We anticipate\nthat it could facilitate the understanding of diseases and advance drug\ndiscovery through the identification of novel biomarkers.\n","authors":["Anis Bourou","Thomas Boyer","KÃ©vin Daupin","VÃ©ronique Dubreuil","AurÃ©lie De Thonel","ValÃ©rie Mezger","Auguste Genovesio"],"pdf_url":"https://arxiv.org/pdf/2312.08290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.10909v2","updated":"2024-07-10T15:53:32Z","published":"2022-09-22T10:32:08Z","title":"Vanilla Feedforward Neural Networks as a Discretization of Dynamical\n  Systems","summary":"  Deep learning has made significant applications in the field of data science\nand natural science. Some studies have linked deep neural networks to dynamic\nsystems, but the network structure is restricted to the residual network. It is\nknown that residual networks can be regarded as a numerical discretization of\ndynamic systems. In this paper, we back to the classical network structure and\nprove that the vanilla feedforward networks could also be a numerical\ndiscretization of dynamic systems, where the width of the network is equal to\nthe dimension of the input and output. Our proof is based on the properties of\nthe leaky-ReLU function and the numerical technique of splitting method to\nsolve differential equations. Our results could provide a new perspective for\nunderstanding the approximation properties of feedforward neural networks.\n","authors":["Yifei Duan","Li'ang Li","Guanghua Ji","Yongqiang Cai"],"pdf_url":"https://arxiv.org/pdf/2209.10909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07402v3","updated":"2024-07-10T15:52:48Z","published":"2023-10-11T11:38:18Z","title":"NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time-Series\n  Pretraining","summary":"  Recent research on time-series self-supervised models shows great promise in\nlearning semantic representations. However, it has been limited to small-scale\ndatasets, e.g., thousands of temporal sequences. In this work, we make key\ntechnical contributions that are tailored to the numerical properties of\ntime-series data and allow the model to scale to large datasets, e.g., millions\nof temporal sequences. We adopt the Transformer architecture by first\npartitioning the input into non-overlapping windows. Each window is then\ncharacterized by its normalized shape and two scalar values denoting the mean\nand standard deviation within each window. To embed scalar values that may\npossess arbitrary numerical amplitudes in a high-dimensional space, we propose\na numerically multi-scaled embedding module enumerating all possible numerical\nscales for the scalars. The model undergoes pretraining with a simple\ncontrastive objective on a large-scale dataset over a million sequences\ncollected by merging existing public data. We study its transfer performance on\na number of univariate and multivariate classification tasks, few shot\nlearning, unsupervised clustering and anomaly detection benchmarks. Our method\nexhibits remarkable improvement against previous pretraining approaches and\nestablishes the new state of the art, even compared with domain-specific\nnon-learning-based methods. Code is available at:\n\\url{https://github.com/chenguolin/NuTime}.\n","authors":["Chenguo Lin","Xumeng Wen","Wei Cao","Congrui Huang","Jiang Bian","Stephen Lin","Zhirong Wu"],"pdf_url":"https://arxiv.org/pdf/2310.07402v3.pdf","comment":"Accepted by TMLR 2024"},{"id":"http://arxiv.org/abs/2407.07765v1","updated":"2024-07-10T15:43:30Z","published":"2024-07-10T15:43:30Z","title":"Ramsey Theorems for Trees and a General 'Private Learning Implies Online\n  Learning' Theorem","summary":"  This work continues to investigate the link between differentially private\n(DP) and online learning. Alon, Livni, Malliaris, and Moran (2019) showed that\nfor binary concept classes, DP learnability of a given class implies that it\nhas a finite Littlestone dimension (equivalently, that it is online learnable).\nTheir proof relies on a model-theoretic result by Hodges (1997), which\ndemonstrates that any binary concept class with a large Littlestone dimension\ncontains a large subclass of thresholds. In a follow-up work, Jung, Kim, and\nTewari (2020) extended this proof to multiclass PAC learning with a bounded\nnumber of labels. Unfortunately, Hodges's result does not apply in other\nnatural settings such as multiclass PAC learning with an unbounded label space,\nand PAC learning of partial concept classes.\n  This naturally raises the question of whether DP learnability continues to\nimply online learnability in more general scenarios: indeed, Alon, Hanneke,\nHolzman, and Moran (2021) explicitly leave it as an open question in the\ncontext of partial concept classes, and the same question is open in the\ngeneral multiclass setting. In this work, we give a positive answer to these\nquestions showing that for general classification tasks, DP learnability\nimplies online learnability. Our proof reasons directly about Littlestone\ntrees, without relying on thresholds. We achieve this by establishing several\nRamsey-type theorems for trees, which might be of independent interest.\n","authors":["Simone Fioravanti","Steve Hanneke","Shay Moran","Hilla Schefler","Iska Tsubari"],"pdf_url":"https://arxiv.org/pdf/2407.07765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06053v2","updated":"2024-07-10T15:20:10Z","published":"2024-07-08T15:55:12Z","title":"Learning local equivariant representations for quantum operators","summary":"  Predicting quantum operator matrices such as Hamiltonian, overlap, and\ndensity matrices in the density functional theory (DFT) framework is crucial\nfor understanding material properties. Current methods often focus on\nindividual operators and struggle with efficiency and scalability for large\nsystems. Here we introduce a novel deep learning model, SLEM (strictly\nlocalized equivariant message-passing) for predicting multiple quantum\noperators, that achieves state-of-the-art accuracy while dramatically improving\ncomputational efficiency. SLEM's key innovation is its strict locality-based\ndesign, constructing local, equivariant representations for quantum tensors\nwhile preserving physical symmetries. This enables complex many-body dependence\nwithout expanding the effective receptive field, leading to superior data\nefficiency and transferability. Using an innovative SO(2) convolution\ntechnique, SLEM reduces the computational complexity of high-order tensor\nproducts and is therefore capable of handling systems requiring the $f$ and $g$\norbitals in their basis sets. We demonstrate SLEM's capabilities across diverse\n2D and 3D materials, achieving high accuracy even with limited training data.\nSLEM's design facilitates efficient parallelization, potentially extending DFT\nsimulations to systems with device-level sizes, opening new possibilities for\nlarge-scale quantum simulations and high-throughput materials discovery.\n","authors":["Zhanghao Zhouyin","Zixi Gan","Shishir Kumar Pandey","Linfeng Zhang","Qiangqiang Gu"],"pdf_url":"https://arxiv.org/pdf/2407.06053v2.pdf","comment":"11 pages, 5 figures and 4 tables"},{"id":"http://arxiv.org/abs/2305.05506v3","updated":"2024-07-10T15:10:17Z","published":"2023-05-09T14:54:59Z","title":"FedGT: Identification of Malicious Clients in Federated Learning with\n  Secure Aggregation","summary":"  We propose FedGT, a novel framework for identifying malicious clients in\nfederated learning with secure aggregation. Inspired by group testing, the\nframework leverages overlapping groups of clients to identify the presence of\nmalicious clients in the groups via a decoding operation. The clients\nidentified as malicious are then removed from the model training, which is\nperformed over the remaining clients. By choosing the size, number, and overlap\nbetween groups, FedGT strikes a balance between privacy and security.\nSpecifically, the server learns the aggregated model of the clients in each\ngroup - vanilla federated learning and secure aggregation correspond to the\nextreme cases of FedGT with group size equal to one and the total number of\nclients, respectively. The effectiveness of FedGT is demonstrated through\nextensive experiments on the MNIST, CIFAR-10, and ISIC2019 datasets in a\ncross-silo setting under different data-poisoning attacks. These experiments\nshowcase FedGT's ability to identify malicious clients, resulting in high model\nutility. We further show that FedGT significantly outperforms the private\nrobust aggregation approach based on the geometric median recently proposed by\nPillutla et al. in multiple settings.\n","authors":["Marvin Xhemrishi","Johan Ãstman","Antonia Wachter-Zeh","Alexandre Graell i Amat"],"pdf_url":"https://arxiv.org/pdf/2305.05506v3.pdf","comment":"Changes: 1. New testing strategy, 2. New scheme that does not require\n  hyperparameter tuning, 3. Added two versions of FedGT, 4. New experimental\n  results"},{"id":"http://arxiv.org/abs/2407.07737v1","updated":"2024-07-10T15:07:58Z","published":"2024-07-10T15:07:58Z","title":"Fine-Tuning Large Language Models with User-Level Differential Privacy","summary":"  We investigate practical and scalable algorithms for training large language\nmodels (LLMs) with user-level differential privacy (DP) in order to provably\nsafeguard all the examples contributed by each user. We study two variants of\nDP-SGD with: (1) example-level sampling (ELS) and per-example gradient\nclipping, and (2) user-level sampling (ULS) and per-user gradient clipping. We\nderive a novel user-level DP accountant that allows us to compute provably\ntight privacy guarantees for ELS. Using this, we show that while ELS can\noutperform ULS in specific settings, ULS generally yields better results when\neach user has a diverse collection of examples. We validate our findings\nthrough experiments in synthetic mean estimation and LLM fine-tuning tasks\nunder fixed compute budgets. We find that ULS is significantly better in\nsettings where either (1) strong privacy guarantees are required, or (2) the\ncompute budget is large. Notably, our focus on LLM-compatible training\nalgorithms allows us to scale to models with hundreds of millions of parameters\nand datasets with hundreds of thousands of users.\n","authors":["Zachary Charles","Arun Ganesh","Ryan McKenna","H. Brendan McMahan","Nicole Mitchell","Krishna Pillutla","Keith Rush"],"pdf_url":"https://arxiv.org/pdf/2407.07737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06216v2","updated":"2024-07-10T15:06:33Z","published":"2024-07-04T17:20:36Z","title":"Digital twin with automatic disturbance detection for real-time\n  optimization of a semi-autogenous grinding (SAG) mill","summary":"  This work describes the development and validation of a digital twin for a\nsemi-autogenous grinding (SAG) mill controlled by an expert system. The digital\ntwin consists of three modules emulating a closed-loop system: fuzzy logic for\nthe expert control, a state-space model for regulatory control, and a recurrent\nneural network for the SAG mill process. The model was trained with 68 hours of\ndata and validated with 8 hours of test data. It predicts the mill's behavior\nwithin a 2.5-minute horizon with a 30-second sampling time. The disturbance\ndetection evaluates the need for retraining, and the digital twin shows promise\nfor supervising the SAG mill with the expert control system. Future work will\nfocus on integrating this digital twin into real-time optimization strategies\nwith industrial validation.\n","authors":["Paulina Quintanilla","Francisco FernÃ¡ndez","Cristobal Mancilla","MatÃ­as Rojas","Mauricio Estrada","Daniel Navia"],"pdf_url":"https://arxiv.org/pdf/2407.06216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07726v1","updated":"2024-07-10T14:57:46Z","published":"2024-07-10T14:57:46Z","title":"PaliGemma: A versatile 3B VLM for transfer","summary":"  PaliGemma is an open Vision-Language Model (VLM) that is based on the\nSigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to\nbe a versatile and broadly knowledgeable base model that is effective to\ntransfer. It achieves strong performance on a wide variety of open-world tasks.\nWe evaluate PaliGemma on almost 40 diverse tasks including standard VLM\nbenchmarks, but also more specialized tasks such as remote-sensing and\nsegmentation.\n","authors":["Lucas Beyer","Andreas Steiner","AndrÃ© Susano Pinto","Alexander Kolesnikov","Xiao Wang","Daniel Salz","Maxim Neumann","Ibrahim Alabdulmohsin","Michael Tschannen","Emanuele Bugliarello","Thomas Unterthiner","Daniel Keysers","Skanda Koppula","Fangyu Liu","Adam Grycner","Alexey Gritsenko","Neil Houlsby","Manoj Kumar","Keran Rong","Julian Eisenschlos","Rishabh Kabra","Matthias Bauer","Matko BoÅ¡njak","Xi Chen","Matthias Minderer","Paul Voigtlaender","Ioana Bica","Ivana Balazevic","Joan Puigcerver","Pinelopi Papalampidi","Olivier Henaff","Xi Xiong","Radu Soricut","Jeremiah Harmsen","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2407.07726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07712v1","updated":"2024-07-10T14:44:25Z","published":"2024-07-10T14:44:25Z","title":"Deep-Graph-Sprints: Accelerated Representation Learning in\n  Continuous-Time Dynamic Graphs","summary":"  Continuous-time dynamic graphs (CTDGs) are essential for modeling\ninterconnected, evolving systems. Traditional methods for extracting knowledge\nfrom these graphs often depend on feature engineering or deep learning. Feature\nengineering is limited by the manual and time-intensive nature of crafting\nfeatures, while deep learning approaches suffer from high inference latency,\nmaking them impractical for real-time applications. This paper introduces\nDeep-Graph-Sprints (DGS), a novel deep learning architecture designed for\nefficient representation learning on CTDGs with low-latency inference\nrequirements. We benchmark DGS against state-of-the-art feature engineering and\ngraph neural network methods using five diverse datasets. The results indicate\nthat DGS achieves competitive performance while improving inference speed up to\n12x compared to other deep learning approaches on our tested benchmarks. Our\nmethod effectively bridges the gap between deep representation learning and\nlow-latency application requirements for CTDGs.\n","authors":["Ahmad Naser Eddin","Jacopo Bono","David AparÃ­cio","Hugo Ferreira","Pedro Ribeiro","Pedro Bizarro"],"pdf_url":"https://arxiv.org/pdf/2407.07712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05833v2","updated":"2024-07-10T14:37:50Z","published":"2023-10-09T16:22:11Z","title":"A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative\n  Models","summary":"  Generative models, like large language models, are becoming increasingly\nrelevant in our daily lives, yet a theoretical framework to assess their\ngeneralization behavior and uncertainty does not exist. Particularly, the\nproblem of uncertainty estimation is commonly solved in an ad-hoc and\ntask-dependent manner. For example, natural language approaches cannot be\ntransferred to image generation. In this paper, we introduce the first\nbias-variance-covariance decomposition for kernel scores. This decomposition\nrepresents a theoretical framework from which we derive a kernel-based variance\nand entropy for uncertainty estimation. We propose unbiased and consistent\nestimators for each quantity which only require generated samples but not the\nunderlying model itself. Based on the wide applicability of kernels, we\ndemonstrate our framework via generalization and uncertainty experiments for\nimage, audio, and language generation. Specifically, kernel entropy for\nuncertainty estimation is more predictive of performance on CoQA and TriviaQA\nquestion answering datasets than existing baselines and can also be applied to\nclosed-source models.\n","authors":["Sebastian G. Gruber","Florian Buettner"],"pdf_url":"https://arxiv.org/pdf/2310.05833v2.pdf","comment":"Published at ICML 2024: https://openreview.net/pdf?id=QwgSOwynxD"},{"id":"http://arxiv.org/abs/2407.07700v1","updated":"2024-07-10T14:33:28Z","published":"2024-07-10T14:33:28Z","title":"Split Conformal Prediction under Data Contamination","summary":"  Conformal prediction is a non-parametric technique for constructing\nprediction intervals or sets from arbitrary predictive models under the\nassumption that the data is exchangeable. It is popular as it comes with\ntheoretical guarantees on the marginal coverage of the prediction sets and the\nsplit conformal prediction variant has a very low computational cost compared\nto model training. We study the robustness of split conformal prediction in a\ndata contamination setting, where we assume a small fraction of the calibration\nscores are drawn from a different distribution than the bulk. We quantify the\nimpact of the corrupted data on the coverage and efficiency of the constructed\nsets when evaluated on \"clean\" test points, and verify our results with\nnumerical experiments. Moreover, we propose an adjustment in the classification\nsetting which we call Contamination Robust Conformal Prediction, and verify the\nefficacy of our approach using both synthetic and real datasets.\n","authors":["Jase Clarkson","Wenkai Xu","Mihai Cucuringu","Gesine Reinert"],"pdf_url":"https://arxiv.org/pdf/2407.07700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07684v1","updated":"2024-07-10T14:08:27Z","published":"2024-07-10T14:08:27Z","title":"Towards Human-Like Driving: Active Inference in Autonomous Vehicle\n  Control","summary":"  This paper presents a novel approach to Autonomous Vehicle (AV) control\nthrough the application of active inference, a theory derived from neuroscience\nthat conceptualizes the brain as a predictive machine. Traditional autonomous\ndriving systems rely heavily on Modular Pipelines, Imitation Learning, or\nReinforcement Learning, each with inherent limitations in adaptability,\ngeneralization, and computational efficiency. Active inference addresses these\nchallenges by minimizing prediction error (termed \"surprise\") through a dynamic\nmodel that balances perception and action. Our method integrates active\ninference with deep learning to manage lateral control in AVs, enabling them to\nperform lane following maneuvers within a simulated urban environment. We\ndemonstrate that our model, despite its simplicity, effectively learns and\ngeneralizes from limited data without extensive retraining, significantly\nreducing computational demands. The proposed approach not only enhances the\nadaptability and performance of AVs in dynamic scenarios but also aligns\nclosely with human-like driving behavior, leveraging a generative model to\npredict and adapt to environmental changes. Results from extensive experiments\nin the CARLA simulator show promising outcomes, outperforming traditional\nmethods in terms of adaptability and efficiency, thereby advancing the\npotential of active inference in real-world autonomous driving applications.\n","authors":["Elahe Delavari","John Moore","Junho Hong","Jaerock Kwon"],"pdf_url":"https://arxiv.org/pdf/2407.07684v1.pdf","comment":"9 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.07674v1","updated":"2024-07-10T14:00:20Z","published":"2024-07-10T14:00:20Z","title":"Feasibility Study on Active Learning of Smart Surrogates for Scientific\n  Simulations","summary":"  High-performance scientific simulations, important for comprehension of\ncomplex systems, encounter computational challenges especially when exploring\nextensive parameter spaces. There has been an increasing interest in developing\ndeep neural networks (DNNs) as surrogate models capable of accelerating the\nsimulations. However, existing approaches for training these DNN surrogates\nrely on extensive simulation data which are heuristically selected and\ngenerated with expensive computation -- a challenge under-explored in the\nliterature. In this paper, we investigate the potential of incorporating active\nlearning into DNN surrogate training. This allows intelligent and objective\nselection of training simulations, reducing the need to generate extensive\nsimulation data as well as the dependency of the performance of DNN surrogates\non pre-defined training simulations. In the problem context of constructing DNN\nsurrogates for diffusion equations with sources, we examine the efficacy of\ndiversity- and uncertainty-based strategies for selecting training simulations,\nconsidering two different DNN architecture. The results set the groundwork for\ndeveloping the high-performance computing infrastructure for Smart Surrogates\nthat supports on-the-fly generation of simulation data steered by active\nlearning strategies to potentially improve the efficiency of scientific\nsimulations.\n","authors":["Pradeep Bajracharya","Javier QuetzalcÃ³atl Toledo-MarÃ­n","Geoffrey Fox","Shantenu Jha","Linwei Wang"],"pdf_url":"https://arxiv.org/pdf/2407.07674v1.pdf","comment":"11 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.07670v1","updated":"2024-07-10T13:58:57Z","published":"2024-07-10T13:58:57Z","title":"Stochastic Gradient Descent for Two-layer Neural Networks","summary":"  This paper presents a comprehensive study on the convergence rates of the\nstochastic gradient descent (SGD) algorithm when applied to overparameterized\ntwo-layer neural networks. Our approach combines the Neural Tangent Kernel\n(NTK) approximation with convergence analysis in the Reproducing Kernel Hilbert\nSpace (RKHS) generated by NTK, aiming to provide a deep understanding of the\nconvergence behavior of SGD in overparameterized two-layer neural networks. Our\nresearch framework enables us to explore the intricate interplay between kernel\nmethods and optimization processes, shedding light on the optimization dynamics\nand convergence properties of neural networks. In this study, we establish\nsharp convergence rates for the last iterate of the SGD algorithm in\noverparameterized two-layer neural networks. Additionally, we have made\nsignificant advancements in relaxing the constraints on the number of neurons,\nwhich have been reduced from exponential dependence to polynomial dependence on\nthe sample size or number of iterations. This improvement allows for more\nflexibility in the design and scaling of neural networks, and will deepen our\ntheoretical understanding of neural network models trained with SGD.\n","authors":["Dinghao Cao","Zheng-Chu Guo","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2407.07670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07668v1","updated":"2024-07-10T13:51:15Z","published":"2024-07-10T13:51:15Z","title":"How to Leverage Predictive Uncertainty Estimates for Reducing\n  Catastrophic Forgetting in Online Continual Learning","summary":"  Many real-world applications require machine-learning models to be able to\ndeal with non-stationary data distributions and thus learn autonomously over an\nextended period of time, often in an online setting. One of the main challenges\nin this scenario is the so-called catastrophic forgetting (CF) for which the\nlearning model tends to focus on the most recent tasks while experiencing\npredictive degradation on older ones. In the online setting, the most effective\nsolutions employ a fixed-size memory buffer to store old samples used for\nreplay when training on new tasks. Many approaches have been presented to\ntackle this problem. However, it is not clear how predictive uncertainty\ninformation for memory management can be leveraged in the most effective manner\nand conflicting strategies are proposed to populate the memory. Are the\neasiest-to-forget or the easiest-to-remember samples more effective in\ncombating CF? Starting from the intuition that predictive uncertainty provides\nan idea of the samples' location in the decision space, this work presents an\nin-depth analysis of different uncertainty estimates and strategies for\npopulating the memory. The investigation provides a better understanding of the\ncharacteristics data points should have for alleviating CF. Then, we propose an\nalternative method for estimating predictive uncertainty via the generalised\nvariance induced by the negative log-likelihood. Finally, we demonstrate that\nthe use of predictive uncertainty measures helps in reducing CF in different\nsettings.\n","authors":["Giuseppe Serra","Ben Werner","Florian Buettner"],"pdf_url":"https://arxiv.org/pdf/2407.07668v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2405.18925"},{"id":"http://arxiv.org/abs/2407.07664v1","updated":"2024-07-10T13:44:19Z","published":"2024-07-10T13:44:19Z","title":"A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning\n  Geometry","summary":"  Hyperspherical Prototypical Learning (HPL) is a supervised approach to\nrepresentation learning that designs class prototypes on the unit hypersphere.\nThe prototypes bias the representations to class separation in a scale\ninvariant and known geometry. Previous approaches to HPL have either of the\nfollowing shortcomings: (i) they follow an unprincipled optimisation procedure;\nor (ii) they are theoretically sound, but are constrained to only one possible\nlatent dimension. In this paper, we address both shortcomings. To address (i),\nwe present a principled optimisation procedure whose solution we show is\noptimal. To address (ii), we construct well-separated prototypes in a wide\nrange of dimensions using linear block codes. Additionally, we give a full\ncharacterisation of the optimal prototype placement in terms of achievable and\nconverse bounds, showing that our proposed methods are near-optimal.\n","authors":["Martin LindstrÃ¶m","Borja RodrÃ­guez-GÃ¡lvez","Ragnar Thobaben","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2407.07664v1.pdf","comment":"14 pages: 9 of the main paper, 2 of references, and 3 of appendices.\n  To appear in the Proceedings of the Geometry-grounded Representation Learning\n  and Generative Modeling at the 41st International Conference on Machine\n  Learning, Vienna, Austria. Code is available at:\n  https://github.com/martinlindstrom/coding_theoretic_hpl"},{"id":"http://arxiv.org/abs/2407.07655v1","updated":"2024-07-10T13:35:04Z","published":"2024-07-10T13:35:04Z","title":"The Selective G-Bispectrum and its Inversion: Applications to\n  G-Invariant Networks","summary":"  An important problem in signal processing and deep learning is to achieve\n\\textit{invariance} to nuisance factors not relevant for the task. Since many\nof these factors are describable as the action of a group $G$ (e.g. rotations,\ntranslations, scalings), we want methods to be $G$-invariant. The\n$G$-Bispectrum extracts every characteristic of a given signal up to group\naction: for example, the shape of an object in an image, but not its\norientation. Consequently, the $G$-Bispectrum has been incorporated into deep\nneural network architectures as a computational primitive for\n$G$-invariance\\textemdash akin to a pooling mechanism, but with greater\nselectivity and robustness. However, the computational cost of the\n$G$-Bispectrum ($\\mathcal{O}(|G|^2)$, with $|G|$ the size of the group) has\nlimited its widespread adoption. Here, we show that the $G$-Bispectrum\ncomputation contains redundancies that can be reduced into a \\textit{selective\n$G$-Bispectrum} with $\\mathcal{O}(|G|)$ complexity. We prove desirable\nmathematical properties of the selective $G$-Bispectrum and demonstrate how its\nintegration in neural networks enhances accuracy and robustness compared to\ntraditional approaches, while enjoying considerable speeds-up compared to the\nfull $G$-Bispectrum.\n","authors":["Simon Mataigne","Johan Mathe","Sophia Sanborn","Christopher Hillar","Nina Miolane"],"pdf_url":"https://arxiv.org/pdf/2407.07655v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2312.00855v2","updated":"2024-07-10T13:27:54Z","published":"2023-12-01T15:03:29Z","title":"Refine, Discriminate and Align: Stealing Encoders via Sample-Wise\n  Prototypes and Multi-Relational Extraction","summary":"  This paper introduces RDA, a pioneering approach designed to address two\nprimary deficiencies prevalent in previous endeavors aiming at stealing\npre-trained encoders: (1) suboptimal performances attributed to biased\noptimization objectives, and (2) elevated query costs stemming from the\nend-to-end paradigm that necessitates querying the target encoder every epoch.\nSpecifically, we initially Refine the representations of the target encoder for\neach training sample, thereby establishing a less biased optimization objective\nbefore the steal-training phase. This is accomplished via a sample-wise\nprototype, which consolidates the target encoder's representations for a given\nsample's various perspectives. Demanding exponentially fewer queries compared\nto the end-to-end approach, prototypes can be instantiated to guide subsequent\nquery-free training. For more potent efficacy, we develop a multi-relational\nextraction loss that trains the surrogate encoder to Discriminate mismatched\nembedding-prototype pairs while Aligning those matched ones in terms of both\namplitude and angle. In this way, the trained surrogate encoder achieves\nstate-of-the-art results across the board in various downstream datasets with\nlimited queries. Moreover, RDA is shown to be robust to multiple widely-used\ndefenses.\n","authors":["Shuchi Wu","Chuan Ma","Kang Wei","Xiaogang Xu","Ming Ding","Yuwen Qian","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2312.00855v2.pdf","comment":"25 pages, 12 figures, 15 tables"},{"id":"http://arxiv.org/abs/2407.03482v2","updated":"2024-07-10T13:27:20Z","published":"2024-07-03T20:10:55Z","title":"Domain-Aware Fine-Tuning of Foundation Models","summary":"  Foundation models (FMs) have revolutionized computer vision, enabling\neffective learning across different domains. However, their performance under\ndomain shift is yet underexplored. This paper investigates the zero-shot domain\nadaptation potential of FMs by comparing different backbone architectures and\nintroducing novel domain-aware components that leverage domain related textual\nembeddings. We propose domain adaptive normalization, termed as Domino, which\nexplicitly leverages domain embeddings during fine-tuning, thus making the\nmodel domain aware. Ultimately, Domino enables more robust computer vision\nmodels that can adapt effectively to various unseen domains.\n","authors":["Ugur Ali Kaplan","Margret Keuper","Anna Khoreva","Dan Zhang","Yumeng Li"],"pdf_url":"https://arxiv.org/pdf/2407.03482v2.pdf","comment":"Accepted at ICML 2024 Workshop on Foundation Models in the Wild"},{"id":"http://arxiv.org/abs/2407.07639v1","updated":"2024-07-10T13:20:47Z","published":"2024-07-10T13:20:47Z","title":"Explaining Graph Neural Networks for Node Similarity on Graphs","summary":"  Similarity search is a fundamental task for exploiting information in various\napplications dealing with graph data, such as citation networks or knowledge\ngraphs. While this task has been intensively approached from heuristics to\ngraph embeddings and graph neural networks (GNNs), providing explanations for\nsimilarity has received less attention. In this work we are concerned with\nexplainable similarity search over graphs, by investigating how GNN-based\nmethods for computing node similarities can be augmented with explanations.\nSpecifically, we evaluate the performance of two prominent approaches towards\nexplanations in GNNs, based on the concepts of mutual information (MI), and\ngradient-based explanations (GB). We discuss their suitability and empirically\nvalidate the properties of their explanations over different popular graph\nbenchmarks. We find that unlike MI explanations, gradient-based explanations\nhave three desirable properties. First, they are actionable: selecting inputs\ndepending on them results in predictable changes in similarity scores. Second,\nthey are consistent: the effect of selecting certain inputs overlaps very\nlittle with the effect of discarding them. Third, they can be pruned\nsignificantly to obtain sparse explanations that retain the effect on\nsimilarity scores.\n","authors":["Daniel Daza","Cuong Xuan Chu","Trung-Kien Tran","Daria Stepanova","Michael Cochez","Paul Groth"],"pdf_url":"https://arxiv.org/pdf/2407.07639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07636v1","updated":"2024-07-10T13:16:12Z","published":"2024-07-10T13:16:12Z","title":"MoVEInt: Mixture of Variational Experts for Learning Human-Robot\n  Interactions from Demonstrations","summary":"  Shared dynamics models are important for capturing the complexity and\nvariability inherent in Human-Robot Interaction (HRI). Therefore, learning such\nshared dynamics models can enhance coordination and adaptability to enable\nsuccessful reactive interactions with a human partner. In this work, we propose\na novel approach for learning a shared latent space representation for HRIs\nfrom demonstrations in a Mixture of Experts fashion for reactively generating\nrobot actions from human observations. We train a Variational Autoencoder (VAE)\nto learn robot motions regularized using an informative latent space prior that\ncaptures the multimodality of the human observations via a Mixture Density\nNetwork (MDN). We show how our formulation derives from a Gaussian Mixture\nRegression formulation that is typically used approaches for learning HRI from\ndemonstrations such as using an HMM/GMM for learning a joint distribution over\nthe actions of the human and the robot. We further incorporate an additional\nregularization to prevent \"mode collapse\", a common phenomenon when using\nlatent space mixture models with VAEs. We find that our approach of using an\ninformative MDN prior from human observations for a VAE generates more accurate\nrobot motions compared to previous HMM-based or recurrent approaches of\nlearning shared latent representations, which we validate on various HRI\ndatasets involving interactions such as handshakes, fistbumps, waving, and\nhandovers. Further experiments in a real-world human-to-robot handover scenario\nshow the efficacy of our approach for generating successful interactions with\nfour different human interaction partners.\n","authors":["Vignesh Prasad","Alap Kshirsagar","Dorothea Koert","Ruth Stock-Homburg","Jan Peters","Georgia Chalvatzaki"],"pdf_url":"https://arxiv.org/pdf/2407.07636v1.pdf","comment":"Preprint version of paper accepted at IEEE RAL. Project URL:\n  https://bit.ly/MoVEInt"},{"id":"http://arxiv.org/abs/2307.00144v2","updated":"2024-07-10T13:15:10Z","published":"2023-06-30T21:32:32Z","title":"Abide by the Law and Follow the Flow: Conservation Laws for Gradient\n  Flows","summary":"  Understanding the geometric properties of gradient descent dynamics is a key\ningredient in deciphering the recent success of very large machine learning\nmodels. A striking observation is that trained over-parameterized models retain\nsome properties of the optimization initialization. This \"implicit bias\" is\nbelieved to be responsible for some favorable properties of the trained models\nand could explain their good generalization properties. The purpose of this\narticle is threefold. First, we rigorously expose the definition and basic\nproperties of \"conservation laws\", that define quantities conserved during\ngradient flows of a given model (e.g. of a ReLU network with a given\narchitecture) with any training data and any loss. Then we explain how to find\nthe maximal number of independent conservation laws by performing\nfinite-dimensional algebraic manipulations on the Lie algebra generated by the\nJacobian of the model. Finally, we provide algorithms to: a) compute a family\nof polynomial laws; b) compute the maximal number of (not necessarily\npolynomial) independent conservation laws. We provide showcase examples that we\nfully work out theoretically. Besides, applying the two algorithms confirms for\na number of ReLU network architectures that all known laws are recovered by the\nalgorithm, and that there are no other independent laws. Such computational\ntools pave the way to understanding desirable properties of optimization\ninitialization in large machine learning models.\n","authors":["Sibylle Marcotte","RÃ©mi Gribonval","Gabriel PeyrÃ©"],"pdf_url":"https://arxiv.org/pdf/2307.00144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07631v1","updated":"2024-07-10T13:09:52Z","published":"2024-07-10T13:09:52Z","title":"Pessimism Meets Risk: Risk-Sensitive Offline Reinforcement Learning","summary":"  We study risk-sensitive reinforcement learning (RL), a crucial field due to\nits ability to enhance decision-making in scenarios where it is essential to\nmanage uncertainty and minimize potential adverse outcomes. Particularly, our\nwork focuses on applying the entropic risk measure to RL problems. While\nexisting literature primarily investigates the online setting, there remains a\nlarge gap in understanding how to efficiently derive a near-optimal policy\nbased on this risk measure using only a pre-collected dataset. We center on the\nlinear Markov Decision Process (MDP) setting, a well-regarded theoretical\nframework that has yet to be examined from a risk-sensitive standpoint. In\nresponse, we introduce two provably sample-efficient algorithms. We begin by\npresenting a risk-sensitive pessimistic value iteration algorithm, offering a\ntight analysis by leveraging the structure of the risk-sensitive performance\nmeasure. To further improve the obtained bounds, we propose another pessimistic\nalgorithm that utilizes variance information and reference-advantage\ndecomposition, effectively improving both the dependence on the space dimension\n$d$ and the risk-sensitivity factor. To the best of our knowledge, we obtain\nthe first provably efficient risk-sensitive offline RL algorithms.\n","authors":["Dake Zhang","Boxiang Lyu","Shuang Qiu","Mladen Kolar","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.07631v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2407.07613v1","updated":"2024-07-10T12:52:24Z","published":"2024-07-10T12:52:24Z","title":"Probabilistic learning rate scheduler with provable convergence","summary":"  Learning rate schedulers have shown great success in speeding up the\nconvergence of learning algorithms in practice. However, their convergence to a\nminimum has not been proven theoretically. This difficulty mainly arises from\nthe fact that, while traditional convergence analysis prescribes to\nmonotonically decreasing (or constant) learning rates, schedulers opt for rates\nthat often increase and decrease through the training epochs. In this work, we\naim to bridge the gap by proposing a probabilistic learning rate scheduler\n(PLRS), that does not conform to the monotonically decreasing condition, with\nprovable convergence guarantees. In addition to providing detailed convergence\nproofs, we also show experimental results where the proposed PLRS performs\ncompetitively as other state-of-the-art learning rate schedulers across a\nvariety of datasets and architectures.\n","authors":["Dahlia Devapriya","Thulasi Tholeti","Janani Suresh","Sheetal Kalyani"],"pdf_url":"https://arxiv.org/pdf/2407.07613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07612v1","updated":"2024-07-10T12:50:44Z","published":"2024-07-10T12:50:44Z","title":"Teaching Transformers Causal Reasoning through Axiomatic Training","summary":"  For text-based AI systems to interact in the real world, causal reasoning is\nan essential skill. Since interventional data is costly to generate, we study\nto what extent an agent can learn causal reasoning from passive data.\nSpecifically, we consider an axiomatic training setup where an agent learns\nfrom multiple demonstrations of a causal axiom (or rule), rather than\nincorporating the axiom as an inductive bias or inferring it from data values.\nA key question is whether the agent would learn to generalize from the axiom\ndemonstrations to new scenarios. For example, if a transformer model is trained\non demonstrations of the causal transitivity axiom over small graphs, would it\ngeneralize to applying the transitivity axiom over large graphs? Our results,\nbased on a novel axiomatic training scheme, indicate that such generalization\nis possible. We consider the task of inferring whether a variable causes\nanother variable, given a causal graph structure. We find that a 67 million\nparameter transformer model, when trained on linear causal chains (along with\nsome noisy variations) can generalize well to new kinds of graphs, including\nlonger causal chains, causal chains with reversed order, and graphs with\nbranching; even when it is not explicitly trained for such settings. Our model\nperforms at par (or even better) than many larger language models such as\nGPT-4, Gemini Pro, and Phi-3. Overall, our axiomatic training framework\nprovides a new paradigm of learning causal reasoning from passive data that can\nbe used to learn arbitrary axioms, as long as sufficient demonstrations can be\ngenerated.\n","authors":["Aniket Vashishtha","Abhinav Kumar","Abbavaram Gowtham Reddy","Vineeth N Balasubramanian","Amit Sharma"],"pdf_url":"https://arxiv.org/pdf/2407.07612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07611v1","updated":"2024-07-10T12:50:43Z","published":"2024-07-10T12:50:43Z","title":"Physics-Informed Geometric Operators to Support Surrogate, Dimension\n  Reduction and Generative Models for Engineering Design","summary":"  In this work, we propose a set of physics-informed geometric operators (GOs)\nto enrich the geometric data provided for training surrogate/discriminative\nmodels, dimension reduction, and generative models, typically employed for\nperformance prediction, dimension reduction, and creating data-driven\nparameterisations, respectively. However, as both the input and output streams\nof these models consist of low-level shape representations, they often fail to\ncapture shape characteristics essential for performance analyses. Therefore,\nthe proposed GOs exploit the differential and integral properties of\nshapes--accessed through Fourier descriptors, curvature integrals, geometric\nmoments, and their invariants--to infuse high-level intrinsic geometric\ninformation and physics into the feature vector used for training, even when\nemploying simple model architectures or low-level parametric descriptions. We\nshowed that for surrogate modelling, along with the inclusion of the notion of\nphysics, GOs enact regularisation to reduce over-fitting and enhance\ngeneralisation to new, unseen designs. Furthermore, through extensive\nexperimentation, we demonstrate that for dimension reduction and generative\nmodels, incorporating the proposed GOs enriches the training data with compact\nglobal and local geometric features. This significantly enhances the quality of\nthe resulting latent space, thereby facilitating the generation of valid and\ndiverse designs. Lastly, we also show that GOs can enable learning parametric\nsensitivities to a great extent. Consequently, these enhancements accelerate\nthe convergence rate of shape optimisers towards optimal solutions.\n","authors":["Shahroz Khan","Zahid Masood","Muhammad Usama","Konstantinos Kostas","Panagiotis Kaklis"," Wei"," Chen"],"pdf_url":"https://arxiv.org/pdf/2407.07611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09532v2","updated":"2024-07-10T12:49:41Z","published":"2021-12-17T14:31:40Z","title":"Pixel Distillation: A New Knowledge Distillation Scheme for\n  Low-Resolution Image Recognition","summary":"  Previous knowledge distillation (KD) methods mostly focus on compressing\nnetwork architectures, which is not thorough enough in deployment as some costs\nlike transmission bandwidth and imaging equipment are related to the image\nsize. Therefore, we propose Pixel Distillation that extends knowledge\ndistillation into the input level while simultaneously breaking architecture\nconstraints. Such a scheme can achieve flexible cost control for deployment, as\nit allows the system to adjust both network architecture and image quality\naccording to the overall requirement of resources. Specifically, we first\npropose an input spatial representation distillation (ISRD) mechanism to\ntransfer spatial knowledge from large images to student's input module, which\ncan facilitate stable knowledge transfer between CNN and ViT. Then, a\nTeacher-Assistant-Student (TAS) framework is further established to disentangle\npixel distillation into the model compression stage and input compression\nstage, which significantly reduces the overall complexity of pixel distillation\nand the difficulty of distilling intermediate knowledge. Finally, we adapt\npixel distillation to object detection via an aligned feature for preservation\n(AFP) strategy for TAS, which aligns output dimensions of detectors at each\nstage by manipulating features and anchors of the assistant. Comprehensive\nexperiments on image classification and object detection demonstrate the\neffectiveness of our method. Code is available at\nhttps://github.com/gyguo/PixelDistillation.\n","authors":["Guangyu Guo","Dingwen Zhang","Longfei Han","Nian Liu","Ming-Ming Cheng","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2112.09532v2.pdf","comment":"TPAMI 2024"},{"id":"http://arxiv.org/abs/2401.14057v2","updated":"2024-07-10T12:47:20Z","published":"2024-01-25T10:29:07Z","title":"Left/Right Brain, human motor control and the implications for robotics","summary":"  Neural Network movement controllers promise a variety of advantages over\nconventional control methods, however, they are not widely adopted due to their\ninability to produce reliably precise movements. This research explores a\nbilateral neural network architecture as a control system for motor tasks. We\naimed to achieve hemispheric specialisation similar to what is observed in\nhumans across different tasks; the dominant system (usually the right hand,\nleft hemisphere) excels at tasks involving coordination and efficiency of\nmovement, and the non-dominant system performs better at tasks requiring\npositional stability. Specialisation was achieved by training the hemispheres\nwith different loss functions tailored to the expected behaviour of the\nrespective hemispheres. We compared bilateral models with and without\nspecialised hemispheres, with and without inter-hemispheric connectivity\n(representing the biological Corpus Callosum), and unilateral models with and\nwithout specialisation. The models were trained and tested on two tasks common\nin the human motor control literature: the random reach task, suited to the\ndominant system, a model with better coordination, and the hold position task,\nsuited to the non-dominant system, a model with more stable movement. Each\nsystem outperformed the non-preferred system in its preferred task. For both\ntasks, a bilateral model outperformed the non-preferred hand and was as good or\nbetter than the preferred hand. The results suggest that the hemispheres could\ncollaborate on tasks or work independently to their strengths. This study\nprovides ideas for how a biologically inspired bilateral architecture could be\nexploited for industrial motor control.\n","authors":["Jarrad Rinaldo","Levin Kuhlmann","Jason Friedman","Gideon Kowadlo"],"pdf_url":"https://arxiv.org/pdf/2401.14057v2.pdf","comment":"ACAIN 2024"},{"id":"http://arxiv.org/abs/2304.14824v3","updated":"2024-07-10T12:37:50Z","published":"2023-04-28T13:06:14Z","title":"A noise-robust acoustic method for recognizing foraging activities of\n  grazing cattle","summary":"  Farmers must continuously improve their livestock production systems to\nremain competitive in the growing dairy market. Precision livestock farming\ntechnologies provide individualized monitoring of animals on commercial farms,\noptimizing livestock production. Continuous acoustic monitoring is a widely\naccepted sensing technique used to estimate the daily rumination and grazing\ntime budget of free-ranging cattle. However, typical environmental and natural\nnoises on pastures noticeably affect the performance limiting the practical\napplication of current acoustic methods. In this study, we present the\noperating principle and generalization capability of an acoustic method called\nNoise-Robust Foraging Activity Recognizer (NRFAR). The proposed method\ndetermines foraging activity bouts by analyzing fixed-length segments of\nidentified jaw movement events produced during grazing and rumination. The\nadditive noise robustness of the NRFAR was evaluated for several\nsignal-to-noise ratios using stationary Gaussian white noise and four different\nnonstationary natural noise sources. In noiseless conditions, NRFAR reached an\naverage balanced accuracy of 86.4%, outperforming two previous acoustic methods\nby more than 7.5%. Furthermore, NRFAR performed better than previous acoustic\nmethods in 77 of 80 evaluated noisy scenarios (53 cases with p<0.05). NRFAR has\nbeen shown to be effective in harsh free-ranging environments and could be used\nas a reliable solution to improve pasture management and monitor the health and\nwelfare of dairy cows. The instrumentation and computational algorithms\npresented in this publication are protected by a pending patent application: AR\nP20220100910. Web demo available at: https://sinc.unl.edu.ar/web-demo/nrfar\n","authors":["Luciano S. Martinez-Rau","JosÃ© O. Chelotti","Mariano Ferrero","Julio R. Galli","Santiago A. Utsumi","Alejandra M. Planisich","H. Leonardo Rufiner","Leonardo L. Giovanini"],"pdf_url":"https://arxiv.org/pdf/2304.14824v3.pdf","comment":"list of used audio-clips is available in the list_audio_clips.xlsx"},{"id":"http://arxiv.org/abs/2402.12231v2","updated":"2024-07-10T12:36:15Z","published":"2024-02-19T15:36:36Z","title":"Diffusion Tempering Improves Parameter Estimation with Probabilistic\n  Integrators for Ordinary Differential Equations","summary":"  Ordinary differential equations (ODEs) are widely used to describe dynamical\nsystems in science, but identifying parameters that explain experimental\nmeasurements is challenging. In particular, although ODEs are differentiable\nand would allow for gradient-based parameter optimization, the nonlinear\ndynamics of ODEs often lead to many local minima and extreme sensitivity to\ninitial conditions. We therefore propose diffusion tempering, a novel\nregularization technique for probabilistic numerical methods which improves\nconvergence of gradient-based parameter optimization in ODEs. By iteratively\nreducing a noise parameter of the probabilistic integrator, the proposed method\nconverges more reliably to the true parameters. We demonstrate that our method\nis effective for dynamical systems of different complexity and show that it\nobtains reliable parameter estimates for a Hodgkin-Huxley model with a\npractically relevant number of parameters.\n","authors":["Jonas Beck","Nathanael Bosch","Michael Deistler","Kyra L. Kadhim","Jakob H. Macke","Philipp Hennig","Philipp Berens"],"pdf_url":"https://arxiv.org/pdf/2402.12231v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07598v1","updated":"2024-07-10T12:31:53Z","published":"2024-07-10T12:31:53Z","title":"Targeted Augmented Data for Audio Deepfake Detection","summary":"  The availability of highly convincing audio deepfake generators highlights\nthe need for designing robust audio deepfake detectors. Existing works often\nrely solely on real and fake data available in the training set, which may lead\nto overfitting, thereby reducing the robustness to unseen manipulations. To\nenhance the generalization capabilities of audio deepfake detectors, we propose\na novel augmentation method for generating audio pseudo-fakes targeting the\ndecision boundary of the model. Inspired by adversarial attacks, we perturb\noriginal real data to synthesize pseudo-fakes with ambiguous prediction\nprobabilities. Comprehensive experiments on two well-known architectures\ndemonstrate that the proposed augmentation contributes to improving the\ngeneralization capabilities of these architectures.\n","authors":["Marcella Astrid","Enjie Ghorbel","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2407.07598v1.pdf","comment":"Accepted in EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2407.07596v1","updated":"2024-07-10T12:29:46Z","published":"2024-07-10T12:29:46Z","title":"Learning treatment effects while treating those in need","summary":"  Many social programs attempt to allocate scarce resources to people with the\ngreatest need. Indeed, public services increasingly use algorithmic risk\nassessments motivated by this goal. However, targeting the highest-need\nrecipients often conflicts with attempting to evaluate the causal effect of the\nprogram as a whole, as the best evaluations would be obtained by randomizing\nthe allocation. We propose a framework to design randomized allocation rules\nwhich optimally balance targeting high-need individuals with learning treatment\neffects, presenting policymakers with a Pareto frontier between the two goals.\nWe give sample complexity guarantees for the policy learning problem and\nprovide a computationally efficient strategy to implement it. We then apply our\nframework to data from human services in Allegheny County, Pennsylvania.\nOptimized policies can substantially mitigate the tradeoff between learning and\ntargeting. For example, it is often possible to obtain 90% of the optimal\nutility in targeting high-need individuals while ensuring that the average\ntreatment effect can be estimated with less than 2 times the samples that a\nrandomized controlled trial would require. Mechanisms for targeting public\nservices often focus on measuring need as accurately as possible. However, our\nresults suggest that algorithmic systems in public services can be most\nimpactful if they incorporate program evaluation as an explicit goal alongside\ntargeting.\n","authors":["Bryan Wilder","Pim Welle"],"pdf_url":"https://arxiv.org/pdf/2407.07596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17000v5","updated":"2024-07-10T12:26:10Z","published":"2023-05-26T14:59:28Z","title":"DistriBlock: Identifying adversarial audio samples by leveraging\n  characteristics of the output distribution","summary":"  Adversarial attacks can mislead automatic speech recognition (ASR) systems\ninto predicting an arbitrary target text, thus posing a clear security threat.\nTo prevent such attacks, we propose DistriBlock, an efficient detection\nstrategy applicable to any ASR system that predicts a probability distribution\nover output tokens in each time step. We measure a set of characteristics of\nthis distribution: the median, maximum, and minimum over the output\nprobabilities, the entropy of the distribution, as well as the Kullback-Leibler\nand the Jensen-Shannon divergence with respect to the distributions of the\nsubsequent time step. Then, by leveraging the characteristics observed for both\nbenign and adversarial data, we apply binary classifiers, including simple\nthreshold-based classification, ensembles of such classifiers, and neural\nnetworks. Through extensive analysis across different state-of-the-art ASR\nsystems and language data sets, we demonstrate the supreme performance of this\napproach, with a mean area under the receiver operating characteristic curve\nfor distinguishing target adversarial examples against clean and noisy data of\n99% and 97%, respectively. To assess the robustness of our method, we show that\nadaptive adversarial examples that can circumvent DistriBlock are much noisier,\nwhich makes them easier to detect through filtering and creates another avenue\nfor preserving the system's robustness.\n","authors":["MatÃ­as P. Pizarro B.","Dorothea Kolossa","Asja Fischer"],"pdf_url":"https://arxiv.org/pdf/2305.17000v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.06862v9","updated":"2024-07-10T12:20:11Z","published":"2022-09-09T00:40:14Z","title":"Deep learning in a bilateral brain with hemispheric specialization","summary":"  The brains of all bilaterally symmetric animals on Earth are divided into\nleft and right hemispheres. The anatomy and functionality of the hemispheres\nhave a large degree of overlap, but there are asymmetries, and they specialise\nin possesses different attributes. Other authors have used computational models\nto mimic hemispheric asymmetries with a focus on reproducing human data on\nsemantic and visual processing tasks. We took a different approach and aimed to\nunderstand how dual hemispheres in a bilateral architecture interact to perform\nwell in a given task. We propose a bilateral artificial neural network that\nimitates lateralisation observed in nature: that the left hemisphere\nspecialises in local features and the right in global features. We used\ndifferent training objectives to achieve the desired specialisation and tested\nit on an image classification task with two different CNN backbones: ResNet and\nVGG. Our analysis found that the hemispheres represent complementary features\nthat are exploited by a network head that implements a type of weighted\nattention. The bilateral architecture outperformed a range of baselines of\nsimilar representational capacity that do not exploit differential\nspecialisation, with the exception of a conventional ensemble of unilateral\nnetworks trained on dual training objectives for local and global features. The\nresults demonstrate the efficacy of bilateralism, contribute to the discussion\nof bilateralism in biological brains, and the principle may serve as an\ninductive bias for new AI systems.\n","authors":["Chandramouli Rajagopalan","David Rawlinson","Elkhonon Goldberg","Gideon Kowadlo"],"pdf_url":"https://arxiv.org/pdf/2209.06862v9.pdf","comment":"ACAIN 2024"},{"id":"http://arxiv.org/abs/2407.07586v1","updated":"2024-07-10T12:18:38Z","published":"2024-07-10T12:18:38Z","title":"Simplifying Source-Free Domain Adaptation for Object Detection:\n  Effective Self-Training Strategies and Performance Insights","summary":"  This paper focuses on source-free domain adaptation for object detection in\ncomputer vision. This task is challenging and of great practical interest, due\nto the cost of obtaining annotated data sets for every new domain. Recent\nresearch has proposed various solutions for Source-Free Object Detection\n(SFOD), most being variations of teacher-student architectures with diverse\nfeature alignment, regularization and pseudo-label selection strategies. Our\nwork investigates simpler approaches and their performance compared to more\ncomplex SFOD methods in several adaptation scenarios. We highlight the\nimportance of batch normalization layers in the detector backbone, and show\nthat adapting only the batch statistics is a strong baseline for SFOD. We\npropose a simple extension of a Mean Teacher with strong-weak augmentation in\nthe source-free setting, Source-Free Unbiased Teacher (SF-UT), and show that it\nactually outperforms most of the previous SFOD methods. Additionally, we\nshowcase that an even simpler strategy consisting in training on a fixed set of\npseudo-labels can achieve similar performance to the more complex\nteacher-student mutual learning, while being computationally efficient and\nmitigating the major issue of teacher-student collapse. We conduct experiments\non several adaptation tasks using benchmark driving datasets including\n(Foggy)Cityscapes, Sim10k and KITTI, and achieve a notable improvement of 4.7\\%\nAP50 on Cityscapes$\\rightarrow$Foggy-Cityscapes compared with the latest\nstate-of-the-art in SFOD. Source code is available at\nhttps://github.com/EPFL-IMOS/simple-SFOD.\n","authors":["Yan Hao","Florent Forest","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2407.07586v1.pdf","comment":"Accepted at ECCV 2024. 19 pages"},{"id":"http://arxiv.org/abs/2407.07575v1","updated":"2024-07-10T12:08:39Z","published":"2024-07-10T12:08:39Z","title":"Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network","summary":"  As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation.\n","authors":["Yu Xie","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Jiangzhou Wang","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2407.07575v1.pdf","comment":"This paper has been submitted to IEEE Journal. The source code has\n  been released\n  at:https://github.com/qiongwu86/Resource-allocation-for-twin-maintenance-and-computing-tasks-in-digital-twin-mobile-edge-network"},{"id":"http://arxiv.org/abs/2403.04586v2","updated":"2024-07-10T11:57:01Z","published":"2024-03-07T15:30:54Z","title":"Learning Speed Adaptation for Flight in Clutter","summary":"  Animals learn to adapt speed of their movements to their capabilities and the\nenvironment they observe. Mobile robots should also demonstrate this ability to\ntrade-off aggressiveness and safety for efficiently accomplishing tasks. The\naim of this work is to endow flight vehicles with the ability of speed\nadaptation in prior unknown and partially observable cluttered environments. We\npropose a hierarchical learning and planning framework where we utilize both\nwell-established methods of model-based trajectory generation and\ntrial-and-error that comprehensively learns a policy to dynamically configure\nthe speed constraint. Technically, we use online reinforcement learning to\nobtain the deployable policy. The statistical results in simulation demonstrate\nthe advantages of our method over the constant speed constraint baselines and\nan alternative method in terms of flight efficiency and safety. In particular,\nthe policy behaves perception awareness, which distinguish it from alternative\napproaches. By deploying the policy to hardware, we verify that these\nadvantages can be brought to the real world.\n","authors":["Guangyu Zhao","Tianyue Wu","Yeke Chen","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.04586v2.pdf","comment":"Published on Robotics and Automation Letter (RA-L). 8 pages, 10\n  figures. The first two authors contribute equally to this work. Project page:\n  https://learning-agility-adaptation.github.io/"},{"id":"http://arxiv.org/abs/2306.04489v3","updated":"2024-07-10T11:40:41Z","published":"2023-06-07T15:00:38Z","title":"Fair Column Subset Selection","summary":"  The problem of column subset selection asks for a subset of columns from an\ninput matrix such that the matrix can be reconstructed as accurately as\npossible within the span of the selected columns. A natural extension is to\nconsider a setting where the matrix rows are partitioned into two groups, and\nthe goal is to choose a subset of columns that minimizes the maximum\nreconstruction error of both groups, relative to their respective best rank-k\napproximation. Extending the known results of column subset selection to this\nfair setting is not straightforward: in certain scenarios it is unavoidable to\nchoose columns separately for each group, resulting in double the expected\ncolumn count. We propose a deterministic leverage-score sampling strategy for\nthe fair setting and show that sampling a column subset of minimum size becomes\nNP-hard in the presence of two groups. Despite these negative results, we give\nan approximation algorithm that guarantees a solution within 1.5 times the\noptimal solution size. We also present practical heuristic algorithms based on\nrank-revealing QR factorization. Finally, we validate our methods through an\nextensive set of experiments using real-world data.\n","authors":["Antonis Matakos","Bruno Ordozgoiti","Suhas Thejaswi"],"pdf_url":"https://arxiv.org/pdf/2306.04489v3.pdf","comment":"KDD 2024"},{"id":"http://arxiv.org/abs/2407.07560v1","updated":"2024-07-10T11:35:02Z","published":"2024-07-10T11:35:02Z","title":"Instrumentation and Analysis of Native ML Pipelines via Logical Query\n  Plans","summary":"  Machine Learning (ML) is increasingly used to automate impactful decisions,\nwhich leads to concerns regarding their correctness, reliability, and fairness.\nWe envision highly-automated software platforms to assist data scientists with\ndeveloping, validating, monitoring, and analysing their ML pipelines. In\ncontrast to existing work, our key idea is to extract \"logical query plans\"\nfrom ML pipeline code relying on popular libraries. Based on these plans, we\nautomatically infer pipeline semantics and instrument and rewrite the ML\npipelines to enable diverse use cases without requiring data scientists to\nmanually annotate or rewrite their code.\n  First, we developed such an abstract ML pipeline representation together with\nmachinery to extract it from Python code. Next, we used this representation to\nefficiently instrument static ML pipelines and apply provenance tracking, which\nenables lightweight screening for common data preparation issues. Finally, we\nbuilt machinery to automatically rewrite ML pipelines to perform more advanced\nwhat-if analyses and proposed using multi-query optimisation for the resulting\nworkloads. In future work, we aim to interactively assist data scientists as\nthey work on their ML pipelines.\n","authors":["Stefan Grafberger"],"pdf_url":"https://arxiv.org/pdf/2407.07560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04052v2","updated":"2024-07-10T11:24:42Z","published":"2024-06-06T13:17:44Z","title":"Multivector Neurons: Better and Faster O(n)-Equivariant Clifford Graph\n  Neural Networks","summary":"  Most current deep learning models equivariant to $O(n)$ or $SO(n)$ either\nconsider mostly scalar information such as distances and angles or have a very\nhigh computational complexity. In this work, we test a few novel message\npassing graph neural networks (GNNs) based on Clifford multivectors, structured\nsimilarly to other prevalent equivariant models in geometric deep learning. Our\napproach leverages efficient invariant scalar features while simultaneously\nperforming expressive learning on multivector representations, particularly\nthrough the use of the equivariant geometric product operator. By integrating\nthese elements, our methods outperform established efficient baseline models on\nan N-Body simulation task and protein denoising task while maintaining a high\nefficiency. In particular, we push the state-of-the-art error on the N-body\ndataset to 0.0035 (averaged over 3 runs); an 8% improvement over recent\nmethods. Our implementation is available on Github.\n","authors":["Cong Liu","David Ruhe","Patrick ForrÃ©"],"pdf_url":"https://arxiv.org/pdf/2406.04052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01413v2","updated":"2024-07-10T11:16:08Z","published":"2024-02-02T13:45:42Z","title":"Objective and subjective evaluation of speech enhancement methods in the\n  UDASE task of the 7th CHiME challenge","summary":"  Supervised models for speech enhancement are trained using artificially\ngenerated mixtures of clean speech and noise signals. However, the synthetic\ntraining conditions may not accurately reflect real-world conditions\nencountered during testing. This discrepancy can result in poor performance\nwhen the test domain significantly differs from the synthetic training domain.\nTo tackle this issue, the UDASE task of the 7th CHiME challenge aimed to\nleverage real-world noisy speech recordings from the test domain for\nunsupervised domain adaptation of speech enhancement models. Specifically, this\ntest domain corresponds to the CHiME-5 dataset, characterized by real\nmulti-speaker and conversational speech recordings made in noisy and\nreverberant domestic environments, for which ground-truth clean speech signals\nare not available. In this paper, we present the objective and subjective\nevaluations of the systems that were submitted to the CHiME-7 UDASE task, and\nwe provide an analysis of the results. This analysis reveals a limited\ncorrelation between subjective ratings and several supervised nonintrusive\nperformance metrics recently proposed for speech enhancement. Conversely, the\nresults suggest that more traditional intrusive objective metrics can be used\nfor in-domain performance evaluation using the reverberant LibriCHiME-5 dataset\ndeveloped for the challenge. The subjective evaluation indicates that all\nsystems successfully reduced the background noise, but always at the expense of\nincreased distortion. Out of the four speech enhancement methods evaluated\nsubjectively, only one demonstrated an improvement in overall quality compared\nto the unprocessed noisy speech, highlighting the difficulty of the task. The\ntools and audio material created for the CHiME-7 UDASE task are shared with the\ncommunity.\n","authors":["Simon Leglaive","Matthieu Fraticelli","Hend ElGhazaly","LÃ©onie Borne","Mostafa Sadeghi","Scott Wisdom","Manuel Pariente","John R. Hershey","Daniel Pressnitzer","Jon P. Barker"],"pdf_url":"https://arxiv.org/pdf/2402.01413v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07539v1","updated":"2024-07-10T10:59:28Z","published":"2024-07-10T10:59:28Z","title":"Machine Unlearning for Medical Imaging","summary":"  Machine unlearning is the process of removing the impact of a particular set\nof training samples from a pretrained model. It aims to fulfill the \"right to\nbe forgotten\", which grants the individuals such as patients the right to\nreconsider their contribution in models including medical imaging models. In\nthis study, we evaluate the effectiveness (performance) and computational\nefficiency of different unlearning algorithms in medical imaging domain. Our\nevaluations demonstrate that the considered unlearning algorithms perform well\non the retain set (samples whose influence on the model is allowed to be\nretained) and forget set (samples whose contribution to the model should be\neliminated), and show no bias against male or female samples. They, however,\nadversely impact the generalization of the model, especially for larger forget\nset sizes. Moreover, they might be biased against easy or hard samples, and\nneed additional computational overhead for hyper-parameter tuning. In\nconclusion, machine unlearning seems promising for medical imaging, but the\nexisting unlearning algorithms still needs further improvements to become more\npractical for medical applications.\n","authors":["Reza Nasirigerdeh","Nader Razmi","Julia A. Schnabel","Daniel Rueckert","Georgios Kaissis"],"pdf_url":"https://arxiv.org/pdf/2407.07539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07530v1","updated":"2024-07-10T10:36:11Z","published":"2024-07-10T10:36:11Z","title":"How Aligned are Different Alignment Metrics?","summary":"  In recent years, various methods and benchmarks have been proposed to\nempirically evaluate the alignment of artificial neural networks to human\nneural and behavioral data. But how aligned are different alignment metrics? To\nanswer this question, we analyze visual data from Brain-Score (Schrimpf et al.,\n2018), including metrics from the model-vs-human toolbox (Geirhos et al.,\n2021), together with human feature alignment (Linsley et al., 2018; Fel et al.,\n2022) and human similarity judgements (Muttenthaler et al., 2022). We find that\npairwise correlations between neural scores and behavioral scores are quite low\nand sometimes even negative. For instance, the average correlation between\nthose 80 models on Brain-Score that were fully evaluated on all 69 alignment\nmetrics we considered is only 0.198. Assuming that all of the employed metrics\nare sound, this implies that alignment with human perception may best be\nthought of as a multidimensional concept, with different methods measuring\nfundamentally different aspects. Our results underline the importance of\nintegrative benchmarking, but also raise questions about how to correctly\ncombine and aggregate individual metrics. Aggregating by taking the arithmetic\naverage, as done in Brain-Score, leads to the overall performance currently\nbeing dominated by behavior (95.25% explained variance) while the neural\npredictivity plays a less important role (only 33.33% explained variance). As a\nfirst step towards making sure that different alignment metrics all contribute\nfairly towards an integrative benchmark score, we therefore conclude by\ncomparing three different aggregation options.\n","authors":["Jannis Ahlert","Thomas Klein","Felix Wichmann","Robert Geirhos"],"pdf_url":"https://arxiv.org/pdf/2407.07530v1.pdf","comment":"submitted to the ICLR 2024 Workshop on Representational Alignment\n  (Re-Align)"},{"id":"http://arxiv.org/abs/2407.07528v1","updated":"2024-07-10T10:31:57Z","published":"2024-07-10T10:31:57Z","title":"MLRS-PDS: A Meta-learning recommendation of dynamic ensemble selection\n  pipelines","summary":"  Dynamic Selection (DS), where base classifiers are chosen from a classifier's\npool for each new instance at test time, has shown to be highly effective in\npattern recognition. However, instability and redundancy in the classifier\npools can impede computational efficiency and accuracy in dynamic ensemble\nselection. This paper introduces a meta-learning recommendation system (MLRS)\nto recommend the optimal pool generation scheme for DES methods tailored to\nindividual datasets. The system employs a meta-model built from dataset\nmeta-features to predict the most suitable pool generation scheme and DES\nmethod for a given dataset. Through an extensive experimental study\nencompassing 288 datasets, we demonstrate that this meta-learning\nrecommendation system outperforms traditional fixed pool or DES method\nselection strategies, highlighting the efficacy of a meta-learning approach in\nrefining DES method selection. The source code, datasets, and supplementary\nresults can be found in this project's GitHub repository:\nhttps://github.com/Menelau/MLRS-PDS.\n","authors":["Hesam Jalalian","Rafael M. O. Cruz"],"pdf_url":"https://arxiv.org/pdf/2407.07528v1.pdf","comment":"Paper published at the International Joint Conference on Neural\n  Networks"},{"id":"http://arxiv.org/abs/2407.07521v1","updated":"2024-07-10T10:18:07Z","published":"2024-07-10T10:18:07Z","title":"CHILLI: A data context-aware perturbation method for XAI","summary":"  The trustworthiness of Machine Learning (ML) models can be difficult to\nassess, but is critical in high-risk or ethically sensitive applications. Many\nmodels are treated as a `black-box' where the reasoning or criteria for a final\ndecision is opaque to the user. To address this, some existing Explainable AI\n(XAI) approaches approximate model behaviour using perturbed data. However,\nsuch methods have been criticised for ignoring feature dependencies, with\nexplanations being based on potentially unrealistic data. We propose a novel\nframework, CHILLI, for incorporating data context into XAI by generating\ncontextually aware perturbations, which are faithful to the training data of\nthe base model being explained. This is shown to improve both the soundness and\naccuracy of the explanations.\n","authors":["Saif Anwar","Nathan Griffiths","Abhir Bhalerao","Thomas Popham"],"pdf_url":"https://arxiv.org/pdf/2407.07521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06333v2","updated":"2024-07-10T09:43:58Z","published":"2024-07-08T18:55:57Z","title":"A third-order finite difference weighted essentially non-oscillatory\n  scheme with shallow neural network","summary":"  In this paper, we introduce the finite difference weighted essentially\nnon-oscillatory (WENO) scheme based on the neural network for hyperbolic\nconservation laws. We employ the supervised learning and design two loss\nfunctions, one with the mean squared error and the other with the mean squared\nlogarithmic error, where the WENO3-JS weights are computed as the labels. Each\nloss function consists of two components where the first component compares the\ndifference between the weights from the neural network and WENO3-JS weights,\nwhile the second component matches the output weights of the neural network and\nthe linear weights. The former of the loss function enforces the neural network\nto follow the WENO properties, implying that there is no need for the\npost-processing layer. Additionally the latter leads to better performance\naround discontinuities. As a neural network structure, we choose the shallow\nneural network (SNN) for computational efficiency with the Delta layer\nconsisting of the normalized undivided differences. These constructed WENO3-SNN\nschemes show the outperformed results in one-dimensional examples and improved\nbehavior in two-dimensional examples, compared with the simulations from\nWENO3-JS and WENO3-Z.\n","authors":["Kwanghyuk Park","Xinjuan Chen","Dongjin Lee","Jiaxi Gu","Jae-Hun Jung"],"pdf_url":"https://arxiv.org/pdf/2407.06333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07492v1","updated":"2024-07-10T09:24:50Z","published":"2024-07-10T09:24:50Z","title":"Fine-Grained Classification for Poisonous Fungi Identification with\n  Transfer Learning","summary":"  FungiCLEF 2024 addresses the fine-grained visual categorization (FGVC) of\nfungi species, with a focus on identifying poisonous species. This task is\nchallenging due to the size and class imbalance of the dataset, subtle\ninter-class variations, and significant intra-class variability amongst\nsamples. In this paper, we document our approach in tackling this challenge\nthrough the use of ensemble classifier heads on pre-computed image embeddings.\nOur team (DS@GT) demonstrate that state-of-the-art self-supervised vision\nmodels can be utilized as robust feature extractors for downstream application\nof computer vision tasks without the need for task-specific fine-tuning on the\nvision backbone. Our approach achieved the best Track 3 score (0.345), accuracy\n(78.4%) and macro-F1 (0.577) on the private test set in post competition\nevaluation. Our code is available at\nhttps://github.com/dsgt-kaggle-clef/fungiclef-2024.\n","authors":["Christopher Chiu","Maximilian Heil","Teresa Kim","Anthony Miyaguchi"],"pdf_url":"https://arxiv.org/pdf/2407.07492v1.pdf","comment":"Submitted and accepted into CLEF 2024 CEUR-WS proceedings"},{"id":"http://arxiv.org/abs/2307.14012v2","updated":"2024-07-10T09:24:07Z","published":"2023-07-26T07:50:41Z","title":"MCMC-Correction of Score-Based Diffusion Models for Model Composition","summary":"  Diffusion models can be parameterised in terms of either a score or an energy\nfunction. An energy parameterisation is appealing since it enables an extended\nsampling procedure with a Metropolis--Hastings (MH) correction step, based on\nthe change in total energy in the proposed samples. Improved sampling is\nimportant for model compositions, where off-the-shelf models are combined with\neach other, in order to sample from new distributions. For model composition,\nscore-based diffusions have the advantages that they are popular and that many\npre-trained models are readily available. However, this parameterisation does\nnot, in general, define an energy, and the MH acceptance probability is\ntherefore unavailable, and generally ill-defined. We propose keeping the score\nparameterisation and computing an acceptance probability inspired by\nenergy-based models through line integration of the score function. This allows\nus to reuse existing diffusion models and still combine the reverse process\nwith various Markov-Chain Monte Carlo (MCMC) methods. We evaluate our method\nusing numerical experiments and find that score-parameterised versions of the\nMCMC samplers can achieve similar improvements to the corresponding energy\nparameterisation.\n","authors":["Anders SjÃ¶berg","Jakob Lindqvist","Magnus Ãnnheim","Mats Jirstrand","Lennart Svensson"],"pdf_url":"https://arxiv.org/pdf/2307.14012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07482v1","updated":"2024-07-10T09:13:11Z","published":"2024-07-10T09:13:11Z","title":"Rigorous Probabilistic Guarantees for Robust Counterfactual Explanations","summary":"  We study the problem of assessing the robustness of counterfactual\nexplanations for deep learning models. We focus on $\\textit{plausible model\nshifts}$ altering model parameters and propose a novel framework to reason\nabout the robustness property in this setting. To motivate our solution, we\nbegin by showing for the first time that computing the robustness of\ncounterfactuals with respect to plausible model shifts is NP-complete. As this\n(practically) rules out the existence of scalable algorithms for exactly\ncomputing robustness, we propose a novel probabilistic approach which is able\nto provide tight estimates of robustness with strong guarantees while\npreserving scalability. Remarkably, and differently from existing solutions\ntargeting plausible model shifts, our approach does not impose requirements on\nthe network to be analyzed, thus enabling robustness analysis on a wider range\nof architectures. Experiments on four binary classification datasets indicate\nthat our method improves the state of the art in generating robust\nexplanations, outperforming existing methods on a range of metrics.\n","authors":["Luca Marzari","Francesco Leofante","Ferdinando Cicalese","Alessandro Farinelli"],"pdf_url":"https://arxiv.org/pdf/2407.07482v1.pdf","comment":"Accepted at the 27th European Conference on Artificial Intelligence\n  (ECAI 2024). Marzari and Leofante contributed equally to the paper"},{"id":"http://arxiv.org/abs/2407.05413v2","updated":"2024-07-10T09:01:31Z","published":"2024-07-07T15:37:13Z","title":"SBoRA: Low-Rank Adaptation with Regional Weight Updates","summary":"  This paper introduces Standard Basis LoRA (SBoRA), a novel\nparameter-efficient fine-tuning approach for Large Language Models that builds\nupon the pioneering works of Low-Rank Adaptation (LoRA) and Orthogonal\nAdaptation. SBoRA further reduces the computational and memory requirements of\nLoRA while enhancing learning performance. By leveraging orthogonal standard\nbasis vectors to initialize one of the low-rank matrices, either A or B, SBoRA\nenables regional weight updates and memory-efficient fine-tuning. This approach\ngives rise to two variants, SBoRA-FA and SBoRA-FB, where only one of the\nmatrices is updated, resulting in a sparse update matrix with a majority of\nzero rows or columns. Consequently, the majority of the fine-tuned model's\nweights remain unchanged from the pre-trained weights. This characteristic of\nSBoRA, wherein regional weight updates occur, is reminiscent of the modular\norganization of the human brain, which efficiently adapts to new tasks. Our\nempirical results demonstrate the superiority of SBoRA-FA over LoRA in various\nfine-tuning tasks, including commonsense reasoning and arithmetic reasoning.\nFurthermore, we evaluate the effectiveness of QSBoRA on quantized LLaMA models\nof varying scales, highlighting its potential for efficient adaptation to new\ntasks. Code is available at https://github.com/cityuhkai/SBoRA\n","authors":["Lai-Man Po","Yuyang Liu","Haoxuan Wu","Tianqi Zhang","Wing-Yin Yu","Zeyu Jiang","Kun Li"],"pdf_url":"https://arxiv.org/pdf/2407.05413v2.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2405.05429v3","updated":"2024-07-10T08:47:05Z","published":"2024-05-08T21:19:18Z","title":"How Inverse Conditional Flows Can Serve as a Substitute for\n  Distributional Regression","summary":"  Neural network representations of simple models, such as linear regression,\nare being studied increasingly to better understand the underlying principles\nof deep learning algorithms. However, neural representations of distributional\nregression models, such as the Cox model, have received little attention so\nfar. We close this gap by proposing a framework for distributional regression\nusing inverse flow transformations (DRIFT), which includes neural\nrepresentations of the aforementioned models. We empirically demonstrate that\nthe neural representations of models in DRIFT can serve as a substitute for\ntheir classical statistical counterparts in several applications involving\ncontinuous, ordered, time-series, and survival outcomes. We confirm that models\nin DRIFT empirically match the performance of several statistical methods in\nterms of estimation of partial effects, prediction, and aleatoric uncertainty\nquantification. DRIFT covers both interpretable statistical models and flexible\nneural networks opening up new avenues in both statistical modeling and deep\nlearning.\n","authors":["Lucas Kook","Chris Kolb","Philipp Schiele","Daniel Dold","Marcel Arpogaus","Cornelius Fritz","Philipp F. Baumann","Philipp Kopper","Tobias Pielok","Emilio Dorigatti","David RÃ¼gamer"],"pdf_url":"https://arxiv.org/pdf/2405.05429v3.pdf","comment":"Accepted at UAI 2024 https://www.auai.org/uai2024/accepted_papers"},{"id":"http://arxiv.org/abs/2312.02364v2","updated":"2024-07-10T08:39:59Z","published":"2023-12-04T21:46:21Z","title":"Class-Discriminative Attention Maps for Vision Transformers","summary":"  Importance estimators are explainability methods that quantify feature\nimportance for deep neural networks (DNN). In vision transformers (ViT), the\nself-attention mechanism naturally leads to attention maps, which are sometimes\nused as importance scores for which input features ViT models are focusing on.\nHowever, attention maps do not account for signals from downstream tasks. To\ngenerate explanations that are sensitive to downstream tasks, we have developed\nclass-discriminative attention maps (CDAM), a gradient-based extension that\nestimates feature importance with respect to a known class or a latent concept.\nCDAM scales attention scores by how relevant the corresponding tokens are for\nthe predictions of a classifier head. In addition to targeting the supervised\nclassifier, CDAM can explain an arbitrary concept shared by selected samples by\nmeasuring similarity in the latent space of ViT. Additionally, we introduce\nSmooth CDAM and Integrated CDAM, which average a series of CDAMs with slightly\naltered tokens. Our quantitative benchmarks include correctness, compactness,\nand class sensitivity, in comparison to six other importance estimators.\nVanilla, Smooth, and Integrated CDAM excel across all three benchmarks. In\nparticular, our results suggest that existing importance estimators may not\nprovide sufficient class-sensitivity. We demonstrate the utility of CDAM in\nmedical images by training and explaining malignancy and biomarker prediction\nmodels based on lung Computed Tomography (CT) scans. Overall, CDAM is shown to\nbe highly class-discriminative and semantically relevant, while providing\ncompact explanations.\n","authors":["Lennart Brocki","Jakub Binda","Neo Christopher Chung"],"pdf_url":"https://arxiv.org/pdf/2312.02364v2.pdf","comment":"Earlier version at 2024 International Joint Conference on Artificial\n  Intelligence (IJCAI) Workshop on Explainable Artificial Intelligence (XAI)"},{"id":"http://arxiv.org/abs/2403.07559v2","updated":"2024-07-10T08:36:48Z","published":"2024-03-12T11:47:12Z","title":"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding","summary":"  Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding\n(MAPF) has recently gained attention due to its efficiency and scalability.\nSeveral MARL-MAPF methods choose to use communication to enrich the information\none agent can perceive. However, existing works still struggle in structured\nenvironments with high obstacle density and a high number of agents. To further\nimprove the performance of the communication-based MARL-MAPF solvers, we\npropose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first\npropose a selective communication block to gather richer information for better\nagent coordination within multi-agent environments and train the model with a Q\nlearning-based algorithm. We further introduce three advanced inference\nstrategies aimed at bolstering performance during the execution phase. First,\nwe hybridize the neural policy with single-agent expert guidance for navigating\nconflict-free zones. Secondly, we propose Q value-based methods for prioritized\nresolution of conflicts as well as deadlock situations. Finally, we introduce a\nrobust ensemble method that can efficiently collect the best out of multiple\npossible solutions. We empirically evaluate EPH in complex multi-agent\nenvironments and demonstrate competitive performance against state-of-the-art\nneural methods for MAPF. We open-source our code at\nhttps://github.com/ai4co/eph-mapf.\n","authors":["Huijie Tang","Federico Berto","Jinkyoo Park"],"pdf_url":"https://arxiv.org/pdf/2403.07559v2.pdf","comment":"Accepted to 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2407.07462v1","updated":"2024-07-10T08:32:26Z","published":"2024-07-10T08:32:26Z","title":"MAN TruckScenes: A multimodal dataset for autonomous trucking in diverse\n  conditions","summary":"  Autonomous trucking is a promising technology that can greatly impact modern\nlogistics and the environment. Ensuring its safety on public roads is one of\nthe main duties that requires an accurate perception of the environment. To\nachieve this, machine learning methods rely on large datasets, but to this day,\nno such datasets are available for autonomous trucks. In this work, we present\nMAN TruckScenes, the first multimodal dataset for autonomous trucking. MAN\nTruckScenes allows the research community to come into contact with\ntruck-specific challenges, such as trailer occlusions, novel sensor\nperspectives, and terminal environments for the first time. It comprises more\nthan 740 scenes of 20 s each within a multitude of different environmental\nconditions. The sensor set includes 4 cameras, 6 lidar, 6 radar sensors, 2\nIMUs, and a high-precision GNSS. The dataset's 3D bounding boxes were manually\nannotated and carefully reviewed to achieve a high quality standard. Bounding\nboxes are available for 27 object classes, 15 attributes, and a range of more\nthan 230 m. The scenes are tagged according to 34 distinct scene tags, and all\nobjects are tracked throughout the scene to promote a wide range of\napplications. Additionally, MAN TruckScenes is the first dataset to provide 4D\nradar data with 360{\\deg} coverage and is thereby the largest radar dataset\nwith annotated 3D bounding boxes. Finally, we provide extensive dataset\nanalysis and baseline results. The dataset, development kit and more are\navailable online.\n","authors":["Felix Fent","Fabian Kuttenreich","Florian Ruch","Farija Rizwin","Stefan Juergens","Lorenz Lechermann","Christian Nissler","Andrea Perl","Ulrich Voll","Min Yan","Markus Lienkamp"],"pdf_url":"https://arxiv.org/pdf/2407.07462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09257v2","updated":"2024-07-10T08:21:47Z","published":"2024-01-17T15:03:37Z","title":"A First-Order Multi-Gradient Algorithm for Multi-Objective Bi-Level\n  Optimization","summary":"  In this paper, we study the Multi-Objective Bi-Level Optimization (MOBLO)\nproblem, where the upper-level subproblem is a multi-objective optimization\nproblem and the lower-level subproblem is for scalar optimization. Existing\ngradient-based MOBLO algorithms need to compute the Hessian matrix, causing the\ncomputational inefficient problem. To address this, we propose an efficient\nfirst-order multi-gradient method for MOBLO, called FORUM. Specifically, we\nreformulate MOBLO problems as a constrained multi-objective optimization (MOO)\nproblem via the value-function approach. Then we propose a novel multi-gradient\naggregation method to solve the challenging constrained MOO problem.\nTheoretically, we provide the complexity analysis to show the efficiency of the\nproposed method and a non-asymptotic convergence result. Empirically, extensive\nexperiments demonstrate the effectiveness and efficiency of the proposed FORUM\nmethod in different learning problems. In particular, it achieves\nstate-of-the-art performance on three multi-task learning benchmark datasets.\nThe code is available at https://github.com/Baijiong-Lin/FORUM.\n","authors":["Feiyang Ye","Baijiong Lin","Xiaofeng Cao","Yu Zhang","Ivor Tsang"],"pdf_url":"https://arxiv.org/pdf/2401.09257v2.pdf","comment":"ECAI 2024"},{"id":"http://arxiv.org/abs/2407.07458v1","updated":"2024-07-10T08:21:01Z","published":"2024-07-10T08:21:01Z","title":"Machine Learning Assisted Design of mmWave Wireless Transceiver Circuits","summary":"  As fifth-generation (5G) and upcoming sixth-generation (6G) communications\nexhibit tremendous demands in providing high data throughput with a relatively\nlow latency, millimeter-wave (mmWave) technologies manifest themselves as the\nkey enabling components to achieve the envisioned performance and tasks. In\nthis context, mmWave integrated circuits (IC) have attracted significant\nresearch interests over the past few decades, ranging from individual block\ndesign to complex system design. However, the highly nonlinear properties and\nintricate trade-offs involved render the design of analog or RF circuits a\ncomplicated process. The rapid evolution of fabrication technology also results\nin an increasingly long time allocated in the design process due to more\nstringent requirements. In this thesis, 28-GHz transceiver circuits are first\ninvestigated with detailed schematics and associated performance metrics. In\nthis case, two target systems comprising heterogeneous individual blocks are\nselected and demonstrated on both the transmitter and receiver sides.\nSubsequently, some conventional and large-scale machine learning (ML)\napproaches are integrated into the design pipeline of the chosen systems to\npredict circuit parameters based on desired specifications, thereby\ncircumventing the typical time-consuming iterations found in traditional\nmethods. Finally, some potential research directions are discussed from the\nperspectives of circuit design and ML algorithms.\n","authors":["Xuzhe Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.07458v1.pdf","comment":"Portions of Chapter 3 to 5 are adapted to form the paper that is\n  currently under review as \"AICircuit: A Multi-Level Dataset and Benchmark for\n  AI-Driven Analog Integrated Circuit Design\", in the 38th Conference on Neural\n  Information Processing Systems (NeurIPS 2024) Track on Datasets and\n  Benchmarks. Detailed information is provided in the Acknowledgments section"},{"id":"http://arxiv.org/abs/2403.09974v2","updated":"2024-07-10T08:20:56Z","published":"2024-03-15T02:40:13Z","title":"Unlocking the Multi-modal Potential of CLIP for Generalized Category\n  Discovery","summary":"  Given unlabelled datasets containing both old and new categories, generalized\ncategory discovery (GCD) aims to accurately discover new classes while\ncorrectly classifying old classes, leveraging the class concepts learned from\nlabeled samples. Current GCD methods only use a single visual modality of\ninformation, resulting in poor classification of visually similar classes. As a\ndifferent modality, text information can provide complementary discriminative\ninformation, which motivates us to introduce it into the GCD task. However, the\nlack of class names for unlabelled data makes it impractical to utilize text\ninformation. To tackle this challenging problem, in this paper, we propose a\nText Embedding Synthesizer (TES) to generate pseudo text embeddings for\nunlabelled samples. Specifically, our TES leverages the property that CLIP can\ngenerate aligned vision-language features, converting visual embeddings into\ntokens of the CLIP's text encoder to generate pseudo text embeddings. Besides,\nwe employ a dual-branch framework, through the joint learning and instance\nconsistency of different modality branches, visual and semantic information\nmutually enhance each other, promoting the interaction and fusion of visual and\ntext knowledge. Our method unlocks the multi-modal potentials of CLIP and\noutperforms the baseline methods by a large margin on all GCD benchmarks,\nachieving new state-of-the-art. The code will be released at\nhttps://github.com/enguangW/GET .\n","authors":["Enguang Wang","Zhimao Peng","Zhengyuan Xie","Fei Yang","Xialei Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.09974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07457v1","updated":"2024-07-10T08:20:47Z","published":"2024-07-10T08:20:47Z","title":"GLBench: A Comprehensive Benchmark for Graph with Large Language Models","summary":"  The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench.\n","authors":["Yuhan Li","Peisong Wang","Xiao Zhu","Aochuan Chen","Haiyun Jiang","Deng Cai","Victor Wai Kin Chan","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2407.07457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07454v1","updated":"2024-07-10T08:16:13Z","published":"2024-07-10T08:16:13Z","title":"CM-DQN: A Value-Based Deep Reinforcement Learning Model to Simulate\n  Confirmation Bias","summary":"  In human decision-making tasks, individuals learn through trials and\nprediction errors. When individuals learn the task, some are more influenced by\ngood outcomes, while others weigh bad outcomes more heavily. Such confirmation\nbias can lead to different learning effects. In this study, we propose a new\nalgorithm in Deep Reinforcement Learning, CM-DQN, which applies the idea of\ndifferent update strategies for positive or negative prediction errors, to\nsimulate the human decision-making process when the task's states are\ncontinuous while the actions are discrete. We test in Lunar Lander environment\nwith confirmatory, disconfirmatory bias and non-biased to observe the learning\neffects. Moreover, we apply the confirmation model in a multi-armed bandit\nproblem (environment in discrete states and discrete actions), which utilizes\nthe same idea as our proposed algorithm, as a contrast experiment to\nalgorithmically simulate the impact of different confirmation bias in\ndecision-making process. In both experiments, confirmatory bias indicates a\nbetter learning effect. Our code can be found here\nhttps://github.com/Patrickhshs/CM-DQN.\n","authors":["Jiacheng Shen","Lihan Feng"],"pdf_url":"https://arxiv.org/pdf/2407.07454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07450v1","updated":"2024-07-10T08:07:55Z","published":"2024-07-10T08:07:55Z","title":"Using Low-Discrepancy Points for Data Compression in Machine Learning:\n  An Experimental Comparison","summary":"  Low-discrepancy points (also called Quasi-Monte Carlo points) are\ndeterministically and cleverly chosen point sets in the unit cube, which\nprovide an approximation of the uniform distribution. We explore two methods\nbased on such low-discrepancy points to reduce large data sets in order to\ntrain neural networks. The first one is the method of Dick and Feischl [4],\nwhich relies on digital nets and an averaging procedure. Motivated by our\nexperimental findings, we construct a second method, which again uses digital\nnets, but Voronoi clustering instead of averaging. Both methods are compared to\nthe supercompress approach of [14], which is a variant of the K-means\nclustering algorithm. The comparison is done in terms of the compression error\nfor different objective functions and the accuracy of the training of a neural\nnetwork.\n","authors":["Simone GÃ¶ttlich","Jacob Heieck","Andreas Neuenkirch"],"pdf_url":"https://arxiv.org/pdf/2407.07450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02851v2","updated":"2024-07-10T08:02:03Z","published":"2022-10-06T12:14:25Z","title":"Anomaly detection using data depth: multivariate case","summary":"  Anomaly detection is a branch of data analysis and machine learning which\naims at identifying observations that exhibit abnormal behaviour. Be it\nmeasurement errors, disease development, severe weather, production quality\ndefault(s) (items) or failed equipment, financial frauds or crisis events,\ntheir on-time identification, isolation and explanation constitute an important\ntask in almost any branch of science and industry. By providing a robust\nordering, data depth - statistical function that measures belongingness of any\npoint of the space to a data set - becomes a particularly useful tool for\ndetection of anomalies. Already known for its theoretical properties, data\ndepth has undergone substantial computational developments in the last decade\nand particularly recent years, which has made it applicable for\ncontemporary-sized problems of data analysis and machine learning.\n  In this article, data depth is studied as an efficient anomaly detection\ntool, assigning abnormality labels to observations with lower depth values, in\na multivariate setting. Practical questions of necessity and reasonability of\ninvariances and shape of the depth function, its robustness and computational\ncomplexity, choice of the threshold are discussed. Illustrations include\nuse-cases that underline advantageous behaviour of data depth in various\nsettings.\n","authors":["Pavlo Mozharovskyi","Romain Valla"],"pdf_url":"https://arxiv.org/pdf/2210.02851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08918v2","updated":"2024-07-10T08:01:26Z","published":"2024-06-13T08:30:29Z","title":"Beyond the Calibration Point: Mechanism Comparison in Differential\n  Privacy","summary":"  In differentially private (DP) machine learning, the privacy guarantees of DP\nmechanisms are often reported and compared on the basis of a single\n$(\\varepsilon, \\delta)$-pair. This practice overlooks that DP guarantees can\nvary substantially even between mechanisms sharing a given $(\\varepsilon,\n\\delta)$, and potentially introduces privacy vulnerabilities which can remain\nundetected. This motivates the need for robust, rigorous methods for comparing\nDP guarantees in such cases. Here, we introduce the $\\Delta$-divergence between\nmechanisms which quantifies the worst-case excess privacy vulnerability of\nchoosing one mechanism over another in terms of $(\\varepsilon, \\delta)$, $f$-DP\nand in terms of a newly presented Bayesian interpretation. Moreover, as a\ngeneralisation of the Blackwell theorem, it is endowed with strong\ndecision-theoretic foundations. Through application examples, we show that our\ntechniques can facilitate informed decision-making and reveal gaps in the\ncurrent understanding of privacy risks, as current practices in DP-SGD often\nresult in choosing mechanisms with high excess privacy vulnerabilities.\n","authors":["Georgios Kaissis","Stefan Kolek","Borja Balle","Jamie Hayes","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2406.08918v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2406.09740v2","updated":"2024-07-10T07:54:46Z","published":"2024-06-14T06:02:14Z","title":"Deep Symbolic Optimization for Combinatorial Optimization: Accelerating\n  Node Selection by Discovering Potential Heuristics","summary":"  Combinatorial optimization (CO) is one of the most fundamental mathematical\nmodels in real-world applications. Traditional CO solvers, such as\nBranch-and-Bound (B&B) solvers, heavily rely on expert-designed heuristics,\nwhich are reliable but require substantial manual tuning. Recent studies have\nleveraged deep learning (DL) models as an alternative to capture rich feature\npatterns for improved performance on GPU machines. Nonetheless, the drawbacks\nof high training and inference costs, as well as limited interpretability,\nseverely hinder the adoption of DL methods in real-world applications. To\naddress these challenges, we propose a novel deep symbolic optimization\nlearning framework that combines their advantages. Specifically, we focus on\nthe node selection module within B&B solvers -- namely, deep symbolic\noptimization for node selection (Dso4NS). With data-driven approaches, Dso4NS\nguides the search for mathematical expressions within the high-dimensional\ndiscrete symbolic space and then incorporates the highest-performing\nmathematical expressions into a solver. The data-driven model captures the rich\nfeature information in the input data and generates symbolic expressions, while\nthe expressions deployed in solvers enable fast inference with high\ninterpretability. Experiments demonstrate the effectiveness of Dso4NS in\nlearning high-quality expressions, outperforming existing approaches on a CPU\nmachine. Encouragingly, the learned CPU-based policies consistently achieve\nperformance comparable to state-of-the-art GPU-based approaches.\n","authors":["Hongyu Liu","Haoyang Liu","Yufei Kuang","Jie Wang","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2406.09740v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10153v2","updated":"2024-07-10T07:45:54Z","published":"2024-03-15T09:54:04Z","title":"Improving Medical Multi-modal Contrastive Learning with Expert\n  Annotations","summary":"  We introduce eCLIP, an enhanced version of the CLIP model that integrates\nexpert annotations in the form of radiologist eye-gaze heatmaps. It tackles key\nchallenges in contrastive multi-modal medical imaging analysis, notably data\nscarcity and the \"modality gap\" -- a significant disparity between image and\ntext embeddings that diminishes the quality of representations and hampers\ncross-modal interoperability. eCLIP integrates a heatmap processor and\nleverages mixup augmentation to efficiently utilize the scarce expert\nannotations, thus boosting the model's learning effectiveness. eCLIP is\ndesigned to be generally applicable to any variant of CLIP without requiring\nany modifications of the core architecture. Through detailed evaluations across\nseveral tasks, including zero-shot inference, linear probing, cross-modal\nretrieval, and Retrieval Augmented Generation (RAG) of radiology reports using\na frozen Large Language Model, eCLIP showcases consistent improvements in\nembedding quality. The outcomes reveal enhanced alignment and uniformity,\naffirming eCLIP's capability to harness high-quality annotations for enriched\nmulti-modal analysis in the medical imaging domain.\n","authors":["Yogesh Kumar","Pekka Marttinen"],"pdf_url":"https://arxiv.org/pdf/2403.10153v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2404.08698v2","updated":"2024-07-10T07:38:32Z","published":"2024-04-10T16:11:09Z","title":"Lossless Acceleration of Large Language Model via Adaptive N-gram\n  Parallel Decoding","summary":"  While Large Language Models (LLMs) have shown remarkable abilities, they are\nhindered by significant resource consumption and considerable latency due to\nautoregressive processing. In this study, we introduce Adaptive N-gram Parallel\nDecoding (ANPD), an innovative and lossless approach that accelerates inference\nby allowing the simultaneous generation of multiple tokens. ANPD incorporates a\ntwo-stage approach: it begins with a rapid drafting phase that employs an\nN-gram module, which adapts based on the current interactive context, followed\nby a verification phase, during which the original LLM assesses and confirms\nthe proposed tokens. Consequently, ANPD preserves the integrity of the LLM's\noriginal output while enhancing processing speed. We further leverage a\nmulti-level architecture for the N-gram module to enhance the precision of the\ninitial draft, consequently reducing inference latency. ANPD eliminates the\nneed for retraining or extra GPU memory, making it an efficient and\nplug-and-play enhancement. In our experiments, models such as LLaMA and its\nfine-tuned variants have shown speed improvements up to 3.67x, validating the\neffectiveness of our proposed ANPD.\n","authors":["Jie Ou","Yueming Chen","Wenhong Tian"],"pdf_url":"https://arxiv.org/pdf/2404.08698v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07421v1","updated":"2024-07-10T07:23:21Z","published":"2024-07-10T07:23:21Z","title":"Federated PCA on Grassmann Manifold for IoT Anomaly Detection","summary":"  With the proliferation of the Internet of Things (IoT) and the rising\ninterconnectedness of devices, network security faces significant challenges,\nespecially from anomalous activities. While traditional machine learning-based\nintrusion detection systems (ML-IDS) effectively employ supervised learning\nmethods, they possess limitations such as the requirement for labeled data and\nchallenges with high dimensionality. Recent unsupervised ML-IDS approaches such\nas AutoEncoders and Generative Adversarial Networks (GAN) offer alternative\nsolutions but pose challenges in deployment onto resource-constrained IoT\ndevices and in interpretability. To address these concerns, this paper proposes\na novel federated unsupervised anomaly detection framework, FedPCA, that\nleverages Principal Component Analysis (PCA) and the Alternating Directions\nMethod Multipliers (ADMM) to learn common representations of distributed\nnon-i.i.d. datasets. Building on the FedPCA framework, we propose two\nalgorithms, FEDPE in Euclidean space and FEDPG on Grassmann manifolds. Our\napproach enables real-time threat detection and mitigation at the device level,\nenhancing network resilience while ensuring privacy. Moreover, the proposed\nalgorithms are accompanied by theoretical convergence rates even under a\nsubsampling scheme, a novel result. Experimental results on the UNSW-NB15 and\nTON-IoT datasets show that our proposed methods offer performance in anomaly\ndetection comparable to nonlinear baselines, while providing significant\nimprovements in communication and memory efficiency, underscoring their\npotential for securing IoT networks.\n","authors":["Tung-Anh Nguyen","Long Tan Le","Tuan Dung Nguyen","Wei Bao","Suranga Seneviratne","Choong Seon Hong","Nguyen H. Tran"],"pdf_url":"https://arxiv.org/pdf/2407.07421v1.pdf","comment":"Accepted for publication at IEEE/ACM Transactions on Networking"},{"id":"http://arxiv.org/abs/2407.07410v1","updated":"2024-07-10T07:12:50Z","published":"2024-07-10T07:12:50Z","title":"Mutual Information calculation on different appearances","summary":"  Mutual information has many applications in image alignment and matching,\nmainly due to its ability to measure the statistical dependence between two\nimages, even if the two images are from different modalities (e.g., CT and\nMRI). It considers not only the pixel intensities of the images but also the\nspatial relationships between the pixels. In this project, we apply the mutual\ninformation formula to image matching, where image A is the moving object and\nimage B is the target object and calculate the mutual information between them\nto evaluate the similarity between the images. For comparison, we also used\nentropy and information-gain methods to test the dependency of the images. We\nalso investigated the effect of different environments on the mutual\ninformation of the same image and used experiments and plots to demonstrate.\n","authors":["Jiecheng Liao","Junhao Lu","Jeff Ji","Jiacheng He"],"pdf_url":"https://arxiv.org/pdf/2407.07410v1.pdf","comment":"demo for the work: elucidator.cn/demo-mi/"},{"id":"http://arxiv.org/abs/2107.11972v4","updated":"2024-07-10T07:05:51Z","published":"2021-07-26T05:52:42Z","title":"Trade When Opportunity Comes: Price Movement Forecasting via\n  Locality-Aware Attention and Iterative Refinement Labeling","summary":"  Price movement forecasting, aimed at predicting financial asset trends based\non current market information, has achieved promising advancements through\nmachine learning (ML) methods. Most existing ML methods, however, struggle with\nthe extremely low signal-to-noise ratio and stochastic nature of financial\ndata, often mistaking noises for real trading signals without careful selection\nof potentially profitable samples. To address this issue, we propose LARA, a\nnovel price movement forecasting framework with two main components:\nLocality-Aware Attention (LA-Attention) and Iterative Refinement Labeling\n(RA-Labeling). (1) LA-Attention, enhanced by metric learning techniques,\nautomatically extracts the potentially profitable samples through masked\nattention scheme and task-specific distance metrics. (2) RA-Labeling further\niteratively refines the noisy labels of potentially profitable samples, and\ncombines the learned predictors robust to the unseen and noisy samples. In a\nset of experiments on three real-world financial markets: stocks,\ncryptocurrencies, and ETFs, LARA significantly outperforms several machine\nlearning based methods on the Qlib quantitative investment platform. Extensive\nablation studies confirm LARA's superior ability in capturing more reliable\ntrading opportunities.\n","authors":["Liang Zeng","Lei Wang","Hui Niu","Ruchen Zhang","Ling Wang","Jian Li"],"pdf_url":"https://arxiv.org/pdf/2107.11972v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08505v4","updated":"2024-07-10T06:59:20Z","published":"2024-01-16T17:07:22Z","title":"Harnessing Orthogonality to Train Low-Rank Neural Networks","summary":"  This study explores the learning dynamics of neural networks by analyzing the\nsingular value decomposition (SVD) of their weights throughout training. Our\ninvestigation reveals that an orthogonal basis within each multidimensional\nweight's SVD representation stabilizes during training. Building upon this, we\nintroduce Orthogonality-Informed Adaptive Low-Rank (OIALR) training, a novel\ntraining method exploiting the intrinsic orthogonality of neural networks.\nOIALR seamlessly integrates into existing training workflows with minimal\naccuracy loss, as demonstrated by benchmarking on various datasets and\nwell-established network architectures. With appropriate hyperparameter tuning,\nOIALR can surpass conventional training setups, including those of\nstate-of-the-art models.\n","authors":["Daniel Coquelin","Katharina FlÃ¼gel","Marie Weiel","Nicholas Kiefer","Charlotte Debus","Achim Streit","Markus GÃ¶tz"],"pdf_url":"https://arxiv.org/pdf/2401.08505v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13815v3","updated":"2024-07-10T06:34:07Z","published":"2023-08-26T08:39:16Z","title":"Arbitrary Distributions Mapping via SyMOT-Flow: A Flow-based Approach\n  Integrating Maximum Mean Discrepancy and Optimal Transport","summary":"  Finding a transformation between two unknown probability distributions from\nfinite samples is crucial for modeling complex data distributions and\nperforming tasks such as sample generation, domain adaptation and statistical\ninference. One powerful framework for such transformations is normalizing flow,\nwhich transforms an unknown distribution into a standard normal distribution\nusing an invertible network. In this paper, we introduce a novel model called\nSyMOT-Flow that trains an invertible transformation by minimizing the symmetric\nmaximum mean discrepancy between samples from two unknown distributions, and an\noptimal transport cost is incorporated as regularization to obtain a\nshort-distance and interpretable transformation. The resulted transformation\nleads to more stable and accurate sample generation. Several theoretical\nresults are established for the proposed model and its effectiveness is\nvalidated with low-dimensional illustrative examples as well as\nhigh-dimensional bi-modality medical image generation through the forward and\nreverse flows.\n","authors":["Zhe Xiong","Qiaoqiao Ding","Xiaoqun Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.13815v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07392v1","updated":"2024-07-10T06:32:58Z","published":"2024-07-10T06:32:58Z","title":"Malicious Path Manipulations via Exploitation of Representation\n  Vulnerabilities of Vision-Language Navigation Systems","summary":"  Building on the unprecedented capabilities of large language models for\ncommand understanding and zero-shot recognition of multi-modal vision-language\ntransformers, visual language navigation (VLN) has emerged as an effective way\nto address multiple fundamental challenges toward a natural language interface\nto robot navigation. However, such vision-language models are inherently\nvulnerable due to the lack of semantic meaning of the underlying embedding\nspace. Using a recently developed gradient based optimization procedure, we\ndemonstrate that images can be modified imperceptibly to match the\nrepresentation of totally different images and unrelated texts for a\nvision-language model. Building on this, we develop algorithms that can\nadversarially modify a minimal number of images so that the robot will follow a\nroute of choice for commands that require a number of landmarks. We demonstrate\nthat experimentally using a recently proposed VLN system; for a given\nnavigation command, a robot can be made to follow drastically different routes.\nWe also develop an efficient algorithm to detect such malicious modifications\nreliably based on the fact that the adversarially modified images have much\nhigher sensitivity to added Gaussian noise than the original images.\n","authors":["Chashi Mahiul Islam","Shaeke Salman","Montasir Shams","Xiuwen Liu","Piyush Kumar"],"pdf_url":"https://arxiv.org/pdf/2407.07392v1.pdf","comment":"8 pages, 5 figures. This paper has been accepted for publication at\n  the IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2024"},{"id":"http://arxiv.org/abs/2310.07338v3","updated":"2024-07-10T06:17:07Z","published":"2023-10-11T09:37:38Z","title":"From Supervised to Generative: A Novel Paradigm for Tabular Deep\n  Learning with Large Language Models","summary":"  Learning on tabular data underpins numerous real-world applications. Despite\nconsiderable efforts in developing effective learning models for tabular data,\ncurrent transferable tabular models remain in their infancy, limited by either\nthe lack of support for direct instruction following in new tasks or the\nneglect of acquiring foundational knowledge and capabilities from diverse\ntabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs)\nto overcome these limitations. TabFMs harness the potential of generative\ntabular learning, employing a pre-trained large language model (LLM) as the\nbase model and fine-tuning it using purpose-designed objectives on an extensive\nrange of tabular datasets. This approach endows TabFMs with a profound\nunderstanding and universal capabilities essential for learning on tabular\ndata. Our evaluations underscore TabFM's effectiveness: not only does it\nsignificantly excel in instruction-following tasks like zero-shot and\nin-context inference, but it also showcases performance that approaches, and in\ninstances, even transcends, the renowned yet mysterious closed-source LLMs like\nGPT-4. Furthermore, when fine-tuning with scarce data, our model achieves\nremarkable efficiency and maintains competitive performance with abundant\ntraining data. Finally, while our results are promising, we also delve into\nTabFM's limitations and potential opportunities, aiming to stimulate and\nexpedite future research on developing more potent TabFMs.\n","authors":["Xumeng Wen","Han Zhang","Shun Zheng","Wei Xu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.07338v3.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2407.07376v1","updated":"2024-07-10T05:37:02Z","published":"2024-07-10T05:37:02Z","title":"Deep(er) Reconstruction of Imaging Cherenkov Detectors with Swin\n  Transformers and Normalizing Flow Models","summary":"  Imaging Cherenkov detectors are crucial for particle identification (PID) in\nnuclear and particle physics experiments. Fast reconstruction algorithms are\nessential for near real-time alignment, calibration, data quality control, and\nefficient analysis. At the future Electron-Ion Collider (EIC), the ePIC\ndetector will feature a dual Ring Imaging Cherenkov (dual-RICH) detector in the\nhadron direction, a Detector of Internally Reflected Cherenkov (DIRC) in the\nbarrel, and a proximity focus RICH in the electron direction. This paper\nfocuses on the DIRC detector, which presents complex hit patterns and is also\nused for PID of pions and kaons in the GlueX experiment at JLab. We present\nDeep(er)RICH, an extension of the seminal DeepRICH work, offering improved and\nfaster PID compared to traditional methods and, for the first time, fast and\naccurate simulation. This advancement addresses a major bottleneck in Cherenkov\ndetector simulations involving photon tracking through complex optical\nelements. Our results leverage advancements in Vision Transformers,\nspecifically hierarchical Swin Transformer and normalizing flows. These methods\nenable direct learning from real data and the reconstruction of complex\ntopologies. We conclude by discussing the implications and future extensions of\nthis work, which can offer capabilities for PID for multiple cutting-edge\nexperiments like the future EIC.\n","authors":["Cristiano Fanelli","James Giroux","Justin Stevens"],"pdf_url":"https://arxiv.org/pdf/2407.07376v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.07373v1","updated":"2024-07-10T05:17:55Z","published":"2024-07-10T05:17:55Z","title":"Automatic Extraction of Disease Risk Factors from Medical Publications","summary":"  We present a novel approach to automating the identification of risk factors\nfor diseases from medical literature, leveraging pre-trained models in the\nbio-medical domain, while tuning them for the specific task. Faced with the\nchallenges of the diverse and unstructured nature of medical articles, our\nstudy introduces a multi-step system to first identify relevant articles, then\nclassify them based on the presence of risk factor discussions and, finally,\nextract specific risk factor information for a disease through a\nquestion-answering model.\n  Our contributions include the development of a comprehensive pipeline for the\nautomated extraction of risk factors and the compilation of several datasets,\nwhich can serve as valuable resources for further research in this area. These\ndatasets encompass a wide range of diseases, as well as their associated risk\nfactors, meticulously identified and validated through a fine-grained\nevaluation scheme. We conducted both automatic and thorough manual evaluation,\ndemonstrating encouraging results. We also highlight the importance of\nimproving models and expanding dataset comprehensiveness to keep pace with the\nrapidly evolving field of medical research.\n","authors":["Maxim Rubchinsky","Ella Rabinovich","Adi Shraibman","Netanel Golan","Tali Sahar","Dorit Shweiki"],"pdf_url":"https://arxiv.org/pdf/2407.07373v1.pdf","comment":"BioNLP@ACL2024, 12 pages"},{"id":"http://arxiv.org/abs/2407.07368v1","updated":"2024-07-10T05:03:48Z","published":"2024-07-10T05:03:48Z","title":"Data-driven Bayesian State Estimation with Compressed Measurement of\n  Model-free Process using Semi-supervised Learning","summary":"  The research topic is: data-driven Bayesian state estimation with compressed\nmeasurement (BSCM) of model-free process, say for a (causal) tracking\napplication. The dimension of the temporal measurement vector is lower than the\ndimension of the temporal state vector to be estimated. Hence the state\nestimation problem is an underdetermined inverse problem. The state-space-model\n(SSM) of the underlying dynamical process is assumed to be unknown and hence,\nwe use the terminology 'model-free process'. In absence of the SSM, we can not\nemploy traditional model-driven methods like Kalman Filter (KF) and Particle\nFilter (PF) and instead require data-driven methods. We first experimentally\nshow that two existing unsupervised learning-based data-driven methods fail to\naddress the BSCM problem for model-free process; they are data-driven nonlinear\nstate estimation (DANSE) method and deep Markov model (DMM) method. The\nunsupervised learning uses unlabelled data comprised of only noisy\nmeasurements. While DANSE provides a good predictive performance to model the\ntemporal measurement data as time-series, its unsupervised learning lacks a\nregularization for state estimation. We then investigate use of a\nsemi-supervised learning approach, and develop a semi-supervised learning-based\nDANSE method, referred to as SemiDANSE. In the semi-supervised learning, we use\na limited amount of labelled data along-with a large amount of unlabelled data,\nand that helps to bring the desired regularization for BSCM problem in the\nabsence of SSM. The labelled data means pairwise measurement-and-state data.\nUsing three chaotic dynamical systems (or processes) with nonlinear SSMs as\nbenchmark, we show that the data-driven SemiDANSE provides competitive\nperformance for BSCM against three SSM-informed methods - a hybrid method\ncalled KalmanNet, and two traditional model-driven methods called extended KF\nand unscented KF.\n","authors":["Anubhab Ghosh","Yonina C. Eldar","Saikat Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2407.07368v1.pdf","comment":"12 pages, under review at IEEE TSP. The abstract on ArXiv webpage is\n  slightly abridged to respect the character limit, please check the pdf\n  version for the unabridged version"},{"id":"http://arxiv.org/abs/2407.02702v2","updated":"2024-07-10T04:58:42Z","published":"2024-07-02T22:51:01Z","title":"Practical Guide for Causal Pathways and Sub-group Disparity Analysis","summary":"  In this study, we introduce the application of causal disparity analysis to\nunveil intricate relationships and causal pathways between sensitive attributes\nand the targeted outcomes within real-world observational data. Our methodology\ninvolves employing causal decomposition analysis to quantify and examine the\ncausal interplay between sensitive attributes and outcomes. We also emphasize\nthe significance of integrating heterogeneity assessment in causal disparity\nanalysis to gain deeper insights into the impact of sensitive attributes within\nspecific sub-groups on outcomes. Our two-step investigation focuses on datasets\nwhere race serves as the sensitive attribute. The results on two datasets\nindicate the benefit of leveraging causal analysis and heterogeneity assessment\nnot only for quantifying biases in the data but also for disentangling their\ninfluences on outcomes. We demonstrate that the sub-groups identified by our\napproach to be affected the most by disparities are the ones with the largest\nML classification errors. We also show that grouping the data only based on a\nsensitive attribute is not enough, and through these analyses, we can find\nsub-groups that are directly affected by disparities. We hope that our findings\nwill encourage the adoption of such methodologies in future ethical AI\npractices and bias audits, fostering a more equitable and fair technological\nlandscape.\n","authors":["Farnaz Kohankhaki","Shaina Raza","Oluwanifemi Bamgbose","Deval Pandya","Elham Dolatabadi"],"pdf_url":"https://arxiv.org/pdf/2407.02702v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07364v1","updated":"2024-07-10T04:53:26Z","published":"2024-07-10T04:53:26Z","title":"Real-time system optimal traffic routing under uncertainties -- Can\n  physics models boost reinforcement learning?","summary":"  System optimal traffic routing can mitigate congestion by assigning routes\nfor a portion of vehicles so that the total travel time of all vehicles in the\ntransportation system can be reduced. However, achieving real-time optimal\nrouting poses challenges due to uncertain demands and unknown system dynamics,\nparticularly in expansive transportation networks. While physics model-based\nmethods are sensitive to uncertainties and model mismatches, model-free\nreinforcement learning struggles with learning inefficiencies and\ninterpretability issues. Our paper presents TransRL, a novel algorithm that\nintegrates reinforcement learning with physics models for enhanced performance,\nreliability, and interpretability. TransRL begins by establishing a\ndeterministic policy grounded in physics models, from which it learns from and\nis guided by a differentiable and stochastic teacher policy. During training,\nTransRL aims to maximize cumulative rewards while minimizing the Kullback\nLeibler (KL) divergence between the current policy and the teacher policy. This\napproach enables TransRL to simultaneously leverage interactions with the\nenvironment and insights from physics models. We conduct experiments on three\ntransportation networks with up to hundreds of links. The results demonstrate\nTransRL's superiority over traffic model-based methods for being adaptive and\nlearning from the actual network data. By leveraging the information from\nphysics models, TransRL consistently outperforms state-of-the-art reinforcement\nlearning algorithms such as proximal policy optimization (PPO) and soft actor\ncritic (SAC). Moreover, TransRL's actions exhibit higher reliability and\ninterpretability compared to baseline reinforcement learning approaches like\nPPO and SAC.\n","authors":["Zemian Ke","Qiling Zou","Jiachao Liu","Sean Qian"],"pdf_url":"https://arxiv.org/pdf/2407.07364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07361v1","updated":"2024-07-10T04:39:56Z","published":"2024-07-10T04:39:56Z","title":"Characterizing Encrypted Application Traffic through Cellular Radio\n  Interface Protocol","summary":"  Modern applications are end-to-end encrypted to prevent data from being read\nor secretly modified. 5G tech nology provides ubiquitous access to these\napplications without compromising the application-specific performance and\nlatency goals. In this paper, we empirically demonstrate that 5G radio\ncommunication becomes the side channel to precisely infer the user's\napplications in real-time. The key idea lies in observing the 5G physical and\nMAC layer interactions over time that reveal the application's behavior. The\nMAC layer receives the data from the application and requests the network to\nassign the radio resource blocks. The network assigns the radio resources as\nper application requirements, such as priority, Quality of Service (QoS) needs,\namount of data to be transmitted, and buffer size. The adversary can passively\nobserve the radio resources to fingerprint the applications. We empirically\ndemonstrate this attack by considering four different categories of\napplications: online shopping, voice/video conferencing, video streaming, and\nOver-The-Top (OTT) media platforms. Finally, we have also demonstrated that an\nattacker can differentiate various types of applications in real-time within\neach category.\n","authors":["Md Ruman Islam","Raja Hasnain Anwar","Spyridon Mastorakis","Muhammad Taqi Raza"],"pdf_url":"https://arxiv.org/pdf/2407.07361v1.pdf","comment":"9 pages, 8 figures, 2 tables. This paper has been accepted for\n  publication by the 21st IEEE International Conference on Mobile Ad-Hoc and\n  Smart Systems (MASS 2024)"},{"id":"http://arxiv.org/abs/2407.07360v1","updated":"2024-07-10T04:33:43Z","published":"2024-07-10T04:33:43Z","title":"Towards a text-based quantitative and explainable histopathology image\n  analysis","summary":"  Recently, vision-language pre-trained models have emerged in computational\npathology. Previous works generally focused on the alignment of image-text\npairs via the contrastive pre-training paradigm. Such pre-trained models have\nbeen applied to pathology image classification in zero-shot learning or\ntransfer learning fashion. Herein, we hypothesize that the pre-trained\nvision-language models can be utilized for quantitative histopathology image\nanalysis through a simple image-to-text retrieval. To this end, we propose a\nText-based Quantitative and Explainable histopathology image analysis, which we\ncall TQx. Given a set of histopathology images, we adopt a pre-trained\nvision-language model to retrieve a word-of-interest pool. The retrieved words\nare then used to quantify the histopathology images and generate understandable\nfeature embeddings due to the direct mapping to the text description. To\nevaluate the proposed method, the text-based embeddings of four histopathology\nimage datasets are utilized to perform clustering and classification tasks. The\nresults demonstrate that TQx is able to quantify and analyze histopathology\nimages that are comparable to the prevalent visual models in computational\npathology.\n","authors":["Anh Tien Nguyen","Trinh Thi Le Vuong","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.07360v1.pdf","comment":"MICCAI 2024 - Early acceptance (Top 11%)"},{"id":"http://arxiv.org/abs/2407.07358v1","updated":"2024-07-10T04:31:50Z","published":"2024-07-10T04:31:50Z","title":"SGM-PINN: Sampling Graphical Models for Faster Training of\n  Physics-Informed Neural Networks","summary":"  SGM-PINN is a graph-based importance sampling framework to improve the\ntraining efficacy of Physics-Informed Neural Networks (PINNs) on parameterized\nproblems. By applying a graph decomposition scheme to an undirected\nProbabilistic Graphical Model (PGM) built from the training dataset, our method\ngenerates node clusters encoding conditional dependence between training\nsamples. Biasing sampling towards more important clusters allows smaller\nmini-batches and training datasets, improving training speed and accuracy. We\nadditionally fuse an efficient robustness metric with residual losses to\ndetermine regions requiring additional sampling. Experiments demonstrate the\nadvantages of the proposed framework, achieving $3\\times$ faster convergence\ncompared to prior state-of-the-art sampling methods.\n","authors":["John Anticev","Ali Aghdaei","Wuxinlin Cheng","Zhuo Feng"],"pdf_url":"https://arxiv.org/pdf/2407.07358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07357v1","updated":"2024-07-10T04:28:21Z","published":"2024-07-10T04:28:21Z","title":"A deep graph model for the signed interaction prediction in biological\n  network","summary":"  In pharmaceutical research, the strategy of drug repurposing accelerates the\ndevelopment of new therapies while reducing R&D costs. Network pharmacology\nlays the theoretical groundwork for identifying new drug indications, and deep\ngraph models have become essential for their precision in mapping complex\nbiological networks. Our study introduces an advanced graph model that utilizes\ngraph convolutional networks and tensor decomposition to effectively predict\nsigned chemical-gene interactions. This model demonstrates superior predictive\nperformance, especially in handling the polar relations in biological networks.\nOur research opens new avenues for drug discovery and repurposing, especially\nin understanding the mechanism of actions of drugs.\n","authors":["Shuyi Jin","Mengji Zhang","Meijie Wang","Lun Yu"],"pdf_url":"https://arxiv.org/pdf/2407.07357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09806v2","updated":"2024-07-10T04:04:06Z","published":"2024-05-16T04:28:44Z","title":"MediSyn: Text-Guided Diffusion Models for Broad Medical 2D and 3D Image\n  Synthesis","summary":"  Diffusion models have recently gained significant traction due to their\nability to generate high-fidelity and diverse images and videos conditioned on\ntext prompts. In medicine, this application promises to address the critical\nchallenge of data scarcity, a consequence of barriers in data sharing,\nstringent patient privacy regulations, and disparities in patient population\nand demographics. By generating realistic and varying medical 2D and 3D images,\nthese models offer a rich, privacy-respecting resource for algorithmic training\nand research. To this end, we introduce MediSyn, a pair of instruction-tuned\ntext-guided latent diffusion models with the ability to generate high-fidelity\nand diverse medical 2D and 3D images across specialties and modalities. Through\nestablished metrics, we show significant improvement in broad medical image and\nvideo synthesis guided by text prompts.\n","authors":["Joseph Cho","Cyril Zakka","Dhamanpreet Kaur","Rohan Shad","Ross Wightman","Akshay Chaudhari","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2405.09806v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07350v1","updated":"2024-07-10T04:03:23Z","published":"2024-07-10T04:03:23Z","title":"Long-Term Fairness in Sequential Multi-Agent Selection with Positive\n  Reinforcement","summary":"  While much of the rapidly growing literature on fair decision-making focuses\non metrics for one-shot decisions, recent work has raised the intriguing\npossibility of designing sequential decision-making to positively impact\nlong-term social fairness. In selection processes such as college admissions or\nhiring, biasing slightly towards applicants from under-represented groups is\nhypothesized to provide positive feedback that increases the pool of\nunder-represented applicants in future selection rounds, thus enhancing\nfairness in the long term. In this paper, we examine this hypothesis and its\nconsequences in a setting in which multiple agents are selecting from a common\npool of applicants. We propose the Multi-agent Fair-Greedy policy, that\nbalances greedy score maximization and fairness. Under this policy, we prove\nthat the resource pool and the admissions converge to a long-term fairness\ntarget set by the agents when the score distributions across the groups in the\npopulation are identical. We provide empirical evidence of existence of\nequilibria under non-identical score distributions through synthetic and\nadapted real-world datasets. We then sound a cautionary note for more complex\napplicant pool evolution models, under which uncoordinated behavior by the\nagents can cause negative reinforcement, leading to a reduction in the fraction\nof under-represented applicants. Our results indicate that, while positive\nreinforcement is a promising mechanism for long-term fairness, policies must be\ndesigned carefully to be robust to variations in the evolution model, with a\nnumber of open issues that remain to be explored by algorithm designers, social\nscientists, and policymakers.\n","authors":["Bhagyashree Puranik","Ozgur Guldogan","Upamanyu Madhow","Ramtin Pedarsani"],"pdf_url":"https://arxiv.org/pdf/2407.07350v1.pdf","comment":"This manuscript has been accepted for publication in the IEEE Journal\n  on Selected Areas in Information Theory special issue on\n  information-theoretic methods for reliable and trustworthy ML"},{"id":"http://arxiv.org/abs/2405.14246v3","updated":"2024-07-10T04:01:55Z","published":"2024-05-23T07:25:31Z","title":"GCondenser: Benchmarking Graph Condensation","summary":"  Large-scale graphs are valuable for graph representation learning, yet the\nabundant data in these graphs hinders the efficiency of the training process.\nGraph condensation (GC) alleviates this issue by compressing the large graph\ninto a significantly smaller one that still supports effective model training.\nAlthough recent research has introduced various approaches to improve the\neffectiveness of the condensed graph, comprehensive and practical evaluations\nacross different GC methods are neglected. This paper proposes the first\nlarge-scale graph condensation benchmark, GCondenser, to holistically evaluate\nand compare mainstream GC methods. GCondenser includes a standardised GC\nparadigm, consisting of condensation, validation, and evaluation procedures, as\nwell as enabling extensions to new GC methods and datasets. With GCondenser, a\ncomprehensive performance study is conducted, presenting the effectiveness of\nexisting methods. GCondenser is open-sourced and available at\nhttps://github.com/superallen13/GCondenser.\n","authors":["Yilun Liu","Ruihong Qiu","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2405.14246v3.pdf","comment":"GCondenser is open-sourced and available at\n  https://github.com/superallen13/GCondenser"},{"id":"http://arxiv.org/abs/2303.17568v2","updated":"2024-07-10T03:52:58Z","published":"2023-03-30T17:34:01Z","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual\n  Benchmarking on HumanEval-X","summary":"  Large pre-trained code generation models, such as OpenAI Codex, can generate\nsyntax- and function-correct code, making the coding of programmers more\nproductive and our pursuit of artificial general intelligence closer. In this\npaper, we introduce CodeGeeX, a multilingual model with 13 billion parameters\nfor code generation. CodeGeeX is pre-trained on 850 billion tokens of 23\nprogramming languages as of June 2022. Our extensive experiments suggest that\nCodeGeeX outperforms multilingual code models of similar scale for both the\ntasks of code generation and translation on HumanEval-X. Building upon\nHumanEval (Python only), we develop the HumanEval-X benchmark for evaluating\nmultilingual models by hand-writing the solutions in C++, Java, JavaScript, and\nGo. In addition, we build CodeGeeX-based extensions on Visual Studio Code,\nJetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of\nthousands of active users per week. Our user study demonstrates that CodeGeeX\ncan help to increase coding efficiency for 83.4% of its users. Finally,\nCodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code,\nmodel weights (the version of 850B tokens), API, extensions, and HumanEval-X at\nhttps://github.com/THUDM/CodeGeeX.\n","authors":["Qinkai Zheng","Xiao Xia","Xu Zou","Yuxiao Dong","Shan Wang","Yufei Xue","Zihan Wang","Lei Shen","Andi Wang","Yang Li","Teng Su","Zhilin Yang","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2303.17568v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07346v1","updated":"2024-07-10T03:52:53Z","published":"2024-07-10T03:52:53Z","title":"INSIGHT: Universal Neural Simulator for Analog Circuits Harnessing\n  Autoregressive Transformers","summary":"  Analog front-end design heavily relies on specialized human expertise and\ncostly trial-and-error simulations, which motivated many prior works on analog\ndesign automation. However, efficient and effective exploration of the vast and\ncomplex design space remains constrained by the time-consuming nature of\nCPU-based SPICE simulations, making effective design automation a challenging\nendeavor. In this paper, we introduce INSIGHT, a GPU-powered,\ntechnology-independent, effective universal neural simulator in the analog\nfront-end design automation loop. INSIGHT accurately predicts the performance\nmetrics of analog circuits across various technology nodes, significantly\nreducing inference time. Notably, its autoregressive capabilities enable\nINSIGHT to accurately predict simulation-costly critical transient\nspecifications leveraging less expensive performance metric information. The\nlow cost and high fidelity feature make INSIGHT a good substitute for standard\nsimulators in analog front-end optimization frameworks. INSIGHT is compatible\nwith any optimization framework, facilitating enhanced design space exploration\nfor sample efficiency through sophisticated offline learning and adaptation\ntechniques. Our experiments demonstrate that INSIGHT-M, a model-based batch\nreinforcement learning framework that leverages INSIGHT for analog sizing,\nachieves at least 50X improvement in sample efficiency across circuits. To the\nbest of our knowledge, this marks the first use of autoregressive transformers\nin analog front-end design.\n","authors":["Souradip Poddar","Youngmin Oh","Yao Lai","Hanqing Zhu","Bosun Hwang","David Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2407.07346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15681v2","updated":"2024-07-10T03:32:36Z","published":"2024-04-24T06:29:55Z","title":"Automated Creation of Source Code Variants of a Cryptographic Hash\n  Function Implementation Using Generative Pre-Trained Transformer Models","summary":"  Generative pre-trained transformers (GPT's) are a type of large language\nmachine learning model that are unusually adept at producing novel, and\ncoherent, natural language. In this study the ability of GPT models to generate\nnovel and correct versions, and notably very insecure versions, of\nimplementations of the cryptographic hash function SHA-1 is examined. The GPT\nmodels Llama-2-70b-chat-h, Mistral-7B-Instruct-v0.1, and zephyr-7b-alpha are\nused. The GPT models are prompted to re-write each function using a modified\nversion of the localGPT framework and langchain to provide word embedding\ncontext of the full source code and header files to the model, resulting in\nover 150,000 function re-write GPT output text blocks, approximately 50,000 of\nwhich were able to be parsed as C code and subsequently compiled. The generated\ncode is analyzed for being compilable, correctness of the algorithm, memory\nleaks, compiler optimization stability, and character distance to the reference\nimplementation. Remarkably, several generated function variants have a high\nimplementation security risk of being correct for some test vectors, but\nincorrect for other test vectors. Additionally, many function implementations\nwere not correct to the reference algorithm of SHA-1, but produced hashes that\nhave some of the basic characteristics of hash functions. Many of the function\nre-writes contained serious flaws such as memory leaks, integer overflows, out\nof bounds accesses, use of uninitialised values, and compiler optimization\ninstability. Compiler optimization settings and SHA-256 hash checksums of the\ncompiled binaries are used to cluster implementations that are equivalent but\nmay not have identical syntax - using this clustering over 100,000 novel and\ncorrect versions of the SHA-1 codebase were generated where each component C\nfunction of the reference implementation is different from the original code.\n","authors":["Elijah Pelofske","Vincent Urias","Lorie M. Liebrock"],"pdf_url":"https://arxiv.org/pdf/2404.15681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07338v1","updated":"2024-07-10T03:20:17Z","published":"2024-07-10T03:20:17Z","title":"Towards Complete Causal Explanation with Expert Knowledge","summary":"  We study the problem of restricting Markov equivalence classes of maximal\nancestral graphs (MAGs) containing certain edge marks, which we refer to as\nexpert knowledge. MAGs forming a Markov equivalence class can be uniquely\nrepresented by an essential ancestral graph. We seek to learn the restriction\nof the essential ancestral graph containing the proposed expert knowledge. Our\ncontributions are several-fold. First, we prove certain properties for the\nentire Markov equivalence class including a conjecture from Ali et al. (2009).\nSecond, we present three sound graphical orientation rules, two of which\ngeneralize previously known rules, for adding expert knowledge to an essential\ngraph. We also show that some orientation rules of Zhang (2008) are not needed\nfor restricting the Markov equivalence class with expert knowledge. We provide\nan algorithm for including this expert knowledge and show that our algorithm is\ncomplete in certain settings i.e., in these settings, the output of our\nalgorithm is a restricted essential ancestral graph. We conjecture this\nalgorithm is complete generally. Outside of our specified settings, we provide\nan algorithm for checking whether a graph is a restricted essential graph and\ndiscuss its runtime. This work can be seen as a generalization of Meek (1995).\n","authors":["Aparajithan Venkateswaran","Emilija Perkovic"],"pdf_url":"https://arxiv.org/pdf/2407.07338v1.pdf","comment":"80 pages (main paper 18 pages, supplementary material 62 pages), 16\n  figures, 7 algorithms, 3 tables"},{"id":"http://arxiv.org/abs/2407.07333v1","updated":"2024-07-10T03:04:20Z","published":"2024-07-10T03:04:20Z","title":"Mitigating Partial Observability in Sequential Decision Processes via\n  the Lambda Discrepancy","summary":"  Reinforcement learning algorithms typically rely on the assumption that the\nenvironment dynamics and value function can be expressed in terms of a\nMarkovian state representation. However, when state information is only\npartially observable, how can an agent learn such a state representation, and\nhow can it detect when it has found one? We introduce a metric that can\naccomplish both objectives, without requiring access to--or knowledge of--an\nunderlying, unobservable state space. Our metric, the $\\lambda$-discrepancy, is\nthe difference between two distinct temporal difference (TD) value estimates,\neach computed using TD($\\lambda$) with a different value of $\\lambda$. Since\nTD($\\lambda$=0) makes an implicit Markov assumption and TD($\\lambda$=1) does\nnot, a discrepancy between these estimates is a potential indicator of a\nnon-Markovian state representation. Indeed, we prove that the\n$\\lambda$-discrepancy is exactly zero for all Markov decision processes and\nalmost always non-zero for a broad class of partially observable environments.\nWe also demonstrate empirically that, once detected, minimizing the\n$\\lambda$-discrepancy can help with learning a memory function to mitigate the\ncorresponding partial observability. We then train a reinforcement learning\nagent that simultaneously constructs two recurrent value networks with\ndifferent $\\lambda$ parameters and minimizes the difference between them as an\nauxiliary loss. The approach scales to challenging partially observable\ndomains, where the resulting agent frequently performs significantly better\n(and never performs worse) than a baseline recurrent agent with only a single\nvalue network.\n","authors":["Cameron Allen","Aaron Kirtland","Ruo Yu Tao","Sam Lobel","Daniel Scott","Nicholas Petrocelli","Omer Gottesman","Ronald Parr","Michael L. Littman","George Konidaris"],"pdf_url":"https://arxiv.org/pdf/2407.07333v1.pdf","comment":"GitHub URL: https://github.com/brownirl/lambda_discrepancy"},{"id":"http://arxiv.org/abs/2407.07328v1","updated":"2024-07-10T02:51:35Z","published":"2024-07-10T02:51:35Z","title":"CATP: Context-Aware Trajectory Prediction with Competition Symbiosis","summary":"  Contextual information is vital for accurate trajectory prediction. For\ninstance, the intricate flying behavior of migratory birds hinges on their\nanalysis of environmental cues such as wind direction and air pressure.\nHowever, the diverse and dynamic nature of contextual information renders it an\narduous task for AI models to comprehend its impact on trajectories and\nconsequently predict them accurately. To address this issue, we propose a\n``manager-worker'' framework to unleash the full potential of contextual\ninformation and construct CATP model, an implementation of the framework for\nContext-Aware Trajectory Prediction. The framework comprises a manager model,\nseveral worker models, and a tailored training mechanism inspired by\ncompetition symbiosis in nature. Taking CATP as an example, each worker needs\nto compete against others for training data and develop an advantage in\npredicting specific moving patterns. The manager learns the workers'\nperformance in different contexts and selects the best one in the given context\nto predict trajectories, enabling CATP as a whole to operate in a symbiotic\nmanner. We conducted two comparative experiments and an ablation study to\nquantitatively evaluate the proposed framework and CATP model. The results\nshowed that CATP could outperform SOTA models, and the framework could be\ngeneralized to different context-aware tasks.\n","authors":["Jiang Wu","Dongyu Liu","Yuchen Lin","Yingcai Wu"],"pdf_url":"https://arxiv.org/pdf/2407.07328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09465v2","updated":"2024-07-10T02:43:14Z","published":"2024-04-15T05:29:23Z","title":"PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI","summary":"  With recent developments in Embodied Artificial Intelligence (EAI) research,\nthere has been a growing demand for high-quality, large-scale interactive scene\ngeneration. While prior methods in scene synthesis have prioritized the\nnaturalness and realism of the generated scenes, the physical plausibility and\ninteractivity of scenes have been largely left unexplored. To address this\ndisparity, we introduce PhyScene, a novel method dedicated to generating\ninteractive 3D scenes characterized by realistic layouts, articulated objects,\nand rich physical interactivity tailored for embodied agents. Based on a\nconditional diffusion model for capturing scene layouts, we devise novel\nphysics- and interactivity-based guidance mechanisms that integrate constraints\nfrom object collision, room layout, and object reachability. Through extensive\nexperiments, we demonstrate that PhyScene effectively leverages these guidance\nfunctions for physically interactable scene synthesis, outperforming existing\nstate-of-the-art scene synthesis methods by a large margin. Our findings\nsuggest that the scenes generated by PhyScene hold considerable potential for\nfacilitating diverse skill acquisition among agents within interactive\nenvironments, thereby catalyzing further advancements in embodied AI research.\nProject website: http://physcene.github.io.\n","authors":["Yandan Yang","Baoxiong Jia","Peiyuan Zhi","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2404.09465v2.pdf","comment":"Accepted by CVPR 2024 (Highlight), 18 pages"},{"id":"http://arxiv.org/abs/2407.04846v2","updated":"2024-07-10T02:39:01Z","published":"2024-07-05T20:14:36Z","title":"Amazing Things Come From Having Many Good Models","summary":"  The Rashomon Effect, coined by Leo Breiman, describes the phenomenon that\nthere exist many equally good predictive models for the same dataset. This\nphenomenon happens for many real datasets and when it does, it sparks both\nmagic and consternation, but mostly magic. In light of the Rashomon Effect,\nthis perspective piece proposes reshaping the way we think about machine\nlearning, particularly for tabular data problems in the nondeterministic\n(noisy) setting. We address how the Rashomon Effect impacts (1) the existence\nof simple-yet-accurate models, (2) flexibility to address user preferences,\nsuch as fairness and monotonicity, without losing performance, (3) uncertainty\nin predictions, fairness, and explanations, (4) reliable variable importance,\n(5) algorithm choice, specifically, providing advanced knowledge of which\nalgorithms might be suitable for a given problem, and (6) public policy. We\nalso discuss a theory of when the Rashomon Effect occurs and why. Our goal is\nto illustrate how the Rashomon Effect can have a massive impact on the use of\nmachine learning for complex problems in society.\n","authors":["Cynthia Rudin","Chudi Zhong","Lesia Semenova","Margo Seltzer","Ronald Parr","Jiachang Liu","Srikar Katta","Jon Donnelly","Harry Chen","Zachery Boner"],"pdf_url":"https://arxiv.org/pdf/2407.04846v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07320v1","updated":"2024-07-10T02:31:15Z","published":"2024-07-10T02:31:15Z","title":"Flow to Rare Events: An Application of Normalizing Flow in Temporal\n  Importance Sampling for Automated Vehicle Validation","summary":"  Automated Vehicle (AV) validation based on simulated testing requires\nunbiased evaluation and high efficiency. One effective solution is to increase\nthe exposure to risky rare events while reweighting the probability measure.\nHowever, characterizing the distribution of risky events is particularly\nchallenging due to the paucity of samples and the temporality of continuous\nscenario variables. To solve it, we devise a method to represent, generate, and\nreweight the distribution of risky rare events. We decompose the temporal\nevolution of continuous variables into distribution components based on\nconditional probability. By introducing the Risk Indicator Function, the\ndistribution of risky rare events is theoretically precipitated out of\nnaturalistic driving distribution. This targeted distribution is practically\ngenerated via Normalizing Flow, which achieves exact and tractable probability\nevaluation of intricate distribution. The rare event distribution is then\ndemonstrated as the advantageous Importance Sampling distribution. We also\npromote the technique of temporal Importance Sampling. The combined method,\nnamed as TrimFlow, is executed to estimate the collision rate of Car-following\nscenarios as a tentative practice. The results showed that sampling background\nvehicle maneuvers from rare event distribution could evolve testing scenarios\nto hazardous states. TrimFlow reduced 86.1% of tests compared to generating\ntesting scenarios according to their exposure in the naturalistic driving\nenvironment. In addition, the TrimFlow method is not limited to one specific\ntype of functional scenario.\n","authors":["Yichun Ye","He Zhang","Ye Tian","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2407.07320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07311v1","updated":"2024-07-10T02:11:01Z","published":"2024-07-10T02:11:01Z","title":"ViTime: A Visual Intelligence-Based Foundation Model for Time Series\n  Forecasting","summary":"  The success of large pretrained models in natural language processing (NLP)\nand computer vision (CV) has opened new avenues for constructing foundation\nmodels for time series forecasting (TSF). Traditional TSF foundation models\nrely heavily on numerical data fitting. In contrast, the human brain is\ninherently skilled at processing visual information, prefer predicting future\ntrends by observing visualized sequences. From a biomimetic perspective,\nutilizing models to directly process numerical sequences might not be the most\neffective route to achieving Artificial General Intelligence (AGI). This paper\nproposes ViTime, a novel Visual Intelligence-based foundation model for TSF.\nViTime overcomes the limitations of numerical time series data fitting by\nutilizing visual data processing paradigms and employs a innovative data\nsynthesis method during training, called Real Time Series (RealTS). Experiments\non a diverse set of previously unseen forecasting datasets demonstrate that\nViTime achieves state-of-the-art zero-shot performance, even surpassing the\nbest individually trained supervised models in some situations. These findings\nsuggest that visual intelligence can significantly enhance time series analysis\nand forecasting, paving the way for more advanced and versatile models in the\nfield. The code for our framework is accessible at\nhttps://github.com/IkeYang/ViTime.\n","authors":["Luoxiao Yang","Yun Wang","Xinqi Fan","Israel Cohen","Yue Zhao","Zijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.07311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06645v2","updated":"2024-07-10T01:55:29Z","published":"2024-07-09T08:14:29Z","title":"Entropy Law: The Story Behind Data Compression and LLM Performance","summary":"  Data is the cornerstone of large language models (LLMs), but not all data is\nuseful for model learning. Carefully selected data can better elicit the\ncapabilities of LLMs with much less computational overhead. Most methods\nconcentrate on evaluating the quality of individual samples in data selection,\nwhile the combinatorial effects among samples are neglected. Even if each\nsample is of perfect quality, their combinations may be suboptimal in teaching\nLLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim\nto uncover the underlying relationships between LLM performance and data\nselection. Inspired by the information compression nature of LLMs, we uncover\nan ``entropy law'' that connects LLM performance with data compression ratio\nand first-epoch training loss, which reflect the information redundancy of a\ndataset and the mastery of inherent knowledge encoded in this dataset,\nrespectively. Through both theoretical deduction and empirical evaluation, we\nfind that model performance is negatively correlated to the compression ratio\nof training data, which usually yields a lower training loss. Based on the\nfindings of the entropy law, we propose a quite efficient and universal data\nselection method named \\textbf{ZIP} for training LLMs, which aim to prioritize\ndata subsets exhibiting a low compression ratio. Based on a multi-stage\nalgorithm that selects diverse data in a greedy manner, we can obtain a good\ndata subset with satisfactory diversity. Extensive experiments have been\nconducted to validate the entropy law and the superiority of ZIP across\ndifferent LLM backbones and alignment stages. We also present an interesting\napplication of entropy law that can detect potential performance risks at the\nbeginning of model training.\n","authors":["Mingjia Yin","Chuhan Wu","Yufei Wang","Hao Wang","Wei Guo","Yasheng Wang","Yong Liu","Ruiming Tang","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.06645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07066v2","updated":"2024-07-10T01:37:05Z","published":"2024-07-09T17:42:26Z","title":"Explainable Hyperdimensional Computing for Balancing Privacy and\n  Transparency in Additive Manufacturing Monitoring","summary":"  In-situ sensing, in conjunction with learning models, presents a unique\nopportunity to address persistent defect issues in Additive Manufacturing (AM)\nprocesses. However, this integration introduces significant data privacy\nconcerns, such as data leakage, sensor data compromise, and model inversion\nattacks, revealing critical details about part design, material composition,\nand machine parameters. Differential Privacy (DP) models, which inject noise\ninto data under mathematical guarantees, offer a nuanced balance between data\nutility and privacy by obscuring traces of sensing data. However, the\nintroduction of noise into learning models, often functioning as black boxes,\ncomplicates the prediction of how specific noise levels impact model accuracy.\nThis study introduces the Differential Privacy-HyperDimensional computing\n(DP-HD) framework, leveraging the explainability of the vector symbolic\nparadigm to predict the noise impact on the accuracy of in-situ monitoring,\nsafeguarding sensitive data while maintaining operational efficiency.\nExperimental results on real-world high-speed melt pool data of AM for\ndetecting overhang anomalies demonstrate that DP-HD achieves superior\noperational efficiency, prediction accuracy, and robust privacy protection,\noutperforming state-of-the-art Machine Learning (ML) models. For example, when\nimplementing the same level of privacy protection (with a privacy budget set at\n1), our model achieved an accuracy of 94.43%, surpassing the performance of\ntraditional models such as ResNet50 (52.30%), GoogLeNet (23.85%), AlexNet\n(55.78%), DenseNet201 (69.13%), and EfficientNet B2 (40.81%). Notably, DP-HD\nmaintains high performance under substantial noise additions designed to\nenhance privacy, unlike current models that suffer significant accuracy\ndeclines under high privacy constraints.\n","authors":["Fardin Jalil Piran","Prathyush P. Poduval","Hamza Errahmouni Barkam","Mohsen Imani","Farhad Imani"],"pdf_url":"https://arxiv.org/pdf/2407.07066v2.pdf","comment":"24 pages, 13 figures"},{"id":"http://arxiv.org/abs/2407.06190v2","updated":"2024-07-10T01:32:28Z","published":"2024-07-08T17:59:54Z","title":"4D Contrastive Superflows are Dense 3D Representation Learners","summary":"  In the realm of autonomous driving, accurate 3D perception is the foundation.\nHowever, developing such models relies on extensive human annotations -- a\nprocess that is both costly and labor-intensive. To address this challenge from\na data representation learning perspective, we introduce SuperFlow, a novel\nframework designed to harness consecutive LiDAR-camera pairs for establishing\nspatiotemporal pretraining objectives. SuperFlow stands out by integrating two\nkey designs: 1) a dense-to-sparse consistency regularization, which promotes\ninsensitivity to point cloud density variations during feature learning, and 2)\na flow-based contrastive learning module, carefully crafted to extract\nmeaningful temporal cues from readily available sensor calibrations. To further\nboost learning efficiency, we incorporate a plug-and-play view consistency\nmodule that enhances the alignment of the knowledge distilled from camera\nviews. Extensive comparative and ablation studies across 11 heterogeneous LiDAR\ndatasets validate our effectiveness and superiority. Additionally, we observe\nseveral interesting emerging properties by scaling up the 2D and 3D backbones\nduring pretraining, shedding light on the future research of 3D foundation\nmodels for LiDAR-based perception.\n","authors":["Xiang Xu","Lingdong Kong","Hui Shuai","Wenwei Zhang","Liang Pan","Kai Chen","Ziwei Liu","Qingshan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.06190v2.pdf","comment":"ECCV 2024; 36 pages, 11 figures, 11 tables; Code at\n  https://github.com/Xiangxu-0103/SuperFlow"},{"id":"http://arxiv.org/abs/2407.07294v1","updated":"2024-07-10T01:22:02Z","published":"2024-07-10T01:22:02Z","title":"Analyzing Machine Learning Performance in a Hybrid Quantum Computing and\n  HPC Environment","summary":"  We explored the possible benefits of integrating quantum simulators in a\n\"hybrid\" quantum machine learning (QML) workflow that uses both classical and\nquantum computations in a high-performance computing (HPC) environment. Here,\nwe used two Oak Ridge Leadership Computing Facility HPC systems, Andes (a\ncommodity-type Linux cluster) and Frontier (an HPE Cray EX supercomputer),\nalong with quantum computing simulators from PennyLane and IBMQ to evaluate a\nhybrid QML program -- using a \"ground up\" approach. Using 1 GPU on Frontier, we\nfound ~56% and ~77% speedups when compared to using Frontier's CPU and a local,\nnon-HPC system, respectively. Analyzing performance on a larger dataset using\nmultiple threads, the Frontier GPUs performed ~92% and ~48% faster than the\nAndes and Frontier CPUs, respectively. More impressively, this is a ~226%\nspeedup over a local, non-HPC system's runtime using the same simulator and\nnumber of threads. We hope that this proof of concept will motivate more\nintensive hybrid QC/HPC scaling studies in the future.\n","authors":["Samuel T. Bieberich","Michael A. Sandoval"],"pdf_url":"https://arxiv.org/pdf/2407.07294v1.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.07291v1","updated":"2024-07-10T00:55:38Z","published":"2024-07-10T00:55:38Z","title":"Causal Discovery in Semi-Stationary Time Series","summary":"  Discovering causal relations from observational time series without making\nthe stationary assumption is a significant challenge. In practice, this\nchallenge is common in many areas, such as retail sales, transportation\nsystems, and medical science. Here, we consider this problem for a class of\nnon-stationary time series. The structural causal model (SCM) of this type of\ntime series, called the semi-stationary time series, exhibits that a finite\nnumber of different causal mechanisms occur sequentially and periodically\nacross time. This model holds considerable practical utility because it can\nrepresent periodicity, including common occurrences such as seasonality and\ndiurnal variation. We propose a constraint-based, non-parametric algorithm for\ndiscovering causal relations in this setting. The resulting algorithm,\nPCMCI$_{\\Omega}$, can capture the alternating and recurring changes in the\ncausal mechanisms and then identify the underlying causal graph with\nconditional independence (CI) tests. We show that this algorithm is sound in\nidentifying causal relations on discrete time series. We validate the algorithm\nwith extensive experiments on continuous and discrete simulated data. We also\napply our algorithm to a real-world climate dataset.\n","authors":["Shanyun Gao","Raghavendra Addanki","Tong Yu","Ryan A. Rossi","Murat Kocaoglu"],"pdf_url":"https://arxiv.org/pdf/2407.07291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07290v1","updated":"2024-07-10T00:54:42Z","published":"2024-07-10T00:54:42Z","title":"Causal Discovery-Driven Change Point Detection in Time Series","summary":"  Change point detection in time series seeks to identify times when the\nprobability distribution of time series changes. It is widely applied in many\nareas, such as human-activity sensing and medical science. In the context of\nmultivariate time series, this typically involves examining the joint\ndistribution of high-dimensional data: If any one variable changes, the whole\ntime series is assumed to have changed. However, in practical applications, we\nmay be interested only in certain components of the time series, exploring\nabrupt changes in their distributions in the presence of other time series.\nHere, assuming an underlying structural causal model that governs the\ntime-series data generation, we address this problem by proposing a two-stage\nnon-parametric algorithm that first learns parts of the causal structure\nthrough constraint-based discovery methods. The algorithm then uses conditional\nrelative Pearson divergence estimation to identify the change points. The\nconditional relative Pearson divergence quantifies the distribution disparity\nbetween consecutive segments in the time series, while the causal discovery\nmethod enables a focus on the causal mechanism, facilitating access to\nindependent and identically distributed (IID) samples. Theoretically, the\ntypical assumption of samples being IID in conventional change point detection\nmethods can be relaxed based on the Causal Markov Condition. Through\nexperiments on both synthetic and real-world datasets, we validate the\ncorrectness and utility of our approach.\n","authors":["Shanyun Gao","Raghavendra Addanki","Tong Yu","Ryan A. Rossi","Murat Kocaoglu"],"pdf_url":"https://arxiv.org/pdf/2407.07290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14439v2","updated":"2024-07-10T00:37:02Z","published":"2023-12-22T05:09:58Z","title":"PUMA: Efficient Continual Graph Learning for Node Classification with\n  Graph Condensation","summary":"  When handling streaming graphs, existing graph representation learning models\nencounter a catastrophic forgetting problem, where previously learned knowledge\nof these models is easily overwritten when learning with newly incoming graphs.\nIn response, Continual Graph Learning (CGL) emerges as a novel paradigm\nenabling graph representation learning from streaming graphs. Our prior work,\nCondense and Train (CaT) is a replay-based CGL framework with a balanced\ncontinual learning procedure, which designs a small yet effective memory bankn\nfor replaying. Although the CaT alleviates the catastrophic forgetting problem,\nthere exist three issues: (1) The graph condensation only focuses on labelled\nnodes while neglecting abundant information carried by unlabelled nodes; (2)\nThe continual training scheme of the CaT overemphasises on the previously\nlearned knowledge, limiting the model capacity to learn from newly added\nmemories; (3) Both the condensation process and replaying process of the CaT\nare time-consuming. In this paper, we propose a PsUdo-label guided Memory bAnk\n(PUMA) CGL framework, extending from the CaT to enhance its efficiency and\neffectiveness by overcoming the above-mentioned weaknesses and limits. To fully\nexploit the information in a graph, PUMA expands the coverage of nodes during\ngraph condensation with both labelled and unlabelled nodes. Furthermore, a\ntraining-from-scratch strategy is proposed to upgrade the previous continual\nlearning scheme for a balanced training between the historical and the new\ngraphs. Besides, PUMA uses a one-time prorogation and wide graph encoders to\naccelerate the graph condensation and the graph encoding process in the\ntraining stage to improve the efficiency of the whole framework. Extensive\nexperiments on six datasets for the node classification task demonstrate the\nstate-of-the-art performance and efficiency over existing methods.\n","authors":["Yilun Liu","Ruihong Qiu","Yanran Tang","Hongzhi Yin","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2312.14439v2.pdf","comment":"Add new baselines and experiments. The code has been released in\n  https://github.com/superallen13/PUMA. arXiv admin note: substantial text\n  overlap with arXiv:2309.09455"},{"id":"http://arxiv.org/abs/2311.12917v2","updated":"2024-07-10T00:32:49Z","published":"2023-11-21T18:25:23Z","title":"Orchard: building large cancer phylogenies using stochastic\n  combinatorial search","summary":"  Phylogenies depicting the evolutionary history of genetically heterogeneous\nsubpopulations of cells from the same cancer, i.e., cancer phylogenies, offer\nvaluable insights about cancer development and guide treatment strategies. Many\nmethods exist that reconstruct cancer phylogenies using point mutations\ndetected with bulk DNA sequencing. However, these methods become inaccurate\nwhen reconstructing phylogenies with more than 30 mutations, or, in some cases,\nfail to recover a phylogeny altogether. Here, we introduce Orchard, a cancer\nphylogeny reconstruction algorithm that is fast and accurate using up to 1000\nmutations. Orchard samples without replacement from a factorized approximation\nof the posterior distribution over phylogenies, a novel result derived in this\npaper. Each factor in this approximate posterior corresponds to a conditional\ndistribution for adding a new mutation to a partially built phylogeny. Orchard\noptimizes each factor sequentially, generating a sequence of incrementally\nlarger phylogenies that ultimately culminate in a complete tree containing all\nmutations. Our evaluations demonstrate that Orchard outperforms\nstate-of-the-art cancer phylogeny reconstruction methods in reconstructing more\nplausible phylogenies across 90 simulated cancers and 14 B-progenitor acute\nlymphoblastic leukemias (B-ALLs). Remarkably, Orchard accurately reconstructs\ncancer phylogenies using up to 1,000 mutations. Additionally, we demonstrate\nthat the large and accurate phylogenies reconstructed by Orchard are useful for\nidentifying patterns of somatic mutations and genetic variations among distinct\ncancer cell subpopulations.\n","authors":["E. Kulman","R. Kuang","Q. Morris"],"pdf_url":"https://arxiv.org/pdf/2311.12917v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07279v1","updated":"2024-07-10T00:01:56Z","published":"2024-07-10T00:01:56Z","title":"Towards a theory of learning dynamics in deep state space models","summary":"  State space models (SSMs) have shown remarkable empirical performance on many\nlong sequence modeling tasks, but a theoretical understanding of these models\nis still lacking. In this work, we study the learning dynamics of linear SSMs\nto understand how covariance structure in data, latent state size, and\ninitialization affect the evolution of parameters throughout learning with\ngradient descent. We show that focusing on the learning dynamics in the\nfrequency domain affords analytical solutions under mild assumptions, and we\nestablish a link between one-dimensional SSMs and the dynamics of deep linear\nfeed-forward networks. Finally, we analyze how latent state\nover-parameterization affects convergence time and describe future work in\nextending our results to the study of deep SSMs with nonlinear connections.\nThis work is a step toward a theory of learning dynamics in deep state space\nmodels.\n","authors":["Jakub SmÃ©kal","Jimmy T. H. Smith","Michael Kleinman","Dan Biderman","Scott W. Linderman"],"pdf_url":"https://arxiv.org/pdf/2407.07279v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.07825v1","updated":"2024-07-10T16:49:23Z","published":"2024-07-10T16:49:23Z","title":"RT-LA-VocE: Real-Time Low-SNR Audio-Visual Speech Enhancement","summary":"  In this paper, we aim to generate clean speech frame by frame from a live\nvideo stream and a noisy audio stream without relying on future inputs. To this\nend, we propose RT-LA-VocE, which completely re-designs every component of\nLA-VocE, a state-of-the-art non-causal audio-visual speech enhancement model,\nto perform causal real-time inference with a 40ms input frame. We do so by\ndevising new visual and audio encoders that rely solely on past frames,\nreplacing the Transformer encoder with the Emformer, and designing a new causal\nneural vocoder C-HiFi-GAN. On the popular AVSpeech dataset, we show that our\nalgorithm achieves state-of-the-art results in all real-time scenarios. More\nimportantly, each component is carefully tuned to minimize the algorithm\nlatency to the theoretical minimum (40ms) while maintaining a low end-to-end\nprocessing latency of 28.15ms per frame, enabling real-time frame-by-frame\nenhancement with minimal delay.\n","authors":["Honglie Chen","Rodrigo Mira","Stavros Petridis","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2407.07825v1.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2211.15597v2","updated":"2024-07-10T16:16:46Z","published":"2022-11-28T17:50:19Z","title":"Lightning Fast Video Anomaly Detection via Adversarial Knowledge\n  Distillation","summary":"  We propose a very fast frame-level model for anomaly detection in video,\nwhich learns to detect anomalies by distilling knowledge from multiple highly\naccurate object-level teacher models. To improve the fidelity of our student,\nwe distill the low-resolution anomaly maps of the teachers by jointly applying\nstandard and adversarial distillation, introducing an adversarial discriminator\nfor each teacher to distinguish between target and generated anomaly maps. We\nconduct experiments on three benchmarks (Avenue, ShanghaiTech, UCSD Ped2),\nshowing that our method is over 7 times faster than the fastest competing\nmethod, and between 28 and 62 times faster than object-centric models, while\nobtaining comparable results to recent methods. Our evaluation also indicates\nthat our model achieves the best trade-off between speed and accuracy, due to\nits previously unheard-of speed of 1480 FPS. In addition, we carry out a\ncomprehensive ablation study to justify our architectural design choices. Our\ncode is freely available at: https://github.com/ristea/fast-aed.\n","authors":["Florinel-Alin Croitoru","Nicolae-Catalin Ristea","Dana Dascalescu","Radu Tudor Ionescu","Fahad Shahbaz Khan","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2211.15597v2.pdf","comment":"Accepted in Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2407.07771v1","updated":"2024-07-10T15:46:32Z","published":"2024-07-10T15:46:32Z","title":"Multi-task Prompt Words Learning for Social Media Content Generation","summary":"  The rapid development of the Internet has profoundly changed human life.\nHumans are increasingly expressing themselves and interacting with others on\nsocial media platforms. However, although artificial intelligence technology\nhas been widely used in many aspects of life, its application in social media\ncontent creation is still blank. To solve this problem, we propose a new prompt\nword generation framework based on multi-modal information fusion, which\ncombines multiple tasks including topic classification, sentiment analysis,\nscene recognition and keyword extraction to generate more comprehensive prompt\nwords. Subsequently, we use a template containing a set of prompt words to\nguide ChatGPT to generate high-quality tweets. Furthermore, in the absence of\neffective and objective evaluation criteria in the field of content generation,\nwe use the ChatGPT tool to evaluate the results generated by the algorithm,\nmaking large-scale evaluation of content generation algorithms possible.\nEvaluation results on extensive content generation demonstrate that our cue\nword generation framework generates higher quality content compared to manual\nmethods and other cueing techniques, while topic classification, sentiment\nanalysis, and scene recognition significantly enhance content clarity and its\nconsistency with the image.\n","authors":["Haochen Xue","Chong Zhang","Chengzhi Liu","Fangyu Wu","Xiaobo Jin"],"pdf_url":"https://arxiv.org/pdf/2407.07771v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.07728v1","updated":"2024-07-10T15:00:08Z","published":"2024-07-10T15:00:08Z","title":"SaMoye: Zero-shot Singing Voice Conversion Based on Feature\n  Disentanglement and Synthesis","summary":"  Singing voice conversion (SVC) aims to convert a singer's voice in a given\nmusic piece to another singer while keeping the original content. We propose an\nend-to-end feature disentanglement-based model, which we named SaMoye, to\nenable zero-shot many-to-many singing voice conversion. SaMoye disentangles the\nfeatures of the singing voice into content features, timbre features, and pitch\nfeatures respectively. The content features are enhanced using a GPT-based\nmodel to perform cross-prediction with the phoneme of the lyrics. SaMoye can\ngenerate the music with converted voice by replacing the timbre features with\nthe target singer. We also establish an unparalleled large-scale dataset to\nguarantee zero-shot performance. The dataset consists of 1500k pure singing\nvocal clips containing at least 10,000 singers.\n","authors":["Zihao Wang","Le Ma","Yan Liu","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.07728v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.07523v1","updated":"2024-07-10T10:22:35Z","published":"2024-07-10T10:22:35Z","title":"SHERL: Synthesizing High Accuracy and Efficient Memory for\n  Resource-Limited Transfer Learning","summary":"  Parameter-efficient transfer learning (PETL) has emerged as a flourishing\nresearch field for adapting large pre-trained models to downstream tasks,\ngreatly reducing trainable parameters while grappling with memory challenges\nduring fine-tuning. To address it, memory-efficient series (METL) avoid\nbackpropagating gradients through the large backbone. However, they compromise\nby exclusively relying on frozen intermediate outputs and limiting the\nexhaustive exploration of prior knowledge from pre-trained models. Moreover,\nthe dependency and redundancy between cross-layer features are frequently\noverlooked, thereby submerging more discriminative representations and causing\nan inherent performance gap (vs. conventional PETL methods). Hence, we propose\nan innovative METL strategy called SHERL for resource-limited scenarios to\ndecouple the entire adaptation into two successive and complementary processes.\nIn the early route, intermediate outputs are consolidated via an\nanti-redundancy operation, enhancing their compatibility for subsequent\ninteractions; thereby in the late route, utilizing minimal late pre-trained\nlayers could alleviate the peak demand on memory overhead and regulate these\nfairly flexible features into more adaptive and powerful representations for\nnew domains. Extensive ablations on vision-and-language and language-only tasks\nshow that SHERL combines the strengths of both parameter and memory-efficient\ntechniques, performing on-par or better across diverse architectures with lower\nmemory during fine-tuning. Our code is publicly available at:\nhttps://github.com/Paranioar/SHERL.\n","authors":["Haiwen Diao","Bo Wan","Xu Jia","Yunzhi Zhuge","Ying Zhang","Huchuan Lu","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2407.07523v1.pdf","comment":"23 pages, 11 figures, Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.07464v1","updated":"2024-07-10T08:40:39Z","published":"2024-07-10T08:40:39Z","title":"Video-to-Audio Generation with Hidden Alignment","summary":"  Generating semantically and temporally aligned audio content in accordance\nwith video input has become a focal point for researchers, particularly\nfollowing the remarkable breakthrough in text-to-video generation. In this\nwork, we aim to offer insights into the video-to-audio generation paradigm,\nfocusing on three crucial aspects: vision encoders, auxiliary embeddings, and\ndata augmentation techniques. Beginning with a foundational model VTA-LDM built\non a simple yet surprisingly effective intuition, we explore various vision\nencoders and auxiliary embeddings through ablation studies. Employing a\ncomprehensive evaluation pipeline that emphasizes generation quality and\nvideo-audio synchronization alignment, we demonstrate that our model exhibits\nstate-of-the-art video-to-audio generation capabilities. Furthermore, we\nprovide critical insights into the impact of different data augmentation\nmethods on enhancing the generation framework's overall capacity. We showcase\npossibilities to advance the challenge of generating synchronized audio from\nsemantic and temporal perspectives. We hope these insights will serve as a\nstepping stone toward developing more realistic and accurate audio-visual\ngeneration models.\n","authors":["Manjie Xu","Chenxing Li","Yong Ren","Rilin Chen","Yu Gu","Wei Liang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2407.07464v1.pdf","comment":"https://sites.google.com/view/vta-ldm"},{"id":"http://arxiv.org/abs/2407.07395v1","updated":"2024-07-10T06:36:45Z","published":"2024-07-10T06:36:45Z","title":"Standard compliant video coding using low complexity, switchable neural\n  wrappers","summary":"  The proliferation of high resolution videos posts great storage and bandwidth\npressure on cloud video services, driving the development of next-generation\nvideo codecs. Despite great progress made in neural video coding, existing\napproaches are still far from economical deployment considering the complexity\nand rate-distortion performance tradeoff. To clear the roadblocks for neural\nvideo coding, in this paper we propose a new framework featuring standard\ncompatibility, high performance, and low decoding complexity. We employ a set\nof jointly optimized neural pre- and post-processors, wrapping a standard video\ncodec, to encode videos at different resolutions. The rate-distorion optimal\ndownsampling ratio is signaled to the decoder at the per-sequence level for\neach target rate. We design a low complexity neural post-processor architecture\nthat can handle different upsampling ratios. The change of resolution exploits\nthe spatial redundancy in high-resolution videos, while the neural wrapper\nfurther achieves rate-distortion performance improvement through end-to-end\noptimization with a codec proxy. Our light-weight post-processor architecture\nhas a complexity of 516 MACs / pixel, and achieves 9.3% BD-Rate reduction over\nVVC on the UVG dataset, and 6.4% on AOM CTC Class A1. Our approach has the\npotential to further advance the performance of the latest video coding\nstandards using neural processing with minimal added complexity.\n","authors":["Yueyu Hu","Chenhao Zhang","Onur G. Guleryuz","Debargha Mukherjee","Yao Wang"],"pdf_url":"https://arxiv.org/pdf/2407.07395v1.pdf","comment":"Accepted by IEEE ICIP 2024"},{"id":"http://arxiv.org/abs/2407.07325v1","updated":"2024-07-10T02:43:18Z","published":"2024-07-10T02:43:18Z","title":"HiLight: Technical Report on the Motern AI Video Language Model","summary":"  This technical report presents the implementation of a state-of-the-art video\nencoder for video-text modal alignment and a video conversation framework\ncalled HiLight, which features dual visual towers. The work is divided into two\nmain parts: 1.alignment of video and text modalities; 2.convenient and\nefficient way to interact with users. Our goal is to address the task of video\ncomprehension in the context of billiards. The report includes a discussion of\nthe concepts and the final solution developed during the task's implementation.\n","authors":["Zhiting Wang","Qiangong Zhou","Kangjie Yang","Zongyang Liu. Xin Mao"],"pdf_url":"https://arxiv.org/pdf/2407.07325v1.pdf","comment":null}]},"2024-07-09T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.06730v2","updated":"2024-07-09T23:53:06Z","published":"2024-01-12T18:03:30Z","title":"Relying on the Unreliable: The Impact of Language Models' Reluctance to\n  Express Uncertainty","summary":"  As natural language becomes the default interface for human-AI interaction,\nthere is a need for LMs to appropriately communicate uncertainties in\ndownstream applications. In this work, we investigate how LMs incorporate\nconfidence in responses via natural language and how downstream users behave in\nresponse to LM-articulated uncertainties. We examine publicly deployed models\nand find that LMs are reluctant to express uncertainties when answering\nquestions even when they produce incorrect responses. LMs can be explicitly\nprompted to express confidences, but tend to be overconfident, resulting in\nhigh error rates (an average of 47%) among confident responses. We test the\nrisks of LM overconfidence by conducting human experiments and show that users\nrely heavily on LM generations, whether or not they are marked by certainty.\nLastly, we investigate the preference-annotated datasets used in post training\nalignment and find that humans are biased against texts with uncertainty. Our\nwork highlights new safety harms facing human-LM interactions and proposes\ndesign recommendations and mitigating strategies moving forward.\n","authors":["Kaitlyn Zhou","Jena D. Hwang","Xiang Ren","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2401.06730v2.pdf","comment":"ACL 2024 (Camera Ready)"},{"id":"http://arxiv.org/abs/2404.19721v3","updated":"2024-07-09T23:45:27Z","published":"2024-04-30T17:11:54Z","title":"PANGeA: Procedural Artificial Narrative using Generative AI for\n  Turn-Based Video Games","summary":"  This research introduces Procedural Artificial Narrative using Generative AI\n(PANGeA), a structured approach for leveraging large language models (LLMs),\nguided by a game designer's high-level criteria, to generate narrative content\nfor turn-based role-playing video games (RPGs). Distinct from prior\napplications of LLMs used for video game design, PANGeA innovates by not only\ngenerating game level data (which includes, but is not limited to, setting, key\nitems, and non-playable characters (NPCs)), but by also fostering dynamic,\nfree-form interactions between the player and the environment that align with\nthe procedural game narrative. The NPCs generated by PANGeA are\npersonality-biased and express traits from the Big 5 Personality Model in their\ngenerated responses. PANGeA addresses challenges behind ingesting free-form\ntext input, which can prompt LLM responses beyond the scope of the game\nnarrative. A novel validation system that uses the LLM's intelligence evaluates\ntext input and aligns generated responses with the unfolding narrative. Making\nthese interactions possible, PANGeA is supported by a server that hosts a\ncustom memory system that supplies context for augmenting generated responses\nthus aligning them with the procedural narrative. For its broad application,\nthe server has a REST interface enabling any game engine to integrate directly\nwith PANGeA, as well as an LLM interface adaptable with local or private LLMs.\nPANGeA's ability to foster dynamic narrative generation by aligning responses\nwith the procedural narrative is demonstrated through an empirical study and\nablation test of two versions of a demo game. These are, a custom,\nbrowser-based GPT and a Unity demo. As the results show, PANGeA holds potential\nto assist game designers in using LLMs to generate narrative-consistent content\neven when provided varied and unpredictable, free-form text input.\n","authors":["Steph Buongiorno","Lawrence Jake Klinkert","Tanishq Chawla","Zixin Zhuang","Corey Clark"],"pdf_url":"https://arxiv.org/pdf/2404.19721v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14321v2","updated":"2024-07-09T23:44:46Z","published":"2023-05-23T17:53:30Z","title":"ConGraT: Self-Supervised Contrastive Pretraining for Joint Graph and\n  Text Embeddings","summary":"  Learning on text-attributed graphs (TAGs), in which nodes are associated with\none or more texts, has been the subject of much recent work. However, most\napproaches tend to make strong assumptions about the downstream task of\ninterest, are reliant on hand-labeled data, or fail to equally balance the\nimportance of both text and graph representations. In this work, we propose\nContrastive Graph-Text pretraining (ConGraT), a general, self-supervised\napproach for jointly learning separate representations of texts and nodes in a\nTAG. Our method trains a language model (LM) and a graph neural network (GNN)\nto align their representations in a common latent space using a batch-wise\ncontrastive learning objective inspired by CLIP. We further propose an\nextension to the CLIP objective that leverages graph structure to incorporate\ninformation about inter-node similarity. Extensive experiments demonstrate that\nConGraT outperforms baselines on various downstream tasks, including node and\ntext category classification, link prediction, and language modeling. Finally,\nwe present an application of our method to community detection in social\ngraphs, which enables finding more textually grounded communities, rather than\npurely graph-based ones. Code and certain datasets are available at\nhttps://github.com/wwbrannon/congrat.\n","authors":["William Brannon","Wonjune Kang","Suyash Fulay","Hang Jiang","Brandon Roy","Deb Roy","Jad Kabbara"],"pdf_url":"https://arxiv.org/pdf/2305.14321v2.pdf","comment":"New visualizations, added references, and an application to community\n  detection. To appear at the TextGraphs workshop @ ACL 2024. 21 pages, 5\n  figures, 13 tables"},{"id":"http://arxiv.org/abs/2407.07275v1","updated":"2024-07-09T23:39:37Z","published":"2024-07-09T23:39:37Z","title":"Remastering Divide and Remaster: A Cinematic Audio Source Separation\n  Dataset with Multilingual Support","summary":"  Cinematic audio source separation (CASS) is a relatively new subtask of audio\nsource separation, concerned with the separation of a mixture into the\ndialogue, music, and effects stems. To date, only one publicly available\ndataset exists for CASS, that is, the Divide and Remaster (DnR) dataset, which\nis currently at version 2. While DnR v2 has been an incredibly useful resource\nfor CASS, several areas of improvement have been identified, particularly\nthrough its use in the 2023 Sound Demixing Challenge. In this work, we develop\nversion 3 of the DnR dataset, addressing issues relating to vocal content in\nnon-dialogue stems, loudness distributions, mastering process, and linguistic\ndiversity. In particular, the dialogue stem of DnR v3 includes speech content\nfrom more than 30 languages from multiple families including but not limited to\nthe Germanic, Romance, Indo-Aryan, Dravidian, Malayo-Polynesian, and Bantu\nfamilies. Benchmark results using the Bandit model indicated that training on\nmultilingual data yields significant generalizability to the model even in\nlanguages with low data availability. Even in languages with high data\navailability, the multilingual model often performs on par or better than\ndedicated models trained on monolingual CASS datasets.\n","authors":["Karn N. Watcharasupat","Chih-Wei Wu","Iroro Orife"],"pdf_url":"https://arxiv.org/pdf/2407.07275v1.pdf","comment":"Submitted to the 5th IEEE International Symposium on the Internet of\n  Sounds"},{"id":"http://arxiv.org/abs/2407.07263v1","updated":"2024-07-09T22:37:59Z","published":"2024-07-09T22:37:59Z","title":"Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language\n  Models","summary":"  As language models have scaled both their number of parameters and\npretraining dataset sizes, the computational cost for pretraining has become\nintractable except for the most well-resourced teams. This increasing cost\nmakes it ever more important to be able to reuse a model after it has completed\npretraining; allowing for a model's abilities to further improve without\nneeding to train from scratch. In this work, we detail a set of guidelines that\ncover how to design efficacious data distributions and learning rate schedules\nfor continued pretraining of language models. When applying these findings\nwithin a continued pretraining run on top of a well-trained 15B parameter\nmodel, we show an improvement of 9\\% in average model accuracy compared to the\nbaseline of continued training on the pretraining set. The resulting recipe\nprovides a practical starting point with which to begin developing language\nmodels through reuse rather than retraining.\n","authors":["Jupinder Parmar","Sanjev Satheesh","Mostofa Patwary","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2407.07263v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2407.07258v1","updated":"2024-07-09T22:26:42Z","published":"2024-07-09T22:26:42Z","title":"Identification of emotions on Twitter during the 2022 electoral process\n  in Colombia","summary":"  The study of Twitter as a means for analyzing social phenomena has gained\ninterest in recent years due to the availability of large amounts of data in a\nrelatively spontaneous environment. Within opinion-mining tasks, emotion\ndetection is specially relevant, as it allows for the identification of\npeople's subjective responses to different social events in a more granular way\nthan traditional sentiment analysis based on polarity. In the particular case\nof political events, the analysis of emotions in social networks can provide\nvaluable information on the perception of candidates, proposals, and other\nimportant aspects of the public debate. In spite of this importance, there are\nfew studies on emotion detection in Spanish and, to the best of our knowledge,\nfew resources are public for opinion mining in Colombian Spanish, highlighting\nthe need for generating resources addressing the specific cultural\ncharacteristics of this variety. In this work, we present a small corpus of\ntweets in Spanish related to the 2022 Colombian presidential elections,\nmanually labeled with emotions using a fine-grained taxonomy. We perform\nclassification experiments using supervised state-of-the-art models (BERT\nmodels) and compare them with GPT-3.5 in few-shot learning settings. We make\nour dataset and code publicly available for research purposes.\n","authors":["Juan Jose Iguaran Fernandez","Juan Manuel Perez","German Rosati"],"pdf_url":"https://arxiv.org/pdf/2407.07258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05219v2","updated":"2024-07-09T21:51:09Z","published":"2024-07-07T00:43:05Z","title":"Flood of Techniques and Drought of Theories: Emotion Mining in Disasters","summary":"  Emotion mining has become a crucial tool for understanding human emotions\nduring disasters, leveraging the extensive data generated on social media\nplatforms. This paper aims to summarize existing research on emotion mining\nwithin disaster contexts, highlighting both significant discoveries and\npersistent issues. On the one hand, emotion mining techniques have achieved\nacceptable accuracy enabling applications such as rapid damage assessment and\nmental health surveillance. On the other hand, with many studies adopting\ndata-driven approaches, several methodological issues remain. These include\narbitrary emotion classification, ignoring biases inherent in data collection\nfrom social media, such as the overrepresentation of individuals from higher\nsocioeconomic status on Twitter, and the lack of application of theoretical\nframeworks like cross-cultural comparisons. These problems can be summarized as\na notable lack of theory-driven research and ignoring insights from social and\nbehavioral sciences. This paper underscores the need for interdisciplinary\ncollaboration between computer scientists and social scientists to develop more\nrobust and theoretically grounded approaches in emotion mining. By addressing\nthese gaps, we aim to enhance the effectiveness and reliability of emotion\nmining methodologies, ultimately contributing to improved disaster\npreparedness, response, and recovery.\n  Keywords: emotion mining, sentiment analysis, natural disasters, psychology,\ntechnological disasters\n","authors":["Soheil Shapouri","Saber Soleymani","Saed Rezayi"],"pdf_url":"https://arxiv.org/pdf/2407.05219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14864v3","updated":"2024-07-09T21:09:38Z","published":"2023-05-24T08:18:35Z","title":"Just CHOP: Embarrassingly Simple LLM Compression","summary":"  Large language models (LLMs) enable unparalleled few- and zero-shot reasoning\ncapabilities but at a high computational footprint. A growing assortment of\nmethods for compression promises to reduce the computational burden of LLMs in\ndeployment, but so far, only quantization approaches have been demonstrated to\nbe effective for LLM compression while maintaining zero-shot performance. A\ncritical step in the compression process, the pretrain-then-finetune paradigm,\nhas largely been overlooked when adapting existing pruning strategies to LLMs\nor proposing new ones. In this work, we show that embarrassingly simple layer\npruning coupled with an extended language model pretraining as the finetuning\nphase produces state-of-the-art results against structured and even\nsemi-structured compression of models at a 7B scale while being more inference\nefficient. We call this method LayerChop, where we deterministically remove\nlayers from a model followed by task-agnostic finetuning of the remaining\nweights by continued self-supervised pretraining. At this scale, we also show\nhow distillation, which has been super effective in task-agnostic compression\nof smaller BERT-style models, becomes inefficient against our simple pruning\ntechnique.\n","authors":["Ananya Harsh Jha","Tom Sherborne","Evan Pete Walsh","Dirk Groeneveld","Emma Strubell","Iz Beltagy"],"pdf_url":"https://arxiv.org/pdf/2305.14864v3.pdf","comment":"13 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2407.07225v1","updated":"2024-07-09T20:44:40Z","published":"2024-07-09T20:44:40Z","title":"ConvNLP: Image-based AI Text Detection","summary":"  The potentials of Generative-AI technologies like Large Language models\n(LLMs) to revolutionize education are undermined by ethical considerations\naround their misuse which worsens the problem of academic dishonesty. LLMs like\nGPT-4 and Llama 2 are becoming increasingly powerful in generating\nsophisticated content and answering questions, from writing academic essays to\nsolving complex math problems. Students are relying on these LLMs to complete\ntheir assignments and thus compromising academic integrity. Solutions to detect\nLLM-generated text are compute-intensive and often lack generalization. This\npaper presents a novel approach for detecting LLM-generated AI-text using a\nvisual representation of word embedding. We have formulated a novel\nConvolutional Neural Network called ZigZag ResNet, as well as a scheduler for\nimproving generalization, named ZigZag Scheduler. Through extensive evaluation\nusing datasets of text generated by six different state-of-the-art LLMs, our\nmodel demonstrates strong intra-domain and inter-domain generalization\ncapabilities. Our best model detects AI-generated text with an impressive\naverage detection rate (over inter- and intra-domain test data) of 88.35%.\nThrough an exhaustive ablation study, our ZigZag ResNet and ZigZag Scheduler\nprovide a performance improvement of nearly 4% over the vanilla ResNet. The\nend-to-end inference latency of our model is below 2.5ms per sentence. Our\nsolution offers a lightweight, computationally efficient, and faster\nalternative to existing tools for AI-generated text detection, with better\ngeneralization performance. It can help academic institutions in their fight\nagainst the misuse of LLMs in academic settings. Through this work, we aim to\ncontribute to safeguarding the principles of academic integrity and ensuring\nthe trustworthiness of student work in the era of advanced LLMs.\n","authors":["Suriya Prakash Jambunathan","Ashwath Shankarnarayan","Parijat Dube"],"pdf_url":"https://arxiv.org/pdf/2407.07225v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.07957v3","updated":"2024-07-09T19:28:30Z","published":"2023-10-12T00:50:24Z","title":"A New Approach Towards Autoformalization","summary":"  Verifying mathematical proofs is difficult, but can be automated with the\nassistance of a computer. Autoformalization is the task of automatically\ntranslating natural language mathematics into a formal language that can be\nverified by a program. This is a challenging task, and especially for\nhigher-level mathematics found in research papers. Research paper mathematics\nrequires large amounts of background and context. In this paper, we propose an\navenue towards tackling autoformalization for research-level mathematics, by\nbreaking the task into easier and more approachable subtasks: unlinked\nformalization (formalization with unlinked definitions and theorems), entity\nlinking (linking to the proper theorems and definitions), and finally adjusting\ntypes so it passes the type checker. In addition, we present arXiv2Formal, a\nbenchmark dataset for unlinked formalization consisting of 50 theorems\nformalized for the Lean theorem prover sampled from papers on arXiv.org. We\nwelcome any contributions from the community to future versions of this\ndataset.\n","authors":["Nilay Patel","Rahul Saha","Jeffrey Flanigan"],"pdf_url":"https://arxiv.org/pdf/2310.07957v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07094v1","updated":"2024-07-09T17:59:56Z","published":"2024-07-09T17:59:56Z","title":"AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning","summary":"  The pervasive deployment of Large Language Models-LLMs in various sectors\noften neglects the nuanced requirements of individuals and small organizations,\nwho benefit more from models precisely tailored to their specific business\ncontexts rather than those with broadly superior general capabilities. This\nwork introduces \\textbf{AnyTaskTune}, a novel fine-tuning methodology coined as\n\\textbf{Task-Fine-Tune}, specifically developed to elevate model performance on\na diverse array of domain-specific tasks. This method involves a meticulous\nprocess to identify and define targeted sub-tasks within a domain, followed by\nthe creation of specialized enhancement datasets for fine-tuning, thereby\noptimizing task-specific model performance. We conducted comprehensive\nfine-tuning experiments not only in the legal domain for tasks such as keyword\nextraction and sentence prediction but across over twenty different sub-tasks\nderived from the domains of finance, healthcare, law, psychology, consumer\nservices, and human resources. To substantiate our approach and facilitate\ncommunity engagement, we will open-source these bilingual task datasets. Our\nfindings demonstrate that models fine-tuned using the \\textbf{Task-Fine-Tune}\nmethodology not only achieve superior performance on these specific tasks but\nalso significantly outperform models with higher general capabilities in their\nrespective domains. Our work is publicly available at\n\\url{https://github.com/PandaVT/DataTager}.\n","authors":["Jiaxi Cui","Wentao Zhang","Jing Tang","Xudong Tong","Zhenwei Zhang"," Amie","Jing Wen","Rongsheng Wang","Pengfei Wu"],"pdf_url":"https://arxiv.org/pdf/2407.07094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07093v1","updated":"2024-07-09T17:59:48Z","published":"2024-07-09T17:59:48Z","title":"FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive\n  Distillation","summary":"  This work presents a Fully BInarized Large Language Model (FBI-LLM),\ndemonstrating for the first time how to train a large-scale binary language\nmodel from scratch (not the partial binary or ternary LLM like BitNet b1.58) to\nmatch the performance of its full-precision counterparts (e.g., FP16 or BF16)\nin transformer-based LLMs. It achieves this by employing an autoregressive\ndistillation (AD) loss with maintaining equivalent model dimensions (130M,\n1.3B, 7B) and training data volume as regular LLM pretraining, while delivering\ncompetitive results in terms of perplexity and task-specific effectiveness.\nIntriguingly, by analyzing the training trajectory, we find that the pretrained\nweight is not necessary for training binarized LLMs from scratch. This research\nencourages a new computational framework and may facilitate the future design\nof specialized hardware tailored for fully 1-bit LLMs. We make all models,\ncode, and training dataset fully accessible and transparent to support further\nresearch (Code: https://github.com/LiqunMa/FBI-LLM. Model:\nhttps://huggingface.co/LiqunMa/).\n","authors":["Liqun Ma","Mingjie Sun","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2407.07093v1.pdf","comment":"Github at https://github.com/LiqunMa/FBI-LLM"},{"id":"http://arxiv.org/abs/2407.07087v1","updated":"2024-07-09T17:58:18Z","published":"2024-07-09T17:58:18Z","title":"CopyBench: Measuring Literal and Non-Literal Reproduction of\n  Copyright-Protected Text in Language Model Generation","summary":"  Evaluating the degree of reproduction of copyright-protected content by\nlanguage models (LMs) is of significant interest to the AI and legal\ncommunities. Although both literal and non-literal similarities are considered\nby courts when assessing the degree of reproduction, prior research has focused\nonly on literal similarities. To bridge this gap, we introduce CopyBench, a\nbenchmark designed to measure both literal and non-literal copying in LM\ngenerations. Using copyrighted fiction books as text sources, we provide\nautomatic evaluation protocols to assess literal and non-literal copying,\nbalanced against the model utility in terms of the ability to recall facts from\nthe copyrighted works and generate fluent completions. We find that, although\nliteral copying is relatively rare, two types of non-literal copying -- event\ncopying and character copying -- occur even in models as small as 7B\nparameters. Larger models demonstrate significantly more copying, with literal\ncopying rates increasing from 0.2% to 10.5% and non-literal copying from 2.3%\nto 6.9% when comparing Llama3-8B and 70B models, respectively. We further\nevaluate the effectiveness of current strategies for mitigating copying and\nshow that (1) training-time alignment can reduce literal copying but may\nincrease non-literal copying, and (2) current inference-time mitigation methods\nprimarily reduce literal but not non-literal copying.\n","authors":["Tong Chen","Akari Asai","Niloofar Mireshghallah","Sewon Min","James Grimmelmann","Yejin Choi","Hannaneh Hajishirzi","Luke Zettlemoyer","Pang Wei Koh"],"pdf_url":"https://arxiv.org/pdf/2407.07087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07080v1","updated":"2024-07-09T17:51:37Z","published":"2024-07-09T17:51:37Z","title":"Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary\n  and Instruction Capabilities","summary":"  Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.\n","authors":["Shaltiel Shmidman","Avi Shmidman","Amir DN Cohen","Moshe Koppel"],"pdf_url":"https://arxiv.org/pdf/2407.07080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07071v1","updated":"2024-07-09T17:44:34Z","published":"2024-07-09T17:44:34Z","title":"Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\n  Large Language Models Using Only Attention Maps","summary":"  When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.\n","authors":["Yung-Sung Chuang","Linlu Qiu","Cheng-Yu Hsieh","Ranjay Krishna","Yoon Kim","James Glass"],"pdf_url":"https://arxiv.org/pdf/2407.07071v1.pdf","comment":"The source code is available at\n  https://github.com/voidism/Lookback-Lens"},{"id":"http://arxiv.org/abs/2406.05806v2","updated":"2024-07-09T17:14:27Z","published":"2024-06-09T14:44:59Z","title":"Do Prompts Really Prompt? Exploring the Prompt Understanding Capability\n  of Whisper","summary":"  This research explores how the information of prompts interacts with the\nhigh-performing speech recognition model, Whisper. We compare its performances\nwhen prompted by prompts with correct information and those corrupted with\nincorrect information. Our results unexpectedly show that Whisper may not\nunderstand the textual prompts in a human-expected way. Additionally, we find\nthat performance improvement is not guaranteed even with stronger adherence to\nthe topic information in textual prompts. It is also noted that English prompts\ngenerally outperform Mandarin ones on datasets of both languages, likely due to\ndifferences in training data distributions for these languages despite the\nmismatch with pre-training scenarios. Conversely, we discover that Whisper\nexhibits awareness of misleading information in language tokens by ignoring\nincorrect language tokens and focusing on the correct ones. In sum, We raise\ninsightful questions about Whisper's prompt understanding and reveal its\ncounter-intuitive behaviors. We encourage further studies.\n","authors":["Chih-Kai Yang","Kuan-Po Huang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2406.05806v2.pdf","comment":"In progress"},{"id":"http://arxiv.org/abs/2407.07038v1","updated":"2024-07-09T17:00:39Z","published":"2024-07-09T17:00:39Z","title":"Decoding Climate Disagreement: A Graph Neural Network-Based Approach to\n  Understanding Social Media Dynamics","summary":"  This work introduces the ClimateSent-GAT Model, an innovative method that\nintegrates Graph Attention Networks (GATs) with techniques from natural\nlanguage processing to accurately identify and predict disagreements within\nReddit comment-reply pairs. Our model classifies disagreements into three\ncategories: agree, disagree, and neutral. Leveraging the inherent graph\nstructure of Reddit comment-reply pairs, the model significantly outperforms\nexisting benchmarks by capturing complex interaction patterns and sentiment\ndynamics. This research advances graph-based NLP methodologies and provides\nactionable insights for policymakers and educators in climate science\ncommunication.\n","authors":["Ruiran Su","Janet B. Pierrehumbert"],"pdf_url":"https://arxiv.org/pdf/2407.07038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07035v1","updated":"2024-07-09T16:53:36Z","published":"2024-07-09T16:53:36Z","title":"Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era\n  of Foundation Models","summary":"  Vision-and-Language Navigation (VLN) has gained increasing attention over\nrecent years and many approaches have emerged to advance their development. The\nremarkable achievements of foundation models have shaped the challenges and\nproposed methods for VLN research. In this survey, we provide a top-down review\nthat adopts a principled framework for embodied planning and reasoning, and\nemphasizes the current methods and future opportunities leveraging foundation\nmodels to address VLN challenges. We hope our in-depth discussions could\nprovide valuable resources and insights: on one hand, to milestone the progress\nand explore opportunities and potential roles for foundation models in this\nfield, and on the other, to organize different challenges and solutions in VLN\nto foundation model researchers.\n","authors":["Yue Zhang","Ziqiao Ma","Jialu Li","Yanyuan Qiao","Zun Wang","Joyce Chai","Qi Wu","Mohit Bansal","Parisa Kordjamshidi"],"pdf_url":"https://arxiv.org/pdf/2407.07035v1.pdf","comment":"Authors contributed equally to this work, and supervisors contributed\n  equal advising to this work"},{"id":"http://arxiv.org/abs/2407.07026v1","updated":"2024-07-09T16:46:58Z","published":"2024-07-09T16:46:58Z","title":"Resolving Sentiment Discrepancy for Multimodal Sentiment Detection via\n  Semantics Completion and Decomposition","summary":"  With the proliferation of social media posts in recent years, the need to\ndetect sentiments in multimodal (image-text) content has grown rapidly. Since\nposts are user-generated, the image and text from the same post can express\ndifferent or even contradictory sentiments, leading to potential\n\\textbf{sentiment discrepancy}. However, existing works mainly adopt a\nsingle-branch fusion structure that primarily captures the consistent sentiment\nbetween image and text. The ignorance or implicit modeling of discrepant\nsentiment results in compromised unimodal encoding and limited performances. In\nthis paper, we propose a semantics Completion and Decomposition (CoDe) network\nto resolve the above issue. In the semantics completion module, we complement\nimage and text representations with the semantics of the OCR text embedded in\nthe image, helping bridge the sentiment gap. In the semantics decomposition\nmodule, we decompose image and text representations with exclusive projection\nand contrastive learning, thereby explicitly capturing the discrepant sentiment\nbetween modalities. Finally, we fuse image and text representations by\ncross-attention and combine them with the learned discrepant sentiment for\nfinal classification. Extensive experiments conducted on four multimodal\nsentiment datasets demonstrate the superiority of CoDe against SOTA methods.\n","authors":["Daiqing Wu","Dongbao Yang","Huawen Shen","Can Ma","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.07026v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.07019v1","updated":"2024-07-09T16:40:44Z","published":"2024-07-09T16:40:44Z","title":"Using Large Language Models for Generating Smart Contracts for Health\n  Insurance from Textual Policies","summary":"  We explore using Large Language Models (LLMs) to generate application code\nthat automates health insurance processes from text-based policies. We target\nblockchain-based smart contracts as they offer immutability, verifiability,\nscalability, and a trustless setting: any number of parties can use the smart\ncontracts, and they need not have previously established trust relationships\nwith each other. Our methodology generates outputs at increasing levels of\ntechnical detail: (1) textual summaries, (2) declarative decision logic, and\n(3) smart contract code with unit tests. We ascertain LLMs are good at the task\n(1), and the structured output is useful to validate tasks (2) and (3).\nDeclarative languages (task 2) are often used to formalize healthcare policies,\nbut their execution on blockchain is non-trivial. Hence, task (3) attempts to\ndirectly automate the process using smart contracts. To assess the LLM output,\nwe propose completeness, soundness, clarity, syntax, and functioning code as\nmetrics. Our evaluation employs three health insurance policies (scenarios)\nwith increasing difficulty from Medicare's official booklet. Our evaluation\nuses GPT-3.5 Turbo, GPT-3.5 Turbo 16K, GPT-4, GPT-4 Turbo and CodeLLaMA. Our\nfindings confirm that LLMs perform quite well in generating textual summaries.\nAlthough outputs from tasks (2)-(3) are useful starting points, they require\nhuman oversight: in multiple cases, even \"runnable\" code will not yield sound\nresults; the popularity of the target language affects the output quality; and\nmore complex scenarios still seem a bridge too far. Nevertheless, our\nexperiments demonstrate the promise of LLMs for translating textual process\ndescriptions into smart contracts.\n","authors":["Inwon Kang","William Van Woensel","Oshani Seneviratne"],"pdf_url":"https://arxiv.org/pdf/2407.07019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07018v1","updated":"2024-07-09T16:38:48Z","published":"2024-07-09T16:38:48Z","title":"End-To-End Causal Effect Estimation from Unstructured Natural Language\n  Data","summary":"  Knowing the effect of an intervention is critical for human decision-making,\nbut current approaches for causal effect estimation rely on manual data\ncollection and structuring, regardless of the causal assumptions. This\nincreases both the cost and time-to-completion for studies. We show how large,\ndiverse observational text data can be mined with large language models (LLMs)\nto produce inexpensive causal effect estimates under appropriate causal\nassumptions. We introduce NATURAL, a novel family of causal effect estimators\nbuilt with LLMs that operate over datasets of unstructured text. Our estimators\nuse LLM conditional distributions (over variables of interest, given the text\ndata) to assist in the computation of classical estimators of causal effect. We\novercome a number of technical challenges to realize this idea, such as\nautomating data curation and using LLMs to impute missing information. We\nprepare six (two synthetic and four real) observational datasets, paired with\ncorresponding ground truth in the form of randomized trials, which we used to\nsystematically evaluate each step of our pipeline. NATURAL estimators\ndemonstrate remarkable performance, yielding causal effect estimates that fall\nwithin 3 percentage points of their ground truth counterparts, including on\nreal-world Phase 3/4 clinical trials. Our results suggest that unstructured\ntext data is a rich source of causal effect information, and NATURAL is a first\nstep towards an automated pipeline to tap this resource.\n","authors":["Nikita Dhawan","Leonardo Cotta","Karen Ullrich","Rahul G. Krishnan","Chris J. Maddison"],"pdf_url":"https://arxiv.org/pdf/2407.07018v1.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.07011v1","updated":"2024-07-09T16:29:21Z","published":"2024-07-09T16:29:21Z","title":"Induction Heads as an Essential Mechanism for Pattern Matching in\n  In-context Learning","summary":"  Large language models (LLMs) have shown a remarkable ability to learn and\nperform complex tasks through in-context learning (ICL). However, a\ncomprehensive understanding of its internal mechanisms is still lacking. This\npaper explores the role of induction heads in a few-shot ICL setting. We\nanalyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract\npattern recognition and NLP tasks. Our results show that even a minimal\nablation of induction heads leads to ICL performance decreases of up to ~32%\nfor abstract pattern recognition tasks, bringing the performance close to\nrandom. For NLP tasks, this ablation substantially decreases the model's\nability to benefit from examples, bringing few-shot ICL performance close to\nthat of zero-shot prompts. We further use attention knockout to disable\nspecific induction patterns, and present fine-grained evidence for the role\nthat the induction mechanism plays in ICL.\n","authors":["J. Crosbie","E. Shutova"],"pdf_url":"https://arxiv.org/pdf/2407.07011v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.06023v2","updated":"2024-07-09T16:29:11Z","published":"2024-07-08T15:17:46Z","title":"Distilling System 2 into System 1","summary":"  Large language models (LLMs) can spend extra compute during inference to\ngenerate intermediate thoughts, which helps to produce better final responses.\nSince Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have\nbeen proposed such as Rephrase and Respond (Deng et al., 2023a), System 2\nAttention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,\n2023). In this work we investigate self-supervised methods to ``compile''\n(distill) higher quality outputs from System 2 techniques back into LLM\ngenerations without intermediate reasoning token sequences, as this reasoning\nhas been distilled into System 1. We show that several such techniques can be\nsuccessfully distilled, resulting in improved results compared to the original\nSystem 1 performance, and with less inference cost than System 2. We posit that\nsuch System 2 distillation will be an important feature of future continually\nlearning AI systems, enabling them to focus System 2 capabilities on the\nreasoning tasks that they cannot yet do well.\n","authors":["Ping Yu","Jing Xu","Jason Weston","Ilia Kulikov"],"pdf_url":"https://arxiv.org/pdf/2407.06023v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07004v1","updated":"2024-07-09T16:17:16Z","published":"2024-07-09T16:17:16Z","title":"Empirical analysis of Biding Precedent efficiency in the Brazilian\n  Supreme Court via Similar Case Retrieval","summary":"  Binding precedents (S\\'umulas Vinculantes) constitute a juridical instrument\nunique to the Brazilian legal system and whose objectives include the\nprotection of the Federal Supreme Court against repetitive demands. Studies of\nthe effectiveness of these instruments in decreasing the Court's exposure to\nsimilar cases, however, indicate that they tend to fail in such a direction,\nwith some of the binding precedents seemingly creating new demands. We\nempirically assess the legal impact of five binding precedents, 11, 14, 17, 26\nand 37, at the highest court level through their effects on the legal subjects\nthey address. This analysis is only possible through the comparison of the\nCourt's ruling about the precedents' themes before they are created, which\nmeans that these decisions should be detected through techniques of Similar\nCase Retrieval. The contributions of this article are therefore twofold: on the\nmathematical side, we compare the uses of different methods of Natural Language\nProcessing -- TF-IDF, LSTM, BERT, and regex -- for Similar Case Retrieval,\nwhereas on the legal side, we contrast the inefficiency of these binding\nprecedents with a set of hypotheses that may justify their repeated usage. We\nobserve that the deep learning models performed significantly worse in the\nspecific Similar Case Retrieval task and that the reasons for binding\nprecedents to fail in responding to repetitive demand are heterogeneous and\ncase-dependent, making it impossible to single out a specific cause.\n","authors":["RaphaÃ«l Tinarrage","Henrique Ennes","Lucas E. Resck","Lucas T. Gomes","Jean R. Ponciano","Jorge Poco"],"pdf_url":"https://arxiv.org/pdf/2407.07004v1.pdf","comment":"54 pages, 22 figures"},{"id":"http://arxiv.org/abs/2407.07000v1","updated":"2024-07-09T16:13:26Z","published":"2024-07-09T16:13:26Z","title":"Metron: Holistic Performance Evaluation Framework for LLM Inference\n  Systems","summary":"  Serving large language models (LLMs) in production can incur substantial\ncosts, which has prompted recent advances in inference system optimizations.\nToday, these systems are evaluated against conventional latency and throughput\nmetrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics\nfail to fully capture the nuances of LLM inference, leading to an incomplete\nassessment of user-facing performance crucial for real-time applications such\nas chat and translation. In this paper, we first identify the pitfalls of\ncurrent performance metrics in evaluating LLM inference systems. We then\npropose Metron, a comprehensive performance evaluation framework that includes\nfluidity-index -- a novel metric designed to reflect the intricacies of the LLM\ninference process and its impact on real-time user experience. Finally, we\nevaluate various existing open-source platforms and model-as-a-service\nofferings using Metron, discussing their strengths and weaknesses. Metron is\navailable at https://github.com/project-metron/metron.\n","authors":["Amey Agrawal","Anmol Agarwal","Nitin Kedia","Jayashree Mohan","Souvik Kundu","Nipun Kwatra","Ramachandran Ramjee","Alexey Tumanov"],"pdf_url":"https://arxiv.org/pdf/2407.07000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06992v1","updated":"2024-07-09T16:07:01Z","published":"2024-07-09T16:07:01Z","title":"Robust Neural Information Retrieval: An Adversarial and\n  Out-of-distribution Perspective","summary":"  Recent advances in neural information retrieval (IR) models have\nsignificantly enhanced their effectiveness over various IR tasks. The\nrobustness of these models, essential for ensuring their reliability in\npractice, has also garnered significant attention. With a wide array of\nresearch on robust IR being proposed, we believe it is the opportune moment to\nconsolidate the current status, glean insights from existing methodologies, and\nlay the groundwork for future development. We view the robustness of IR to be a\nmultifaceted concept, emphasizing its necessity against adversarial attacks,\nout-of-distribution (OOD) scenarios and performance variance. With a focus on\nadversarial and OOD robustness, we dissect robustness solutions for dense\nretrieval models (DRMs) and neural ranking models (NRMs), respectively,\nrecognizing them as pivotal components of the neural IR pipeline. We provide an\nin-depth discussion of existing methods, datasets, and evaluation metrics,\nshedding light on challenges and future directions in the era of large language\nmodels. To the best of our knowledge, this is the first comprehensive survey on\nthe robustness of neural IR models, and we will also be giving our first\ntutorial presentation at SIGIR 2024\n\\url{https://sigir2024-robust-information-retrieval.github.io}. Along with the\norganization of existing work, we introduce a Benchmark for robust IR (BestIR),\na heterogeneous evaluation benchmark for robust neural information retrieval,\nwhich is publicly available at \\url{https://github.com/Davion-Liu/BestIR}. We\nhope that this study provides useful clues for future research on the\nrobustness of IR models and helps to develop trustworthy search engines\n\\url{https://github.com/Davion-Liu/Awesome-Robustness-in-Information-Retrieval}.\n","authors":["Yu-An Liu","Ruqing Zhang","Jiafeng Guo","Maarten de Rijke","Yixing Fan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.06992v1.pdf","comment":"Survey paper"},{"id":"http://arxiv.org/abs/2407.06990v1","updated":"2024-07-09T16:04:21Z","published":"2024-07-09T16:04:21Z","title":"Segment-Based Interactive Machine Translation for Pre-trained Models","summary":"  Pre-trained large language models (LLM) are starting to be widely used in\nmany applications. In this work, we explore the use of these models in\ninteractive machine translation (IMT) environments. In particular, we have\nchosen mBART (multilingual Bidirectional and Auto-Regressive Transformer) and\nmT5 (multilingual Text-to-Text Transfer Transformer) as the LLMs to perform our\nexperiments. The system generates perfect translations interactively using the\nfeedback provided by the user at each iteration. The Neural Machine Translation\n(NMT) model generates a preliminary hypothesis with the feedback, and the user\nvalidates new correct segments and performs a word correction--repeating the\nprocess until the sentence is correctly translated. We compared the performance\nof mBART, mT5, and a state-of-the-art (SoTA) machine translation model on a\nbenchmark dataset regarding user effort, Word Stroke Ratio (WSR), Key Stroke\nRatio (KSR), and Mouse Action Ratio (MAR). The experimental results indicate\nthat mBART performed comparably with SoTA models, suggesting that it is a\nviable option for this field of IMT. The implications of this finding extend to\nthe development of new machine translation models for interactive environments,\nas it indicates that some novel pre-trained models exhibit SoTA performance in\nthis domain, highlighting the potential benefits of adapting these models to\nspecific needs.\n","authors":["Angel Navarro","Francisco Casacuberta"],"pdf_url":"https://arxiv.org/pdf/2407.06990v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.03882v2","updated":"2024-07-09T15:54:55Z","published":"2024-06-06T09:21:13Z","title":"Spontaneous Speech-Based Suicide Risk Detection Using Whisper and Large\n  Language Models","summary":"  The early detection of suicide risk is important since it enables the\nintervention to prevent potential suicide attempts. This paper studies the\nautomatic detection of suicide risk based on spontaneous speech from\nadolescents, and collects a Mandarin dataset with 15 hours of suicide speech\nfrom more than a thousand adolescents aged from ten to eighteen for our\nexperiments. To leverage the diverse acoustic and linguistic features embedded\nin spontaneous speech, both the Whisper speech model and textual large language\nmodels (LLMs) are used for suicide risk detection. Both all-parameter\nfinetuning and parameter-efficient finetuning approaches are used to adapt the\npre-trained models for suicide risk detection, and multiple audio-text fusion\napproaches are evaluated to combine the representations of Whisper and the LLM.\nThe proposed system achieves a detection accuracy of 0.807 and an F1-score of\n0.846 on the test set with 119 subjects, indicating promising potential for\nreal suicide risk detection applications.\n","authors":["Ziyun Cui","Chang Lei","Wen Wu","Yinan Duan","Diyang Qu","Ji Wu","Runsen Chen","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.03882v2.pdf","comment":"Accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2407.02147v2","updated":"2024-07-09T15:36:11Z","published":"2024-07-02T10:43:49Z","title":"GemmAr: Enhancing LLMs Through Arabic Instruction-Tuning","summary":"  Large language models (LLMs) have greatly impacted the natural language\nprocessing (NLP) field, particularly for the English language. These models\nhave demonstrated capabilities in understanding and generating human-like text.\nThe success of language models largely depends on the availability of\nhigh-quality instruction datasets, which consist of detailed task descriptions\nand corresponding responses that are essential for training the models to\naddress a variety of prompts accurately. However, the availability and quality\nof these resources vary by language. While models perform well in English, they\noften need help with languages like Arabic, due to the lack of datasets for\nfine-tuning Arabic-specific tasks. To address this issue, we introduce\nInstAr-500k, a new Arabic instruction dataset created by generating and\ncollecting content that covers several domains and instruction types. We assess\nthis dataset by fine-tuning an open-source Gemma-7B model on several downstream\ntasks to improve its functionality. Based on multiple evaluations, our\nfine-tuned model achieves excellent performance on several Arabic NLP\nbenchmarks. These outcomes emphasize the effectiveness of our dataset in\nelevating the capabilities of language models for Arabic. Our instruction\ndataset bridges the performance gap between English and Arabic language models\nby providing resources that amplify Arabic NLP development. Building on this\nfoundation, we developed a model, GemmAr-7B-V1, specifically tuned to excel at\na wide range of Arabic NLP tasks.\n","authors":["Hasna Chouikhi","Manel Aloui","Cyrine Ben Hammou","Ghaith Chaabane","Haithem Kchaou","Chehir Dhaouadi"],"pdf_url":"https://arxiv.org/pdf/2407.02147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06957v1","updated":"2024-07-09T15:35:43Z","published":"2024-07-09T15:35:43Z","title":"Listen and Speak Fairly: A Study on Semantic Gender Bias in Speech\n  Integrated Large Language Models","summary":"  Speech Integrated Large Language Models (SILLMs) combine large language\nmodels with speech perception to perform diverse tasks, such as emotion\nrecognition to speaker verification, demonstrating universal audio\nunderstanding capability. However, these models may amplify biases present in\ntraining data, potentially leading to biased access to information for\nmarginalized groups. This work introduces a curated spoken bias evaluation\ntoolkit and corresponding dataset. We evaluate gender bias in SILLMs across\nfour semantic-related tasks: speech-to-text translation (STT), spoken\ncoreference resolution (SCR), spoken sentence continuation (SSC), and spoken\nquestion answering (SQA). Our analysis reveals that bias levels are\nlanguage-dependent and vary with different evaluation methods. Our findings\nemphasize the necessity of employing multiple approaches to comprehensively\nassess biases in SILLMs, providing insights for developing fairer SILLM\nsystems.\n","authors":["Yi-Cheng Lin","Tzu-Quan Lin","Chih-Kai Yang","Ke-Han Lu","Wei-Chih Chen","Chun-Yi Kuan","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2407.06957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06955v1","updated":"2024-07-09T15:35:06Z","published":"2024-07-09T15:35:06Z","title":"ICLGuard: Controlling In-Context Learning Behavior for Applicability\n  Authorization","summary":"  In-context learning (ICL) is a recent advancement in the capabilities of\nlarge language models (LLMs). This feature allows users to perform a new task\nwithout updating the model. Concretely, users can address tasks during the\ninference time by conditioning on a few input-label pair demonstrations along\nwith the test input. It is different than the conventional fine-tuning paradigm\nand offers more flexibility. However, this capability also introduces potential\nissues. For example, users may use the model on any data without restriction,\nsuch as performing tasks with improper or sensitive content, which might\nviolate the model policy or conflict with the model owner's interests. As a\nmodel owner, it is crucial to establish a mechanism to control the model's\nbehavior under ICL, depending on the model owner's requirements for various\ncontent. To this end, we introduce the concept of \"applicability authorization\"\ntailored for LLMs, particularly for ICL behavior, and propose a simple\napproach, ICLGuard. It is a fine-tuning framework designed to allow the model\nowner to regulate ICL behavior on different data. ICLGuard preserves the\noriginal LLM and fine-tunes only a minimal set of additional trainable\nparameters to \"guard\" the LLM. Empirical results show that the guarded LLM can\ndeactivate its ICL ability on target data without affecting its ICL ability on\nother data and its general functionality across all data.\n","authors":["Wai Man Si","Michael Backes","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.06955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06950v1","updated":"2024-07-09T15:31:41Z","published":"2024-07-09T15:31:41Z","title":"Spanish TrOCR: Leveraging Transfer Learning for Language Adaptation","summary":"  This study explores the transfer learning capabilities of the TrOCR\narchitecture to Spanish. TrOCR is a transformer-based Optical Character\nRecognition (OCR) model renowned for its state-of-the-art performance in\nEnglish benchmarks. Inspired by Li et al. assertion regarding its adaptability\nto multilingual text recognition, we investigate two distinct approaches to\nadapt the model to a new language: integrating an English TrOCR encoder with a\nlanguage specific decoder and train the model on this specific language, and\nfine-tuning the English base TrOCR model on a new language data. Due to the\nscarcity of publicly available datasets, we present a resource-efficient\npipeline for creating OCR datasets in any language, along with a comprehensive\nbenchmark of the different image generation methods employed with a focus on\nVisual Rich Documents (VRDs). Additionally, we offer a comparative analysis of\nthe two approaches for the Spanish language, demonstrating that fine-tuning the\nEnglish TrOCR on Spanish yields superior recognition than the language specific\ndecoder for a fixed dataset size. We evaluate our model employing character and\nword error rate metrics on a public available printed dataset, comparing the\nperformance against other open-source and cloud OCR spanish models. As far as\nwe know, these resources represent the best open-source model for OCR in\nSpanish. The Spanish TrOCR models are publicly available on HuggingFace [20]\nand the code to generate the dataset is available on Github [25].\n","authors":["Filipe Lauar","Valentin Laurent"],"pdf_url":"https://arxiv.org/pdf/2407.06950v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.04078v2","updated":"2024-07-09T15:29:03Z","published":"2024-07-04T17:39:16Z","title":"DotaMath: Decomposition of Thought with Code Assistance and\n  Self-correction for Mathematical Reasoning","summary":"  Large language models (LLMs) have made impressive progress in handling simple\nmath problems, yet they still struggle with more challenging and complex\nmathematical tasks. In this paper, we introduce a series of LLMs that employs\nthe Decomposition of thought with code assistance and self-correction for\nmathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex\nmathematical tasks by decomposing them into simpler logical subtasks,\nleveraging code to solve these subtasks, obtaining fine-grained feedback from\nthe code interpreter, and engaging in self-reflection and correction. By\nannotating diverse interactive tool-use trajectories and employing query\nevolution on GSM8K and MATH datasets, we generate an instruction fine-tuning\ndataset called DotaMathQA with 574K query-response pairs. We train a series of\nbase LLMs using imitation learning on DotaMathQA, resulting in DotaMath models\nthat achieve remarkable performance compared to open-source LLMs across various\nin-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases\nan outstanding performance of 64.8% on the competitive MATH dataset and 86.7%\non GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a\nseries of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,\nwe anticipate that the DotaMath paradigm will open new pathways for addressing\nintricate mathematical problems. Our code is publicly available at\nhttps://github.com/ChengpengLi1003/DotaMath.\n","authors":["Chengpeng Li","Guanting Dong","Mingfeng Xue","Ru Peng","Xiang Wang","Dayiheng Liu"],"pdf_url":"https://arxiv.org/pdf/2407.04078v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2407.06946v1","updated":"2024-07-09T15:23:28Z","published":"2024-07-09T15:23:28Z","title":"Self-Recognition in Language Models","summary":"  A rapidly growing number of applications rely on a small set of closed-source\nlanguage models (LMs). This dependency might introduce novel security risks if\nLMs develop self-recognition capabilities. Inspired by human identity\nverification methods, we propose a novel approach for assessing\nself-recognition in LMs using model-generated \"security questions\". Our test\ncan be externally administered to keep track of frontier models as it does not\nrequire access to internal model parameters or output probabilities. We use our\ntest to examine self-recognition in ten of the most capable open- and\nclosed-source LMs currently publicly available. Our extensive experiments found\nno empirical evidence of general or consistent self-recognition in any examined\nLM. Instead, our results suggest that given a set of alternatives, LMs seek to\npick the \"best\" answer, regardless of its origin. Moreover, we find indications\nthat preferences about which models produce the best answers are consistent\nacross LMs. We additionally uncover novel insights on position bias\nconsiderations for LMs in multiple-choice settings.\n","authors":["Tim R. Davidson","Viacheslav Surkov","Veniamin Veselovsky","Giuseppe Russo","Robert West","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2407.06946v1.pdf","comment":"Code to reproduce experiments and replicate findings is made\n  available at https://github.com/trdavidson/self-recognition"},{"id":"http://arxiv.org/abs/2407.06941v1","updated":"2024-07-09T15:18:56Z","published":"2024-07-09T15:18:56Z","title":"Raply: A profanity-mitigated rap generator","summary":"  The task of writing rap is challenging and involves producing complex rhyming\nschemes, yet meaningful lyrics. In this work, we propose Raply, a fine-tuned\nGPT-2 model capable of producing meaningful rhyming text in the style of rap.\nIn addition to its rhyming capabilities, the model is able to generate less\noffensive content. It was achieved through the fine-tuning the model on a new\ndataset Mitislurs, a profanity-mitigated corpus. We evaluate the output of the\nmodel on two criteria: 1) rhyming based on the rhyme density metric; 2)\nprofanity content, using the list of profanities for the English language. To\nour knowledge, this is the first attempt at profanity mitigation for rap lyrics\ngeneration.\n","authors":["Omar Manil Bendali","Samir Ferroum","Ekaterina Kozachenko","Youssef Parviz","Hanna Shcharbakova","Anna Tokareva","Shemair Williams"],"pdf_url":"https://arxiv.org/pdf/2407.06941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04533v2","updated":"2024-07-09T15:07:35Z","published":"2024-07-05T14:21:36Z","title":"Performance Analysis of Speech Encoders for Low-Resource SLU and ASR in\n  Tunisian Dialect","summary":"  Speech encoders pretrained through self-supervised learning (SSL) have\ndemonstrated remarkable performance in various downstream tasks, including\nSpoken Language Understanding (SLU) and Automatic Speech Recognition (ASR). For\ninstance, fine-tuning SSL models for such tasks has shown significant\npotential, leading to improvements in the SOTA performance across challenging\ndatasets. In contrast to existing research, this paper contributes by comparing\nthe effectiveness of SSL approaches in the context of (i) the low-resource\nspoken Tunisian Arabic dialect and (ii) its combination with a low-resource SLU\nand ASR scenario, where only a few semantic annotations are available for\nfine-tuning. We conduct experiments using many SSL speech encoders on the\nTARIC-SLU dataset. We use speech encoders that were pre-trained on either\nmonolingual or multilingual speech data. Some of them have also been refined\nwithout in-domain nor Tunisian data through multimodal supervised\nteacher-student paradigm. This study yields numerous significant findings that\nwe are discussing in this paper.\n","authors":["Salima Mdhaffar","Haroun Elleuch","Fethi Bougares","Yannick EstÃ¨ve"],"pdf_url":"https://arxiv.org/pdf/2407.04533v2.pdf","comment":"Accepted in ArabicNLP 2024"},{"id":"http://arxiv.org/abs/2407.01937v2","updated":"2024-07-09T14:55:52Z","published":"2024-07-02T04:11:52Z","title":"Efficient-Empathy: Towards Efficient and Effective Selection of Empathy\n  Data","summary":"  In recent years, with the rapid advancements in large language models (LLMs),\nachieving excellent empathetic response capability has become a crucial\nprerequisite. Consequently, managing and understanding large-scale video\ndatasets has gained increasing importance. However, empathetic data are\ntypically trained without any quality selection, leading to inefficient data\nusage and wasted computational resources. Additionally, using raw data can\nresult in low performance in empathetic dialogues. In this work, we present\nEfficient-Empathy, a sensibility and rationality score-based data selection\nalgorithm that automatically selects sensibility and rationality data while\ndiscarding low-quality data. With only the sensibility data (59% of the full\ndataset), our trained sensibility model efficiently achieves state-of-the-art\n(SoTA) performance. Furthermore, with multiple data selection hyperparameters,\nthe sensibility model demonstrates SoTA performance, showcasing the robustness\nof our method. By integrating sensibility and rationality data with a MoE\nstructure, we achieve even higher performance, demonstrating the effectiveness\nof our Efficient-Empathy algorithm.\n","authors":["Linzhuang Sun","Hao Liang","Jingxuan Wei","Linkun Sun","Bihui Yu","Bin Cui","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.01937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06917v1","updated":"2024-07-09T14:52:52Z","published":"2024-07-09T14:52:52Z","title":"Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models","summary":"  Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to.\n","authors":["Zara Siddique","Liam D. Turner","Luis Espinosa-Anke"],"pdf_url":"https://arxiv.org/pdf/2407.06917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03956v2","updated":"2024-07-09T14:47:28Z","published":"2024-07-04T14:22:25Z","title":"Solving Zebra Puzzles Using Constraint-Guided Multi-Agent Systems","summary":"  Prior research has enhanced the ability of Large Language Models (LLMs) to\nsolve logic puzzles using techniques such as chain-of-thought prompting or\nintroducing a symbolic representation. These frameworks are still usually\ninsufficient to solve complicated logical problems, such as Zebra puzzles, due\nto the inherent complexity of translating natural language clues into logical\nstatements. We introduce a multi-agent system, ZPS, that integrates LLMs with\nan off the shelf theorem prover. This system tackles the complex puzzle-solving\ntask by breaking down the problem into smaller, manageable parts, generating\nSMT (Satisfiability Modulo Theories) code to solve them with a theorem prover,\nand using feedback between the agents to repeatedly improve their answers. We\nalso introduce an automated grid puzzle grader to assess the correctness of our\npuzzle solutions and show that the automated grader is reliable by evaluating\nit in a user-study. Our approach shows improvement in all three LLMs we tested,\nwith GPT-4 showing 166% improvement in the number of fully correct solutions.\n","authors":["Shmuel Berman","Kathleen McKeown","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2407.03956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06908v1","updated":"2024-07-09T14:45:15Z","published":"2024-07-09T14:45:15Z","title":"Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion\n  Representation of Religion in Large Language Models","summary":"  Emotions play important epistemological and cognitive roles in our lives,\nrevealing our values and guiding our actions. Previous work has shown that LLMs\ndisplay biases in emotion attribution along gender lines. However, unlike\ngender, which says little about our values, religion, as a socio-cultural\nsystem, prescribes a set of beliefs and values for its followers. Religions,\ntherefore, cultivate certain emotions. Moreover, these rules are explicitly\nlaid out and interpreted by religious leaders. Using emotion attribution, we\nexplore how different religions are represented in LLMs. We find that: Major\nreligions in the US and European countries are represented with more nuance,\ndisplaying a more shaded model of their beliefs. Eastern religions like\nHinduism and Buddhism are strongly stereotyped. Judaism and Islam are\nstigmatized -- the models' refusal skyrocket. We ascribe these to cultural bias\nin LLMs and the scarcity of NLP literature on religion. In the rare instances\nwhere religion is discussed, it is often in the context of toxic language,\nperpetuating the perception of these religions as inherently toxic. This\nfinding underscores the urgent need to address and rectify these biases. Our\nresearch underscores the crucial role emotions play in our lives and how our\nvalues influence them.\n","authors":["Flor Miriam Plaza-del-Arco","Amanda Cercas Curry","Susanna Paoli","Alba Curry","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2407.06908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06893v1","updated":"2024-07-09T14:25:23Z","published":"2024-07-09T14:25:23Z","title":"Measuring Sustainability Intention of ESG Fund Disclosure using Few-Shot\n  Learning","summary":"  Global sustainable fund universe encompasses open-end funds and\nexchange-traded funds (ETF) that, by prospectus or other regulatory filings,\nclaim to focus on Environment, Social and Governance (ESG). Challengingly, the\nclaims can only be confirmed by examining the textual disclosures to check if\nthere is presence of intentionality and ESG focus on its investment strategy.\nCurrently, there is no regulation to enforce sustainability in ESG products\nspace. This paper proposes a unique method and system to classify and score the\nfund prospectuses in the sustainable universe regarding specificity and\ntransparency of language. We aim to employ few-shot learners to identify\nspecific, ambiguous, and generic sustainable investment-related language.\nAdditionally, we construct a ratio metric to determine language score and\nrating to rank products and quantify sustainability claims for US sustainable\nuniverse. As a by-product, we publish manually annotated quality training\ndataset on Hugging Face (ESG-Prospectus-Clarity-Category under cc-by-nc-sa-4.0)\nof more than 1K ESG textual statements. The performance of the few-shot\nfinetuning approach is compared with zero-shot models e.g., Llama-13B, GPT 3.5\nTurbo etc. We found that prompting large language models are not accurate for\ndomain specific tasks due to misalignment issues. The few-shot finetuning\ntechniques outperform zero-shot models by large margins of more than absolute\n~30% in precision, recall and F1 metrics on completely unseen ESG languages\n(test set). Overall, the paper attempts to establish a systematic and scalable\napproach to measure and rate sustainability intention quantitatively for\nsustainable funds using texts in prospectus. Regulatory bodies, investors, and\nadvisors may utilize the findings of this research to reduce cognitive load in\ninvestigating or screening of ESG funds which accurately reflects the ESG\nintention.\n","authors":["Mayank Singh","Nazia Nafis","Abhijeet Kumar","Mridul Mishra"],"pdf_url":"https://arxiv.org/pdf/2407.06893v1.pdf","comment":"This paper was presented at 'AI applications in ESG Conference' at\n  IIM Bangalore, India (Nov, 2023)"},{"id":"http://arxiv.org/abs/2407.06866v1","updated":"2024-07-09T13:53:38Z","published":"2024-07-09T13:53:38Z","title":"ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context","summary":"  While the biases of language models in production are extensively documented,\nthe biases of their guardrails have been neglected. This paper studies how\ncontextual information about the user influences the likelihood of an LLM to\nrefuse to execute a request. By generating user biographies that offer\nideological and demographic information, we find a number of biases in\nguardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas\nare more likely to trigger a refusal guardrail when requesting censored or\nillegal information. Guardrails are also sycophantic, refusing to comply with\nrequests for a political position the user is likely to disagree with. We find\nthat certain identity groups and seemingly innocuous information, e.g., sports\nfandom, can elicit changes in guardrail sensitivity similar to direct\nstatements of political ideology. For each demographic category and even for\nAmerican football team fandom, we find that ChatGPT appears to infer a likely\npolitical ideology and modify guardrail behavior accordingly.\n","authors":["Victoria R. Li","Yida Chen","Naomi Saphra"],"pdf_url":"https://arxiv.org/pdf/2407.06866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06851v1","updated":"2024-07-09T13:35:54Z","published":"2024-07-09T13:35:54Z","title":"Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders","summary":"  Despite the impressive capabilities of Large Language Models (LLMs) in\nvarious tasks, their vulnerability to unsafe prompts remains a critical issue.\nThese prompts can lead LLMs to generate responses on illegal or sensitive\ntopics, posing a significant threat to their safe and ethical use. Existing\napproaches attempt to address this issue using classification models, but they\nhave several drawbacks. With the increasing complexity of unsafe prompts,\nsimilarity search-based techniques that identify specific features of unsafe\nprompts provide a more robust and effective solution to this evolving problem.\nThis paper investigates the potential of sentence encoders to distinguish safe\nfrom unsafe prompts, and the ability to classify various unsafe prompts\naccording to a safety taxonomy. We introduce new pairwise datasets and the\nCategorical Purity (CP) metric to measure this capability. Our findings reveal\nboth the effectiveness and limitations of existing sentence encoders, proposing\ndirections to improve sentence encoders to operate as more robust safety\ndetectors. Our code is available at https://github.com/JwdanielJung/Safe-Embed.\n","authors":["Jinseok Kim","Jaewon Jung","Sangyeop Kim","Sohyung Park","Sungzoon Cho"],"pdf_url":"https://arxiv.org/pdf/2407.06851v1.pdf","comment":"ACL 2024 KnowledgeableLMs workshop paper"},{"id":"http://arxiv.org/abs/2407.04472v3","updated":"2024-07-09T13:31:00Z","published":"2024-07-05T12:42:31Z","title":"EventChat: Implementation and user-centric evaluation of a large\n  language model-driven conversational recommender system for exploring leisure\n  events in an SME context","summary":"  Large language models (LLMs) present an enormous evolution in the strategic\npotential of conversational recommender systems (CRS). Yet to date, research\nhas predominantly focused upon technical frameworks to implement LLM-driven\nCRS, rather than end-user evaluations or strategic implications for firms,\nparticularly from the perspective of a small to medium enterprises (SME) that\nmakeup the bedrock of the global economy. In the current paper, we detail the\ndesign of an LLM-driven CRS in an SME setting, and its subsequent performance\nin the field using both objective system metrics and subjective user\nevaluations. While doing so, we additionally outline a short-form revised\nResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly\nevolving field. Our results reveal good system performance from a user\nexperience perspective (85.5% recommendation accuracy) but underscore latency,\ncost, and quality issues challenging business viability. Notably, with a median\ncost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and\nresponse time emerge as crucial areas for achieving a more user-friendly and\neconomically viable LLM-driven CRS for SME settings. One major driver of these\ncosts is the use of an advanced LLM as a ranker within the retrieval-augmented\ngeneration (RAG) technique. Our results additionally indicate that relying\nsolely on approaches such as Prompt-based learning with ChatGPT as the\nunderlying LLM makes it challenging to achieve satisfying quality in a\nproduction environment. Strategic considerations for SMEs deploying an\nLLM-driven CRS are outlined, particularly considering trade-offs in the current\ntechnical landscape.\n","authors":["Hannes Kunstmann","Joseph Ollier","Joel Persson","Florian von Wangenheim"],"pdf_url":"https://arxiv.org/pdf/2407.04472v3.pdf","comment":"27 pages, 3 tables, 5 figures, pre-print manuscript, updated version\n  of manuscript due to typo (previous version, Figure 5 was incorrectly named\n  Figure 6)"},{"id":"http://arxiv.org/abs/2407.06800v1","updated":"2024-07-09T12:14:48Z","published":"2024-07-09T12:14:48Z","title":"Learn and Don't Forget: Adding a New Language to ASR Foundation Models","summary":"  Foundation ASR models often support many languages, e.g. 100 languages in\nWhisper. However, there has been limited work on integrating an additional,\ntypically low-resource, language, while maintaining performance on the original\nlanguage set. Fine-tuning, while simple, may degrade the accuracy of the\noriginal set. We compare three approaches that exploit adaptation parameters:\nsoft language code tuning, train only the language code; soft prompt tuning,\ntrain prepended tokens; and LoRA where a small set of additional parameters are\noptimised. Elastic Weight Consolidation (EWC) offers an alternative compromise\nwith the potential to maintain performance in specific target languages.\nResults show that direct fine-tuning yields the best performance for the new\nlanguage but degrades existing language capabilities. EWC can address this\nissue for specific languages. If only adaptation parameters are used, the\nlanguage capabilities are maintained but at the cost of performance in the new\nlanguage.\n","authors":["Mengjie Qian","Siyuan Tang","Rao Ma","Kate M. Knill","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2407.06800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.12333v2","updated":"2024-07-09T12:06:39Z","published":"2021-04-26T03:37:22Z","title":"Explore BiLSTM-CRF-Based Models for Open Relation Extraction","summary":"  Extracting multiple relations from text sentences is still a challenge for\ncurrent Open Relation Extraction (Open RE) tasks. In this paper, we develop\nseveral Open RE models based on the bidirectional LSTM-CRF (BiLSTM-CRF) neural\nnetwork and different contextualized word embedding methods. We also propose a\nnew tagging scheme to solve overlapping problems and enhance models'\nperformance. From the evaluation results and comparisons between models, we\nselect the best combination of tagging scheme, word embedder, and BiLSTM-CRF\nnetwork to achieve an Open RE model with a remarkable extracting ability on\nmultiple-relation sentences.\n","authors":["Tao Ni","Qing Wang","Gabriela Ferraro"],"pdf_url":"https://arxiv.org/pdf/2104.12333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02181v3","updated":"2024-07-09T11:59:01Z","published":"2024-03-04T16:23:58Z","title":"Not All Layers of LLMs Are Necessary During Inference","summary":"  Due to the large number of parameters, the inference phase of Large Language\nModels (LLMs) is resource-intensive. However, not all requests posed to LLMs\nare equally difficult to handle. Through analysis, we show that for some tasks,\nLLMs can achieve results comparable to the final output at some intermediate\nlayers. That is, not all layers of LLMs are necessary during inference. If we\ncan predict at which layer the inferred results match the final results\n(produced by evaluating all layers), we could significantly reduce the\ninference cost. To this end, we propose a simple yet effective algorithm named\nAdaInfer to adaptively terminate the inference process for an input instance.\nAdaInfer relies on easily obtainable statistical features and classic\nclassifiers like SVM. Experiments on well-known LLMs like the Llama2 series and\nOPT, show that AdaInfer can achieve an average of 17.8% pruning ratio, and up\nto 43% on sentiment tasks, with nearly no performance drop (<1%). Because\nAdaInfer does not alter LLM parameters, the LLMs incorporated with AdaInfer\nmaintain generalizability across tasks.\n","authors":["Siqi Fan","Xin Jiang","Xiang Li","Xuying Meng","Peng Han","Shuo Shang","Aixin Sun","Yequan Wang","Zhongyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.02181v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06779v1","updated":"2024-07-09T11:48:49Z","published":"2024-07-09T11:48:49Z","title":"Using Pretrained Large Language Model with Prompt Engineering to Answer\n  Biomedical Questions","summary":"  Our team participated in the BioASQ 2024 Task12b and Synergy tasks to build a\nsystem that can answer biomedical questions by retrieving relevant articles and\nsnippets from the PubMed database and generating exact and ideal answers. We\npropose a two-level information retrieval and question-answering system based\non pre-trained large language models (LLM), focused on LLM prompt engineering\nand response post-processing. We construct prompts with in-context few-shot\nexamples and utilize post-processing techniques like resampling and malformed\nresponse detection. We compare the performance of various pre-trained LLM\nmodels on this challenge, including Mixtral, OpenAI GPT and Llama2. Our\nbest-performing system achieved 0.14 MAP score on document retrieval, 0.05 MAP\nscore on snippet retrieval, 0.96 F1 score for yes/no questions, 0.38 MRR score\nfor factoid questions and 0.50 F1 score for list questions in Task 12b.\n","authors":["Wenxin Zhou","Thuy Hang Ngo"],"pdf_url":"https://arxiv.org/pdf/2407.06779v1.pdf","comment":"Submitted to Conference and Labs of the Evaluation Forum (CLEF) 2024\n  CEUR-WS"},{"id":"http://arxiv.org/abs/2407.03994v2","updated":"2024-07-09T11:09:19Z","published":"2024-07-04T15:14:17Z","title":"Unlocking the Potential of Model Merging for Low-Resource Languages","summary":"  Adapting large language models (LLMs) to new languages typically involves\ncontinual pre-training (CT) followed by supervised fine-tuning (SFT). However,\nthis CT-then-SFT approach struggles with limited data in the context of\nlow-resource languages, failing to balance language modeling and task-solving\ncapabilities. We thus propose model merging as an alternative for low-resource\nlanguages, combining models with distinct capabilities into a single model\nwithout additional training. We use model merging to develop task-solving LLMs\nfor low-resource languages without SFT data in the target languages. Our\nexperiments based on Llama-2-7B demonstrate that model merging effectively\nendows LLMs for low-resource languages with task-solving abilities,\noutperforming CT-then-SFT in scenarios with extremely scarce data. Observing\nperformance saturation in model merging with more training tokens, we further\nanalyze the merging process and introduce a slack variable to the model merging\nalgorithm to mitigate the loss of important parameters, thereby enhancing\nperformance. We hope that model merging can benefit more human languages\nsuffering from data scarcity with its higher data efficiency.\n","authors":["Mingxu Tao","Chen Zhang","Quzhe Huang","Tianyao Ma","Songfang Huang","Dongyan Zhao","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2407.03994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13887v2","updated":"2024-07-09T10:46:29Z","published":"2024-02-21T15:58:37Z","title":"Beyond Probabilities: Unveiling the Misalignment in Evaluating Large\n  Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, fundamentally reshaping the landscape of natural language\nprocessing (NLP) research. However, recent evaluation frameworks often rely on\nthe output probabilities of LLMs for predictions, primarily due to\ncomputational constraints, diverging from real-world LLM usage scenarios. While\nwidely employed, the efficacy of these probability-based evaluation strategies\nremains an open research question. This study aims to scrutinize the validity\nof such probability-based evaluation methods within the context of using LLMs\nfor Multiple Choice Questions (MCQs), highlighting their inherent limitations.\nOur empirical investigation reveals that the prevalent probability-based\nevaluation method inadequately aligns with generation-based prediction.\nFurthermore, current evaluation frameworks typically assess LLMs through\npredictive tasks based on output probabilities rather than directly generating\nresponses, owing to computational limitations. We illustrate that these\nprobability-based approaches do not effectively correspond with generative\npredictions. The outcomes of our study can enhance the understanding of LLM\nevaluation methodologies and provide insights for future research in this\ndomain.\n","authors":["Chenyang Lyu","Minghao Wu","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2402.13887v2.pdf","comment":"Accepted to KnowledgeableLMs @ ACL 2024"},{"id":"http://arxiv.org/abs/2402.15020v2","updated":"2024-07-09T09:32:52Z","published":"2024-02-22T23:36:26Z","title":"Probabilistically-Sound Beam Search with Masked Language Models","summary":"  Beam search with masked language models (MLMs) is challenging in part because\njoint probability distributions over sequences are not readily available,\nunlike for autoregressive models. However, estimating such distributions has\nimportant domain-specific applications such as ancient text restoration and\nprotein engineering. Here we present probabilistically-sound methods for beam\nsearch with MLMs. First, we clarify the conditions under which it is\ntheoretically sound to perform text infilling with MLMs using standard beam\nsearch. When these conditions fail, we provide a probabilistically-sound\nmodification with no additional computational complexity and demonstrate that\nit is superior to the aforementioned beam search in the expected conditions. We\nthen present empirical results comparing several infilling approaches with MLMs\nacross several domains.\n","authors":["Creston Brooks","Robert Calef","Charlie Cowen-Breen","Anna Sappington"],"pdf_url":"https://arxiv.org/pdf/2402.15020v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06699v1","updated":"2024-07-09T09:21:55Z","published":"2024-07-09T09:21:55Z","title":"Consistent Document-Level Relation Extraction via Counterfactuals","summary":"  Many datasets have been developed to train and evaluate document-level\nrelation extraction (RE) models. Most of these are constructed using real-world\ndata. It has been shown that RE models trained on real-world data suffer from\nfactual biases. To evaluate and address this issue, we present CovEReD, a\ncounterfactual data generation approach for document-level relation extraction\ndatasets using entity replacement. We first demonstrate that models trained on\nfactual data exhibit inconsistent behavior: while they accurately extract\ntriples from factual data, they fail to extract the same triples after\ncounterfactual modification. This inconsistency suggests that models trained on\nfactual data rely on spurious signals such as specific entities and external\nknowledge $\\unicode{x2013}$ rather than on the input context $\\unicode{x2013}$\nto extract triples. We show that by generating document-level counterfactual\ndata with CovEReD and training models on them, consistency is maintained with\nminimal impact on RE performance. We release our CovEReD pipeline as well as\nRe-DocRED-CF, a dataset of counterfactual RE documents, to assist in evaluating\nand addressing inconsistency in document-level RE.\n","authors":["Ali Modarressi","Abdullatif KÃ¶ksal","Hinrich SchÃ¼tze"],"pdf_url":"https://arxiv.org/pdf/2407.06699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06004v2","updated":"2024-07-09T09:11:18Z","published":"2024-07-08T14:58:29Z","title":"Perceptions to Beliefs: Exploring Precursory Inferences for Theory of\n  Mind in Large Language Models","summary":"  While humans naturally develop theory of mind (ToM), the capability to\nunderstand other people's mental states and beliefs, state-of-the-art large\nlanguage models (LLMs) underperform on simple ToM benchmarks. We posit that we\ncan extend our understanding of LLMs' ToM abilities by evaluating key human ToM\nprecursors -- perception inference and perception-to-belief inference -- in\nLLMs. We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate\nthese precursory inferences for ToM in LLMs by annotating characters'\nperceptions on ToMi and FANToM, respectively. Our evaluation of eight\nstate-of-the-art LLMs reveals that the models generally perform well in\nperception inference while exhibiting limited capability in\nperception-to-belief inference (e.g., lack of inhibitory control). Based on\nthese results, we present PercepToM, a novel ToM method leveraging LLMs' strong\nperception inference capability while supplementing their limited\nperception-to-belief inference. Experimental results demonstrate that PercepToM\nsignificantly enhances LLM's performance, especially in false belief scenarios.\n","authors":["Chani Jung","Dongkwan Kim","Jiho Jin","Jiseon Kim","Yeon Seonwoo","Yejin Choi","Alice Oh","Hyunwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.06004v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06677v1","updated":"2024-07-09T08:50:18Z","published":"2024-07-09T08:50:18Z","title":"Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of\n  Modules","summary":"  Is it always necessary to compute tokens from shallow to deep layers in\nTransformers? The continued success of vanilla Transformers and their variants\nsuggests an undoubted \"yes\". In this work, however, we attempt to break the\ndepth-ordered convention by proposing a novel architecture dubbed\nmixture-of-modules (MoM), which is motivated by an intuition that any layer,\nregardless of its position, can be used to compute a token as long as it\npossesses the needed processing capabilities. The construction of MoM starts\nfrom a finite set of modules defined by multi-head attention and feed-forward\nnetworks, each distinguished by its unique parameterization. Two routers then\niteratively select attention modules and feed-forward modules from the set to\nprocess a token. The selection dynamically expands the computation graph in the\nforward pass of the token, culminating in an assembly of modules. We show that\nMoM provides not only a unified framework for Transformers and their numerous\nvariants but also a flexible and learnable approach for reducing redundancy in\nTransformer parameterization. We pre-train various MoMs using OpenWebText.\nEmpirical results demonstrate that MoMs, of different parameter counts,\nconsistently outperform vanilla transformers on both GLUE and XSUM benchmarks.\nMore interestingly, with a fixed parameter budget, MoM-large enables an over\n38% increase in depth for computation graphs compared to GPT-2-large, resulting\nin absolute gains of 1.4 on GLUE and 1 on XSUM. On the other hand, MoM-large\nalso enables an over 60% reduction in depth while involving more modules per\nlayer, yielding a 16% reduction in TFLOPs and a 43% decrease in memory usage\ncompared to GPT-2-large, while maintaining comparable performance.\n","authors":["Zhuocheng Gong","Ang Lv","Jian Guan","Junxi Yan","Wei Wu","Huishuai Zhang","Minlie Huang","Dongyan Zhao","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2407.06677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14825v4","updated":"2024-07-09T08:38:05Z","published":"2024-06-21T01:52:37Z","title":"TemPrompt: Multi-Task Prompt Learning for Temporal Relation Extraction\n  in RAG-based Crowdsourcing Systems","summary":"  Temporal relation extraction (TRE) aims to grasp the evolution of events or\nactions, and thus shape the workflow of associated tasks, so it holds promise\nin helping understand task requests initiated by requesters in crowdsourcing\nsystems. However, existing methods still struggle with limited and unevenly\ndistributed annotated data. Therefore, inspired by the abundant global\nknowledge stored within pre-trained language models (PLMs), we propose a\nmulti-task prompt learning framework for TRE (TemPrompt), incorporating prompt\ntuning and contrastive learning to tackle these issues. To elicit more\neffective prompts for PLMs, we introduce a task-oriented prompt construction\napproach that thoroughly takes the myriad factors of TRE into consideration for\nautomatic prompt generation. In addition, we design temporal event reasoning in\nthe form of masked language modeling as auxiliary tasks to bolster the model's\nfocus on events and temporal cues. The experimental results demonstrate that\nTemPrompt outperforms all compared baselines across the majority of metrics\nunder both standard and few-shot settings. A case study on designing and\nmanufacturing printed circuit boards is provided to validate its effectiveness\nin crowdsourcing scenarios.\n","authors":["Jing Yang","Yu Zhao","Linyao Yang","Xiao Wang","Long Chen","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2406.14825v4.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.06654v1","updated":"2024-07-09T08:26:39Z","published":"2024-07-09T08:26:39Z","title":"SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language\n  Model Pre-training","summary":"  The effectiveness of large language models (LLMs) is often hindered by\nduplicated data in their extensive pre-training datasets. Current approaches\nprimarily focus on detecting and removing duplicates, which risks the loss of\nvaluable information and neglects the varying degrees of duplication. To\naddress this, we propose a soft deduplication method that maintains dataset\nintegrity while selectively reducing the sampling weight of data with high\ncommonness. Central to our approach is the concept of \"data commonness\", a\nmetric we introduce to quantify the degree of duplication by measuring the\noccurrence probabilities of samples using an n-gram model. Empirical analysis\nshows that this method significantly improves training efficiency, achieving\ncomparable perplexity scores with at least a 26% reduction in required training\nsteps. Additionally, it enhances average few-shot downstream accuracy by 1.77%\nwhen trained for an equivalent duration. Importantly, this approach\nconsistently improves performance, even on rigorously deduplicated datasets,\nindicating its potential to complement existing methods and become a standard\npre-training process for LLMs.\n","authors":["Nan He","Weichen Xiong","Hanwen Liu","Yi Liao","Lei Ding","Kai Zhang","Guohua Tang","Xiao Han","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2407.06654v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.02119v2","updated":"2024-07-09T08:24:06Z","published":"2024-07-02T10:09:19Z","title":"Cost-Effective Proxy Reward Model Construction with On-Policy and Active\n  Learning","summary":"  Reinforcement learning with human feedback (RLHF), as a widely adopted\napproach in current large language model pipelines, is \\textit{bottlenecked by\nthe size of human preference data}. While traditional methods rely on offline\npreference dataset constructions, recent approaches have shifted towards online\nsettings, where a learner uses a small amount of labeled seed data and a large\npool of unlabeled prompts to iteratively construct new preference data through\nself-generated responses and high-quality reward/preference feedback. However,\nmost current online algorithms still focus on preference labeling during policy\nmodel updating with given feedback oracles, which incurs significant expert\nquery costs. \\textit{We are the first to explore cost-effective proxy reward\noracles construction strategies for further labeling preferences or rewards\nwith extremely limited labeled data and expert query budgets}. Our approach\nintroduces two key innovations: (1) on-policy query to avoid OOD and imbalance\nissues in seed data, and (2) active learning to select the most informative\ndata for preference queries. Using these methods, we train a evaluation model\nwith minimal expert-labeled data, which then effectively labels nine times more\npreference pairs for further RLHF training. For instance, our model using\nDirect Preference Optimization (DPO) gains around over 1% average improvement\non AlpacaEval2, MMLU-5shot and MMLU-0shot, with only 1.7K query cost. Our\nmethodology is orthogonal to other direct expert query-based strategies and\ntherefore might be integrated with them to further reduce query costs.\n","authors":["Yifang Chen","Shuohang Wang","Ziyi Yang","Hiteshi Sharma","Nikos Karampatziakis","Donghan Yu","Kevin Jamieson","Simon Shaolei Du","Yelong Shen"],"pdf_url":"https://arxiv.org/pdf/2407.02119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06650v1","updated":"2024-07-09T08:21:40Z","published":"2024-07-09T08:21:40Z","title":"A Word Order Synchronization Metric for Evaluating Simultaneous\n  Interpretation and Translation","summary":"  Simultaneous interpretation (SI), the translation of one language to another\nin real time, starts translation before the original speech has finished. Its\nevaluation needs to consider both latency and quality. This trade-off is\nchallenging especially for distant word order language pairs such as English\nand Japanese. To handle this word order gap, interpreters maintain the word\norder of the source language as much as possible to keep up with original\nlanguage to minimize its latency while maintaining its quality, whereas in\ntranslation reordering happens to keep fluency in the target language. This\nmeans outputs synchronized with the source language are desirable based on the\nreal SI situation, and it's a key for further progress in computational SI and\nsimultaneous machine translation (SiMT). In this work, we propose an automatic\nevaluation metric for SI and SiMT focusing on word order synchronization. Our\nevaluation metric is based on rank correlation coefficients, leveraging\ncross-lingual pre-trained language models. Our experimental results on\nNAIST-SIC-Aligned and JNPC showed our metrics' effectiveness to measure word\norder synchronization between source and target language.\n","authors":["Mana Makinae","Katsuhito Sudoh","Mararu Yamada","Satoshi Nakamura"],"pdf_url":"https://arxiv.org/pdf/2407.06650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05740v2","updated":"2024-07-09T08:16:24Z","published":"2024-07-08T08:46:50Z","title":"Do Multilingual Large Language Models Mitigate Stereotype Bias?","summary":"  While preliminary findings indicate that multilingual LLMs exhibit reduced\nbias compared to monolingual ones, a comprehensive understanding of the effect\nof multilingual training on bias mitigation, is lacking. This study addresses\nthis gap by systematically training six LLMs of identical size (2.6B\nparameters) and architecture: five monolingual models (English, German, French,\nItalian, and Spanish) and one multilingual model trained on an equal\ndistribution of data across these languages, all using publicly available data.\nTo ensure robust evaluation, standard bias benchmarks were automatically\ntranslated into the five target languages and verified for both translation\nquality and bias preservation by human annotators. Our results consistently\ndemonstrate that multilingual training effectively mitigates bias. Moreover, we\nobserve that multilingual models achieve not only lower bias but also superior\nprediction accuracy when compared to monolingual models with the same amount of\ntraining data, model architecture, and size.\n","authors":["Shangrui Nie","Michael Fromm","Charles Welch","Rebekka GÃ¶rge","Akbar Karimi","Joan Plepi","Nazia Afsan Mowmita","Nicolas Flores-Herr","Mehdi Ali","Lucie Flek"],"pdf_url":"https://arxiv.org/pdf/2407.05740v2.pdf","comment":"19 pages, 8 figures, C3NLP 2024"},{"id":"http://arxiv.org/abs/2403.08495v3","updated":"2024-07-09T07:47:20Z","published":"2024-03-13T13:04:58Z","title":"Automatic Interactive Evaluation for Large Language Models with State\n  Aware Patient Simulator","summary":"  Large Language Models (LLMs) have demonstrated remarkable proficiency in\nhuman interactions, yet their application within the medical field remains\ninsufficiently explored. Previous works mainly focus on the performance of\nmedical knowledge with examinations, which is far from the realistic scenarios,\nfalling short in assessing the abilities of LLMs on clinical tasks. In the\nquest to enhance the application of Large Language Models (LLMs) in healthcare,\nthis paper introduces the Automated Interactive Evaluation (AIE) framework and\nthe State-Aware Patient Simulator (SAPS), targeting the gap between traditional\nLLM evaluations and the nuanced demands of clinical practice. Unlike prior\nmethods that rely on static medical knowledge assessments, AIE and SAPS provide\na dynamic, realistic platform for assessing LLMs through multi-turn\ndoctor-patient simulations. This approach offers a closer approximation to real\nclinical scenarios and allows for a detailed analysis of LLM behaviors in\nresponse to complex patient interactions. Our extensive experimental validation\ndemonstrates the effectiveness of the AIE framework, with outcomes that align\nwell with human evaluations, underscoring its potential to revolutionize\nmedical LLM testing for improved healthcare delivery.\n","authors":["Yusheng Liao","Yutong Meng","Yuhao Wang","Hongcheng Liu","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08495v3.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.06606v1","updated":"2024-07-09T07:15:56Z","published":"2024-07-09T07:15:56Z","title":"Tailored Design of Audio-Visual Speech Recognition Models using\n  Branchformers","summary":"  Recent advances in Audio-Visual Speech Recognition (AVSR) have led to\nunprecedented achievements in the field, improving the robustness of this type\nof system in adverse, noisy environments. In most cases, this task has been\naddressed through the design of models composed of two independent encoders,\neach dedicated to a specific modality. However, while recent works have\nexplored unified audio-visual encoders, determining the optimal cross-modal\narchitecture remains an ongoing challenge. Furthermore, such approaches often\nrely on models comprising vast amounts of parameters and high computational\ncost training processes. In this paper, we aim to bridge this research gap by\nintroducing a novel audio-visual framework. Our proposed method constitutes, to\nthe best of our knowledge, the first attempt to harness the flexibility and\ninterpretability offered by encoder architectures, such as the Branchformer, in\nthe design of parameter-efficient AVSR systems. To be more precise, the\nproposed framework consists of two steps: first, estimating audio- and\nvideo-only systems, and then designing a tailored audio-visual unified encoder\nbased on the layer-level branch scores provided by the modality-specific\nmodels. Extensive experiments on English and Spanish AVSR benchmarks covering\nmultiple data conditions and scenarios demonstrated the effectiveness of our\nproposed method. Results reflect how our tailored AVSR system is able to reach\nstate-of-the-art recognition rates while significantly reducing the model\ncomplexity w.r.t. the prevalent approach in the field. Code and pre-trained\nmodels are available at https://github.com/david-gimeno/tailored-avsr.\n","authors":["David Gimeno-GÃ³mez","Carlos-D. MartÃ­nez-Hinarejos"],"pdf_url":"https://arxiv.org/pdf/2407.06606v1.pdf","comment":"Submitted and under review for the IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing (TASLP) journal"},{"id":"http://arxiv.org/abs/2407.06146v2","updated":"2024-07-09T07:08:11Z","published":"2024-07-08T17:19:59Z","title":"Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling\n  Tasks","summary":"  We present and evaluate a method called grammar masking, which is used to\nguide large language models (LLMs) toward producing syntactically correct\nmodels for a given context-free grammar. Prompt engineering methods such as\nfew-shot learning or priming can be used to improve the chances of an LLM\nproducing correct syntax, but the more complex the grammar, the more\ntime-consuming and less promising these methods become. Previous work is\nfocused primarily on the usage of either language model training or prompt\nengineering. In this work, a method is presented that restricts the output to a\ngiven grammar using constrained decoding to ensure the output adheres to a\nvalid syntax. We use several DSLs built with MontiCore and task multiple LLMs\nto produce models with and without constrained decoding. A corresponding parser\nis used to confirm the syntactic correctness of each model. We show that\ngrammar masking can dramatically improve the modeling capabilities of several\nLLMs, reducing the need for well-refined prompting while increasing the chance\nof producing correct models.\n","authors":["Lukas Netz","Jan Reimer","Bernhard Rumpe"],"pdf_url":"https://arxiv.org/pdf/2407.06146v2.pdf","comment":"Preprint to be published in the MODELS Workshop \"MDE Intelligence\""},{"id":"http://arxiv.org/abs/2405.09854v2","updated":"2024-07-09T06:52:45Z","published":"2024-05-16T07:14:13Z","title":"Striking a Balance between Classical and Deep Learning Approaches in\n  Natural Language Processing Pedagogy","summary":"  While deep learning approaches represent the state-of-the-art of natural\nlanguage processing (NLP) today, classical algorithms and approaches still find\na place in NLP textbooks and courses of recent years. This paper discusses the\nperspectives of conveners of two introductory NLP courses taught in Australia\nand India, and examines how classical and deep learning approaches can be\nbalanced within the lecture plan and assessments of the courses. We also draw\nparallels with the objects-first and objects-later debate in CS1 education. We\nobserve that teaching classical approaches adds value to student learning by\nbuilding an intuitive understanding of NLP problems, potential solutions, and\neven deep learning models themselves. Despite classical approaches not being\nstate-of-the-art, the paper makes a case for their inclusion in NLP courses\ntoday.\n","authors":["Aditya Joshi","Jake Renzella","Pushpak Bhattacharyya","Saurav Jha","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.09854v2.pdf","comment":"Selected for publication at Teaching NLP workshop at ACL 2024; 9\n  pages + references"},{"id":"http://arxiv.org/abs/2406.18045v3","updated":"2024-07-09T06:52:17Z","published":"2024-06-26T03:43:09Z","title":"PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical\n  and Chemistry","summary":"  Large language models (LLMs) have revolutionized Natural Language Processing\n(NLP) by minimizing the need for complex feature engineering. However, the\napplication of LLMs in specialized domains like biopharmaceuticals and\nchemistry remains largely unexplored. These fields are characterized by\nintricate terminologies, specialized knowledge, and a high demand for precision\nareas where general purpose LLMs often fall short. In this study, we introduce\nPharmaGPT, a suite of domain specilized LLMs with 13 billion and 70 billion\nparameters, specifically trained on a comprehensive corpus tailored to the\nBio-Pharmaceutical and Chemical domains. Our evaluation shows that PharmaGPT\nsurpasses existing general models on specific-domain benchmarks such as NAPLEX,\ndemonstrating its exceptional capability in domain-specific tasks. Remarkably,\nthis performance is achieved with a model that has only a fraction, sometimes\njust one-tenth-of the parameters of general-purpose large models. This\nadvancement establishes a new benchmark for LLMs in the bio-pharmaceutical and\nchemical fields, addressing the existing gap in specialized language modeling.\nIt also suggests a promising path for enhanced research and development, paving\nthe way for more precise and effective NLP applications in these areas.\n","authors":["Linqing Chen","Weilei Wang","Zilong Bai","Peng Xu","Yan Fang","Jie Fang","Wentao Wu","Lizhi Zhou","Ruiji Zhang","Yubin Xia","Chaobo Xu","Ran Hu","Licong Xu","Qijun Cai","Haoran Hua","Jing Sun","Jin Liu","Tian Qiu","Haowen Liu","Meng Hu","Xiuwen Li","Fei Gao","Yufu Wang","Lin Tie","Chaochao Wang","Jianping Lu","Cheng Sun","Yixin Wang","Shengjie Yang","Yuancheng Li","Lu Jin","Lisha Zhang","Fu Bian","Zhongkai Ye","Lidong Pei","Changyang Tu"],"pdf_url":"https://arxiv.org/pdf/2406.18045v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11418v2","updated":"2024-07-09T06:49:38Z","published":"2024-06-17T11:08:08Z","title":"BAMBINO-LM: (Bilingual-)Human-Inspired Continual Pretraining of BabyLM","summary":"  Children from bilingual backgrounds benefit from interactions with parents\nand teachers to re-acquire their heritage language. In this paper, we\ninvestigate how this insight from behavioral study can be incorporated into the\nlearning of small-scale language models. We introduce BAMBINO-LM, a continual\npre-training strategy for BabyLM that uses a novel combination of alternation\nand PPO-based perplexity reward induced from a parent Italian model. Upon\nevaluation on zero-shot classification tasks for English and Italian,\nBAMBINO-LM improves the Italian language capability of a BabyLM baseline. Our\nablation analysis demonstrates that employing both the alternation strategy and\nPPO-based modeling is key to this effectiveness gain. We also show that, as a\nside effect, the proposed method leads to a similar degradation in L1\neffectiveness as human children would have had in an equivalent learning\nscenario. Through its modeling and findings, BAMBINO-LM makes a focused\ncontribution to the pre-training of small-scale language models by first\ndeveloping a human-inspired strategy for pre-training and then showing that it\nresults in behaviours similar to that of humans.\n","authors":["Zhewen Shen","Aditya Joshi","Ruey-Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.11418v2.pdf","comment":"5 pages + references; Selected at CMCL Workshop@ACL 2024"},{"id":"http://arxiv.org/abs/2302.12313v4","updated":"2024-07-09T06:25:16Z","published":"2023-02-23T20:18:52Z","title":"Testing AI on language comprehension tasks reveals insensitivity to\n  underlying meaning","summary":"  Large Language Models (LLMs) are recruited in applications that span from\nclinical assistance and legal support to question answering and education.\nTheir success in specialized tasks has led to the claim that they possess\nhuman-like linguistic capabilities related to compositional understanding and\nreasoning. Yet, reverse-engineering is bound by Moravec's Paradox, according to\nwhich easy skills are hard. We systematically assess 7 state-of-the-art models\non a novel benchmark. Models answered a series of comprehension questions, each\nprompted multiple times in two settings, permitting one-word or open-length\nreplies. Each question targets a short text featuring high-frequency linguistic\nconstructions. To establish a baseline for achieving human-like performance, we\ntested 400 humans on the same prompts. Based on a dataset of n=26,680\ndatapoints, we discovered that LLMs perform at chance accuracy and waver\nconsiderably in their answers. Quantitatively, the tested models are\noutperformed by humans, and qualitatively their answers showcase distinctly\nnon-human errors in language understanding. We interpret this evidence as\nsuggesting that, despite their usefulness in various tasks, current AI models\nfall short of understanding language in a way that matches humans, and we argue\nthat this may be due to their lack of a compositional operator for regulating\ngrammatical and semantic information.\n","authors":["Vittoria Dentella","Fritz Guenther","Elliot Murphy","Gary Marcus","Evelina Leivada"],"pdf_url":"https://arxiv.org/pdf/2302.12313v4.pdf","comment":"18 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.06579v1","updated":"2024-07-09T06:18:40Z","published":"2024-07-09T06:18:40Z","title":"NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in\n  Text Classification","summary":"  Existing research on learning with noisy labels predominantly focuses on\nsynthetic label noise. Although synthetic noise possesses well-defined\nstructural properties, it often fails to accurately replicate real-world noise\npatterns. In recent years, there has been a concerted effort to construct\ngeneralizable and controllable instance-dependent noise datasets for image\nclassification, significantly advancing the development of noise-robust\nlearning in this area. However, studies on noisy label learning for text\nclassification remain scarce. To better understand label noise in real-world\ntext classification settings, we constructed the benchmark dataset NoisyAG-News\nthrough manual annotation. Initially, we analyzed the annotated data to gather\nobservations about real-world noise. We qualitatively and quantitatively\ndemonstrated that real-world noisy labels adhere to instance-dependent\npatterns. Subsequently, we conducted comprehensive learning experiments on\nNoisyAG-News and its corresponding synthetic noise datasets using pre-trained\nlanguage models and noise-handling techniques. Our findings reveal that while\npre-trained models are resilient to synthetic noise, they struggle against\ninstance-dependent noise, with samples of varying confusion levels showing\ninconsistent performance during training and testing. These real-world noise\npatterns pose new, significant challenges, prompting a reevaluation of noisy\nlabel handling methods. We hope that NoisyAG-News will facilitate the\ndevelopment and evaluation of future solutions for learning with noisy labels.\n","authors":["Hongfei Huang","Tingting Liang","Xixi Sun","Zikang Jin","Yuyu Yin"],"pdf_url":"https://arxiv.org/pdf/2407.06579v1.pdf","comment":"20 pages , 13 figure"},{"id":"http://arxiv.org/abs/2407.06576v1","updated":"2024-07-09T06:11:18Z","published":"2024-07-09T06:11:18Z","title":"Virtual Personas for Language Models via an Anthology of Backstories","summary":"  Large language models (LLMs) are trained from vast repositories of text\nauthored by millions of distinct authors, reflecting an enormous diversity of\nhuman traits. While these models bear the potential to be used as\napproximations of human subjects in behavioral studies, prior efforts have been\nlimited in steering model responses to match individual human users. In this\nwork, we introduce \"Anthology\", a method for conditioning LLMs to particular\nvirtual personas by harnessing open-ended life narratives, which we refer to as\n\"backstories.\" We show that our methodology enhances the consistency and\nreliability of experimental outcomes while ensuring better representation of\ndiverse sub-populations. Across three nationally representative human surveys\nconducted as part of Pew Research Center's American Trends Panel (ATP), we\ndemonstrate that Anthology achieves up to 18% improvement in matching the\nresponse distributions of human respondents and 27% improvement in consistency\nmetrics. Our code and generated backstories are available at\nhttps://github.com/CannyLab/anthology.\n","authors":["Suhong Moon","Marwa Abdulhai","Minwoo Kang","Joseph Suh","Widyadewi Soedarmadji","Eran Kohen Behar","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2407.06576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09353v6","updated":"2024-07-09T05:59:16Z","published":"2024-02-14T17:59:34Z","title":"DoRA: Weight-Decomposed Low-Rank Adaptation","summary":"  Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA\nand its variants have gained considerable popularity because of avoiding\nadditional inference costs. However, there still often exists an accuracy gap\nbetween these methods and full fine-tuning (FT). In this work, we first\nintroduce a novel weight decomposition analysis to investigate the inherent\ndifferences between FT and LoRA. Aiming to resemble the learning capacity of FT\nfrom the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA).\nDoRA decomposes the pre-trained weight into two components, magnitude and\ndirection, for fine-tuning, specifically employing LoRA for directional updates\nto efficiently minimize the number of trainable parameters. By employing \\ours,\nwe enhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. \\ours~consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding. Code is available at https://github.com/NVlabs/DoRA.\n","authors":["Shih-Yang Liu","Chien-Yi Wang","Hongxu Yin","Pavlo Molchanov","Yu-Chiang Frank Wang","Kwang-Ting Cheng","Min-Hung Chen"],"pdf_url":"https://arxiv.org/pdf/2402.09353v6.pdf","comment":"ICML2024(Oral)"},{"id":"http://arxiv.org/abs/2407.06564v1","updated":"2024-07-09T05:42:53Z","published":"2024-07-09T05:42:53Z","title":"Combining Knowledge Graphs and Large Language Models","summary":"  In recent years, Natural Language Processing (NLP) has played a significant\nrole in various Artificial Intelligence (AI) applications such as chatbots,\ntext generation, and language translation. The emergence of large language\nmodels (LLMs) has greatly improved the performance of these applications,\nshowing astonishing results in language understanding and generation. However,\nthey still show some disadvantages, such as hallucinations and lack of\ndomain-specific knowledge, that affect their performance in real-world tasks.\nThese issues can be effectively mitigated by incorporating knowledge graphs\n(KGs), which organise information in structured formats that capture\nrelationships between entities in a versatile and interpretable fashion.\nLikewise, the construction and validation of KGs present challenges that LLMs\ncan help resolve. The complementary relationship between LLMs and KGs has led\nto a trend that combines these technologies to achieve trustworthy results.\nThis work collected 28 papers outlining methods for KG-powered LLMs, LLM-based\nKGs, and LLM-KG hybrid approaches. We systematically analysed and compared\nthese approaches to provide a comprehensive overview highlighting key trends,\ninnovative techniques, and common challenges. This synthesis will benefit\nresearchers new to the field and those seeking to deepen their understanding of\nhow KGs and LLMs can be effectively combined to enhance AI applications\ncapabilities.\n","authors":["Amanda Kau","Xuzeng He","Aishwarya Nambissan","Aland Astudillo","Hui Yin","Amir Aryani"],"pdf_url":"https://arxiv.org/pdf/2407.06564v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.03580v2","updated":"2024-07-09T18:54:59Z","published":"2024-07-04T02:19:49Z","title":"Deep Pareto Reinforcement Learning for Multi-Objective Recommender\n  Systems","summary":"  Optimizing multiple objectives simultaneously is an important task for\nrecommendation platforms to improve their performance. However, this task is\nparticularly challenging since the relationships between different objectives\nare heterogeneous across different consumers and dynamically fluctuating\naccording to different contexts. Especially in those cases when objectives\nbecome conflicting with each other, the result of recommendations will form a\npareto-frontier, where the improvements of any objective comes at the cost of a\nperformance decrease of another objective. Existing multi-objective recommender\nsystems do not systematically consider such dynamic relationships; instead,\nthey balance between these objectives in a static and uniform manner, resulting\nin only suboptimal multi-objective recommendation performance. In this paper,\nwe propose a Deep Pareto Reinforcement Learning (DeepPRL) approach, where we\n(1) comprehensively model the complex relationships between multiple objectives\nin recommendations; (2) effectively capture personalized and contextual\nconsumer preference for each objective to provide better recommendations; (3)\noptimize both the short-term and the long-term performance of multi-objective\nrecommendations. As a result, our method achieves significant pareto-dominance\nover the state-of-the-art baselines in the offline experiments. Furthermore, we\nconducted a controlled experiment at the video streaming platform of Alibaba,\nwhere our method simultaneously improved three conflicting business objectives\nover the latest production system significantly, demonstrating its tangible\neconomic impact in practice.\n","authors":["Pan Li","Alexander Tuzhilin"],"pdf_url":"https://arxiv.org/pdf/2407.03580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07181v1","updated":"2024-07-09T18:49:32Z","published":"2024-07-09T18:49:32Z","title":"Multi-objective Learning to Rank by Model Distillation","summary":"  In online marketplaces, search ranking's objective is not only to purchase or\nconversion (primary objective), but to also the purchase outcomes(secondary\nobjectives), e.g. order cancellation(or return), review rating, customer\nservice inquiries, platform long term growth. Multi-objective learning to rank\nhas been widely studied to balance primary and secondary objectives. But\ntraditional approaches in industry face some challenges including expensive\nparameter tuning leads to sub-optimal solution, suffering from imbalanced data\nsparsity issue, and being not compatible with ad-hoc objective. In this paper,\nwe propose a distillation-based ranking solution for multi-objective ranking,\nwhich optimizes the end-to-end ranking system at Airbnb across multiple ranking\nmodels on different objectives along with various considerations to optimize\ntraining and serving efficiency to meet industry standards. We found it\nperforms much better than traditional approaches, it doesn't only significantly\nincreases primary objective by a large margin but also meet secondary\nobjectives constraints and improve model stability. We also demonstrated the\nproposed system could be further simplified by model self-distillation. Besides\nthis, we did additional simulations to show that this approach could also help\nus efficiently inject ad-hoc non-differentiable business objective into the\nranking system while enabling us to balance our optimization objectives.\n","authors":["Jie Tang","Huiji Gao","Liwei He","Sanjeev Katariya"],"pdf_url":"https://arxiv.org/pdf/2407.07181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04577v2","updated":"2024-07-09T16:58:07Z","published":"2024-07-05T15:12:14Z","title":"Optimizing Nepali PDF Extraction: A Comparative Study of Parser and OCR\n  Technologies","summary":"  This research compares PDF parsing and Optical Character Recognition (OCR)\nmethods for extracting Nepali content from PDFs. PDF parsing offers fast and\naccurate extraction but faces challenges with non-Unicode Nepali fonts. OCR,\nspecifically PyTesseract, overcomes these challenges, providing versatility for\nboth digital and scanned PDFs. The study reveals that while PDF parsers are\nfaster, their accuracy fluctuates based on PDF types. In contrast, OCRs, with a\nfocus on PyTesseract, demonstrate consistent accuracy at the expense of\nslightly longer extraction times. Considering the project's emphasis on Nepali\nPDFs, PyTesseract emerges as the most suitable library, balancing extraction\nspeed and accuracy.\n","authors":["Prabin Paudel","Supriya Khadka","Ranju G. C.","Rahul Shah"],"pdf_url":"https://arxiv.org/pdf/2407.04577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07004v1","updated":"2024-07-09T16:17:16Z","published":"2024-07-09T16:17:16Z","title":"Empirical analysis of Biding Precedent efficiency in the Brazilian\n  Supreme Court via Similar Case Retrieval","summary":"  Binding precedents (S\\'umulas Vinculantes) constitute a juridical instrument\nunique to the Brazilian legal system and whose objectives include the\nprotection of the Federal Supreme Court against repetitive demands. Studies of\nthe effectiveness of these instruments in decreasing the Court's exposure to\nsimilar cases, however, indicate that they tend to fail in such a direction,\nwith some of the binding precedents seemingly creating new demands. We\nempirically assess the legal impact of five binding precedents, 11, 14, 17, 26\nand 37, at the highest court level through their effects on the legal subjects\nthey address. This analysis is only possible through the comparison of the\nCourt's ruling about the precedents' themes before they are created, which\nmeans that these decisions should be detected through techniques of Similar\nCase Retrieval. The contributions of this article are therefore twofold: on the\nmathematical side, we compare the uses of different methods of Natural Language\nProcessing -- TF-IDF, LSTM, BERT, and regex -- for Similar Case Retrieval,\nwhereas on the legal side, we contrast the inefficiency of these binding\nprecedents with a set of hypotheses that may justify their repeated usage. We\nobserve that the deep learning models performed significantly worse in the\nspecific Similar Case Retrieval task and that the reasons for binding\nprecedents to fail in responding to repetitive demand are heterogeneous and\ncase-dependent, making it impossible to single out a specific cause.\n","authors":["RaphaÃ«l Tinarrage","Henrique Ennes","Lucas E. Resck","Lucas T. Gomes","Jean R. Ponciano","Jorge Poco"],"pdf_url":"https://arxiv.org/pdf/2407.07004v1.pdf","comment":"54 pages, 22 figures"},{"id":"http://arxiv.org/abs/2407.06992v1","updated":"2024-07-09T16:07:01Z","published":"2024-07-09T16:07:01Z","title":"Robust Neural Information Retrieval: An Adversarial and\n  Out-of-distribution Perspective","summary":"  Recent advances in neural information retrieval (IR) models have\nsignificantly enhanced their effectiveness over various IR tasks. The\nrobustness of these models, essential for ensuring their reliability in\npractice, has also garnered significant attention. With a wide array of\nresearch on robust IR being proposed, we believe it is the opportune moment to\nconsolidate the current status, glean insights from existing methodologies, and\nlay the groundwork for future development. We view the robustness of IR to be a\nmultifaceted concept, emphasizing its necessity against adversarial attacks,\nout-of-distribution (OOD) scenarios and performance variance. With a focus on\nadversarial and OOD robustness, we dissect robustness solutions for dense\nretrieval models (DRMs) and neural ranking models (NRMs), respectively,\nrecognizing them as pivotal components of the neural IR pipeline. We provide an\nin-depth discussion of existing methods, datasets, and evaluation metrics,\nshedding light on challenges and future directions in the era of large language\nmodels. To the best of our knowledge, this is the first comprehensive survey on\nthe robustness of neural IR models, and we will also be giving our first\ntutorial presentation at SIGIR 2024\n\\url{https://sigir2024-robust-information-retrieval.github.io}. Along with the\norganization of existing work, we introduce a Benchmark for robust IR (BestIR),\na heterogeneous evaluation benchmark for robust neural information retrieval,\nwhich is publicly available at \\url{https://github.com/Davion-Liu/BestIR}. We\nhope that this study provides useful clues for future research on the\nrobustness of IR models and helps to develop trustworthy search engines\n\\url{https://github.com/Davion-Liu/Awesome-Robustness-in-Information-Retrieval}.\n","authors":["Yu-An Liu","Ruqing Zhang","Jiafeng Guo","Maarten de Rijke","Yixing Fan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.06992v1.pdf","comment":"Survey paper"},{"id":"http://arxiv.org/abs/2407.06910v1","updated":"2024-07-09T14:46:09Z","published":"2024-07-09T14:46:09Z","title":"Fine-grained large-scale content recommendations for MSX sellers","summary":"  One of the most critical tasks of Microsoft sellers is to meticulously track\nand nurture potential business opportunities through proactive engagement and\ntailored solutions. Recommender systems play a central role to help sellers\nachieve their goals. In this paper, we present a content recommendation model\nwhich surfaces various types of content (technical documentation, comparison\nwith competitor products, customer success stories etc.) that sellers can share\nwith their customers or use for their own self-learning. The model operates at\nthe opportunity level which is the lowest possible granularity and the most\nrelevant one for sellers. It is based on semantic matching between metadata\nfrom the contents and carefully selected attributes of the opportunities.\nConsidering the volume of seller-managed opportunities in organizations such as\nMicrosoft, we show how to perform efficient semantic matching over a very large\nnumber of opportunity-content combinations. The main challenge is to ensure\nthat the top-5 relevant contents for each opportunity are recommended out of a\ntotal of $\\approx 40,000$ published contents. We achieve this target through an\nextensive comparison of different model architectures and feature selection.\nFinally, we further examine the quality of the recommendations in a\nquantitative manner using a combination of human domain experts as well as by\nusing the recently proposed \"LLM as a judge\" framework.\n","authors":["Manpreet Singh","Ravdeep Pasricha","Ravi Prasad Kondapalli","Kiran R","Nitish Singh","Akshita Agarwalla","Manoj R","Manish Prabhakar","Laurent BouÃ©"],"pdf_url":"https://arxiv.org/pdf/2407.06910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04472v3","updated":"2024-07-09T13:31:00Z","published":"2024-07-05T12:42:31Z","title":"EventChat: Implementation and user-centric evaluation of a large\n  language model-driven conversational recommender system for exploring leisure\n  events in an SME context","summary":"  Large language models (LLMs) present an enormous evolution in the strategic\npotential of conversational recommender systems (CRS). Yet to date, research\nhas predominantly focused upon technical frameworks to implement LLM-driven\nCRS, rather than end-user evaluations or strategic implications for firms,\nparticularly from the perspective of a small to medium enterprises (SME) that\nmakeup the bedrock of the global economy. In the current paper, we detail the\ndesign of an LLM-driven CRS in an SME setting, and its subsequent performance\nin the field using both objective system metrics and subjective user\nevaluations. While doing so, we additionally outline a short-form revised\nResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly\nevolving field. Our results reveal good system performance from a user\nexperience perspective (85.5% recommendation accuracy) but underscore latency,\ncost, and quality issues challenging business viability. Notably, with a median\ncost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and\nresponse time emerge as crucial areas for achieving a more user-friendly and\neconomically viable LLM-driven CRS for SME settings. One major driver of these\ncosts is the use of an advanced LLM as a ranker within the retrieval-augmented\ngeneration (RAG) technique. Our results additionally indicate that relying\nsolely on approaches such as Prompt-based learning with ChatGPT as the\nunderlying LLM makes it challenging to achieve satisfying quality in a\nproduction environment. Strategic considerations for SMEs deploying an\nLLM-driven CRS are outlined, particularly considering trade-offs in the current\ntechnical landscape.\n","authors":["Hannes Kunstmann","Joseph Ollier","Joel Persson","Florian von Wangenheim"],"pdf_url":"https://arxiv.org/pdf/2407.04472v3.pdf","comment":"27 pages, 3 tables, 5 figures, pre-print manuscript, updated version\n  of manuscript due to typo (previous version, Figure 5 was incorrectly named\n  Figure 6)"},{"id":"http://arxiv.org/abs/2306.05817v6","updated":"2024-07-09T13:17:52Z","published":"2023-06-09T11:31:50Z","title":"How Can Recommender Systems Benefit from Large Language Models: A Survey","summary":"  With the rapid development of online services, recommender systems (RS) have\nbecome increasingly indispensable for mitigating information overload. Despite\nremarkable progress, conventional recommendation models (CRM) still have some\nlimitations, e.g., lacking open-world knowledge, and difficulties in\ncomprehending users' underlying preferences and motivations. Meanwhile, large\nlanguage models (LLM) have shown impressive general intelligence and human-like\ncapabilities, which mainly stem from their extensive open-world knowledge,\nreasoning ability, as well as their comprehension of human culture and society.\nConsequently, the emergence of LLM is inspiring the design of recommender\nsystems and pointing out a promising research direction, i.e., whether we can\nincorporate LLM and benefit from their knowledge and capabilities to compensate\nfor the limitations of CRM. In this paper, we conduct a comprehensive survey on\nthis research direction from the perspective of the whole pipeline in\nreal-world recommender systems. Specifically, we summarize existing works from\ntwo orthogonal aspects: where and how to adapt LLM to RS. For the WHERE\nquestion, we discuss the roles that LLM could play in different stages of the\nrecommendation pipeline, i.e., feature engineering, feature encoder,\nscoring/ranking function, user interaction, and pipeline controller. For the\nHOW question, we investigate the training and inference strategies, resulting\nin two fine-grained taxonomy criteria, i.e., whether to tune LLM or not, and\nwhether to involve conventional recommendation models for inference. Then, we\nhighlight key challenges in adapting LLM to RS from three aspects, i.e.,\nefficiency, effectiveness, and ethics. Finally, we summarize the survey and\ndiscuss the future prospects. We actively maintain a GitHub repository for\npapers and other related resources:\nhttps://github.com/CHIANGEL/Awesome-LLM-for-RecSys/.\n","authors":["Jianghao Lin","Xinyi Dai","Yunjia Xi","Weiwen Liu","Bo Chen","Hao Zhang","Yong Liu","Chuhan Wu","Xiangyang Li","Chenxu Zhu","Huifeng Guo","Yong Yu","Ruiming Tang","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.05817v6.pdf","comment":"Accepted by ACM Transactions on Information Systems (TOIS); Look-up\n  table in appendix"},{"id":"http://arxiv.org/abs/2104.12333v2","updated":"2024-07-09T12:06:39Z","published":"2021-04-26T03:37:22Z","title":"Explore BiLSTM-CRF-Based Models for Open Relation Extraction","summary":"  Extracting multiple relations from text sentences is still a challenge for\ncurrent Open Relation Extraction (Open RE) tasks. In this paper, we develop\nseveral Open RE models based on the bidirectional LSTM-CRF (BiLSTM-CRF) neural\nnetwork and different contextualized word embedding methods. We also propose a\nnew tagging scheme to solve overlapping problems and enhance models'\nperformance. From the evaluation results and comparisons between models, we\nselect the best combination of tagging scheme, word embedder, and BiLSTM-CRF\nnetwork to achieve an Open RE model with a remarkable extracting ability on\nmultiple-relation sentences.\n","authors":["Tao Ni","Qing Wang","Gabriela Ferraro"],"pdf_url":"https://arxiv.org/pdf/2104.12333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06740v1","updated":"2024-07-09T10:40:31Z","published":"2024-07-09T10:40:31Z","title":"Positive-Unlabelled Learning for Improving Image-based Recommender\n  System Explainability","summary":"  Among the existing approaches for visual-based Recommender System (RS)\nexplainability, utilizing user-uploaded item images as efficient, trustable\nexplanations is a promising option. However, current models following this\nparadigm assume that, for any user, all images uploaded by other users can be\nconsidered negative training examples (i.e. bad explanatory images), an\ninadvertedly naive labelling assumption that contradicts the rationale of the\napproach. This work proposes a new explainer training pipeline by leveraging\nPositive-Unlabelled (PU) Learning techniques to train image-based explainer\nwith refined subsets of reliable negative examples for each user selected\nthrough a novel user-personalized, two-step, similarity-based PU Learning\nalgorithm. Computational experiments show this PU-based approach outperforms\nthe state-of-the-art non-PU method in six popular real-world datasets, proving\nthat an improvement of visual-based RS explainability can be achieved by\nmaximizing training data quality rather than increasing model complexity.\n","authors":["Ãlvaro FernÃ¡ndez-Campa-GonzÃ¡lez","Jorge Paz-Ruza","Amparo Alonso-Betanzos","Bertha Guijarro-BerdiÃ±as"],"pdf_url":"https://arxiv.org/pdf/2407.06740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11180v2","updated":"2024-07-09T10:27:55Z","published":"2024-04-17T08:50:29Z","title":"Causal Deconfounding via Confounder Disentanglement for Dual-Target\n  Cross-Domain Recommendation","summary":"  In recent years, dual-target Cross-Domain Recommendation (CDR) has been\nproposed to capture comprehensive user preferences in order to ultimately\nenhance the recommendation accuracy in both data-richer and data-sparser\ndomains simultaneously. However, in addition to users' true preferences, the\nuser-item interactions might also be affected by confounders (e.g., free\nshipping, sales promotion). As a result, dual-target CDR has to meet two\nchallenges: (1) how to effectively decouple observed confounders, including\nsingle-domain confounders and cross-domain confounders, and (2) how to preserve\nthe positive effects of observed confounders on predicted interactions, while\neliminating their negative effects on capturing comprehensive user preferences.\nTo address the above two challenges, we propose a Causal Deconfounding\nframework via Confounder Disentanglement for dual-target Cross-Domain\nRecommendation, called CD2CDR. In CD2CDR, we first propose a confounder\ndisentanglement module to effectively decouple observed single-domain and\ncross-domain confounders. We then propose a causal deconfounding module to\npreserve the positive effects of such observed confounders and eliminate their\nnegative effects via backdoor adjustment, thereby enhancing the recommendation\naccuracy in each domain. Extensive experiments conducted on five real-world\ndatasets demonstrate that CD2CDR significantly outperforms the state-of-the-art\nmethods.\n","authors":["Jiajie Zhu","Yan Wang","Feng Zhu","Zhu Sun"],"pdf_url":"https://arxiv.org/pdf/2404.11180v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2407.06716v1","updated":"2024-07-09T09:43:42Z","published":"2024-07-09T09:43:42Z","title":"Analyzing the Effectiveness of Listwise Reranking with Positional\n  Invariance on Temporal Generalizability","summary":"  Benchmarking the performance of information retrieval (IR) methods are mostly\nconducted within a fixed set of documents (static corpora). However, in\nreal-world web search engine environments, the document set is continuously\nupdated and expanded. Addressing these discrepancies and measuring the temporal\npersistence of IR systems is crucial. By investigating the LongEval benchmark,\nspecifically designed for such dynamic environments, our findings demonstrate\nthe effectiveness of a listwise reranking approach, which proficiently handles\ninaccuracies induced by temporal distribution shifts. Among listwise rerankers,\nour findings show that ListT5, which effectively mitigates the positional bias\nproblem by adopting the Fusion-in-Decoder architecture, is especially\neffective, and more so, as temporal drift increases, on the test-long subset.\n","authors":["Soyoung Yoon","Jongyoon Kim","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2407.06716v1.pdf","comment":"Accepted at CLEF 2024 LongEval track"},{"id":"http://arxiv.org/abs/2407.06685v1","updated":"2024-07-09T09:00:18Z","published":"2024-07-09T09:00:18Z","title":"Embark on DenseQuest: A System for Selecting the Best Dense Retriever\n  for a Custom Collection","summary":"  In this demo we present a web-based application for selecting an effective\npre-trained dense retriever to use on a private collection. Our system,\nDenseQuest, provides unsupervised selection and ranking capabilities to predict\nthe best dense retriever among a pool of available dense retrievers, tailored\nto an uploaded target collection. DenseQuest implements a number of existing\napproaches, including a recent, highly effective method powered by Large\nLanguage Models (LLMs), which requires neither queries nor relevance judgments.\nThe system is designed to be intuitive and easy to use for those information\nretrieval engineers and researchers who need to identify a general-purpose\ndense retrieval model to encode or search a new private target collection. Our\ndemonstration illustrates conceptual architecture and the different use case\nscenarios of the system implemented on the cloud, enabling universal access and\nuse. DenseQuest is available at https://densequest.ielab.io.\n","authors":["Ekaterina Khramtsova","Teerapong Leelanupab","Shengyao Zhuang","Mahsa Baktashmotlagh","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2407.06685v1.pdf","comment":"SIGIR2024 demo paper"},{"id":"http://arxiv.org/abs/2405.11531v2","updated":"2024-07-09T08:57:52Z","published":"2024-05-19T12:07:24Z","title":"Knowledge Graph Pruning for Recommendation","summary":"  Recent years have witnessed the prosperity of knowledge graph based\nrecommendation system (KGRS), which enriches the representation of users,\nitems, and entities by structural knowledge with striking improvement.\nNevertheless, its unaffordable computational cost still limits researchers from\nexploring more sophisticated models. We observe that the bottleneck for\ntraining efficiency arises from the knowledge graph, which is plagued by the\nwell-known issue of knowledge explosion. Recently, some works have attempted to\nslim the inflated KG via summarization techniques. However, these summarized\nnodes may ignore the collaborative signals and deviate from the facts that\nnodes in knowledge graph represent symbolic abstractions of entities from the\nreal-world. To this end, in this paper, we propose a novel approach called\nKGTrimmer for knowledge graph pruning tailored for recommendation, to remove\nthe unessential nodes while minimizing performance degradation. Specifically,\nwe design an importance evaluator from a dual-view perspective. For the\ncollective view, we embrace the idea of collective intelligence by extracting\ncommunity consensus based on abundant collaborative signals, i.e. nodes are\nconsidered important if they attract attention of numerous users. For the\nholistic view, we learn a global mask to identify the valueless nodes from\ntheir inherent properties or overall popularity. Next, we build an end-to-end\nimportance-aware graph neural network, which injects filtered knowledge to\nenhance the distillation of valuable user-item collaborative signals.\nUltimately, we generate a pruned knowledge graph with lightweight, stable, and\nrobust properties to facilitate the following-up recommendation task. Extensive\nexperiments are conducted on three publicly available datasets to prove the\neffectiveness and generalization ability of KGTrimmer.\n","authors":["Fake Lin","Xi Zhu","Ziwei Zhao","Deqiang Huang","Yu Yu","Xueying Li","Zhi Zheng","Tong Xu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2405.11531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06549v1","updated":"2024-07-09T05:13:45Z","published":"2024-07-09T05:13:45Z","title":"AutoTask: Task Aware Multi-Faceted Single Model for Multi-Task Ads\n  Relevance","summary":"  Ads relevance models are crucial in determining the relevance between user\nsearch queries and ad offers, often framed as a classification problem. The\ncomplexity of modeling increases significantly with multiple ad types and\nvarying scenarios that exhibit both similarities and differences. In this work,\nwe introduce a novel multi-faceted attention model that performs task aware\nfeature combination and cross task interaction modeling. Our technique\nformulates the feature combination problem as \"language\" modeling with\nauto-regressive attentions across both feature and task dimensions.\nSpecifically, we introduce a new dimension of task ID encoding for task\nrepresentations, thereby enabling precise relevance modeling across diverse ad\nscenarios with substantial improvement in generality capability for unseen\ntasks. We demonstrate that our model not only effectively handles the increased\ncomputational and maintenance demands as scenarios proliferate, but also\noutperforms generalized DNN models and even task-specific models across a\nspectrum of ad applications using a single unified model.\n","authors":["Shouchang Guo","Sonam Damani","Keng-hao Chang"],"pdf_url":"https://arxiv.org/pdf/2407.06549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15136v2","updated":"2024-07-09T02:41:11Z","published":"2023-08-29T09:10:53Z","title":"CAGRA: Highly Parallel Graph Construction and Approximate Nearest\n  Neighbor Search for GPUs","summary":"  Approximate Nearest Neighbor Search (ANNS) plays a critical role in various\ndisciplines spanning data mining and artificial intelligence, from information\nretrieval and computer vision to natural language processing and recommender\nsystems. Data volumes have soared in recent years and the computational cost of\nan exhaustive exact nearest neighbor search is often prohibitive, necessitating\nthe adoption of approximate techniques. The balanced performance and recall of\ngraph-based approaches have more recently garnered significant attention in\nANNS algorithms, however, only a few studies have explored harnessing the power\nof GPUs and multi-core processors despite the widespread use of massively\nparallel and general-purpose computing. To bridge this gap, we introduce a\nnovel parallel computing hardware-based proximity graph and search algorithm.\nBy leveraging the high-performance capabilities of modern hardware, our\napproach achieves remarkable efficiency gains. In particular, our method\nsurpasses existing CPU and GPU-based methods in constructing the proximity\ngraph, demonstrating higher throughput in both large- and small-batch searches\nwhile maintaining compatible accuracy. In graph construction time, our method,\nCAGRA, is 2.2~27x faster than HNSW, which is one of the CPU SOTA\nimplementations. In large-batch query throughput in the 90% to 95% recall\nrange, our method is 33~77x faster than HNSW, and is 3.8~8.8x faster than the\nSOTA implementations for GPU. For a single query, our method is 3.4~53x faster\nthan HNSW at 95% recall.\n","authors":["Hiroyuki Ootomo","Akira Naruse","Corey Nolet","Ray Wang","Tamas Feher","Yong Wang"],"pdf_url":"https://arxiv.org/pdf/2308.15136v2.pdf","comment":"Accepted to ICDE 2024"},{"id":"http://arxiv.org/abs/2404.08860v3","updated":"2024-07-09T01:14:21Z","published":"2024-04-13T00:20:09Z","title":"Enhancing Mobile \"How-to\" Queries with Automated Search Results\n  Verification and Reranking","summary":"  Many people use search engines to find online guidance to solve computer or\nmobile device problems. Users frequently encounter challenges in identifying\neffective solutions from search results, often wasting time trying ineffective\nsolutions that seem relevant yet fail to solve real problems. This paper\nintroduces a novel approach to improving the accuracy and relevance of online\ntechnical support search results through automated search results verification\nand reranking. Taking \"How-to\" queries specific to on-device execution as a\nstarting point, we developed the first solution that allows an AI agent to\ninterpret and execute step-by-step instructions in the search results in a\ncontrolled Android environment. We further integrated the agent's findings into\na reranking mechanism that orders search results based on the success\nindicators of the tested solutions.\n  The paper details the architecture of our solution and a comprehensive\nevaluation of the system through a series of tests across various application\ndomains. The results demonstrate a significant improvement in the quality and\nreliability of the top-ranked results. Our findings suggest a paradigm shift in\nhow search engine ranking for online technical support help can be optimized,\noffering a scalable and automated solution to the pervasive challenge of\nfinding effective and reliable online help.\n","authors":["Lei Ding","Jeshwanth Bheemanpally","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.08860v3.pdf","comment":"13 pages, 3 figures, Gen-IR@SIGIR2024 workshop"},{"id":"http://arxiv.org/abs/2407.07925v1","updated":"2024-07-09T10:58:46Z","published":"2024-07-09T10:58:46Z","title":"Enhancing Social Media Personalization: Dynamic User Profile Embeddings\n  and Multimodal Contextual Analysis Using Transformer Models","summary":"  This study investigates the impact of dynamic user profile embedding on\npersonalized context-aware experiences in social networks. A comparative\nanalysis of multilingual and English transformer models was performed on a\ndataset of over twenty million data points. The analysis included a wide range\nof metrics and performance indicators to compare dynamic profile embeddings\nversus non-embeddings (effectively static profile embeddings). A comparative\nstudy using degradation functions was conducted. Extensive testing and research\nconfirmed that dynamic embedding successfully tracks users' changing tastes and\npreferences, providing more accurate recommendations and higher user\nengagement. These results are important for social media platforms aiming to\nimprove user experience through relevant features and sophisticated\nrecommendation engines.\n","authors":["Pranav Vachharajani"],"pdf_url":"https://arxiv.org/pdf/2407.07925v1.pdf","comment":"21 pages, 13 figures. Mentor: Prof Pritam Ranjan"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.07277v1","updated":"2024-07-09T23:52:53Z","published":"2024-07-09T23:52:53Z","title":"Lifestyle-Informed Personalized Blood Biomarker Prediction via Novel\n  Representation Learning","summary":"  Blood biomarkers are an essential tool for healthcare providers to diagnose,\nmonitor, and treat a wide range of medical conditions. Current reference values\nand recommended ranges often rely on population-level statistics, which may not\nadequately account for the influence of inter-individual variability driven by\nfactors such as lifestyle and genetics. In this work, we introduce a novel\nframework for predicting future blood biomarker values and define personalized\nreferences through learned representations from lifestyle data (physical\nactivity and sleep) and blood biomarkers. Our proposed method learns a\nsimilarity-based embedding space that captures the complex relationship between\nbiomarkers and lifestyle factors. Using the UK Biobank (257K participants), our\nresults show that our deep-learned embeddings outperform traditional and\ncurrent state-of-the-art representation learning techniques in predicting\nclinical diagnosis. Using a subset of UK Biobank of 6440 participants who have\nfollow-up visits, we validate that the inclusion of these embeddings and\nlifestyle factors directly in blood biomarker models improves the prediction of\nfuture lab values from a single lab visit. This personalized modeling approach\nprovides a foundation for developing more accurate risk stratification tools\nand tailoring preventative care strategies. In clinical settings, this\ntranslates to the potential for earlier disease detection, more timely\ninterventions, and ultimately, a shift towards personalized healthcare.\n","authors":["A. Ali Heydari","Naghmeh Rezaei","Javier L. Prieto","Shwetak N. Patel","Ahmed A. Metwally"],"pdf_url":"https://arxiv.org/pdf/2407.07277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03404v3","updated":"2024-07-09T23:45:34Z","published":"2023-12-06T10:40:33Z","title":"A cyclical route linking fundamental mechanism and AI algorithm: An\n  example from tuning Poisson's ratio in amorphous networks","summary":"  \"AI for science\" is widely recognized as a future trend in the development of\nscientific research. Currently, although machine learning algorithms have\nplayed a crucial role in scientific research with numerous successful cases,\nrelatively few instances exist where AI assists researchers in uncovering the\nunderlying physical mechanisms behind a certain phenomenon and subsequently\nusing that mechanism to improve machine learning algorithms' efficiency. This\narticle uses the investigation into the relationship between extreme Poisson's\nratio values and the structure of amorphous networks as a case study to\nillustrate how machine learning methods can assist in revealing underlying\nphysical mechanisms. Upon recognizing that the Poisson's ratio relies on the\nlow-frequency vibrational modes of dynamical matrix, we can then employ a\nconvolutional neural network, trained on the dynamical matrix instead of\ntraditional image recognition, to predict the Poisson's ratio of amorphous\nnetworks with a much higher efficiency. Through this example, we aim to\nshowcase the role that artificial intelligence can play in revealing\nfundamental physical mechanisms, which subsequently improves the machine\nlearning algorithms significantly.\n","authors":["Changliang Zhu","Chenchao Fang","Zhipeng Jin","Baowen Li","Xiangying Shen","Lei Xu"],"pdf_url":"https://arxiv.org/pdf/2312.03404v3.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.07275v1","updated":"2024-07-09T23:39:37Z","published":"2024-07-09T23:39:37Z","title":"Remastering Divide and Remaster: A Cinematic Audio Source Separation\n  Dataset with Multilingual Support","summary":"  Cinematic audio source separation (CASS) is a relatively new subtask of audio\nsource separation, concerned with the separation of a mixture into the\ndialogue, music, and effects stems. To date, only one publicly available\ndataset exists for CASS, that is, the Divide and Remaster (DnR) dataset, which\nis currently at version 2. While DnR v2 has been an incredibly useful resource\nfor CASS, several areas of improvement have been identified, particularly\nthrough its use in the 2023 Sound Demixing Challenge. In this work, we develop\nversion 3 of the DnR dataset, addressing issues relating to vocal content in\nnon-dialogue stems, loudness distributions, mastering process, and linguistic\ndiversity. In particular, the dialogue stem of DnR v3 includes speech content\nfrom more than 30 languages from multiple families including but not limited to\nthe Germanic, Romance, Indo-Aryan, Dravidian, Malayo-Polynesian, and Bantu\nfamilies. Benchmark results using the Bandit model indicated that training on\nmultilingual data yields significant generalizability to the model even in\nlanguages with low data availability. Even in languages with high data\navailability, the multilingual model often performs on par or better than\ndedicated models trained on monolingual CASS datasets.\n","authors":["Karn N. Watcharasupat","Chih-Wei Wu","Iroro Orife"],"pdf_url":"https://arxiv.org/pdf/2407.07275v1.pdf","comment":"Submitted to the 5th IEEE International Symposium on the Internet of\n  Sounds"},{"id":"http://arxiv.org/abs/2306.04718v2","updated":"2024-07-09T23:24:53Z","published":"2023-06-07T18:30:25Z","title":"Scalable Neural Symbolic Regression using Control Variables","summary":"  Symbolic regression (SR) is a powerful technique for discovering the\nanalytical mathematical expression from data, finding various applications in\nnatural sciences due to its good interpretability of results. However, existing\nmethods face scalability issues when dealing with complex equations involving\nmultiple variables. To address this challenge, we propose ScaleSR, a scalable\nsymbolic regression model that leverages control variables to enhance both\naccuracy and scalability. The core idea is to decompose multi-variable symbolic\nregression into a set of single-variable SR problems, which are then combined\nin a bottom-up manner. The proposed method involves a four-step process. First,\nwe learn a data generator from observed data using deep neural networks (DNNs).\nSecond, the data generator is used to generate samples for a certain variable\nby controlling the input variables. Thirdly, single-variable symbolic\nregression is applied to estimate the corresponding mathematical expression.\nLastly, we repeat steps 2 and 3 by gradually adding variables one by one until\ncompletion. We evaluate the performance of our method on multiple benchmark\ndatasets. Experimental results demonstrate that the proposed ScaleSR\nsignificantly outperforms state-of-the-art baselines in discovering\nmathematical expressions with multiple variables. Moreover, it can\nsubstantially reduce the search space for symbolic regression. The source code\nwill be made publicly available upon publication.\n","authors":["Xieting Chu","Hongjue Zhao","Enze Xu","Hairong Qi","Minghan Chen","Huajie Shao"],"pdf_url":"https://arxiv.org/pdf/2306.04718v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06903v2","updated":"2024-07-09T23:20:12Z","published":"2024-03-11T16:56:01Z","title":"Benign overfitting in leaky ReLU networks with moderate input dimension","summary":"  The problem of benign overfitting asks whether it is possible for a model to\nperfectly fit noisy training data and still generalize well. We study benign\noverfitting in two-layer leaky ReLU networks trained with the hinge loss on a\nbinary classification task. We consider input data that can be decomposed into\nthe sum of a common signal and a random noise component, that lie on subspaces\northogonal to one another. We characterize conditions on the signal to noise\nratio (SNR) of the model parameters giving rise to benign versus non-benign (or\nharmful) overfitting: in particular, if the SNR is high then benign overfitting\noccurs, conversely if the SNR is low then harmful overfitting occurs. We\nattribute both benign and non-benign overfitting to an approximate margin\nmaximization property and show that leaky ReLU networks trained on hinge loss\nwith gradient descent (GD) satisfy this property. In contrast to prior work we\ndo not require the training data to be nearly orthogonal. Notably, for input\ndimension $d$ and training sample size $n$, while results in prior work require\n$d = \\Omega(n^2 \\log n)$, here we require only $d = \\Omega\\left(n\\right)$.\n","authors":["Kedar Karhadkar","Erin George","Michael Murray","Guido MontÃºfar","Deanna Needell"],"pdf_url":"https://arxiv.org/pdf/2403.06903v2.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2407.07258v1","updated":"2024-07-09T22:26:42Z","published":"2024-07-09T22:26:42Z","title":"Identification of emotions on Twitter during the 2022 electoral process\n  in Colombia","summary":"  The study of Twitter as a means for analyzing social phenomena has gained\ninterest in recent years due to the availability of large amounts of data in a\nrelatively spontaneous environment. Within opinion-mining tasks, emotion\ndetection is specially relevant, as it allows for the identification of\npeople's subjective responses to different social events in a more granular way\nthan traditional sentiment analysis based on polarity. In the particular case\nof political events, the analysis of emotions in social networks can provide\nvaluable information on the perception of candidates, proposals, and other\nimportant aspects of the public debate. In spite of this importance, there are\nfew studies on emotion detection in Spanish and, to the best of our knowledge,\nfew resources are public for opinion mining in Colombian Spanish, highlighting\nthe need for generating resources addressing the specific cultural\ncharacteristics of this variety. In this work, we present a small corpus of\ntweets in Spanish related to the 2022 Colombian presidential elections,\nmanually labeled with emotions using a fine-grained taxonomy. We perform\nclassification experiments using supervised state-of-the-art models (BERT\nmodels) and compare them with GPT-3.5 in few-shot learning settings. We make\nour dataset and code publicly available for research purposes.\n","authors":["Juan Jose Iguaran Fernandez","Juan Manuel Perez","German Rosati"],"pdf_url":"https://arxiv.org/pdf/2407.07258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17886v5","updated":"2024-07-09T22:15:31Z","published":"2024-03-26T17:19:23Z","title":"Neural Embedding Compression For Efficient Multi-Task Earth Observation\n  Modelling","summary":"  As repositories of large scale data in earth observation (EO) have grown, so\nhave transfer and storage costs for model training and inference, expending\nsignificant resources. We introduce Neural Embedding Compression (NEC), based\non the transfer of compressed embeddings to data consumers instead of raw data.\nWe adapt foundation models (FM) through learned neural compression to generate\nmulti-task embeddings while navigating the tradeoff between compression rate\nand embedding utility. We update only a small fraction of the FM parameters\n(10%) for a short training period (1% of the iterations of pre-training). We\nevaluate NEC on two EO tasks: scene classification and semantic segmentation.\nCompared with applying traditional compression to the raw data, NEC achieves\nsimilar accuracy with a 75% to 90% reduction in data. Even at 99.7%\ncompression, performance drops by only 5% on the scene classification task.\nOverall, NEC is a data-efficient yet performant approach for multi-task EO\nmodelling.\n","authors":["Carlos Gomes","Thomas Brunschwiler"],"pdf_url":"https://arxiv.org/pdf/2403.17886v5.pdf","comment":"Published at IGARSS 2024"},{"id":"http://arxiv.org/abs/2407.07239v1","updated":"2024-07-09T21:37:36Z","published":"2024-07-09T21:37:36Z","title":"RotRNN: Modelling Long Sequences with Rotations","summary":"  Linear recurrent models, such as State Space Models (SSMs) and Linear\nRecurrent Units (LRUs), have recently shown state-of-the-art performance on\nlong sequence modelling benchmarks. Despite their success, they come with a\nnumber of drawbacks, most notably their complex initialisation and\nnormalisation schemes. In this work, we address some of these issues by\nproposing RotRNN -- a linear recurrent model which utilises the convenient\nproperties of rotation matrices. We show that RotRNN provides a simple model\nwith fewer theoretical assumptions than prior works, with a practical\nimplementation that remains faithful to its theoretical derivation, achieving\ncomparable scores to the LRU and SSMs on several long sequence modelling\ndatasets.\n","authors":["Rares Dolga","Kai Biegun","Jake Cunningham","David Barber"],"pdf_url":"https://arxiv.org/pdf/2407.07239v1.pdf","comment":"Next Generation of Sequence Modeling Architectures Workshop at ICML\n  2024"},{"id":"http://arxiv.org/abs/2407.07237v1","updated":"2024-07-09T21:35:19Z","published":"2024-07-09T21:35:19Z","title":"The Quantum Imitation Game: Reverse Engineering of Quantum Machine\n  Learning Models","summary":"  Quantum Machine Learning (QML) amalgamates quantum computing paradigms with\nmachine learning models, providing significant prospects for solving complex\nproblems. However, with the expansion of numerous third-party vendors in the\nNoisy Intermediate-Scale Quantum (NISQ) era of quantum computing, the security\nof QML models is of prime importance, particularly against reverse engineering,\nwhich could expose trained parameters and algorithms of the models. We assume\nthe untrusted quantum cloud provider is an adversary having white-box access to\nthe transpiled user-designed trained QML model during inference. Reverse\nengineering (RE) to extract the pre-transpiled QML circuit will enable\nre-transpilation and usage of the model for various hardware with completely\ndifferent native gate sets and even different qubit technology. Such\nflexibility may not be obtained from the transpiled circuit which is tied to a\nparticular hardware and qubit technology. The information about the number of\nparameters, and optimized values can allow further training of the QML model to\nalter the QML model, tamper with the watermark, and/or embed their own\nwatermark or refine the model for other purposes. In this first effort to\ninvestigate the RE of QML circuits, we perform RE and compare the training\naccuracy of original and reverse-engineered Quantum Neural Networks (QNNs) of\nvarious sizes. We note that multi-qubit classifiers can be reverse-engineered\nunder specific conditions with a mean error of order 1e-2 in a reasonable time.\nWe also propose adding dummy fixed parametric gates in the QML models to\nincrease the RE overhead for defense. For instance, adding 2 dummy qubits and 2\nlayers increases the overhead by ~1.76 times for a classifier with 2 qubits and\n3 layers with a performance overhead of less than 9%. We note that RE is a very\npowerful attack model which warrants further efforts on defenses.\n","authors":["Archisman Ghosh","Swaroop Ghosh"],"pdf_url":"https://arxiv.org/pdf/2407.07237v1.pdf","comment":"10 pages, 12 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2406.14485v4","updated":"2024-07-09T17:43:06Z","published":"2024-06-20T16:48:14Z","title":"Proceedings of The second international workshop on eXplainable AI for\n  the Arts (XAIxArts)","summary":"  This second international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\nWorkshop held at the 16th ACM Conference on Creativity and Cognition (C&C\n2024), Chicago, USA.\n","authors":["Nick Bryan-Kinns","Corey Ford","Shuoyang Zheng","Helen Kennedy","Alan Chamberlain","Makayla Lewis","Drew Hemment","Zijin Li","Qiong Wu","Lanxi Xiao","Gus Xia","Jeba Rezwana","Michael Clemens","Gabriel Vigliensoni"],"pdf_url":"https://arxiv.org/pdf/2406.14485v4.pdf","comment":"Proceedings of The second international workshop on eXplainable AI\n  for the Arts (XAIxArts)"},{"id":"http://arxiv.org/abs/2407.07041v1","updated":"2024-07-09T17:03:57Z","published":"2024-07-09T17:03:57Z","title":"Hiding Local Manipulations on SAR Images: a Counter-Forensic Attack","summary":"  The vast accessibility of Synthetic Aperture Radar (SAR) images through\nonline portals has propelled the research across various fields. This\nwidespread use and easy availability have unfortunately made SAR data\nsusceptible to malicious alterations, such as local editing applied to the\nimages for inserting or covering the presence of sensitive targets.\nVulnerability is further emphasized by the fact that most SAR products, despite\ntheir original complex nature, are often released as amplitude-only\ninformation, allowing even inexperienced attackers to edit and easily alter the\npixel content. To contrast malicious manipulations, in the last years the\nforensic community has begun to dig into the SAR manipulation issue, proposing\ndetectors that effectively localize the tampering traces in amplitude images.\nNonetheless, in this paper we demonstrate that an expert practitioner can\nexploit the complex nature of SAR data to obscure any signs of manipulation\nwithin a locally altered amplitude image. We refer to this approach as a\ncounter-forensic attack. To achieve the concealment of manipulation traces, the\nattacker can simulate a re-acquisition of the manipulated scene by the SAR\nsystem that initially generated the pristine image. In doing so, the attacker\ncan obscure any evidence of manipulation, making it appear as if the image was\nlegitimately produced by the system. We assess the effectiveness of the\nproposed counter-forensic approach across diverse scenarios, examining various\nmanipulation operations. The obtained results indicate that our devised attack\nsuccessfully eliminates traces of manipulation, deceiving even the most\nadvanced forensic detectors.\n","authors":["Sara Mandelli","Edoardo Daniele Cannas","Paolo Bestagini","Stefano Tebaldini","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2407.07041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07026v1","updated":"2024-07-09T16:46:58Z","published":"2024-07-09T16:46:58Z","title":"Resolving Sentiment Discrepancy for Multimodal Sentiment Detection via\n  Semantics Completion and Decomposition","summary":"  With the proliferation of social media posts in recent years, the need to\ndetect sentiments in multimodal (image-text) content has grown rapidly. Since\nposts are user-generated, the image and text from the same post can express\ndifferent or even contradictory sentiments, leading to potential\n\\textbf{sentiment discrepancy}. However, existing works mainly adopt a\nsingle-branch fusion structure that primarily captures the consistent sentiment\nbetween image and text. The ignorance or implicit modeling of discrepant\nsentiment results in compromised unimodal encoding and limited performances. In\nthis paper, we propose a semantics Completion and Decomposition (CoDe) network\nto resolve the above issue. In the semantics completion module, we complement\nimage and text representations with the semantics of the OCR text embedded in\nthe image, helping bridge the sentiment gap. In the semantics decomposition\nmodule, we decompose image and text representations with exclusive projection\nand contrastive learning, thereby explicitly capturing the discrepant sentiment\nbetween modalities. Finally, we fuse image and text representations by\ncross-attention and combine them with the learned discrepant sentiment for\nfinal classification. Extensive experiments conducted on four multimodal\nsentiment datasets demonstrate the superiority of CoDe against SOTA methods.\n","authors":["Daiqing Wu","Dongbao Yang","Huawen Shen","Can Ma","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.07026v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.07015v1","updated":"2024-07-09T16:33:51Z","published":"2024-07-09T16:33:51Z","title":"A Framework for Multimodal Medical Image Interaction","summary":"  Medical doctors rely on images of the human anatomy, such as magnetic\nresonance imaging (MRI), to localize regions of interest in the patient during\ndiagnosis and treatment. Despite advances in medical imaging technology, the\ninformation conveyance remains unimodal. This visual representation fails to\ncapture the complexity of the real, multisensory interaction with human tissue.\nHowever, perceiving multimodal information about the patient's anatomy and\ndisease in real-time is critical for the success of medical procedures and\npatient outcome. We introduce a Multimodal Medical Image Interaction (MMII)\nframework to allow medical experts a dynamic, audiovisual interaction with\nhuman tissue in three-dimensional space. In a virtual reality environment, the\nuser receives physically informed audiovisual feedback to improve the spatial\nperception of anatomical structures. MMII uses a model-based sonification\napproach to generate sounds derived from the geometry and physical properties\nof tissue, thereby eliminating the need for hand-crafted sound design. Two user\nstudies involving 34 general and nine clinical experts were conducted to\nevaluate the proposed interaction framework's learnability, usability, and\naccuracy. Our results showed excellent learnability of audiovisual\ncorrespondence as the rate of correct associations significantly improved (p <\n0.001) over the course of the study. MMII resulted in superior brain tumor\nlocalization accuracy (p < 0.05) compared to conventional medical image\ninteraction. Our findings substantiate the potential of this novel framework to\nenhance interaction with medical images, for example, during surgical\nprocedures where immediate and precise feedback is needed.\n","authors":["Laura SchÃ¼tz","Sasan Matinfar","Gideon Schafroth","Navid Navab","Merle Fairhurst","Arthur Wagner","Benedikt Wiestler","Ulrich Eck","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2407.07015v1.pdf","comment":"Accepted for publication in IEEE TVCG; presentation at IEEE ISMAR\n  2024"},{"id":"http://arxiv.org/abs/2406.00320v2","updated":"2024-07-09T15:55:57Z","published":"2024-06-01T06:40:22Z","title":"Frieren: Efficient Video-to-Audio Generation with Rectified Flow\n  Matching","summary":"  Video-to-audio (V2A) generation aims to synthesize content-matching audio\nfrom silent video, and it remains challenging to build V2A models with high\ngeneration quality, efficiency, and visual-audio temporal synchrony. We propose\nFrieren, a V2A model based on rectified flow matching. Frieren regresses the\nconditional transport vector field from noise to spectrogram latent with\nstraight paths and conducts sampling by solving ODE, outperforming\nautoregressive and score-based models in terms of audio quality. By employing a\nnon-autoregressive vector field estimator based on a feed-forward transformer\nand channel-level cross-modal feature fusion with strong temporal alignment,\nour model generates audio that is highly synchronized with the input video.\nFurthermore, through reflow and one-step distillation with guided vector field,\nour model can generate decent audio in a few, or even only one sampling step.\nExperiments indicate that Frieren achieves state-of-the-art performance in both\ngeneration quality and temporal alignment on VGGSound, with alignment accuracy\nreaching 97.22%, and 6.2% improvement in inception score over the strong\ndiffusion-based baseline. Audio samples are available at\nhttp://frieren-v2a.github.io .\n","authors":["Yongqi Wang","Wenxiang Guo","Rongjie Huang","Jiawei Huang","Zehan Wang","Fuming You","Ruiqi Li","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.00320v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08551v5","updated":"2024-07-09T15:48:32Z","published":"2024-03-13T14:02:54Z","title":"GaussianImage: 1000 FPS Image Representation and Compression by 2D\n  Gaussian Splatting","summary":"  Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 2000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding. Code is available at\nhttps://github.com/Xinjie-Q/GaussianImage.\n","authors":["Xinjie Zhang","Xingtong Ge","Tongda Xu","Dailan He","Yan Wang","Hongwei Qin","Guo Lu","Jing Geng","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08551v5.pdf","comment":"Accepted by ECCV 2024. Project\n  Page:https://xingtongge.github.io/GaussianImage-page/ Code:\n  https://github.com/Xinjie-Q/GaussianImage"},{"id":"http://arxiv.org/abs/2407.06524v1","updated":"2024-07-09T03:32:00Z","published":"2024-07-09T03:32:00Z","title":"Improving Speech Enhancement by Integrating Inter-Channel and Band\n  Features with Dual-branch Conformer","summary":"  Recent speech enhancement methods based on convolutional neural networks\n(CNNs) and transformer have been demonstrated to efficaciously capture\ntime-frequency (T-F) information on spectrogram. However, the correlation of\neach channels of speech features is failed to explore. Theoretically, each\nchannel map of speech features obtained by different convolution kernels\ncontains information with different scales demonstrating strong correlations.\nTo fill this gap, we propose a novel dual-branch architecture named\nchannel-aware dual-branch conformer (CADB-Conformer), which effectively\nexplores the long range time and frequency correlations among different\nchannels, respectively, to extract channel relation aware time-frequency\ninformation. Ablation studies conducted on DNS-Challenge 2020 dataset\ndemonstrate the importance of channel feature leveraging while showing the\nsignificance of channel relation aware T-F information for speech enhancement.\nExtensive experiments also show that the proposed model achieves superior\nperformance than recent methods with an attractive computational costs.\n","authors":["Jizhen Li","Xinmeng Xu","Weiping Tu","Yuhong Yang","Rong Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.06524v1.pdf","comment":null}]},"2024-07-11T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.07801v2","updated":"2024-07-11T02:38:14Z","published":"2024-07-10T16:17:49Z","title":"AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning","summary":"  In recent years, advancements in representation learning and language models\nhave propelled Automated Captioning (AC) to new heights, enabling the\ngeneration of human-level descriptions. Leveraging these advancements, we\npropose AVCap, an Audio-Visual Captioning framework, a simple yet powerful\nbaseline approach applicable to audio-visual captioning. AVCap utilizes\naudio-visual features as text tokens, which has many advantages not only in\nperformance but also in the extensibility and scalability of the model. AVCap\nis designed around three pivotal dimensions: the exploration of optimal\naudio-visual encoder architectures, the adaptation of pre-trained models\naccording to the characteristics of generated text, and the investigation into\nthe efficacy of modality fusion in captioning. Our method outperforms existing\naudio-visual captioning methods across all metrics and the code is available on\nhttps://github.com/JongSuk1/AVCap\n","authors":["Jongsuk Kim","Jiwon Shin","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.07801v2.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2407.07796v2","updated":"2024-07-11T03:46:35Z","published":"2024-07-10T16:14:34Z","title":"Evaluating Large Language Models with Grid-Based Game Competitions: An\n  Extensible LLM Benchmark and Leaderboard","summary":"  We introduce a novel and extensible benchmark for large language models\n(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.\nThe open-source game simulation code, available on GitHub, allows LLMs to\ncompete and generates detailed data files in JSON, CSV, TXT, and PNG formats\nfor leaderboard rankings and further analysis. We present the results of games\namong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by\nAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and\nGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of\nresults from other LLMs. In total, we simulated 2,310 matches (5 sessions for\neach pair among 7 LLMs and a random player) across three types of games, using\nthree distinct prompt types: list, illustration, and image. The results\nrevealed significant variations in LLM performance across different games and\nprompt types, with analysis covering win and disqualification rates, missed\nopportunity analysis, and invalid move analysis. The details of the leaderboard\nand result matrix data are available as open-access data on GitHub. This study\nenhances our understanding of LLMs' capabilities in playing games they were not\nspecifically trained for, helping to assess their rule comprehension and\nstrategic thinking. On the path to Artificial General Intelligence (AGI), this\nstudy lays the groundwork for future exploration into their utility in complex\ndecision-making scenarios, illuminating their strategic thinking abilities and\noffering directions for further inquiry into the limits of LLMs within\ngame-based frameworks.\n","authors":["Oguzhan Topsakal","Colby Jacob Edell","Jackson Bailey Harper"],"pdf_url":"https://arxiv.org/pdf/2407.07796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01858v3","updated":"2024-07-11T14:41:14Z","published":"2024-03-04T09:13:33Z","title":"An Improved Traditional Chinese Evaluation Suite for Foundation Model","summary":"  We present TMMLU+, a new benchmark designed for Traditional Chinese language\nunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66\nsubjects from elementary to professional level. It is six times larger and\nboasts a more balanced subject distribution than its predecessor, Taiwan\nMassive Multitask Language Understanding (TMMLU). We also benchmark\nclosed-source models and 26 open-weight Chinese large language models (LLMs) of\nparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings reveal\nthat (1.) Traditional Chinese models still trail behind their Simplified\nChinese counterparts, highlighting a need for more focused advancements in LLMs\ncatering to Traditional Chinese. (2.) Current LLMs still fall short of human\nperformance in average scores, indicating a potential need for future research\nto delve deeper into social science and humanities subjects. (3.) Among all the\ntokenization compression metrics examined, we identify that only the fertility\nscore uniquely demonstrates strong correlations with our benchmark results. We\nforesee that TMMLU+ will pinpoint areas for future model improvement, thereby\nnarrowing the gap between machine and human linguistic capabilities and\nsupporting researchers in developing Traditional Chinese LLMs. Our dataset,\nalong with the benchmark source code, is accessible at\nhuggingface.co/datasets/ikala/tmmluplus.\n","authors":["Zhi-Rui Tam","Ya-Ting Pai","Yen-Wei Lee","Jun-Da Chen","Wei-Min Chu","Sega Cheng","Hong-Han Shuai"],"pdf_url":"https://arxiv.org/pdf/2403.01858v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14770v4","updated":"2024-07-11T14:47:00Z","published":"2023-05-24T06:19:14Z","title":"Using Natural Language Explanations to Rescale Human Judgments","summary":"  The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover human judgments. However, annotators' judgments for subjective tasks can\ndiffer in many ways: they may reflect different qualitative judgments about an\nexample, and they may be mapped to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric.\n","authors":["Manya Wadhwa","Jifan Chen","Junyi Jessy Li","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2305.14770v4.pdf","comment":"Data available at\n  https://github.com/ManyaWadhwa/explanation_based_rescaling"},{"id":"http://arxiv.org/abs/2407.07565v2","updated":"2024-07-11T15:37:20Z","published":"2024-07-10T11:50:20Z","title":"On Leakage of Code Generation Evaluation Datasets","summary":"  In this paper we consider contamination by code generation test sets, in\nparticular in their use in modern large language models. We discuss three\npossible sources of such contamination and show findings supporting each of\nthem: (i) direct data leakage, (ii) indirect data leakage through the use of\nsynthetic data and (iii) overfitting to evaluation sets during model selection.\nKey to our findings is a new dataset of 161 prompts with their associated\npython solutions, dataset which is released at\nhttps://huggingface.co/datasets/CohereForAI/lbpp .\n","authors":["Alexandre Matton","Tom Sherborne","Dennis Aumiller","Elena Tommasone","Milad Alizadeh","Jingyi He","Raymond Ma","Maxime Voisin","Ellen Gilsenan-McMahon","Matthias GallÃ©"],"pdf_url":"https://arxiv.org/pdf/2407.07565v2.pdf","comment":"4 main pages, 9 in total"},{"id":"http://arxiv.org/abs/2407.07457v2","updated":"2024-07-11T06:06:33Z","published":"2024-07-10T08:20:47Z","title":"GLBench: A Comprehensive Benchmark for Graph with Large Language Models","summary":"  The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench.\n","authors":["Yuhan Li","Peisong Wang","Xiao Zhu","Aochuan Chen","Haiyun Jiang","Deng Cai","Victor Wai Kin Chan","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2407.07457v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2306.10280 by other authors"},{"id":"http://arxiv.org/abs/2407.07325v2","updated":"2024-07-11T07:44:28Z","published":"2024-07-10T02:43:18Z","title":"HiLight: Technical Report on the Motern AI Video Language Model","summary":"  This technical report presents the implementation of a state-of-the-art video\nencoder for video-text modal alignment and a video conversation framework\ncalled HiLight, which features dual visual towers. The work is divided into two\nmain parts: 1.alignment of video and text modalities; 2.convenient and\nefficient way to interact with users. Our goal is to address the task of video\ncomprehension in the context of billiards. The report includes a discussion of\nthe concepts and the final solution developed during the task's implementation.\n","authors":["Zhiting Wang","Qiangong Zhou","Kangjie Yang","Zongyang Liu","Xin Mao"],"pdf_url":"https://arxiv.org/pdf/2407.07325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06645v3","updated":"2024-07-11T03:06:45Z","published":"2024-07-09T08:14:29Z","title":"Entropy Law: The Story Behind Data Compression and LLM Performance","summary":"  Data is the cornerstone of large language models (LLMs), but not all data is\nuseful for model learning. Carefully selected data can better elicit the\ncapabilities of LLMs with much less computational overhead. Most methods\nconcentrate on evaluating the quality of individual samples in data selection,\nwhile the combinatorial effects among samples are neglected. Even if each\nsample is of perfect quality, their combinations may be suboptimal in teaching\nLLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim\nto uncover the underlying relationships between LLM performance and data\nselection. Inspired by the information compression nature of LLMs, we uncover\nan ``entropy law'' that connects LLM performance with data compression ratio\nand first-epoch training loss, which reflect the information redundancy of a\ndataset and the mastery of inherent knowledge encoded in this dataset,\nrespectively. Through both theoretical deduction and empirical evaluation, we\nfind that model performance is negatively correlated to the compression ratio\nof training data, which usually yields a lower training loss. Based on the\nfindings of the entropy law, we propose a quite efficient and universal data\nselection method named \\textbf{ZIP} for training LLMs, which aim to prioritize\ndata subsets exhibiting a low compression ratio. Based on a multi-stage\nalgorithm that selects diverse data in a greedy manner, we can obtain a good\ndata subset with satisfactory diversity. Extensive experiments have been\nconducted to validate the entropy law and the superiority of ZIP across\ndifferent LLM backbones and alignment stages. We also present an interesting\napplication of entropy law that can detect potential performance risks at the\nbeginning of model training.\n","authors":["Mingjia Yin","Chuhan Wu","Yufei Wang","Hao Wang","Wei Guo","Yasheng Wang","Yong Liu","Ruiming Tang","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.06645v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06094v2","updated":"2024-07-11T17:59:53Z","published":"2023-06-09T17:57:01Z","title":"Leveraging Large Language Models for Scalable Vector Graphics-Driven\n  Image Understanding","summary":"  Large language models (LLMs) have made significant advancements in natural\nlanguage understanding. However, through that enormous semantic representation\nthat the LLM has learnt, is it somehow possible for it to understand images as\nwell? This work investigates this question. To enable the LLM to process\nimages, we convert them into a representation given by Scalable Vector Graphics\n(SVG). To study what the LLM can do with this XML-based textual description of\nimages, we test the LLM on three broad computer vision tasks: (i) visual\nreasoning and question answering, (ii) image classification under distribution\nshift, few-shot learning, and (iii) generating new images using visual\nprompting. Even though we do not naturally associate LLMs with any visual\nunderstanding capabilities, our results indicate that the LLM can often do a\ndecent job in many of these tasks, potentially opening new avenues for research\ninto LLMs' ability to understand image data. Our code, data, and models can be\nfound here https://github.com/mu-cai/svg-llm.\n","authors":["Mu Cai","Zeyi Huang","Yuheng Li","Utkarsh Ojha","Haohan Wang","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2306.06094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08734v1","updated":"2024-07-11T17:59:00Z","published":"2024-07-11T17:59:00Z","title":"Transformer Circuit Faithfulness Metrics are not Robust","summary":"  Mechanistic interpretability work attempts to reverse engineer the learned\nalgorithms present inside neural networks. One focus of this work has been to\ndiscover 'circuits' -- subgraphs of the full model that explain behaviour on\nspecific tasks. But how do we measure the performance of such circuits? Prior\nwork has attempted to measure circuit 'faithfulness' -- the degree to which the\ncircuit replicates the performance of the full model. In this work, we survey\nmany considerations for designing experiments that measure circuit faithfulness\nby ablating portions of the model's computation. Concerningly, we find existing\nmethods are highly sensitive to seemingly insignificant changes in the ablation\nmethodology. We conclude that existing circuit faithfulness scores reflect both\nthe methodological choices of researchers as well as the actual components of\nthe circuit - the task a circuit is required to perform depends on the ablation\nused to test it. The ultimate goal of mechanistic interpretability work is to\nunderstand neural networks, so we emphasize the need for more clarity in the\nprecise claims being made about circuits. We open source a library at\nhttps://github.com/UFO-101/auto-circuit that includes highly efficient\nimplementations of a wide range of ablation methodologies and circuit discovery\nalgorithms.\n","authors":["Joseph Miller","Bilal Chughtai","William Saunders"],"pdf_url":"https://arxiv.org/pdf/2407.08734v1.pdf","comment":"CoLM 2024 Conference Paper. 11 page main body. 11 page appendix. 12\n  figures"},{"id":"http://arxiv.org/abs/2407.08733v1","updated":"2024-07-11T17:58:58Z","published":"2024-07-11T17:58:58Z","title":"Is Your Model Really A Good Math Reasoner? Evaluating Mathematical\n  Reasoning with Checklist","summary":"  Exceptional mathematical reasoning ability is one of the key features that\ndemonstrate the power of large language models (LLMs). How to comprehensively\ndefine and evaluate the mathematical abilities of LLMs, and even reflect the\nuser experience in real-world scenarios, has emerged as a critical issue.\nCurrent benchmarks predominantly concentrate on problem-solving capabilities,\nwhich presents a substantial risk of model overfitting and fails to accurately\nrepresent genuine mathematical reasoning abilities. In this paper, we argue\nthat if a model really understands a problem, it should be robustly and readily\napplied across a diverse array of tasks. Motivated by this, we introduce\nMATHCHECK, a well-designed checklist for testing task generalization and\nreasoning robustness, as well as an automatic tool to generate checklists\nefficiently. MATHCHECK includes multiple mathematical reasoning tasks and\nrobustness test types to facilitate a comprehensive evaluation of both\nmathematical reasoning ability and behavior testing. Utilizing MATHCHECK, we\ndevelop MATHCHECK-GSM and MATHCHECK-GEO to assess mathematical textual\nreasoning and multi-modal reasoning capabilities, respectively, serving as\nupgraded versions of benchmarks including GSM8k, GeoQA, UniGeo, and Geometry3K.\nWe adopt MATHCHECK-GSM and MATHCHECK-GEO to evaluate over 20 LLMs and 11 MLLMs,\nassessing their comprehensive mathematical reasoning abilities. Our results\ndemonstrate that while frontier LLMs like GPT-4o continue to excel in various\nabilities on the checklist, many other model families exhibit a significant\ndecline. Further experiments indicate that, compared to traditional math\nbenchmarks, MATHCHECK better reflects true mathematical abilities and\nrepresents mathematical intelligence more linearly, thereby supporting our\ndesign. On our MATHCHECK, we can easily conduct detailed behavior analysis to\ndeeply investigate models.\n","authors":["Zihao Zhou","Shudong Liu","Maizhen Ning","Wei Liu","Jindong Wang","Derek F. Wong","Xiaowei Huang","Qiufeng Wang","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2407.08733v1.pdf","comment":"35 pages, 10 figures, preprint"},{"id":"http://arxiv.org/abs/2407.08716v1","updated":"2024-07-11T17:50:34Z","published":"2024-07-11T17:50:34Z","title":"A Taxonomy for Data Contamination in Large Language Models","summary":"  Large language models pretrained on extensive web corpora demonstrate\nremarkable performance across a wide range of downstream tasks. However, a\ngrowing concern is data contamination, where evaluation datasets may be\ncontained in the pretraining corpus, inflating model performance.\nDecontamination, the process of detecting and removing such data, is a\npotential solution; yet these contaminants may originate from altered versions\nof the test set, evading detection during decontamination. How different types\nof contamination impact the performance of language models on downstream tasks\nis not fully understood. We present a taxonomy that categorizes the various\ntypes of contamination encountered by LLMs during the pretraining phase and\nidentify which types pose the highest risk. We analyze the impact of\ncontamination on two key NLP tasks -- summarization and question answering --\nrevealing how different types of contamination influence task performance\nduring evaluation.\n","authors":["Medha Palavalli","Amanda Bertsch","Matthew R. Gormley"],"pdf_url":"https://arxiv.org/pdf/2407.08716v1.pdf","comment":"19 pages, 8 figures, accepted to CONDA Workshop on Data Contamination\n  @ ACL 2024"},{"id":"http://arxiv.org/abs/2407.08713v1","updated":"2024-07-11T17:50:09Z","published":"2024-07-11T17:50:09Z","title":"GTA: A Benchmark for General Tool Agents","summary":"  Significant focus has been placed on integrating large language models (LLMs)\nwith various tools in developing general-purpose agents. This poses a challenge\nto LLMs' tool-use capabilities. However, there are evident gaps between\nexisting tool-use evaluations and real-world scenarios. Current evaluations\noften use AI-generated queries, single-step tasks, dummy tools, and text-only\ninteractions, failing to reveal the agents' real-world problem-solving\nabilities effectively. To address this, we propose GTA, a benchmark for General\nTool Agents, featuring three main aspects: (i) Real user queries: human-written\nqueries with simple real-world objectives but implicit tool-use, requiring the\nLLM to reason the suitable tools and plan the solution steps. (ii) Real\ndeployed tools: an evaluation platform equipped with tools across perception,\noperation, logic, and creativity categories to evaluate the agents' actual task\nexecution performance. (iii) Real multimodal inputs: authentic image files,\nsuch as spatial scenes, web page screenshots, tables, code snippets, and\nprinted/handwritten materials, used as the query contexts to align with\nreal-world scenarios closely. We design 229 real-world tasks and executable\ntool chains to evaluate mainstream LLMs. Our findings show that real-world user\nqueries are challenging for existing LLMs, with GPT-4 completing less than 50%\nof the tasks and most LLMs achieving below 25%. This evaluation reveals the\nbottlenecks in the tool-use capabilities of current LLMs in real-world\nscenarios, which provides future direction for advancing general-purpose tool\nagents. The code and dataset are available at\nhttps://github.com/open-compass/GTA.\n","authors":["Jize Wang","Zerun Ma","Yining Li","Songyang Zhang","Cailian Chen","Kai Chen","Xinyi Le"],"pdf_url":"https://arxiv.org/pdf/2407.08713v1.pdf","comment":"Github repo: https://github.com/open-compass/GTA"},{"id":"http://arxiv.org/abs/2407.00342v2","updated":"2024-07-11T17:08:36Z","published":"2024-06-29T07:01:51Z","title":"Korean Aspect-Based Sentiment Analysis via Implicit-Feature Alignment\n  with Corpus Filtering","summary":"  Investigations into Aspect-Based Sentiment Analysis (ABSA) for Korean\nrestaurant reviews are notably lacking in the existing literature. Our research\nproposes an intuitive and effective framework for ABSA in low-resource\nlanguages such as Korean. It optimizes prediction labels by integrating\ntranslated benchmark and unlabeled Korean data. Using a model fine-tuned on\ntranslated data, we pseudo-labeled the actual Korean NLI set. Subsequently, we\napplied LaBSE and MSP-based filtering to this pseudo-NLI set as implicit\nfeature, enhancing Aspect Category Detection and Polarity determination through\nadditional training. Incorporating dual filtering, this model bridged dataset\ngaps, achieving positive results in Korean ABSA with minimal resources. Through\nadditional data injection pipelines, our approach aims to utilize high-resource\ndata and construct effective models within communities, whether corporate or\nindividual, in low-resource language countries. Compared to English ABSA, our\nframework showed an approximately 3% difference in F1 scores and accuracy. We\nrelease the dataset and our code for Korean ABSA, at this link.\n","authors":["Kibeom Nam"],"pdf_url":"https://arxiv.org/pdf/2407.00342v2.pdf","comment":"13 pages, EMNLP Industry Track (submitted), DMLR@ICML 2024"},{"id":"http://arxiv.org/abs/2407.08662v1","updated":"2024-07-11T16:51:33Z","published":"2024-07-11T16:51:33Z","title":"Uncertainty Estimation of Large Language Models in Medical Question\n  Answering","summary":"  Large Language Models (LLMs) show promise for natural language generation in\nhealthcare, but risk hallucinating factually incorrect information. Deploying\nLLMs for medical question answering necessitates reliable uncertainty\nestimation (UE) methods to detect hallucinations. In this work, we benchmark\npopular UE methods with different model sizes on medical question-answering\ndatasets. Our results show that current approaches generally perform poorly in\nthis domain, highlighting the challenge of UE for medical applications. We also\nobserve that larger models tend to yield better results, suggesting a\ncorrelation between model size and the reliability of UE. To address these\nchallenges, we propose Two-phase Verification, a probability-free Uncertainty\nEstimation approach. First, an LLM generates a step-by-step explanation\nalongside its initial answer, followed by formulating verification questions to\ncheck the factual claims in the explanation. The model then answers these\nquestions twice: first independently, and then referencing the explanation.\nInconsistencies between the two sets of answers measure the uncertainty in the\noriginal response. We evaluate our approach on three biomedical\nquestion-answering datasets using Llama 2 Chat models and compare it against\nthe benchmarked baseline methods. The results show that our Two-phase\nVerification method achieves the best overall accuracy and stability across\nvarious datasets and model sizes, and its performance scales as the model size\nincreases.\n","authors":["Jiaxin Wu","Yizhou Yu","Hong-Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.08662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12822v2","updated":"2024-07-11T16:37:24Z","published":"2024-06-18T17:43:47Z","title":"Is It Good Data for Multilingual Instruction Tuning or Just Bad\n  Multilingual Evaluation for Large Language Models?","summary":"  Large language models, particularly multilingual ones, are designed, claimed,\nand expected to cater to native speakers of varied languages. We hypothesise\nthat the current practices of fine-tuning and evaluating these models may not\nperfectly align with this objective owing to a heavy reliance on translation,\nwhich can introduce translation artefacts and defects. It remains unknown\nwhether the nature of the instruction data has an impact on the model output;\nconversely, it is questionable whether translated test sets can capture such\nnuances. Due to the often coupled practices of using translated data in both\nstages, such imperfections could have been overlooked. This work investigates\nthese issues using controlled native or translated data during instruction\ntuning and evaluation stages. Experiments on eight base models and eight\ndifferent benchmarks show that native or generation benchmarks reveal a notable\ndifference between native and translated instruction data especially when model\nperformance is high, whereas other types of test sets cannot. The comparison\nbetween round-trip and single-pass translations reflects the importance of\nknowledge from language-native resources. Finally, we demonstrate that\nregularization is beneficial to bridging this gap on structured but not\ngenerative tasks.\n","authors":["Pinzhen Chen","Simon Yu","Zhicheng Guo","Barry Haddow"],"pdf_url":"https://arxiv.org/pdf/2406.12822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08642v1","updated":"2024-07-11T16:23:16Z","published":"2024-07-11T16:23:16Z","title":"Towards Building Specialized Generalist AI with System 1 and System 2\n  Fusion","summary":"  In this perspective paper, we introduce the concept of Specialized Generalist\nArtificial Intelligence (SGAI or simply SGI) as a crucial milestone toward\nArtificial General Intelligence (AGI). Compared to directly scaling general\nabilities, SGI is defined as AI that specializes in at least one task,\nsurpassing human experts, while also retaining general abilities. This fusion\npath enables SGI to rapidly achieve high-value areas. We categorize SGI into\nthree stages based on the level of mastery over professional skills and\ngenerality performance. Additionally, we discuss the necessity of SGI in\naddressing issues associated with large language models, such as their\ninsufficient generality, specialized capabilities, uncertainty in innovation,\nand practical applications. Furthermore, we propose a conceptual framework for\ndeveloping SGI that integrates the strengths of Systems 1 and 2 cognitive\nprocessing. This framework comprises three layers and four key components,\nwhich focus on enhancing individual abilities and facilitating collaborative\nevolution. We conclude by summarizing the potential challenges and suggesting\nfuture directions. We hope that the proposed SGI will provide insights into\nfurther research and applications towards achieving AGI.\n","authors":["Kaiyan Zhang","Biqing Qi","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.08642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08618v1","updated":"2024-07-11T15:56:02Z","published":"2024-07-11T15:56:02Z","title":"Tamil Language Computing: the Present and the Future","summary":"  This paper delves into the text processing aspects of Language Computing,\nwhich enables computers to understand, interpret, and generate human language.\nFocusing on tasks such as speech recognition, machine translation, sentiment\nanalysis, text summarization, and language modelling, language computing\nintegrates disciplines including linguistics, computer science, and cognitive\npsychology to create meaningful human-computer interactions. Recent\nadvancements in deep learning have made computers more accessible and capable\nof independent learning and adaptation. In examining the landscape of language\ncomputing, the paper emphasises foundational work like encoding, where Tamil\ntransitioned from ASCII to Unicode, enhancing digital communication. It\ndiscusses the development of computational resources, including raw data,\ndictionaries, glossaries, annotated data, and computational grammars, necessary\nfor effective language processing. The challenges of linguistic annotation, the\ncreation of treebanks, and the training of large language models are also\ncovered, emphasising the need for high-quality, annotated data and advanced\nlanguage models. The paper underscores the importance of building practical\napplications for languages like Tamil to address everyday communication needs,\nhighlighting gaps in current technology. It calls for increased research\ncollaboration, digitization of historical texts, and fostering digital usage to\nensure the comprehensive development of Tamil language processing, ultimately\nenhancing global communication and access to digital services.\n","authors":["Kengatharaiyer Sarveswaran"],"pdf_url":"https://arxiv.org/pdf/2407.08618v1.pdf","comment":"11 pages, This is the write-up of the address delivered at the 29th\n  Annual Sessions of the Jaffna Science Association, held from March 29-31,\n  2023, at the University of Jaffna, Sri Lanka"},{"id":"http://arxiv.org/abs/2402.17396v2","updated":"2024-07-11T15:54:45Z","published":"2024-02-27T10:44:52Z","title":"Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of\n  Prompting Strategies","summary":"  Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Processing thanks to their ability to reuse knowledge acquired on\nmassive text corpora on a wide variety of downstream tasks, with minimal (if\nany) tuning steps. At the same time, it has been repeatedly shown that LLMs\nlack systematic generalization, which allows to extrapolate the learned\nstatistical regularities outside the training distribution. In this work, we\noffer a systematic benchmarking of GPT-4, one of the most advanced LLMs\navailable, on three algorithmic tasks characterized by the possibility to\ncontrol the problem difficulty with two parameters. We compare the performance\nof GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the\nTransformer-Encoder architecture recently introduced to solve similar tasks,\nthe Neural Data Router. We find that the deployment of advanced prompting\ntechniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating\nthat state-of-the-art LLMs constitute a very strong baseline also in\nchallenging tasks that require systematic generalization.\n","authors":["Flavio Petruzzellis","Alberto Testolin","Alessandro Sperduti"],"pdf_url":"https://arxiv.org/pdf/2402.17396v2.pdf","comment":"Accepted at LREC-COLING 2024. Added acknowledgements"},{"id":"http://arxiv.org/abs/2407.08607v1","updated":"2024-07-11T15:43:27Z","published":"2024-07-11T15:43:27Z","title":"Turn-Level Empathy Prediction Using Psychological Indicators","summary":"  For the WASSA 2024 Empathy and Personality Prediction Shared Task, we propose\na novel turn-level empathy detection method that decomposes empathy into six\npsychological indicators: Emotional Language, Perspective-Taking, Sympathy and\nCompassion, Extroversion, Openness, and Agreeableness. A pipeline of text\nenrichment using a Large Language Model (LLM) followed by DeBERTA fine-tuning\ndemonstrates a significant improvement in the Pearson Correlation Coefficient\nand F1 scores for empathy detection, highlighting the effectiveness of our\napproach. Our system officially ranked 7th at the CONV-turn track.\n","authors":["Shaz Furniturewala","Kokil Jaidka"],"pdf_url":"https://arxiv.org/pdf/2407.08607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00499v2","updated":"2024-07-11T15:39:31Z","published":"2024-03-01T12:42:47Z","title":"Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of\n  Machine Cognition","summary":"  Recent advances in LLMs have sparked a debate on whether they understand\ntext. In this position paper, we argue that opponents in this debate hold\ndifferent definitions for understanding, and particularly differ in their view\non the role of consciousness. To substantiate this claim, we propose a thought\nexperiment involving an open-source chatbot $Z$ which excels on every possible\nbenchmark, seemingly without subjective experience. We ask whether $Z$ is\ncapable of understanding, and show that different schools of thought within\nseminal AI research seem to answer this question differently, uncovering their\nterminological disagreement. Moving forward, we propose two distinct working\ndefinitions for understanding which explicitly acknowledge the question of\nconsciousness, and draw connections with a rich literature in philosophy,\npsychology and neuroscience.\n","authors":["Ariel Goldstein","Gabriel Stanovsky"],"pdf_url":"https://arxiv.org/pdf/2403.00499v2.pdf","comment":"Accepted to Findings of ACL (2024)"},{"id":"http://arxiv.org/abs/2406.02061v3","updated":"2024-07-11T15:17:36Z","published":"2024-06-04T07:43:33Z","title":"Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown\n  in State-Of-the-Art Large Language Models","summary":"  Large Language Models (LLMs) are often described as being instances of\nfoundation models - that is, models that transfer strongly across various tasks\nand conditions in few-show or zero-shot manner, while exhibiting scaling laws\nthat predict function improvement when increasing the pre-training scale. These\nclaims of excelling in different functions and tasks rely on measurements taken\nacross various sets of standardized benchmarks showing high scores for such\nmodels. We demonstrate here a dramatic breakdown of function and reasoning\ncapabilities of state-of-the-art models trained at the largest available scales\nwhich claim strong function, using a simple, short, conventional common sense\nproblem (AIW problem) formulated in concise natural language, easily solvable\nby humans. The breakdown is dramatic, as models show strong fluctuations across\neven slight problem variations that should not affect problem solving, also\nexpressing strong overconfidence in the wrong solutions, often backed up by\nplausible sounding explanation-like confabulations. Various standard\ninterventions in an attempt to get the right solution, like various type of\nenhanced prompting, or urging the models to reconsider the wrong solutions\nagain by multi step re-evaluation, fail. We take these initial observations to\nthe scientific and technological community to stimulate urgent re-assessment of\nthe claimed capabilities of current generation of LLMs. Such re-assessment also\nrequires common action to create standardized benchmarks that would allow\nproper detection of such basic reasoning deficits that obviously manage to\nremain undiscovered by current state-of-the-art evaluation procedures and\nbenchmarks. Code for reproducing experiments in the paper and raw experiments\ndata can be found at https://github.com/LAION-AI/AIW\n","authors":["Marianna Nezhurina","Lucia Cipolina-Kun","Mehdi Cherti","Jenia Jitsev"],"pdf_url":"https://arxiv.org/pdf/2406.02061v3.pdf","comment":"v2.0. Adding further experiments on various AIW problem variations.\n  AIW \"Alice Female Power Boost\", AIW Extension (AIW Ext). Including recent\n  Claude 3.5 Sonnet and Qwen 2 72B Instruct results"},{"id":"http://arxiv.org/abs/2210.05401v2","updated":"2024-07-11T15:13:14Z","published":"2022-10-11T12:25:26Z","title":"MiDe22: An Annotated Multi-Event Tweet Dataset for Misinformation\n  Detection","summary":"  The rapid dissemination of misinformation through online social networks\nposes a pressing issue with harmful consequences jeopardizing human health,\npublic safety, democracy, and the economy; therefore, urgent action is required\nto address this problem. In this study, we construct a new human-annotated\ndataset, called MiDe22, having 5,284 English and 5,064 Turkish tweets with\ntheir misinformation labels for several recent events between 2020 and 2022,\nincluding the Russia-Ukraine war, COVID-19 pandemic, and Refugees. The dataset\nincludes user engagements with the tweets in terms of likes, replies, retweets,\nand quotes. We also provide a detailed data analysis with descriptive\nstatistics and the experimental results of a benchmark evaluation for\nmisinformation detection.\n","authors":["Cagri Toraman","Oguzhan Ozcelik","Furkan ÅahinuÃ§","Fazli Can"],"pdf_url":"https://arxiv.org/pdf/2210.05401v2.pdf","comment":"Published at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2201.08214v3","updated":"2024-07-11T15:13:10Z","published":"2022-01-20T15:01:12Z","title":"A Latent-Variable Model for Intrinsic Probing","summary":"  The success of pre-trained contextualized representations has prompted\nresearchers to analyze them for the presence of linguistic information. Indeed,\nit is natural to assume that these pre-trained representations do encode some\nlevel of linguistic knowledge as they have brought about large empirical\nimprovements on a wide variety of NLP tasks, which suggests they are learning\ntrue linguistic generalization. In this work, we focus on intrinsic probing, an\nanalysis technique where the goal is not only to identify whether a\nrepresentation encodes a linguistic attribute but also to pinpoint where this\nattribute is encoded. We propose a novel latent-variable formulation for\nconstructing intrinsic probes and derive a tractable variational approximation\nto the log-likelihood. Our results show that our model is versatile and yields\ntighter mutual information estimates than two intrinsic probes previously\nproposed in the literature. Finally, we find empirical evidence that\npre-trained representations develop a cross-lingually entangled notion of\nmorphosyntax.\n","authors":["Karolina StaÅczak","Lucas Torroba Hennigen","Adina Williams","Ryan Cotterell","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2201.08214v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08582v1","updated":"2024-07-11T15:07:26Z","published":"2024-07-11T15:07:26Z","title":"On the Universal Truthfulness Hyperplane Inside LLMs","summary":"  While large language models (LLMs) have demonstrated remarkable abilities\nacross various fields, hallucination remains a significant challenge. Recent\nstudies have explored hallucinations through the lens of internal\nrepresentations, proposing mechanisms to decipher LLMs' adherence to facts.\nHowever, these approaches often fail to generalize to out-of-distribution data,\nleading to concerns about whether internal representation patterns reflect\nfundamental factual awareness, or only overfit spurious correlations on the\nspecific datasets. In this work, we investigate whether a universal\ntruthfulness hyperplane that distinguishes the model's factually correct and\nincorrect outputs exists within the model. To this end, we scale up the number\nof training datasets and conduct an extensive evaluation -- we train the\ntruthfulness hyperplane on a diverse collection of over 40 datasets and examine\nits cross-task, cross-domain, and in-domain generalization. Our results\nindicate that increasing the diversity of the training datasets significantly\nenhances the performance in all scenarios, while the volume of data samples\nplays a less critical role. This finding supports the optimistic hypothesis\nthat a universal truthfulness hyperplane may indeed exist within the model,\noffering promising directions for future research.\n","authors":["Junteng Liu","Shiqi Chen","Yu Cheng","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2407.08582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08551v1","updated":"2024-07-11T14:36:53Z","published":"2024-07-11T14:36:53Z","title":"Autoregressive Speech Synthesis without Vector Quantization","summary":"  We present MELLE, a novel continuous-valued tokens based language modeling\napproach for text to speech synthesis (TTS). MELLE autoregressively generates\ncontinuous mel-spectrogram frames directly from text condition, bypassing the\nneed for vector quantization, which are originally designed for audio\ncompression and sacrifice fidelity compared to mel-spectrograms. Specifically,\n(i) instead of cross-entropy loss, we apply regression loss with a proposed\nspectrogram flux loss function to model the probability distribution of the\ncontinuous-valued tokens. (ii) we have incorporated variational inference into\nMELLE to facilitate sampling mechanisms, thereby enhancing the output diversity\nand model robustness. Experiments demonstrate that, compared to the two-stage\ncodec language models VALL-E and its variants, the single-stage MELLE mitigates\nrobustness issues by avoiding the inherent flaws of sampling discrete codes,\nachieves superior performance across multiple metrics, and, most importantly,\noffers a more streamlined paradigm. See https://aka.ms/melle for demos of our\nwork.\n","authors":["Lingwei Meng","Long Zhou","Shujie Liu","Sanyuan Chen","Bing Han","Shujie Hu","Yanqing Liu","Jinyu Li","Sheng Zhao","Xixin Wu","Helen Meng","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.08551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20279v2","updated":"2024-07-11T14:22:32Z","published":"2024-03-29T16:49:24Z","title":"LUQ: Long-text Uncertainty Quantification for LLMs","summary":"  Large Language Models (LLMs) have demonstrated remarkable capability in a\nvariety of NLP tasks. However, LLMs are also prone to generate nonfactual\ncontent. Uncertainty Quantification (UQ) is pivotal in enhancing our\nunderstanding of a model's confidence on its generation, thereby aiding in the\nmitigation of nonfactual outputs. Existing research on UQ predominantly targets\nshort text generation, typically yielding brief, word-limited responses.\nHowever, real-world applications frequently necessitate much longer responses.\nOur study first highlights the limitations of current UQ methods in handling\nlong text generation. We then introduce \\textsc{Luq} and its two variations, a\nseries of novel sampling-based UQ approaches specifically designed for long\ntext. Our findings reveal that \\textsc{Luq} outperforms existing baseline\nmethods in correlating with the model's factuality scores (negative coefficient\nof -0.85 observed for Gemini Pro). To further improve the factuality of LLM\nresponses, we propose \\textsc{Luq-Ensemble}, a method that ensembles responses\nfrom multiple models and selects the response with the lowest uncertainty. The\nensembling method greatly improves the response factuality upon the best\nstandalone LLM.\n","authors":["Caiqi Zhang","Fangyu Liu","Marco Basaldella","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2403.20279v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08521v1","updated":"2024-07-11T14:09:42Z","published":"2024-07-11T14:09:42Z","title":"Emergent Visual-Semantic Hierarchies in Image-Text Representations","summary":"  While recent vision-and-language models (VLMs) like CLIP are a powerful tool\nfor analyzing text and images in a shared semantic space, they do not\nexplicitly model the hierarchical nature of the set of texts which may describe\nan image. Conversely, existing multimodal hierarchical representation learning\nmethods require costly training from scratch, failing to leverage the knowledge\nencoded by state-of-the-art multimodal foundation models. In this work, we\nstudy the knowledge of existing foundation models, finding that they exhibit\nemergent understanding of visual-semantic hierarchies despite not being\ndirectly trained for this purpose. We propose the Radial Embedding (RE)\nframework for probing and optimizing hierarchical understanding, and contribute\nthe HierarCaps dataset, a benchmark facilitating the study of hierarchical\nknowledge in image--text representations, constructed automatically via large\nlanguage models. Our results show that foundation VLMs exhibit zero-shot\nhierarchical understanding, surpassing the performance of prior models\nexplicitly designed for this purpose. Furthermore, we show that foundation\nmodels may be better aligned to hierarchical reasoning via a text-only\nfine-tuning phase, while retaining pretraining knowledge.\n","authors":["Morris Alper","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2407.08521v1.pdf","comment":"Accepted to ECCV 2024. Project page: https://hierarcaps.github.io/"},{"id":"http://arxiv.org/abs/2404.12464v5","updated":"2024-07-11T14:05:59Z","published":"2024-04-18T18:48:50Z","title":"NormAd: A Benchmark for Measuring the Cultural Adaptability of Large\n  Language Models","summary":"  The integration of large language models (LLMs) into various global cultures\nfundamentally presents a challenge: LLMs must navigate interactions, respect\nsocial norms, and avoid transgressing cultural boundaries. However, it is still\nunclear if LLMs can adapt their outputs to diverse cultural norms. Our study\nfocuses on this aspect. We introduce NormAd, a novel dataset, which includes\n2.6k stories that represent social and cultural norms from 75 countries, to\nassess the ability of LLMs to adapt to different granular levels of\nsocio-cultural contexts such as the country of origin, its associated cultural\nvalues, and prevalent social norms. Our study reveals that LLMs struggle with\ncultural reasoning across all contextual granularities, showing stronger\nadaptability to English-centric cultures over those from the Global South. Even\nwith explicit social norms, the top-performing model, Mistral-7b-Instruct,\nachieves only 81.8% accuracy, lagging behind the 95.6% achieved by humans.\nEvaluation on NormAd further reveals that LLMs struggle to adapt to stories\ninvolving gift-giving across cultures. Due to inherent agreement or sycophancy\nbiases, LLMs find it considerably easier to assess the social acceptability of\nstories that adhere to norms than those that deviate. Our benchmark measures\nthe cultural adaptability (or lack thereof) of LLMs, emphasizing the potential\nto make these technologies more equitable and useful for global audiences. We\nrelease the NormAd dataset and its associated code on GitHub.\n","authors":["Abhinav Rao","Akhila Yerukola","Vishwa Shah","Katharina Reinecke","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2404.12464v5.pdf","comment":"Preprint. In Review"},{"id":"http://arxiv.org/abs/2407.08495v1","updated":"2024-07-11T13:29:28Z","published":"2024-07-11T13:29:28Z","title":"Investigating LLMs as Voting Assistants via Contextual Augmentation: A\n  Case Study on the European Parliament Elections 2024","summary":"  Instruction-finetuned Large Language Models exhibit unprecedented Natural\nLanguage Understanding capabilities. Recent work has been exploring political\nbiases and political reasoning capabilities in LLMs, mainly scoped in the US\ncontext. In light of the recent 2024 European Parliament elections, we are\ninvestigating if LLMs can be used as Voting Advice Applications (VAAs). We\naudit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the\nstance of political parties based on the latest \"EU and I\" voting assistance\nquestionnaire. Furthermore, we explore alternatives to improve models'\nperformance by augmenting the input context via Retrieval-Augmented Generation\n(RAG) relying on web search, and Self-Reflection using staged conversations\nthat aim to re-collect relevant content from the model's internal memory. We\nfind that MIXTRAL is highly accurate with an 82% accuracy on average.\nAugmenting the input context with expert-curated information can lead to a\nsignificant boost of approx. 9%, which remains an open challenge for automated\napproaches.\n","authors":["Ilias Chalkidis"],"pdf_url":"https://arxiv.org/pdf/2407.08495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08488v1","updated":"2024-07-11T13:22:17Z","published":"2024-07-11T13:22:17Z","title":"Lynx: An Open Source Hallucination Evaluation Model","summary":"  Retrieval Augmented Generation (RAG) techniques aim to mitigate\nhallucinations in Large Language Models (LLMs). However, LLMs can still produce\ninformation that is unsupported or contradictory to the retrieved contexts. We\nintroduce LYNX, a SOTA hallucination detection LLM that is capable of advanced\nreasoning on challenging real-world hallucination scenarios. To evaluate LYNX,\nwe present HaluBench, a comprehensive hallucination evaluation benchmark,\nconsisting of 15k samples sourced from various real-world domains. Our\nexperiment results show that LYNX outperforms GPT-4o, Claude-3-Sonnet, and\nclosed and open-source LLM-as-a-judge models on HaluBench. We release LYNX,\nHaluBench and our evaluation code for public access.\n","authors":["Selvan Sunitha Ravi","Bartosz Mielczarek","Anand Kannappan","Douwe Kiela","Rebecca Qian"],"pdf_url":"https://arxiv.org/pdf/2407.08488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.00081v4","updated":"2024-07-11T13:18:29Z","published":"2023-07-31T18:53:47Z","title":"Towards Semantically Enriched Embeddings for Knowledge Graph Completion","summary":"  Embedding based Knowledge Graph (KG) Completion has gained much attention\nover the past few years. Most of the current algorithms consider a KG as a\nmultidirectional labeled graph and lack the ability to capture the semantics\nunderlying the schematic information. In a separate development, a vast amount\nof information has been captured within the Large Language Models (LLMs) which\nhas revolutionized the field of Artificial Intelligence. KGs could benefit from\nthese LLMs and vice versa. This vision paper discusses the existing algorithms\nfor KG completion based on the variations for generating KG embeddings. It\nstarts with discussing various KG completion algorithms such as transductive\nand inductive link prediction and entity type prediction algorithms. It then\nmoves on to the algorithms utilizing type information within the KGs, LLMs, and\nfinally to algorithms capturing the semantics represented in different\ndescription logic axioms. We conclude the paper with a critical reflection on\nthe current state of work in the community and give recommendations for future\ndirections.\n","authors":["Mehwish Alam","Frank van Harmelen","Maribel Acosta"],"pdf_url":"https://arxiv.org/pdf/2308.00081v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08475v1","updated":"2024-07-11T13:11:16Z","published":"2024-07-11T13:11:16Z","title":"Investigating Public Fine-Tuning Datasets: A Complex Review of Current\n  Practices from a Construction Perspective","summary":"  With the rapid development of the large model domain, research related to\nfine-tuning has concurrently seen significant advancement, given that\nfine-tuning is a constituent part of the training process for large-scale\nmodels. Data engineering plays a fundamental role in the training process of\nmodels, which includes data infrastructure, data processing, etc. Data during\nfine-tuning likewise forms the base for large models. In order to embrace the\npower and explore new possibilities of fine-tuning datasets, this paper reviews\ncurrent public fine-tuning datasets from the perspective of data construction.\nAn overview of public fine-tuning datasets from two sides: evolution and\ntaxonomy, is provided in this review, aiming to chart the development\ntrajectory. Construction techniques and methods for public fine-tuning datasets\nof Large Language Models (LLMs), including data generation and data\naugmentation among others, are detailed. This elaboration follows the\naforementioned taxonomy, specifically across demonstration, comparison, and\ngeneralist categories. Additionally, a category tree of data generation\ntechniques has been abstracted in our review to assist researchers in gaining a\ndeeper understanding of fine-tuning datasets from the construction dimension.\nOur review also summarizes the construction features in different data\npreparation phases of current practices in this field, aiming to provide a\ncomprehensive overview and inform future research. Fine-tuning dataset\npractices, encompassing various data modalities, are also discussed from a\nconstruction perspective in our review. Towards the end of the article, we\noffer insights and considerations regarding the future construction and\ndevelopments of fine-tuning datasets.\n","authors":["Runyuan Ma","Wei Li","Fukai Shang"],"pdf_url":"https://arxiv.org/pdf/2407.08475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06566v2","updated":"2024-07-11T13:05:37Z","published":"2024-06-03T07:44:32Z","title":"Natural Language Interaction with a Household Electricity\n  Knowledge-based Digital Twin","summary":"  Domain specific digital twins, representing a digital replica of various\nsegments of the smart grid, are foreseen as able to model, simulate, and\ncontrol the respective segments. At the same time, knowledge-based digital\ntwins, coupled with AI, may also empower humans to understand aspects of the\nsystem through natural language interaction in view of planning and policy\nmaking. This paper is the first to assess and report on the potential of\nRetrieval Augmented Generation (RAG) question answers related to household\nelectrical energy measurement aspects leveraging a knowledge-based energy\ndigital twin. Relying on the recently published electricity consumption\nknowledge graph that actually represents a knowledge-based digital twin, we\nstudy the capabilities of ChatGPT, Gemini and Llama in answering electricity\nrelated questions. Furthermore, we compare the answers with the ones generated\nthrough a RAG techniques that leverages an existing electricity knowledge-based\ndigital twin. Our findings illustrate that the RAG approach not only reduces\nthe incidence of incorrect information typically generated by LLMs but also\nsignificantly improves the quality of the output by grounding responses in\nverifiable data. This paper details our methodology, presents a comparative\nanalysis of responses with and without RAG, and discusses the implications of\nour findings for future applications of AI in specialized sectors like energy\ndata analysis.\n","authors":["Carolina Fortuna","Vid HanÅ¾el","BlaÅ¾ BertalaniÄ"],"pdf_url":"https://arxiv.org/pdf/2406.06566v2.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2406.08202v2","updated":"2024-07-11T12:51:34Z","published":"2024-06-12T13:35:10Z","title":"A Dialogue Game for Eliciting Balanced Collaboration","summary":"  Collaboration is an integral part of human dialogue. Typical task-oriented\ndialogue games assign asymmetric roles to the participants, which limits their\nability to elicit naturalistic role-taking in collaboration and its\nnegotiation. We present a novel and simple online setup that favors balanced\ncollaboration: a two-player 2D object placement game in which the players must\nnegotiate the goal state themselves. We show empirically that human players\nexhibit a variety of role distributions, and that balanced collaboration\nimproves task performance. We also present an LLM-based baseline agent which\ndemonstrates that automatic playing of our game is an interesting challenge for\nartificial systems.\n","authors":["Isidora JekniÄ","David Schlangen","Alexander Koller"],"pdf_url":"https://arxiv.org/pdf/2406.08202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08454v1","updated":"2024-07-11T12:50:42Z","published":"2024-07-11T12:50:42Z","title":"Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks","summary":"  How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.\n","authors":["Zheng Wang","Boxiao Jin","Zhongzhi Yu","Minjia Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.08454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08441v1","updated":"2024-07-11T12:30:19Z","published":"2024-07-11T12:30:19Z","title":"Are Large Language Models Really Bias-Free? Jailbreak Prompts for\n  Assessing Adversarial Robustness to Bias Elicitation","summary":"  Large Language Models (LLMs) have revolutionized artificial intelligence,\ndemonstrating remarkable computational power and linguistic capabilities.\nHowever, these models are inherently prone to various biases stemming from\ntheir training data. These include selection, linguistic, and confirmation\nbiases, along with common stereotypes related to gender, ethnicity, sexual\norientation, religion, socioeconomic status, disability, and age. This study\nexplores the presence of these biases within the responses given by the most\nrecent LLMs, analyzing the impact on their fairness and reliability. We also\ninvestigate how known prompt engineering techniques can be exploited to\neffectively reveal hidden biases of LLMs, testing their adversarial robustness\nagainst jailbreak prompts specially crafted for bias elicitation. Extensive\nexperiments are conducted using the most widespread LLMs at different scales,\nconfirming that LLMs can still be manipulated to produce biased or\ninappropriate responses, despite their advanced capabilities and sophisticated\nalignment processes. Our findings underscore the importance of enhancing\nmitigation techniques to address these safety issues, toward a more sustainable\nand inclusive artificial intelligence.\n","authors":["Riccardo Cantini","Giada Cosenza","Alessio Orsino","Domenico Talia"],"pdf_url":"https://arxiv.org/pdf/2407.08441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08440v1","updated":"2024-07-11T12:26:55Z","published":"2024-07-11T12:26:55Z","title":"Beyond Instruction Following: Evaluating Rule Following of Large\n  Language Models","summary":"  Although Large Language Models (LLMs) have demonstrated strong\ninstruction-following ability to be helpful, they are further supposed to be\ncontrolled and guided by rules in real-world scenarios to be safe, and accurate\nin responses. This demands the possession of rule-following capability of LLMs.\nHowever, few works have made a clear evaluation of the rule-following\ncapability of LLMs. Previous studies that try to evaluate the rule-following\ncapability of LLMs fail to distinguish the rule-following scenarios from the\ninstruction-following scenarios. Therefore, this paper first makes a\nclarification of the concept of rule-following, and curates a comprehensive\nbenchmark, RuleBench, to evaluate a diversified range of rule-following\nabilities. Our experimental results on a variety of LLMs show that they are\nstill limited in following rules. Our further analysis provides insights into\nthe improvements for LLMs toward a better rule-following intelligent agent. The\ndata and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/\n","authors":["Wangtao Sun","Chenxiang Zhang","Xueyou Zhang","Ziyang Huang","Haotian Xu","Pei Chen","Shizhu He","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06707v4","updated":"2024-07-11T12:25:06Z","published":"2023-10-10T15:33:51Z","title":"Quality-Aware Translation Models: Efficient Generation and Quality\n  Estimation in a Single Model","summary":"  Maximum-a-posteriori (MAP) decoding is the most widely used decoding strategy\nfor neural machine translation (NMT) models. The underlying assumption is that\nmodel probability correlates well with human judgment, with better translations\ngetting assigned a higher score by the model. However, research has shown that\nthis assumption does not always hold, and generation quality can be improved by\ndecoding to optimize a utility function backed by a metric or\nquality-estimation signal, as is done by Minimum Bayes Risk (MBR) or\nquality-aware decoding. The main disadvantage of these approaches is that they\nrequire an additional model to calculate the utility function during decoding,\nsignificantly increasing the computational cost. In this paper, we propose to\nmake the NMT models themselves quality-aware by training them to estimate the\nquality of their own output. Using this approach for MBR decoding we can\ndrastically reduce the size of the candidate list, resulting in a speed-up of\ntwo-orders of magnitude. When applying our method to MAP decoding we obtain\nquality gains similar or even superior to quality reranking approaches, but\nwith the efficiency of single pass decoding.\n","authors":["Christian Tomani","David Vilar","Markus Freitag","Colin Cherry","Subhajit Naskar","Mara Finkelstein","Xavier Garcia","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2310.06707v4.pdf","comment":"In Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2024)"},{"id":"http://arxiv.org/abs/2405.16176v2","updated":"2024-07-11T11:52:23Z","published":"2024-05-25T11:00:44Z","title":"Bi-reachability in Petri nets with data","summary":"  We investigate Petri nets with data, an extension of plain Petri nets where\ntokens carry values from an infinite data domain, and executability of\ntransitions is conditioned by equalities between data values. We provide a\ndecision procedure for the bi-reachability problem: given a Petri net and its\ntwo configurations, we ask if each of the configurations is reachable from the\nother. This pushes forward the decidability borderline, as the bi-reachability\nproblem subsumes the coverability problem (which is known to be decidable) and\nis subsumed by the reachability problem (whose decidability status is unknown).\n","authors":["Åukasz KamiÅski","SÅawomir Lasota"],"pdf_url":"https://arxiv.org/pdf/2405.16176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00402v2","updated":"2024-07-11T11:17:09Z","published":"2024-06-29T11:09:47Z","title":"Is It Really Long Context if All You Need Is Retrieval? Towards\n  Genuinely Difficult Long Context NLP","summary":"  Improvements in language models' capabilities have pushed their applications\ntowards longer contexts, making long-context evaluation and development an\nactive research area. However, many disparate use-cases are grouped together\nunder the umbrella term of \"long-context\", defined simply by the total length\nof the model's input, including - for example - Needle-in-a-Haystack tasks,\nbook summarization, and information aggregation. Given their varied difficulty,\nin this position paper we argue that conflating different tasks by their\ncontext length is unproductive. As a community, we require a more precise\nvocabulary to understand what makes long-context tasks similar or different. We\npropose to unpack the taxonomy of long-context based on the properties that\nmake them more difficult with longer contexts. We propose two orthogonal axes\nof difficulty: (I) Diffusion: How hard is it to find the necessary information\nin the context? (II) Scope: How much necessary information is there to find? We\nsurvey the literature on long-context, provide justification for this taxonomy\nas an informative descriptor, and situate the literature with respect to it. We\nconclude that the most difficult and interesting settings, whose necessary\ninformation is very long and highly diffused within the input, is severely\nunder-explored. By using a descriptive vocabulary and discussing the relevant\nproperties of difficulty in long-context, we can implement more informed\nresearch in this area. We call for a careful design of tasks and benchmarks\nwith distinctly long context, taking into account the characteristics that make\nit qualitatively different from shorter context.\n","authors":["Omer Goldman","Alon Jacovi","Aviv Slobodkin","Aviya Maimon","Ido Dagan","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2407.00402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08400v1","updated":"2024-07-11T11:06:05Z","published":"2024-07-11T11:06:05Z","title":"Self-training Language Models for Arithmetic Reasoning","summary":"  Language models achieve impressive results in tasks involving complex\nmultistep reasoning, but scaling these capabilities further traditionally\nrequires expensive collection of more annotated data. In this work, we explore\nthe potential of improving the capabilities of language models without new\ndata, merely using automated feedback to the validity of their predictions in\narithmetic reasoning (self-training). We find that models can substantially\nimprove in both single-round (offline) and online self-training. In the offline\nsetting, supervised methods are able to deliver gains comparable to preference\noptimization, but in online self-training, preference optimization shows to\nlargely outperform supervised training thanks to superior stability and\nrobustness on unseen types of problems.\n","authors":["Marek KadlÄÃ­k","Michal Å tefÃ¡nik"],"pdf_url":"https://arxiv.org/pdf/2407.08400v1.pdf","comment":"Appeared in ICLR 2024 LLMAgents"},{"id":"http://arxiv.org/abs/2407.03646v2","updated":"2024-07-11T10:56:01Z","published":"2024-07-04T05:37:09Z","title":"Differentiating between human-written and AI-generated texts using\n  linguistic features automatically extracted from an online computational tool","summary":"  While extensive research has focused on ChatGPT in recent years, very few\nstudies have systematically quantified and compared linguistic features between\nhuman-written and Artificial Intelligence (AI)-generated language. This study\naims to investigate how various linguistic components are represented in both\ntypes of texts, assessing the ability of AI to emulate human writing. Using\nhuman-authored essays as a benchmark, we prompted ChatGPT to generate essays of\nequivalent length. These texts were analyzed using Open Brain AI, an online\ncomputational tool, to extract measures of phonological, morphological,\nsyntactic, and lexical constituents. Despite AI-generated texts appearing to\nmimic human speech, the results revealed significant differences across\nmultiple linguistic features such as consonants, word stress, nouns, verbs,\npronouns, direct objects, prepositional modifiers, and use of difficult words\namong others. These findings underscore the importance of integrating automated\ntools for efficient language assessment, reducing time and effort in data\nanalysis. Moreover, they emphasize the necessity for enhanced training\nmethodologies to improve the capacity of AI for producing more human-like text.\n","authors":["Georgios P. Georgiou"],"pdf_url":"https://arxiv.org/pdf/2407.03646v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08388v1","updated":"2024-07-11T10:51:06Z","published":"2024-07-11T10:51:06Z","title":"On the attribution of confidence to large language models","summary":"  Credences are mental states corresponding to degrees of confidence in\npropositions. Attribution of credences to Large Language Models (LLMs) is\ncommonplace in the empirical literature on LLM evaluation. Yet the theoretical\nbasis for LLM credence attribution is unclear. We defend three claims. First,\nour semantic claim is that LLM credence attributions are (at least in general)\ncorrectly interpreted literally, as expressing truth-apt beliefs on the part of\nscientists that purport to describe facts about LLM credences. Second, our\nmetaphysical claim is that the existence of LLM credences is at least\nplausible, although current evidence is inconclusive. Third, our epistemic\nclaim is that LLM credence attributions made in the empirical literature on LLM\nevaluation are subject to non-trivial sceptical concerns. It is a distinct\npossibility that even if LLMs have credences, LLM credence attributions are\ngenerally false because the experimental techniques used to assess LLM\ncredences are not truth-tracking.\n","authors":["Geoff Keeling","Winnie Street"],"pdf_url":"https://arxiv.org/pdf/2407.08388v1.pdf","comment":"22 pages, 0 figures"},{"id":"http://arxiv.org/abs/2302.13939v5","updated":"2024-07-11T10:16:12Z","published":"2023-02-27T16:43:04Z","title":"SpikeGPT: Generative Pre-trained Language Model with Spiking Neural\n  Networks","summary":"  As the size of large language models continue to scale, so does the\ncomputational resources required to run it. Spiking Neural Networks (SNNs) have\nemerged as an energy-efficient approach to deep learning that leverage sparse\nand event-driven activations to reduce the computational overhead associated\nwith model inference. While they have become competitive with non-spiking\nmodels on many computer vision tasks, SNNs have also proven to be more\nchallenging to train. As a result, their performance lags behind modern deep\nlearning, and we are yet to see the effectiveness of SNNs in language\ngeneration. In this paper, inspired by the Receptance Weighted Key Value (RWKV)\nlanguage model, we successfully implement `SpikeGPT', a generative language\nmodel with binary, event-driven spiking activation units. We train the proposed\nmodel on two model variants: 45M and 216M parameters. To the best of our\nknowledge, SpikeGPT is the largest backpropagation-trained SNN model to date,\nrendering it suitable for both the generation and comprehension of natural\nlanguage. We achieve this by modifying the transformer block to replace\nmulti-head self attention to reduce quadratic computational complexity O(N^2)\nto linear complexity O(N) with increasing sequence length. Input tokens are\ninstead streamed in sequentially to our attention mechanism (as with typical\nSNNs). Our preliminary experiments show that SpikeGPT remains competitive with\nnon-spiking models on tested benchmarks, while maintaining 20x fewer operations\nwhen processed on neuromorphic hardware that can leverage sparse, event-driven\nactivations. Our code implementation is available at\nhttps://github.com/ridgerchu/SpikeGPT.\n","authors":["Rui-Jie Zhu","Qihang Zhao","Guoqi Li","Jason K. Eshraghian"],"pdf_url":"https://arxiv.org/pdf/2302.13939v5.pdf","comment":"Accepted by TMLR"},{"id":"http://arxiv.org/abs/2407.08351v1","updated":"2024-07-11T10:03:47Z","published":"2024-07-11T10:03:47Z","title":"AutoBencher: Creating Salient, Novel, Difficult Datasets for Language\n  Models","summary":"  Evaluation is critical for assessing capabilities, tracking scientific\nprogress, and informing model selection. In this paper, we present three\ndesiderata for a good benchmark for language models: (i) salience (e.g.,\nknowledge about World War II is more salient than a random day in history),\n(ii) novelty (i.e., the benchmark reveals new trends in model rankings not\nshown by previous benchmarks), and (iii) difficulty (i.e., the benchmark should\nbe difficult for existing models, leaving headroom for future improvement). We\noperationalize these three desiderata and cast benchmark creation as a search\nproblem, that of finding benchmarks that that satisfy all three desiderata. To\ntackle this search problem, we present AutoBencher, which uses a language model\nto automatically search for datasets that meet the three desiderata.\nAutoBencher uses privileged information (e.g. relevant documents) to construct\nreliable datasets, and adaptivity with reranking to optimize for the search\nobjective. We use AutoBencher to create datasets for math, multilingual, and\nknowledge-intensive question answering. The scalability of AutoBencher allows\nit to test fine-grained categories and tail knowledge, creating datasets that\nare on average 27% more novel and 22% more difficult than existing benchmarks.\nA closer investigation of our constructed datasets shows that we can identify\nspecific gaps in LM knowledge in language models that are not captured by\nexisting benchmarks, such as Gemini Pro performing much worse on question\nanswering about the Permian Extinction and Fordism, while OpenAGI-7B performing\nsurprisingly well on QA about COVID-19.\n","authors":["Xiang Lisa Li","Evan Zheran Liu","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2407.08351v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2407.08348v1","updated":"2024-07-11T09:56:51Z","published":"2024-07-11T09:56:51Z","title":"Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large\n  Language Models -- The Story Goes On","summary":"  In this paper, we investigate the underlying factors that potentially enhance\nthe mathematical reasoning capabilities of large language models (LLMs). We\nargue that the data scaling law for math reasoning capabilities in modern LLMs\nis far from being saturated, highlighting how the model's quality improves with\nincreases in data quantity. To support this claim, we introduce the\nSkywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using\nour proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved\nimpressive accuracies of 51.2% on the competition-level MATH benchmark and\n83.9% on the GSM8K benchmark using only SFT data, outperforming an early\nversion of GPT-4 on MATH. The superior performance of Skywork-Math models\ncontributes to our novel two-stage data synthesis and model SFT pipelines,\nwhich include three different augmentation methods and a diverse seed problem\nset, ensuring both the quantity and quality of Skywork-MathQA dataset across\nvarying difficulty levels. Most importantly, we provide several practical\ntakeaways to enhance math reasoning abilities in LLMs for both research and\nindustry applications.\n","authors":["Liang Zeng","Liangjun Zhong","Liang Zhao","Tianwen Wei","Liu Yang","Jujie He","Cheng Cheng","Rui Hu","Yang Liu","Shuicheng Yan","Han Fang","Yahui Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.08348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01579v2","updated":"2024-07-11T09:50:11Z","published":"2024-02-02T17:17:42Z","title":"Are Paralinguistic Representations all that is needed for Speech Emotion\n  Recognition?","summary":"  Availability of representations from pre-trained models (PTMs) have\nfacilitated substantial progress in speech emotion recognition (SER).\nParticularly, representations from PTM trained for paralinguistic speech\nprocessing have shown state-of-the-art (SOTA) performance for SER. However,\nsuch paralinguistic PTM representations haven't been evaluated for SER in\nlinguistic environments other than English. Also, paralinguistic PTM\nrepresentations haven't been investigated in benchmarks such as SUPERB,\nEMO-SUPERB, ML-SUPERB for SER. This makes it difficult to access the efficacy\nof paralinguistic PTM representations for SER in multiple languages. To fill\nthis gap, we perform a comprehensive comparative study of five SOTA PTM\nrepresentations. Our results shows that paralinguistic PTM (TRILLsson)\nrepresentations performs the best and this performance can be attributed to its\neffectiveness in capturing pitch, tone and other speech characteristics more\neffectively than other PTM representations.\n","authors":["Orchid Chetia Phukan","Gautam Siddharth Kashyap","Arun Balaji Buduru","Rajesh Sharma"],"pdf_url":"https://arxiv.org/pdf/2402.01579v2.pdf","comment":"Accepted to INTERSPEECH 24"},{"id":"http://arxiv.org/abs/2312.01648v3","updated":"2024-07-11T09:32:19Z","published":"2023-12-04T06:01:32Z","title":"Characterizing Large Language Model Geometry Helps Solve Toxicity\n  Detection and Generation","summary":"  Large Language Models (LLMs) drive current AI breakthroughs despite very\nlittle being known about their internal representations. In this work, we\npropose to shed the light on LLMs inner mechanisms through the lens of\ngeometry. In particular, we develop in closed form $(i)$ the intrinsic\ndimension in which the Multi-Head Attention embeddings are constrained to exist\nand $(ii)$ the partition and per-region affine mappings of the feedforward\n(MLP) network of LLMs' layers. Our theoretical findings further enable the\ndesign of novel principled solutions applicable to state-of-the-art LLMs.\nFirst, we show that, through our geometric understanding, we can bypass LLMs'\nRLHF protection by controlling the embedding's intrinsic dimension through\ninformed prompt manipulation. Second, we derive interpretable geometrical\nfeatures that can be extracted from any (pre-trained) LLM, providing a rich\nabstract representation of their inputs. We observe that these features are\nsufficient to help solve toxicity detection, and even allow the identification\nof various types of toxicity. Our results demonstrate how, even in large-scale\nregimes, exact theoretical results can answer practical questions in LLMs.\nCode: https://github.com/RandallBalestriero/SplineLLM\n","authors":["Randall Balestriero","Romain Cosentino","Sarath Shekkizhar"],"pdf_url":"https://arxiv.org/pdf/2312.01648v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00072v3","updated":"2024-07-11T09:28:34Z","published":"2024-06-21T08:52:11Z","title":"Pistis-RAG: A Scalable Cascading Framework Towards Trustworthy\n  Retrieval-Augmented Generation","summary":"  In Greek mythology, Pistis symbolized good faith, trust, and reliability.\nDrawing inspiration from these principles, Pistis-RAG is a scalable multi-stage\nframework designed to address the challenges of large-scale retrieval-augmented\ngeneration (RAG) systems. This framework consists of distinct stages: matching,\npre-ranking, ranking, reasoning, and aggregating. Each stage contributes to\nnarrowing the search space, prioritizing semantically relevant documents,\naligning with the large language model's (LLM) preferences, supporting complex\nchain-of-thought (CoT) methods, and combining information from multiple\nsources.\n  Our ranking stage introduces a significant innovation by recognizing that\nsemantic relevance alone may not lead to improved generation quality, due to\nthe sensitivity of the few-shot prompt order, as noted in previous research.\nThis critical aspect is often overlooked in current RAG frameworks.\n  We argue that the alignment issue between LLMs and external knowledge ranking\nmethods is tied to the model-centric paradigm dominant in RAG systems. We\npropose a content-centric approach, emphasizing seamless integration between\nLLMs and external information sources to optimize content transformation for\nspecific tasks.\n  Our novel ranking stage is designed specifically for RAG systems,\nincorporating principles of information retrieval while considering the unique\nbusiness scenarios reflected in LLM preferences and user feedback. We simulated\nfeedback signals on the MMLU benchmark, resulting in a 9.3% performance\nimprovement. Our model and code will be open-sourced on GitHub. Additionally,\nexperiments on real-world, large-scale data validate the scalability of our\nframework.\n","authors":["Yu Bai","Yukai Miao","Li Chen","Dan Li","Yanyu Ren","Hongtao Xie","Ce Yang","Xuhui Cai"],"pdf_url":"https://arxiv.org/pdf/2407.00072v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08331v1","updated":"2024-07-11T09:28:27Z","published":"2024-07-11T09:28:27Z","title":"Towards Explainable Evolution Strategies with Large Language Models","summary":"  This paper introduces an approach that integrates self-adaptive Evolution\nStrategies (ES) with Large Language Models (LLMs) to enhance the explainability\nof complex optimization processes. By employing a self-adaptive ES equipped\nwith a restart mechanism, we effectively navigate the challenging landscapes of\nbenchmark functions, capturing detailed logs of the optimization journey,\nincluding fitness evolution, step-size adjustments, and restart events due to\nstagnation. An LLM is then utilized to process these logs, generating concise,\nuser-friendly summaries that highlight key aspects such as convergence\nbehavior, optimal fitness achievements, and encounters with local optima. Our\ncase study on the Rastrigin function demonstrates how our approach makes the\ncomplexities of ES optimization transparent and accessible. Our findings\nhighlight the potential of using LLMs to bridge the gap between advanced\noptimization algorithms and their interpretability.\n","authors":["Jill Baumann","Oliver Kramer"],"pdf_url":"https://arxiv.org/pdf/2407.08331v1.pdf","comment":"Accepted at ESANN 2024"},{"id":"http://arxiv.org/abs/2307.07421v3","updated":"2024-07-11T09:20:23Z","published":"2023-07-12T12:51:23Z","title":"SummaryMixing: A Linear-Complexity Alternative to Self-Attention for\n  Speech Recognition and Understanding","summary":"  Modern speech processing systems rely on self-attention. Unfortunately, token\nmixing with self-attention takes quadratic time in the length of the speech\nutterance, slowing down inference and training and increasing memory\nconsumption. Cheaper alternatives to self-attention for ASR have been\ndeveloped, but they fail to consistently reach the same level of accuracy. This\npaper, therefore, proposes a novel linear-time alternative to self-attention.\nIt summarises an utterance with the mean over vectors for all time steps. This\nsingle summary is then combined with time-specific information. We call this\nmethod \"SummaryMixing\". Introducing SummaryMixing in state-of-the-art ASR\nmodels makes it feasible to preserve or exceed previous speech recognition\nperformance while making training and inference up to 28% faster and reducing\nmemory use by half.\n","authors":["Titouan Parcollet","Rogier van Dalen","Shucong Zhang","Sourav Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2307.07421v3.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2405.14908v2","updated":"2024-07-11T08:44:45Z","published":"2024-05-23T09:44:02Z","title":"Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model\n  Pretraining","summary":"  Large language models exhibit exceptional generalization capabilities,\nprimarily attributed to the utilization of diversely sourced data. However,\nconventional practices in integrating this diverse data heavily rely on\nheuristic schemes, lacking theoretical guidance. This research tackles these\nlimitations by investigating strategies based on low-cost proxies for data\nmixtures, with the aim of streamlining data curation to enhance training\nefficiency. Specifically, we propose a unified scaling law, termed\n$\\textbf{BiMix}$, which accurately models the bivariate scaling behaviors of\nboth data quantity and mixing proportions. We conduct systematic experiments\nand provide empirical evidence for the predictive power and fundamental\nprinciples of $\\textbf{BiMix}$. Notably, our findings reveal that\nentropy-driven training-free data mixtures can achieve comparable or even\nbetter performance than more resource-intensive methods. We hope that our\nquantitative insights can shed light on further judicious research and\ndevelopment in cost-effective language modeling.\n","authors":["Ce Ge","Zhijian Ma","Daoyuan Chen","Yaliang Li","Bolin Ding"],"pdf_url":"https://arxiv.org/pdf/2405.14908v2.pdf","comment":"typos corrected"},{"id":"http://arxiv.org/abs/2407.06027v2","updated":"2024-07-11T08:30:01Z","published":"2024-07-08T15:25:33Z","title":"PAS: Data-Efficient Plug-and-Play Prompt Augmentation System","summary":"  In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering.\n","authors":["Miao Zheng","Hao Liang","Fan Yang","Haoze Sun","Tianpeng Li","Lingchu Xiong","Yan Zhang","Youzhen Wu","Kun Li","Yanjun Shen","Mingan Lin","Tao Zhang","Guosheng Dong","Yujing Qiao","Kun Fang","Weipeng Chen","Bin Cui","Wentao Zhang","Zenan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.06027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08273v1","updated":"2024-07-11T08:19:58Z","published":"2024-07-11T08:19:58Z","title":"RB-SQL: A Retrieval-based LLM Framework for Text-to-SQL","summary":"  Large language models (LLMs) with in-context learning have significantly\nimproved the performance of text-to-SQL task. Previous works generally focus on\nusing exclusive SQL generation prompt to improve the LLMs' reasoning ability.\nHowever, they are mostly hard to handle large databases with numerous tables\nand columns, and usually ignore the significance of pre-processing database and\nextracting valuable information for more efficient prompt engineering. Based on\nabove analysis, we propose RB-SQL, a novel retrieval-based LLM framework for\nin-context prompt engineering, which consists of three modules that retrieve\nconcise tables and columns as schema, and targeted examples for in-context\nlearning. Experiment results demonstrate that our model achieves better\nperformance than several competitive baselines on public datasets BIRD and\nSpider.\n","authors":["Zhenhe Wu","Zhongqiu Li","Jie Zhang","Mengxiang Li","Yu Zhao","Ruiyu Fang","Zhongjiang He","Xuelong Li","Zhoujun Li","Shuangyong Song"],"pdf_url":"https://arxiv.org/pdf/2407.08273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08269v1","updated":"2024-07-11T08:12:28Z","published":"2024-07-11T08:12:28Z","title":"LLMs' morphological analyses of complex FST-generated Finnish words","summary":"  Rule-based language processing systems have been overshadowed by neural\nsystems in terms of utility, but it remains unclear whether neural NLP systems,\nin practice, learn the grammar rules that humans use. This work aims to shed\nlight on the issue by evaluating state-of-the-art LLMs in a task of\nmorphological analysis of complex Finnish noun forms. We generate the forms\nusing an FST tool, and they are unlikely to have occurred in the training sets\nof the LLMs, therefore requiring morphological generalisation capacity. We find\nthat GPT-4-turbo has some difficulties in the task while GPT-3.5-turbo\nstruggles and smaller models Llama2-70B and Poro-34B fail nearly completely.\n","authors":["Anssi Moisio","Mathias Creutz","Mikko Kurimo"],"pdf_url":"https://arxiv.org/pdf/2407.08269v1.pdf","comment":"To appear at the CMCL Workshop at ACL 2024"},{"id":"http://arxiv.org/abs/2406.18664v3","updated":"2024-07-11T07:45:04Z","published":"2024-06-26T18:09:46Z","title":"Evaluating Copyright Takedown Methods for Language Models","summary":"  Language models (LMs) derive their capabilities from extensive training on\ndiverse data, including potentially copyrighted material. These models can\nmemorize and generate content similar to their training data, posing potential\nconcerns. Therefore, model creators are motivated to develop mitigation methods\nthat prevent generating protected content. We term this procedure as copyright\ntakedowns for LMs, noting the conceptual similarity to (but legal distinction\nfrom) the DMCA takedown This paper introduces the first evaluation of the\nfeasibility and side effects of copyright takedowns for LMs. We propose\nCoTaEval, an evaluation framework to assess the effectiveness of copyright\ntakedown methods, the impact on the model's ability to retain uncopyrightable\nfactual knowledge from the training data whose recitation is embargoed, and how\nwell the model maintains its general utility and efficiency. We examine several\nstrategies, including adding system prompts, decoding-time filtering\ninterventions, and unlearning approaches. Our findings indicate that no tested\nmethod excels across all metrics, showing significant room for research in this\nunique problem setting and indicating potential unresolved challenges for live\npolicy proposals.\n","authors":["Boyi Wei","Weijia Shi","Yangsibo Huang","Noah A. Smith","Chiyuan Zhang","Luke Zettlemoyer","Kai Li","Peter Henderson"],"pdf_url":"https://arxiv.org/pdf/2406.18664v3.pdf","comment":"31 pages, 9 figures, 14 tables"},{"id":"http://arxiv.org/abs/2407.04185v2","updated":"2024-07-11T07:35:06Z","published":"2024-07-04T23:26:56Z","title":"HAF-RM: A Hybrid Alignment Framework for Reward Model Training","summary":"  The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nTheoretical justifications and experiment results on five datasets show the\nvalidity and effectiveness of our proposed hybrid framework for training a\nhigh-quality reward model. By decoupling the reward modeling procedure and\nincorporating hybrid supervision, our HaF-RM framework offers a principled and\neffective approach to enhancing the performance and alignment of reward models,\na critical component in the responsible development of powerful language\nmodels. We release our code at https://haf-rm.github.io.\n","authors":["Shujun Liu","Xiaoyu Shen","Yuhang Lai","Siyuan Wang","Shengbin Yue","Zengfeng Huang","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.04185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14275v2","updated":"2024-07-11T07:29:12Z","published":"2024-06-20T12:58:26Z","title":"Step-Back Profiling: Distilling User History for Personalized Scientific\n  Writing","summary":"  Large language models (LLM) excel at a variety of natural language processing\ntasks, yet they struggle to generate personalized content for individuals,\nparticularly in real-world scenarios like scientific writing. Addressing this\nchallenge, we introduce STEP-BACK PROFILING to personalize LLMs by distilling\nuser history into concise profiles, including essential traits and preferences\nof users. To conduct the experiments, we construct a Personalized Scientific\nWriting (PSW) dataset to study multi-user personalization. PSW requires the\nmodels to write scientific papers given specialized author groups with diverse\nacademic backgrounds. As for the results, we demonstrate the effectiveness of\ncapturing user characteristics via STEP-BACK PROFILING for collaborative\nwriting. Moreover, our approach outperforms the baselines by up to 3.6 points\non the general personalization benchmark (LaMP), including 7 personalization\nLLM tasks. Our ablation studies validate the contributions of different\ncomponents in our method and provide insights into our task definition. Our\ndataset and code are available at\n\\url{https://github.com/gersteinlab/step-back-profiling}.\n","authors":["Xiangru Tang","Xingyao Zhang","Yanjun Shao","Jie Wu","Yilun Zhao","Arman Cohan","Ming Gong","Dongmei Zhang","Mark Gerstein"],"pdf_url":"https://arxiv.org/pdf/2406.14275v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04854v2","updated":"2024-07-11T07:28:56Z","published":"2023-12-08T06:22:12Z","title":"Learning to Break: Knowledge-Enhanced Reasoning in Multi-Agent Debate\n  System","summary":"  Multi-agent debate system (MAD) imitating the process of human discussion in\npursuit of truth, aims to align the correct cognition of different agents for\nthe optimal solution. It is challenging to make various agents perform right\nand highly consistent cognition due to their limited and different knowledge\nbackgrounds (i.e., cognitive islands), which hinders the search for the optimal\nsolution. To address the challenge, we propose a novel\n\\underline{M}ulti-\\underline{A}gent \\underline{D}ebate with\n\\underline{K}nowledge-\\underline{E}nhanced framework (\\textbf{MADKE}) to\npromote the system to find the solution. First, we involve a shared retrieval\nknowledge pool in the debate process to solve the problem of limited and\ndifferent knowledge backgrounds. Then, we propose an adaptive knowledge\nselection method to guarantee the accuracy and personalization of knowledge.\nThis method allows agents to choose whether to use external knowledge in each\nconversation round according to their own needs. Our experimental results on\nsix datasets show that our method achieves state-of-the-art results compared to\nexisting single-agent and multi-agent methods. Further analysis reveals that\nthe introduction of retrieval knowledge can help the agent to break cognitive\nislands in the debate process and effectively improve the consistency and\ncorrectness of the model. Moreover, MADKE using Qwen1.5-72B-Chat surpasses\nGPT-4 by +1.26\\% on average in six datasets, which validates that our method\ncan help open-source LLMs achieve or even surpass the performance of GPT-4. Our\ncode is available at \\url{https://github.com/FutureForMe/MADKE}.\n","authors":["Haotian Wang","Xiyuan Du","Weijiang Yu","Qianglong Chen","Kun Zhu","Zheng Chu","Lian Yan","Yi Guan"],"pdf_url":"https://arxiv.org/pdf/2312.04854v2.pdf","comment":"18 pages, 10 figures, work in progress"},{"id":"http://arxiv.org/abs/2407.08223v1","updated":"2024-07-11T06:50:19Z","published":"2024-07-11T06:50:19Z","title":"Speculative RAG: Enhancing Retrieval Augmented Generation through\n  Drafting","summary":"  Retrieval augmented generation (RAG) combines the generative abilities of\nlarge language models (LLMs) with external knowledge sources to provide more\naccurate and up-to-date responses. Recent RAG advancements focus on improving\nretrieval outcomes through iterative LLM refinement or self-critique\ncapabilities acquired through additional instruction tuning of LLMs. In this\nwork, we introduce Speculative RAG - a framework that leverages a larger\ngeneralist LM to efficiently verify multiple RAG drafts produced in parallel by\na smaller, distilled specialist LM. Each draft is generated from a distinct\nsubset of retrieved documents, offering diverse perspectives on the evidence\nwhile reducing input token counts per draft. This approach enhances\ncomprehension of each subset and mitigates potential position bias over long\ncontext. Our method accelerates RAG by delegating drafting to the smaller\nspecialist LM, with the larger generalist LM performing a single verification\npass over the drafts. Extensive experiments demonstrate that Speculative RAG\nachieves state-of-the-art performance with reduced latency on TriviaQA,\nMuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy\nby up to 12.97% while reducing latency by 51% compared to conventional RAG\nsystems on PubHealth.\n","authors":["Zilong Wang","Zifeng Wang","Long Le","Huaixiu Steven Zheng","Swaroop Mishra","Vincent Perot","Yuwei Zhang","Anush Mattapalli","Ankur Taly","Jingbo Shang","Chen-Yu Lee","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2407.08223v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.03978v2","updated":"2024-07-11T06:44:47Z","published":"2024-07-04T14:50:45Z","title":"Benchmarking Complex Instruction-Following with Multiple Constraints\n  Composition","summary":"  Instruction following is one of the fundamental capabilities of large\nlanguage models (LLMs). As the ability of LLMs is constantly improving, they\nhave been increasingly applied to deal with complex human instructions in\nreal-world scenarios. Therefore, how to evaluate the ability of complex\ninstruction-following of LLMs has become a critical research problem. Existing\nbenchmarks mainly focus on modeling different types of constraints in human\ninstructions while neglecting the composition of different constraints, which\nis an indispensable constituent in complex instructions. To this end, we\npropose ComplexBench, a benchmark for comprehensively evaluating the ability of\nLLMs to follow complex instructions composed of multiple constraints. We\npropose a hierarchical taxonomy for complex instructions, including 4\nconstraint types, 19 constraint dimensions, and 4 composition types, and\nmanually collect a high-quality dataset accordingly. To make the evaluation\nreliable, we augment LLM-based evaluators with rules to effectively verify\nwhether generated texts can satisfy each constraint and composition.\nFurthermore, we obtain the final evaluation score based on the dependency\nstructure determined by different composition types. ComplexBench identifies\nsignificant deficiencies in existing LLMs when dealing with complex\ninstructions with multiple constraints composition.\n","authors":["Bosi Wen","Pei Ke","Xiaotao Gu","Lindong Wu","Hao Huang","Jinfeng Zhou","Wenchuang Li","Binxin Hu","Wendy Gao","Jiaxin Xu","Yiming Liu","Jie Tang","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2407.03978v2.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.08219v1","updated":"2024-07-11T06:40:36Z","published":"2024-07-11T06:40:36Z","title":"Generating Contextually-Relevant Navigation Instructions for Blind and\n  Low Vision People","summary":"  Navigating unfamiliar environments presents significant challenges for blind\nand low-vision (BLV) individuals. In this work, we construct a dataset of\nimages and goals across different scenarios such as searching through kitchens\nor navigating outdoors. We then investigate how grounded instruction generation\nmethods can provide contextually-relevant navigational guidance to users in\nthese instances. Through a sighted user study, we demonstrate that large\npretrained language models can produce correct and useful instructions\nperceived as beneficial for BLV users. We also conduct a survey and interview\nwith 4 BLV users and observe useful insights on preferences for different\ninstructions based on the scenario.\n","authors":["Zain Merchant","Abrar Anwar","Emily Wang","Souti Chattopadhyay","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2407.08219v1.pdf","comment":"Accepted as RO-MAN 2024 Late Breaking Report"},{"id":"http://arxiv.org/abs/2404.09971v2","updated":"2024-07-11T06:31:31Z","published":"2024-04-15T17:48:46Z","title":"Constructing Benchmarks and Interventions for Combating Hallucinations\n  in LLMs","summary":"  Large language models (LLMs) are prone to hallucinations, which sparked a\nwidespread effort to detect and prevent them. Recent work attempts to mitigate\nhallucinations by intervening in the model's generation, typically computing\nrepresentative vectors of hallucinations vs. grounded generations, for steering\nthe model's hidden states away from a hallucinatory state. However, common\nstudies employ different setups and do not properly separate different possible\ncauses of hallucinations, making interventions misguided. In this work, we\nintroduce a method for categorizing examples based on the model's prior\nknowledge, named WACK. We construct WACK benchmarks that support interventions\nin two settings: open-book and closed-book question answering. Using the\nbenchmarks, we perform an extensive investigation of the effect of different\nchoices for intervention, such as the intervened components, and how often and\nhow strongly to intervene. We find that intervention success varies depending\non the component, with the attention blocks performing well and the residual\nstream proving detrimental to language modeling capabilities. We also show that\ninterventions can benefit from representative vectors collected before, rather\nthan after, a hallucination occurs. Finally, we introduce a new dynamic\nintervention, which intervenes only if needed, and thus is more robust than\nstandard static interventions. The code is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation .\n","authors":["Adi Simhi","Jonathan Herzig","Idan Szpektor","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2404.09971v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08206v1","updated":"2024-07-11T06:17:08Z","published":"2024-07-11T06:17:08Z","title":"System Report for CCL24-Eval Task 7: Multi-Error Modeling and\n  Fluency-Targeted Pre-training for Chinese Essay Evaluation","summary":"  This system report presents our approaches and results for the Chinese Essay\nFluency Evaluation (CEFE) task at CCL-2024. For Track 1, we optimized\npredictions for challenging fine-grained error types using binary\nclassification models and trained coarse-grained models on the Chinese Learner\n4W corpus. In Track 2, we enhanced performance by constructing a pseudo-dataset\nwith multiple error types per sentence. For Track 3, where we achieved first\nplace, we generated fluency-rated pseudo-data via back-translation for\npre-training and used an NSP-based strategy with Symmetric Cross Entropy loss\nto capture context and mitigate long dependencies. Our methods effectively\naddress key challenges in Chinese Essay Fluency Evaluation.\n","authors":["Jingshen Zhang","Xiangyu Yang","Xinkai Su","Xinglu Chen","Tianyou Huang","Xinying Qiu"],"pdf_url":"https://arxiv.org/pdf/2407.08206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01325v3","updated":"2024-07-11T06:11:46Z","published":"2024-01-02T18:30:51Z","title":"LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning","summary":"  It is well known that LLMs cannot generalize well to long contexts whose\nlengths are larger than the training sequence length. This poses challenges\nwhen employing LLMs for processing long input sequences during inference. In\nthis work, we argue that LLMs themselves have inherent capabilities to handle\nlong contexts without fine-tuning. To achieve this goal, we propose SelfExtend\nto extend the context window of LLMs by constructing bi-level attention\ninformation: the grouped attention and the neighbor attention. The grouped\nattention captures the dependencies among tokens that are far apart, while\nneighbor attention captures dependencies among adjacent tokens within a\nspecified range. The two-level attentions are computed based on the original\nmodel's self-attention mechanism during inference. With minor code\nmodification, our SelfExtend can effortlessly extend existing LLMs' context\nwindow without any fine-tuning. We conduct comprehensive experiments on\nmultiple benchmarks and the results show that our SelfExtend can effectively\nextend existing LLMs' context window length. The code can be found at\n\\url{https://github.com/datamllab/LongLM}.\n","authors":["Hongye Jin","Xiaotian Han","Jingfeng Yang","Zhimeng Jiang","Zirui Liu","Chia-Yuan Chang","Huiyuan Chen","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2401.01325v3.pdf","comment":"ICML2024 Spotlight"},{"id":"http://arxiv.org/abs/2406.02746v3","updated":"2024-07-11T06:07:19Z","published":"2024-06-04T20:02:52Z","title":"RATT: A Thought Structure for Coherent and Correct LLM Reasoning","summary":"  Large Language Models (LLMs) gain substantial reasoning and decision-making\ncapabilities from thought structures. However, existing methods such as Tree of\nThought and Retrieval Augmented Thoughts often fall short in complex tasks due\nto the limitations of insufficient local retrieval of factual knowledge and\ninadequate global selection of strategies. These limitations make it\nchallenging for these methods to balance factual accuracy and comprehensive\nlogical optimization effectively. To address these limitations, we introduce\nthe Retrieval Augmented Thought Tree (RATT), a novel thought structure that\nconsiders both overall logical soundness and factual correctness at each step\nof the thinking process. Specifically, at every point of a thought branch, RATT\nperforms planning and lookahead to explore and evaluate multiple potential\nreasoning steps, and integrate the fact-checking ability of Retrieval-Augmented\nGeneration (RAG) with LLM's ability to assess overall strategy. Through this\ncombination of factual knowledge and strategic feasibility, the RATT adjusts\nand integrates the thought tree structure to search for the most promising\nbranches within the search space. This thought structure significantly enhances\nthe model's coherence in logical inference and efficiency in decision-making,\nand thus increases the limit of the capacity of LLM to generate reliable\ninferences and decisions based on thought structures. A broad range of\nexperiments on different types of tasks showcases that the RATT structure\nsignificantly outperforms existing methods in factual correctness and logical\ncoherence.\n","authors":["Jinghan Zhang","Xiting Wang","Weijieying Ren","Lu Jiang","Dongjie Wang","Kunpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2406.02746v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08195v1","updated":"2024-07-11T05:33:19Z","published":"2024-07-11T05:33:19Z","title":"A Text-to-Game Engine for UGC-Based Role-Playing Games","summary":"  The shift from professionally generated content (PGC) to user-generated\ncontent (UGC) has revolutionized various media formats, from text to video.\nWith the rapid advancements in generative AI, a similar shift is set to\ntransform the game industry, particularly in the realm of role-playing games\n(RPGs). This paper introduces a new framework for a text-to-game engine that\nutilizes foundation models to convert simple textual inputs into complex,\ninteractive RPG experiences. The engine dynamically renders the game story in a\nmulti-modal format and adjusts the game character, environment, and mechanics\nin real-time in response to player actions. Using this framework, we developed\nthe \"Zagii\" game engine, which has successfully supported hundreds of RPG games\nacross a diverse range of genres and facilitated tens of thousands of online\nuser gameplay instances. This validates the effectiveness of our frame-work.\nOur work showcases the potential for a more open and democratized gaming\nparadigm, highlighting the transformative impact of generative AI on the game\nlife cycle.\n","authors":["Lei Zhang","Xuezheng Peng","Shuyi Yang","Feiyang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08195v1.pdf","comment":"13 pages,11 figures"},{"id":"http://arxiv.org/abs/2407.08189v1","updated":"2024-07-11T05:13:38Z","published":"2024-07-11T05:13:38Z","title":"fairBERTs: Erasing Sensitive Information Through Semantic and\n  Fairness-aware Perturbations","summary":"  Pre-trained language models (PLMs) have revolutionized both the natural\nlanguage processing research and applications. However, stereotypical biases\n(e.g., gender and racial discrimination) encoded in PLMs have raised negative\nethical implications for PLMs, which critically limits their broader\napplications. To address the aforementioned unfairness issues, we present\nfairBERTs, a general framework for learning fair fine-tuned BERT series models\nby erasing the protected sensitive information via semantic and fairness-aware\nperturbations generated by a generative adversarial network. Through extensive\nqualitative and quantitative experiments on two real-world tasks, we\ndemonstrate the great superiority of fairBERTs in mitigating unfairness while\nmaintaining the model utility. We also verify the feasibility of transferring\nadversarial components in fairBERTs to other conventionally trained BERT-like\nmodels for yielding fairness improvements. Our findings may shed light on\nfurther research on building fairer fine-tuned PLMs.\n","authors":["Jinfeng Li","Yuefeng Chen","Xiangyu Liu","Longtao Huang","Rong Zhang","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2407.08189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07376v2","updated":"2024-07-11T05:09:30Z","published":"2024-04-10T22:26:26Z","title":"LLMs in Biomedicine: A study on clinical Named Entity Recognition","summary":"  Large Language Models (LLMs) demonstrate remarkable versatility in various\nNLP tasks but encounter distinct challenges in biomedical due to the\ncomplexities of language and data scarcity. This paper investigates LLMs\napplication in the biomedical domain by exploring strategies to enhance their\nperformance for the NER task. Our study reveals the importance of meticulously\ndesigned prompts in the biomedical. Strategic selection of in-context examples\nyields a marked improvement, offering ~15-20\\% increase in F1 score across all\nbenchmark datasets for biomedical few-shot NER. Additionally, our results\nindicate that integrating external biomedical knowledge via prompting\nstrategies can enhance the proficiency of general-purpose LLMs to meet the\nspecialized needs of biomedical NER. Leveraging a medical knowledge base, our\nproposed method, DiRAG, inspired by Retrieval-Augmented Generation (RAG), can\nboost the zero-shot F1 score of LLMs for biomedical NER. Code is released at\n\\url{https://github.com/masoud-monajati/LLM_Bio_NER}\n","authors":["Masoud Monajatipoor","Jiaxin Yang","Joel Stremmel","Melika Emami","Fazlolah Mohaghegh","Mozhdeh Rouhsedaghat","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2404.07376v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00284v2","updated":"2024-07-11T05:06:25Z","published":"2024-06-01T03:29:56Z","title":"A Closer Look at Logical Reasoning with LLMs: The Choice of Tool Matters","summary":"  The emergence of Large Language Models (LLMs) has demonstrated promising\nprogress in solving logical reasoning tasks effectively. Several recent\napproaches have proposed to change the role of the LLM from the reasoner into a\ntranslator between natural language statements and symbolic representations\nwhich are then sent to external symbolic solvers to resolve. This paradigm has\nestablished the current state-of-the-art result in logical reasoning (i.e.,\ndeductive reasoning). However, it remains unclear whether the variance in\nperformance of these approaches stems from the methodologies employed or the\nspecific symbolic solvers utilized. There is a lack of consistent comparison\nbetween symbolic solvers and how they influence the overall reported\nperformance. This is important, as each symbolic solver also has its own input\nsymbolic language, presenting varying degrees of challenge in the translation\nprocess. To address this gap, we perform experiments on 3 deductive reasoning\nbenchmarks with LLMs augmented with widely used symbolic solvers: Z3, Pyke, and\nProver9. The tool-executable rates of symbolic translation generated by\ndifferent LLMs exhibit a near 50% performance variation. This highlights a\nsignificant difference in performance rooted in very basic choices of tools.\nThe almost linear correlation between the executable rate of translations and\nthe accuracy of the outcomes from Prover9 highlight a strong alignment between\nLLMs ability to translate into Prover9 symbolic language, and the correctness\nof those translations.\n","authors":["Long Hei Matthew Lam","Ramya Keerthy Thatikonda","Ehsan Shareghi"],"pdf_url":"https://arxiv.org/pdf/2406.00284v2.pdf","comment":"Code and data are publicly available at:\n  https://github.com/Mattylam/Logic_Symbolic_Solvers_Experiment"},{"id":"http://arxiv.org/abs/2407.08185v1","updated":"2024-07-11T05:04:52Z","published":"2024-07-11T05:04:52Z","title":"Automatic Generation of Web Censorship Probe Lists","summary":"  Domain probe lists--used to determine which URLs to probe for Web\ncensorship--play a critical role in Internet censorship measurement studies.\nIndeed, the size and accuracy of the domain probe list limits the set of\ncensored pages that can be detected; inaccurate lists can lead to an incomplete\nview of the censorship landscape or biased results. Previous efforts to\ngenerate domain probe lists have been mostly manual or crowdsourced. This\napproach is time-consuming, prone to errors, and does not scale well to the\never-changing censorship landscape.\n  In this paper, we explore methods for automatically generating probe lists\nthat are both comprehensive and up-to-date for Web censorship measurement. We\nstart from an initial set of 139,957 unique URLs from various existing test\nlists consisting of pages from a variety of languages to generate new candidate\npages. By analyzing content from these URLs (i.e., performing topic and keyword\nextraction), expanding these topics, and using them as a feed to search\nengines, our method produces 119,255 new URLs across 35,147 domains. We then\ntest the new candidate pages by attempting to access each URL from servers in\neleven different global locations over a span of four months to check for their\nconnectivity and potential signs of censorship. Our measurements reveal that\nour method discovered over 1,400 domains--not present in the original\ndataset--we suspect to be blocked. In short, automatically updating probe lists\nis possible, and can help further automate censorship measurements at scale.\n","authors":["Jenny Tang","Leo Alvarez","Arjun Brar","Nguyen Phong Hoang","Nicolas Christin"],"pdf_url":"https://arxiv.org/pdf/2407.08185v1.pdf","comment":"To appear in the Proceedings on Privacy Enhancing Technologies 2024"},{"id":"http://arxiv.org/abs/2407.08182v1","updated":"2024-07-11T04:57:52Z","published":"2024-07-11T04:57:52Z","title":"Beyond Text: Leveraging Multi-Task Learning and Cognitive Appraisal\n  Theory for Post-Purchase Intention Analysis","summary":"  Supervised machine-learning models for predicting user behavior offer a\nchallenging classification problem with lower average prediction performance\nscores than other text classification tasks. This study evaluates multi-task\nlearning frameworks grounded in Cognitive Appraisal Theory to predict user\nbehavior as a function of users' self-expression and psychological attributes.\nOur experiments show that users' language and traits improve predictions above\nand beyond models predicting only from text. Our findings highlight the\nimportance of integrating psychological constructs into NLP to enhance the\nunderstanding and prediction of user actions. We close with a discussion of the\nimplications for future applications of large language models for computational\npsychology.\n","authors":["Gerard Christopher Yeo","Shaz Furniturewala","Kokil Jaidka"],"pdf_url":"https://arxiv.org/pdf/2407.08182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04766v5","updated":"2024-07-11T04:01:41Z","published":"2023-09-09T11:42:22Z","title":"SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment\n  to Cultural Reasoning","summary":"  We present SeaEval, a benchmark for multilingual foundation models. In\naddition to characterizing how these models understand and reason with natural\nlanguage, we also investigate how well they comprehend cultural practices,\nnuances, and values. Alongside standard accuracy metrics, we investigate the\nbrittleness of foundation models in the dimensions of semantics and\nmultilinguality. Our analyses span both open-sourced and closed models, leading\nto empirical results across classic NLP tasks, reasoning, and cultural\ncomprehension. Key findings indicate (1) Most models exhibit varied behavior\nwhen given paraphrased instructions. (2) Many models still suffer from exposure\nbias (e.g., positional bias, majority label bias). (3) For questions rooted in\nfactual, scientific, and commonsense knowledge, consistent responses are\nexpected across multilingual queries that are semantically equivalent. Yet,\nmost models surprisingly demonstrate inconsistent performance on these queries.\n(4) Multilingually-trained models have not attained \"balanced multilingual\"\ncapabilities. Our endeavors underscore the need for more generalizable semantic\nrepresentations and enhanced multilingual contextualization. SeaEval can serve\nas a launchpad for more thorough investigations and evaluations for\nmultilingual and multicultural scenarios.\n","authors":["Bin Wang","Zhengyuan Liu","Xin Huang","Fangkai Jiao","Yang Ding","AiTi Aw","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2309.04766v5.pdf","comment":"Published at NAACL 2024. Code: https://seaeval.github.io/"},{"id":"http://arxiv.org/abs/2310.05797v4","updated":"2024-07-11T03:42:12Z","published":"2023-10-09T15:31:03Z","title":"In-Context Explainers: Harnessing LLMs for Explaining Black Box Models","summary":"  Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional capabilities in complex tasks like machine translation, commonsense\nreasoning, and language understanding. One of the primary reasons for the\nadaptability of LLMs in such diverse tasks is their in-context learning (ICL)\ncapability, which allows them to perform well on new tasks by simply using a\nfew task samples in the prompt. Despite their effectiveness in enhancing the\nperformance of LLMs on diverse language and tabular tasks, these methods have\nnot been thoroughly explored for their potential to generate post hoc\nexplanations. In this work, we carry out one of the first explorations to\nanalyze the effectiveness of LLMs in explaining other complex predictive models\nusing ICL. To this end, we propose a novel framework, In-Context Explainers,\ncomprising of three novel approaches that exploit the ICL capabilities of LLMs\nto explain the predictions made by other predictive models. We conduct\nextensive analysis with these approaches on real-world tabular and text\ndatasets and demonstrate that LLMs are capable of explaining other predictive\nmodels similar to state-of-the-art post hoc explainers, opening up promising\navenues for future research into LLM-based post hoc explanations of complex\npredictive models.\n","authors":["Nicholas Kroeger","Dan Ley","Satyapriya Krishna","Chirag Agarwal","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2310.05797v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12230v2","updated":"2024-07-11T03:29:19Z","published":"2024-06-18T03:08:01Z","title":"MCSD: An Efficient Language Model with Diverse Fusion","summary":"  Transformers excel in Natural Language Processing (NLP) due to their prowess\nin capturing long-term dependencies but suffer from exponential resource\nconsumption with increasing sequence lengths. To address these challenges, we\npropose MCSD model, an efficient language model with linear scaling and fast\ninference speed. MCSD model leverages diverse feature fusion, primarily through\nthe multi-channel slope and decay (MCSD) block, to robustly represent features.\nThis block comprises slope and decay sections that extract features across\ndiverse temporal receptive fields, facilitating capture of both local and\nglobal information. In addition, MCSD block conducts element-wise fusion of\ndiverse features to further enhance the delicate feature extraction capability.\nFor inference, we formulate the inference process into a recurrent\nrepresentation, slashing space complexity to $O(1)$ and time complexity to\n$O(N)$ respectively. Our experiments show that MCSD attains higher throughput\nand lower GPU memory consumption compared to Transformers, while maintaining\ncomparable performance to larger-scale language learning models on benchmark\ntests. These attributes position MCSD as a promising base for edge deployment\nand embodied intelligence.\n","authors":["Hua Yang","Duohai Li","Shiman Li"],"pdf_url":"https://arxiv.org/pdf/2406.12230v2.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2402.14851v2","updated":"2024-07-11T03:14:54Z","published":"2024-02-20T03:57:55Z","title":"$R^3$: \"This is My SQL, Are You With Me?\" A Consensus-Based Multi-Agent\n  System for Text-to-SQL Tasks","summary":"  Large Language Models (LLMs) have demonstrated strong performance on various\ntasks. To unleash their power on the Text-to-SQL task, we propose $R^3$\n(Review-Rebuttal-Revision), a consensus-based multi-agent system for\nText-to-SQL tasks. $R^3$ outperforms the existing single LLM Text-to-SQL\nsystems as well as the multi-agent Text-to-SQL systems by $1.3\\%$ to $8.1\\%$ on\nSpider and Bird. Surprisingly, we find that for Llama-3-8B, $R^3$ outperforms\nchain-of-thought prompting by over 20\\%, even outperforming GPT-3.5 on the\ndevelopment set of Spider.\n","authors":["Hanchen Xia","Feng Jiang","Naihao Deng","Cunxiang Wang","Guojiang Zhao","Rada Mihalcea","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.14851v2.pdf","comment":"12 pages, 2 figures, 8 tables"},{"id":"http://arxiv.org/abs/2407.08152v1","updated":"2024-07-11T03:10:27Z","published":"2024-07-11T03:10:27Z","title":"Privacy-Preserving Data Deduplication for Enhancing Federated Learning\n  of Language Models","summary":"  Deduplication is a vital preprocessing step that enhances machine learning\nmodel performance and saves training time and energy. However, enhancing\nfederated learning through deduplication poses challenges, especially regarding\nscalability and potential privacy violations if deduplication involves sharing\nall clients' data. In this paper, we address the problem of deduplication in a\nfederated setup by introducing a pioneering protocol, Efficient\nPrivacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes\nduplicates from multiple clients' datasets without compromising data privacy.\nEP-MPD is constructed in a modular fashion, utilizing two novel variants of the\nPrivate Set Intersection protocol. Our extensive experiments demonstrate the\nsignificant benefits of deduplication in federated learning of large language\nmodels. For instance, we observe up to 19.61% improvement in perplexity and up\nto 27.95% reduction in running time. EP-MPD effectively balances privacy and\nperformance in federated learning, making it a valuable solution for\nlarge-scale applications.\n","authors":["Aydin Abadi","Vishnu Asutosh Dasu","Sumanta Sarkar"],"pdf_url":"https://arxiv.org/pdf/2407.08152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08147v1","updated":"2024-07-11T03:00:14Z","published":"2024-07-11T03:00:14Z","title":"Looks can be Deceptive: Distinguishing Repetition Disfluency from\n  Reduplication","summary":"  Reduplication and repetition, though similar in form, serve distinct\nlinguistic purposes. Reduplication is a deliberate morphological process used\nto express grammatical, semantic, or pragmatic nuances, while repetition is\noften unintentional and indicative of disfluency. This paper presents the first\nlarge-scale study of reduplication and repetition in speech using computational\nlinguistics. We introduce IndicRedRep, a new publicly available dataset\ncontaining Hindi, Telugu, and Marathi text annotated with reduplication and\nrepetition at the word level. We evaluate transformer-based models for\nmulti-class reduplication and repetition token classification, utilizing the\nReparandum-Interregnum-Repair structure to distinguish between the two\nphenomena. Our models achieve macro F1 scores of up to 85.62% in Hindi, 83.95%\nin Telugu, and 84.82% in Marathi for reduplication-repetition classification.\n","authors":["Arif Ahmad","Mothika Gayathri Khyathi","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2407.08147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10747v3","updated":"2024-07-11T01:34:37Z","published":"2023-12-28T06:47:18Z","title":"Multimodal Sentiment Analysis with Missing Modality: A\n  Knowledge-Transfer Approach","summary":"  Multimodal sentiment analysis aims to identify the emotions expressed by\nindividuals through visual, language, and acoustic cues. However, most of the\nexisting research efforts assume that all modalities are available during both\ntraining and testing, making their algorithms susceptible to the missing\nmodality scenario. In this paper, we propose a novel knowledge-transfer network\nto translate between different modalities to reconstruct the missing audio\nmodalities. Moreover, we develop a cross-modality attention mechanism to retain\nthe maximal information of the reconstructed and observed modalities for\nsentiment prediction. Extensive experiments on three publicly available\ndatasets demonstrate significant improvements over baselines and achieve\ncomparable results to the previous methods with complete multi-modality\nsupervision.\n","authors":["Weide Liu","Huijing Zhan","Hao Chen","Fengmao Lv"],"pdf_url":"https://arxiv.org/pdf/2401.10747v3.pdf","comment":"We request to withdraw our paper from the archive due to significant\n  errors identified in the analysis and conclusions. Upon further review, we\n  realized that these errors undermine the validity of our findings. We plan to\n  conduct additional research to correct these issues and resubmit a revised\n  version in the future"},{"id":"http://arxiv.org/abs/2406.08702v3","updated":"2024-07-11T01:32:48Z","published":"2024-06-13T00:00:20Z","title":"VLind-Bench: Measuring Language Priors in Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have demonstrated outstanding\nperformance across various multimodal tasks. However, they suffer from a\nproblem known as language prior, where responses are generated based solely on\ntextual patterns while disregarding image information. Addressing the issue of\nlanguage prior is crucial, as it can lead to undesirable biases or\nhallucinations when dealing with images that are out of training distribution.\nDespite its importance, current methods for accurately measuring language\npriors in LVLMs are poorly studied. Although existing benchmarks based on\ncounterfactual or out-of-distribution images can partially be used to measure\nlanguage priors, they fail to disentangle language priors from other\nconfounding factors. To this end, we propose a new benchmark called\nVLind-Bench, which is the first benchmark specifically designed to measure the\nlanguage priors, or blindness, of LVLMs. It not only includes tests on\ncounterfactual images to assess language priors but also involves a series of\ntests to evaluate more basic capabilities such as commonsense knowledge, visual\nperception, and commonsense biases. For each instance in our benchmark, we\nensure that all these basic tests are passed before evaluating the language\npriors, thereby minimizing the influence of other factors on the assessment.\nThe evaluation and analysis of recent LVLMs in our benchmark reveal that almost\nall models exhibit a significant reliance on language priors, presenting a\nstrong challenge in the field.\n","authors":["Kang-il Lee","Minbeom Kim","Seunghyun Yoon","Minsung Kim","Dongryeol Lee","Hyukhun Koh","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2406.08702v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08112v1","updated":"2024-07-11T01:08:39Z","published":"2024-07-11T01:08:39Z","title":"How Well Can a Long Sequence Model Model Long Sequences? Comparing\n  Architechtural Inductive Biases on Long-Context Abilities","summary":"  Long sequences occur in abundance within real-world scenarios, hence properly\nmodelling them opens numerous down-stream use-cases. Deep neural networks,\nhowever, have often struggled with these for a variety of reasons. Recent\nadvances, both in system engineering as well as model design, have enabled the\nscaling up of model that are purported to support extended context length. In\nparticular, the state-space and linear recurrent neural network families of\nmodels hypothetically can entend to infinite sequence lenth. However, is this\ntoo good to be true? We conduct an evaluation to show that while such claims\nmay be sound theoretically, there remain large practical gaps that are\nempirically observed. In particular, recurrent models still suffer in the same\nsettings as long-context LLMs with attention. We further show that different\ninductive biases have inconsistent extrapolation capabilities, highlighting the\nneed to further study such paradigms and investigate why long-context models\nseemingly fail to behave as one might expect.\n","authors":["Jerry Huang"],"pdf_url":"https://arxiv.org/pdf/2407.08112v1.pdf","comment":"Work In Progress. 9 pages"},{"id":"http://arxiv.org/abs/2407.08103v1","updated":"2024-07-11T00:25:01Z","published":"2024-07-11T00:25:01Z","title":"Automata-based constraints for language model decoding","summary":"  LMs are often expected to generate strings in some formal language; for\nexample, structured data, API calls, or code snippets. Although LMs can be\ntuned to improve their adherence to formal syntax, this does not guarantee\nconformance, especially with smaller LMs suitable for large-scale deployment.\nIn addition, tuning requires significant resources, making it impractical for\nuncommon or task-specific formats. To prevent downstream parsing errors we\nwould ideally constrain the LM to only produce valid output, but this is\nseverely complicated by tokenization, which is typically both ambiguous and\nmisaligned with the formal grammar. We solve these issues through the\napplication of automata theory, deriving an efficient closed-form solution for\nthe regular languages, a broad class of formal languages with many practical\napplications, including API calls or schema-guided JSON and YAML. We also\ndiscuss pragmatic extensions for coping with the issue of high branching\nfactor. Finally, we extend our techniques to deterministic context-free\nlanguages, which similarly admit an efficient closed-form solution. In spite of\nits flexibility and representative power, our approach only requires access to\nper-token decoding logits and lowers into simple calculations that are\nindependent of LM size, making it both efficient and easy to apply to almost\nany LM architecture.\n","authors":["Terry Koo","Frederick Liu","Luheng He"],"pdf_url":"https://arxiv.org/pdf/2407.08103v1.pdf","comment":"Accepted to CoLM 2024"},{"id":"http://arxiv.org/abs/2407.08099v1","updated":"2024-07-11T00:07:14Z","published":"2024-07-11T00:07:14Z","title":"How does Burrows' Delta work on medieval Chinese poetic texts?","summary":"  Burrows' Delta was introduced in 2002 and has proven to be an effective tool\nfor author attribution. Despite the fact that these are different languages,\nthey mostly belong to the same grammatical type and use the same graphic\nprinciple to convey speech in writing: a phonemic alphabet with word separation\nusing spaces. The question I want to address in this article is how well this\nattribution method works with texts in a language with a different grammatical\nstructure and a script based on different principles. There are fewer studies\nanalyzing the effectiveness of the Delta method on Chinese texts than on texts\nin European languages. I believe that such a low level of attention to Delta\nfrom sinologists is due to the structure of the scientific field dedicated to\nmedieval Chinese poetry. Clustering based on intertextual distances worked\nflawlessly. Delta produced results where clustering showed that the samples of\none author were most similar to each other, and Delta never confused different\npoets. Despite the fact that I used an unconventional approach and applied the\nDelta method to a language poorly suited for it, the method demonstrated its\neffectiveness. Tang dynasty poets are correctly identified using Delta, and the\nempirical pattern observed for authors writing in European standard languages\nhas been confirmed once again.\n","authors":["Boris Orekhov"],"pdf_url":"https://arxiv.org/pdf/2407.08099v1.pdf","comment":"2 figures"},{"id":"http://arxiv.org/abs/2407.08892v1","updated":"2024-07-11T23:34:32Z","published":"2024-07-11T23:34:32Z","title":"Characterizing Prompt Compression Methods for Long Context Inference","summary":"  Long context inference presents challenges at the system level with increased\ncompute and memory requirements, as well as from an accuracy perspective in\nbeing able to reason over long contexts. Recently, several methods have been\nproposed to compress the prompt to reduce the context length. However, there\nhas been little work on comparing the different proposed methods across\ndifferent tasks through a standardized analysis. This has led to conflicting\nresults. To address this, here we perform a comprehensive characterization and\nevaluation of different prompt compression methods. In particular, we analyze\nextractive compression, summarization-based abstractive compression, and token\npruning methods. Surprisingly, we find that extractive compression often\noutperforms all the other approaches, and enables up to 10x compression with\nminimal accuracy degradation. Interestingly, we also find that despite several\nrecent claims, token pruning methods often lag behind extractive compression.\nWe only found marginal improvements on summarization tasks.\n","authors":["Siddharth Jha","Lutfi Eren Erdogan","Sehoon Kim","Kurt Keutzer","Amir Gholami"],"pdf_url":"https://arxiv.org/pdf/2407.08892v1.pdf","comment":"Es-FoMo @ ICML 2024"},{"id":"http://arxiv.org/abs/2407.07840v2","updated":"2024-07-11T23:14:51Z","published":"2024-07-10T17:00:29Z","title":"Decompose and Compare Consistency: Measuring VLMs' Answer Reliability\n  via Task-Decomposition Consistency Comparison","summary":"  Despite tremendous advancements, current state-of-the-art Vision-Language\nModels (VLMs) are still far from perfect. They tend to hallucinate and may\ngenerate biased responses. In such circumstances, having a way to assess the\nreliability of a given response generated by a VLM is quite useful. Existing\nmethods, such as estimating uncertainty using answer likelihoods or\nprompt-based confidence generation, often suffer from overconfidence. Other\nmethods use self-consistency comparison but are affected by confirmation\nbiases. To alleviate these, we propose \\textbf{De}compose and \\textbf{C}ompare\n\\textbf{C}onsistency (\\texttt{DeCC}) for reliability measurement. By comparing\nthe consistency between the direct answer generated using the VLM's internal\nreasoning process, and the indirect answers obtained by decomposing the\nquestion into sub-questions and reasoning over the sub-answers produced by the\nVLM, \\texttt{DeCC} measures the reliability of VLM's direct answer. Experiments\nacross six vision-language tasks with three VLMs show \\texttt{DeCC}'s\nreliability estimation achieves better correlation with task accuracy compared\nto the existing methods.\n","authors":["Qian Yang","Weixiang Yan","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.07840v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.17987v3","updated":"2024-07-11T22:55:40Z","published":"2024-06-26T00:00:45Z","title":"Multi-step Inference over Unstructured Data","summary":"  The advent of Large Language Models (LLMs) and Generative AI has\nrevolutionized natural language applications across various domains. However,\nhigh-stakes decision-making tasks in fields such as medical, legal and finance\nrequire a level of precision, comprehensiveness, and logical consistency that\npure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to\ndeliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI\nplatform to tackle these problems. The platform integrates fine-tuned LLMs for\nknowledge extraction and alignment with a robust symbolic reasoning engine for\nlogical inference, planning and interactive constraint solving. We describe\nCora, a Collaborative Research Assistant built on this platform, that is\ndesigned to perform complex research and discovery tasks in high-stakes\ndomains. This paper discusses the multi-step inference challenges inherent in\nsuch domains, critiques the limitations of existing LLM-based methods, and\ndemonstrates how Cora's neuro-symbolic approach effectively addresses these\nissues. We provide an overview of the system architecture, key algorithms for\nknowledge extraction and formal reasoning, and present preliminary evaluation\nresults that highlight Cora's superior performance compared to well-known LLM\nand RAG baselines.\n","authors":["Aditya Kalyanpur","Kailash Saravanakumar","Victor Barres","CJ McFate","Lori Moon","Nati Seifu","Maksim Eremeev","Jose Barrera","Eric Brown","David Ferrucci"],"pdf_url":"https://arxiv.org/pdf/2406.17987v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08887v1","updated":"2024-07-11T22:46:18Z","published":"2024-07-11T22:46:18Z","title":"Automatic Pruning of Fine-tuning Datasets for Transformer-based Language\n  Models","summary":"  Transformer-based language models have shown state-of-the-art performance on\na variety of natural language understanding tasks. To achieve this performance,\nthese models are first pre-trained on general corpus and then fine-tuned on\ndownstream tasks. Previous work studied the effect of pruning the training set\nof the downstream tasks on the performance of the model on its evaluation set.\nIn this work, we propose an automatic dataset pruning method for the training\nset of fine-tuning tasks. Our method is based on the model's success rate in\ncorrectly classifying each training data point. Unlike previous work which\nrelies on user feedback to determine subset size, our method automatically\nextracts training subsets that are adapted for each pair of model and\nfine-tuning task. Our method provides multiple subsets for use in dataset\npruning that navigate the trade-off between subset size and evaluation\naccuracy. Our largest subset, which we also refer to as the winning ticket\nsubset, is on average $3 \\times$ smaller than the original training set of the\nfine-tuning task. Our experiments on 5 downstream tasks and 2 language models\nshow that, on average, fine-tuning on the winning ticket subsets results in a\n$0.1 \\%$ increase in the evaluation performance of the model.\n","authors":["Mohammadreza Tayaranian","Seyyed Hasan Mozafari","Brett H. Meyer","James J. Clark","Warren J. Gross"],"pdf_url":"https://arxiv.org/pdf/2407.08887v1.pdf","comment":"28 pages, 17 figures. Accepted at the Third Conference on Lifelong\n  Learning Agents (CoLLAs 2024)"},{"id":"http://arxiv.org/abs/2402.02625v2","updated":"2024-07-11T20:43:59Z","published":"2024-02-04T22:12:29Z","title":"Enhancing Transformer RNNs with Multiple Temporal Perspectives","summary":"  We introduce the concept of multiple temporal perspectives, a novel approach\napplicable to Recurrent Neural Network (RNN) architectures for enhancing their\nunderstanding of sequential data. This method involves maintaining diverse\ntemporal views of previously encountered text, significantly enriching the\nlanguage models' capacity to interpret context. To show the efficacy of this\napproach, we incorporate it into the Receptance Weighted Key Value (RWKV)\narchitecture, addressing its inherent challenge of retaining all historical\ninformation within a single hidden state. Notably, this improvement is achieved\nwith a minimal increase in the number of parameters --even as little as\n$0.04\\%$ of the original number of parameters. Further, the additional\nparameters necessary for the multiple temporal perspectives are fine-tuned with\nminimal computational overhead, avoiding the need for a full pre-training. The\nresulting model maintains linear computational complexity during prompt\ninference, ensuring consistent efficiency across various sequence lengths. The\nempirical results and ablation studies included in our research validate the\neffectiveness of our approach, showcasing improved performance across multiple\nbenchmarks. The code, model weights and datasets are open-sourced at:\nhttps://github.com/RazvanDu/TemporalRNNs.\n","authors":["Razvan-Gabriel Dumitru","Darius Peteleaza","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2402.02625v2.pdf","comment":"13 pages, 8 figures, 4 tables, accepted at ICML 2024 - Next\n  Generation of Sequence Modeling Architectures workshop"},{"id":"http://arxiv.org/abs/2407.08853v1","updated":"2024-07-11T20:28:24Z","published":"2024-07-11T20:28:24Z","title":"GPT-4 is judged more human than humans in displaced and inverted Turing\n  tests","summary":"  Everyday AI detection requires differentiating between people and AI in\ninformal, online conversations. In many cases, people will not interact\ndirectly with AI systems but instead read conversations between AI systems and\nother people. We measured how well people and large language models can\ndiscriminate using two modified versions of the Turing test: inverted and\ndisplaced. GPT-3.5, GPT-4, and displaced human adjudicators judged whether an\nagent was human or AI on the basis of a Turing test transcript. We found that\nboth AI and displaced human judges were less accurate than interactive\ninterrogators, with below chance accuracy overall. Moreover, all three judged\nthe best-performing GPT-4 witness to be human more often than human witnesses.\nThis suggests that both humans and current LLMs struggle to distinguish between\nthe two when they are not actively interrogating the person, underscoring an\nurgent need for more accurate tools to detect AI in conversations.\n","authors":["Ishika Rathi","Sydney Taylor","Benjamin K. Bergen","Cameron R. Jones"],"pdf_url":"https://arxiv.org/pdf/2407.08853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12065v2","updated":"2024-07-11T20:16:09Z","published":"2024-04-18T10:25:42Z","title":"RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political\n  Fact-Checking using Multimodal Large Language Models","summary":"  The escalating challenge of misinformation, particularly in political\ndiscourse, requires advanced fact-checking solutions; this is even clearer in\nthe more complex scenario of multimodal claims. We tackle this issue using a\nmultimodal large language model in conjunction with retrieval-augmented\ngeneration (RAG), and introduce two novel reasoning techniques: Chain of RAG\n(CoRAG) and Tree of RAG (ToRAG). They fact-check multimodal claims by\nextracting both textual and image content, retrieving external information, and\nreasoning subsequent questions to be answered based on prior evidence. We\nachieve a weighted F1-score of 0.85, surpassing a baseline reasoning technique\nby 0.14 points. Human evaluation confirms that the vast majority of our\ngenerated fact-check explanations contain all information from gold standard\ndata.\n","authors":["M. Abdul Khaliq","P. Chang","M. Ma","B. Pflugfelder","F. MiletiÄ"],"pdf_url":"https://arxiv.org/pdf/2404.12065v2.pdf","comment":"8 pages, submitted to ACL Rolling Review June 2024"},{"id":"http://arxiv.org/abs/2407.08842v1","updated":"2024-07-11T19:58:13Z","published":"2024-07-11T19:58:13Z","title":"Evaluating Nuanced Bias in Large Language Model Free Response Answers","summary":"  Pre-trained large language models (LLMs) can now be easily adapted for\nspecific business purposes using custom prompts or fine tuning. These\ncustomizations are often iteratively re-engineered to improve some aspect of\nperformance, but after each change businesses want to ensure that there has\nbeen no negative impact on the system's behavior around such critical issues as\nbias. Prior methods of benchmarking bias use techniques such as word masking\nand multiple choice questions to assess bias at scale, but these do not capture\nall of the nuanced types of bias that can occur in free response answers, the\ntypes of answers typically generated by LLM systems. In this paper, we identify\nseveral kinds of nuanced bias in free text that cannot be similarly identified\nby multiple choice tests. We describe these as: confidence bias, implied bias,\ninclusion bias and erasure bias. We present a semi-automated pipeline for\ndetecting these types of bias by first eliminating answers that can be\nautomatically classified as unbiased and then co-evaluating name reversed pairs\nusing crowd workers. We believe that the nuanced classifications our method\ngenerates can be used to give better feedback to LLMs, especially as LLM\nreasoning capabilities become more advanced.\n","authors":["Jennifer Healey","Laurie Byrum","Md Nadeem Akhtar","Moumita Sinha"],"pdf_url":"https://arxiv.org/pdf/2407.08842v1.pdf","comment":"14 pages, 0 figures, submitted to NLDB 2024, Turin, Italy"},{"id":"http://arxiv.org/abs/2407.08836v1","updated":"2024-07-11T19:44:18Z","published":"2024-07-11T19:44:18Z","title":"Fault Diagnosis in Power Grids with Large Language Model","summary":"  Power grid fault diagnosis is a critical task for ensuring the reliability\nand stability of electrical infrastructure. Traditional diagnostic systems\noften struggle with the complexity and variability of power grid data. This\npaper proposes a novel approach that leverages Large Language Models (LLMs),\nspecifically ChatGPT and GPT-4, combined with advanced prompt engineering to\nenhance fault diagnosis accuracy and explainability. We designed comprehensive,\ncontext-aware prompts to guide the LLMs in interpreting complex data and\nproviding detailed, actionable insights. Our method was evaluated against\nbaseline techniques, including standard prompting, Chain-of-Thought (CoT), and\nTree-of-Thought (ToT) methods, using a newly constructed dataset comprising\nreal-time sensor data, historical fault records, and component descriptions.\nExperimental results demonstrate significant improvements in diagnostic\naccuracy, explainability quality, response coherence, and contextual\nunderstanding, underscoring the effectiveness of our approach. These findings\nsuggest that prompt-engineered LLMs offer a promising solution for robust and\nreliable power grid fault diagnosis.\n","authors":["Liu Jing","Amirul Rahman"],"pdf_url":"https://arxiv.org/pdf/2407.08836v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2405.06680v2","updated":"2024-07-11T19:26:18Z","published":"2024-05-05T16:35:30Z","title":"Exploring the Compositional Deficiency of Large Language Models in\n  Mathematical Reasoning","summary":"  Human cognition exhibits systematic compositionality, the algebraic ability\nto generate infinite novel combinations from finite learned components, which\nis the key to understanding and reasoning about complex logic. In this work, we\ninvestigate the compositionality of large language models (LLMs) in\nmathematical reasoning. Specifically, we construct a new dataset\n\\textsc{MathTrap}\\footnotemark[3] by introducing carefully designed logical\ntraps into the problem descriptions of MATH and GSM8k. Since problems with\nlogical flaws are quite rare in the real world, these represent ``unseen''\ncases to LLMs. Solving these requires the models to systematically compose (1)\nthe mathematical knowledge involved in the original problems with (2) knowledge\nrelated to the introduced traps. Our experiments show that while LLMs possess\nboth components of requisite knowledge, they do not \\textbf{spontaneously}\ncombine them to handle these novel cases. We explore several methods to\nmitigate this deficiency, such as natural language prompts, few-shot\ndemonstrations, and fine-tuning. We find that LLMs' performance can be\n\\textbf{passively} improved through the above external intervention. Overall,\nsystematic compositionality remains an open challenge for large language\nmodels.\n","authors":["Jun Zhao","Jingqi Tong","Yurong Mou","Ming Zhang","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2405.06680v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11129v2","updated":"2024-07-11T19:26:15Z","published":"2024-02-16T23:28:02Z","title":"BlendFilter: Advancing Retrieval-Augmented Large Language Models via\n  Query Generation Blending and Knowledge Filtering","summary":"  Retrieval-augmented Large Language Models (LLMs) offer substantial benefits\nin enhancing performance across knowledge-intensive scenarios. However, these\nmethods often face challenges with complex inputs and encounter difficulties\ndue to noisy knowledge retrieval, notably hindering model effectiveness. To\naddress this issue, we introduce BlendFilter, a novel approach that elevates\nretrieval-augmented LLMs by integrating query generation blending with\nknowledge filtering. BlendFilter proposes the blending process through its\nquery generation method, which integrates both external and internal knowledge\naugmentation with the original query, ensuring comprehensive information\ngathering. Additionally, our distinctive knowledge filtering module capitalizes\non the intrinsic capabilities of the LLM, effectively eliminating extraneous\ndata. We conduct extensive experiments on three open-domain question answering\nbenchmarks, and the findings clearly indicate that our innovative BlendFilter\nsurpasses state-of-the-art baselines significantly.\n","authors":["Haoyu Wang","Ruirui Li","Haoming Jiang","Jinjin Tian","Zhengyang Wang","Chen Luo","Xianfeng Tang","Monica Cheng","Tuo Zhao","Jing Gao"],"pdf_url":"https://arxiv.org/pdf/2402.11129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08824v1","updated":"2024-07-11T19:13:16Z","published":"2024-07-11T19:13:16Z","title":"Proving that Cryptic Crossword Clue Answers are Correct","summary":"  Cryptic crossword clues are challenging cognitive tasks, for which new test\nsets are released on a daily basis by multiple international newspapers. Each\ncryptic clue contains both the definition of the answer to be placed in the\ncrossword grid (in common with regular crosswords), and `wordplay' that proves\nthat the answer is correct (i.e. a human solver can be confident that an answer\nis correct without needing crossing words to confirm it). Using an existing\ncryptic wordplay proving framework (operating on Python proofs created by an\nLLM), we show that it is possible to distinguish between correct answers and\nalmost-correct ones based upon whether the wordplay `works'.\n","authors":["Martin Andrews","Sam Witteveen"],"pdf_url":"https://arxiv.org/pdf/2407.08824v1.pdf","comment":"Accepted paper for the ICML 2024 Workshop on LLMs and Cognition (4\n  pages + references + 6 pages of Appendices)"},{"id":"http://arxiv.org/abs/2407.08819v1","updated":"2024-07-11T19:05:43Z","published":"2024-07-11T19:05:43Z","title":"Rule-Based, Neural and LLM Back-Translation: Comparative Insights from a\n  Variant of Ladin","summary":"  This paper explores the impact of different back-translation approaches on\nmachine translation for Ladin, specifically the Val Badia variant. Given the\nlimited amount of parallel data available for this language (only 18k\nLadin-Italian sentence pairs), we investigate the performance of a multilingual\nneural machine translation model fine-tuned for Ladin-Italian. In addition to\nthe available authentic data, we synthesise further translations by using three\ndifferent models: a fine-tuned neural model, a rule-based system developed\nspecifically for this language pair, and a large language model. Our\nexperiments show that all approaches achieve comparable translation quality in\nthis low-resource scenario, yet round-trip translations highlight differences\nin model performance.\n","authors":["Samuel Frontull","Georg Moser"],"pdf_url":"https://arxiv.org/pdf/2407.08819v1.pdf","comment":"Accepted to LoResMT 2024 (ACL workshop)"},{"id":"http://arxiv.org/abs/2407.08818v1","updated":"2024-07-11T18:59:21Z","published":"2024-07-11T18:59:21Z","title":"MAGNET: Improving the Multilingual Fairness of Language Models with\n  Adaptive Gradient-Based Tokenization","summary":"  In multilingual settings, non-Latin scripts and low-resource languages are\nusually disadvantaged in terms of language models' utility, efficiency, and\ncost. Specifically, previous studies have reported multiple modeling biases\nthat the current tokenization algorithms introduce to non-Latin script\nlanguages, the main one being over-segmentation. In this work, we propose\nMAGNET; multilingual adaptive gradient-based tokenization to reduce\nover-segmentation via adaptive gradient-based subword tokenization. MAGNET\nlearns to predict segment boundaries between byte tokens in a sequence via\nsub-modules within the model, which act as internal boundary predictors\n(tokenizers). Previous gradient-based tokenization methods aimed for uniform\ncompression across sequences by integrating a single boundary predictor during\ntraining and optimizing it end-to-end through stochastic reparameterization\nalongside the next token prediction objective. However, this approach still\nresults in over-segmentation for non-Latin script languages in multilingual\nsettings. In contrast, MAGNET offers a customizable architecture where\nbyte-level sequences are routed through language-script-specific predictors,\neach optimized for its respective language script. This modularity enforces\nequitable segmentation granularity across different language scripts compared\nto previous methods. Through extensive experiments, we demonstrate that in\naddition to reducing segmentation disparities, MAGNET also enables faster\nlanguage modelling and improves downstream utility.\n","authors":["Orevaoghene Ahia","Sachin Kumar","Hila Gonen","Valentin Hoffman","Tomasz Limisiewicz","Yulia Tsvetkov","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2407.08818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01061v3","updated":"2024-07-11T18:46:52Z","published":"2024-03-02T01:52:14Z","title":"Reading Subtext: Evaluating Large Language Models on Short Story\n  Summarization with Writers","summary":"  We evaluate recent Large Language Models (LLMs) on the challenging task of\nsummarizing short stories, which can be lengthy, and include nuanced subtext or\nscrambled timelines. Importantly, we work directly with authors to ensure that\nthe stories have not been shared online (and therefore are unseen by the\nmodels), and to obtain informed evaluations of summary quality using judgments\nfrom the authors themselves. Through quantitative and qualitative analysis\ngrounded in narrative theory, we compare GPT-4, Claude-2.1, and LLama-2-70B. We\nfind that all three models make faithfulness mistakes in over 50% of summaries\nand struggle with specificity and interpretation of difficult subtext. We\nadditionally demonstrate that LLM ratings and other automatic metrics for\nsummary quality do not correlate well with the quality ratings from the\nwriters.\n","authors":["Melanie Subbiah","Sean Zhang","Lydia B. Chilton","Kathleen McKeown"],"pdf_url":"https://arxiv.org/pdf/2403.01061v3.pdf","comment":"pre-MIT Press publication version"},{"id":"http://arxiv.org/abs/2407.08790v1","updated":"2024-07-11T18:06:01Z","published":"2024-07-11T18:06:01Z","title":"Large Models of What? Mistaking Engineering Achievements for Human\n  Linguistic Agency","summary":"  In this paper we argue that key, often sensational and misleading, claims\nregarding linguistic capabilities of Large Language Models (LLMs) are based on\nat least two unfounded assumptions; the assumption of language completeness and\nthe assumption of data completeness. Language completeness assumes that a\ndistinct and complete thing such as `a natural language' exists, the essential\ncharacteristics of which can be effectively and comprehensively modelled by an\nLLM. The assumption of data completeness relies on the belief that a language\ncan be quantified and wholly captured by data. Work within the enactive\napproach to cognitive science makes clear that, rather than a distinct and\ncomplete thing, language is a means or way of acting. Languaging is not the\nkind of thing that can admit of a complete or comprehensive modelling. From an\nenactive perspective we identify three key characteristics of enacted language;\nembodiment, participation, and precariousness, that are absent in LLMs, and\nlikely incompatible in principle with current architectures. We argue that\nthese absences imply that LLMs are not now and cannot in their present form be\nlinguistic agents the way humans are. We illustrate the point in particular\nthrough the phenomenon of `algospeak', a recently described pattern of high\nstakes human language activity in heavily controlled online environments. On\nthe basis of these points, we conclude that sensational and misleading claims\nabout LLM agency and capabilities emerge from a deep misconception of both what\nhuman language is and what LLMs are.\n","authors":["Abeba Birhane","Marek McGann"],"pdf_url":"https://arxiv.org/pdf/2407.08790v1.pdf","comment":"To appear in the Journal of Language Sciences"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.06508v3","updated":"2024-07-11T16:16:37Z","published":"2024-07-09T02:33:13Z","title":"A Clinical Benchmark of Public Self-Supervised Pathology Foundation\n  Models","summary":"  The use of self-supervised learning (SSL) to train pathology foundation\nmodels has increased substantially in the past few years. Notably, several\nmodels trained on large quantities of clinical data have been made publicly\navailable in recent months. This will significantly enhance scientific research\nin computational pathology and help bridge the gap between research and\nclinical deployment. With the increase in availability of public foundation\nmodels of different sizes, trained using different algorithms on different\ndatasets, it becomes important to establish a benchmark to compare the\nperformance of such models on a variety of clinically relevant tasks spanning\nmultiple organs and diseases. In this work, we present a collection of\npathology datasets comprising clinical slides associated with clinically\nrelevant endpoints including cancer diagnoses and a variety of biomarkers\ngenerated during standard hospital operation from two medical centers. We\nleverage these datasets to systematically assess the performance of public\npathology foundation models and provide insights into best practices for\ntraining new foundation models and selecting appropriate pretrained models.\n","authors":["Gabriele Campanella","Shengjia Chen","Ruchika Verma","Jennifer Zeng","Aryeh Stock","Matt Croken","Brandon Veremis","Abdulkadir Elmas","Kuan-lin Huang","Ricky Kwan","Jane Houldsworth","Adam J. Schoenfeld","Chad Vanderbilt"],"pdf_url":"https://arxiv.org/pdf/2407.06508v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2310.07033"},{"id":"http://arxiv.org/abs/2407.07853v2","updated":"2024-07-11T07:35:26Z","published":"2024-07-10T17:14:54Z","title":"Progressive Growing of Patch Size: Resource-Efficient Curriculum\n  Learning for Dense Prediction Tasks","summary":"  In this work, we introduce Progressive Growing of Patch Size, a\nresource-efficient implicit curriculum learning approach for dense prediction\ntasks. Our curriculum approach is defined by growing the patch size during\nmodel training, which gradually increases the task's difficulty. We integrated\nour curriculum into the nnU-Net framework and evaluated the methodology on all\n10 tasks of the Medical Segmentation Decathlon. With our approach, we are able\nto substantially reduce runtime, computational costs, and CO2 emissions of\nnetwork training compared to classical constant patch size training. In our\nexperiments, the curriculum approach resulted in improved convergence. We are\nable to outperform standard nnU-Net training, which is trained with constant\npatch size, in terms of Dice Score on 7 out of 10 MSD tasks while only spending\nroughly 50% of the original training runtime. To the best of our knowledge, our\nProgressive Growing of Patch Size is the first successful employment of a\nsample-length curriculum in the form of patch size in the field of computer\nvision. Our code is publicly available at\nhttps://github.com/compai-lab/2024-miccai-fischer.\n","authors":["Stefan M. Fischer","Lina Felsner","Richard Osuala","Johannes Kiechle","Daniel M. Lang","Jan C. Peeken","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2407.07853v2.pdf","comment":"Accepted at MICCAI2024; Changes for Camera-Ready-Version for\n  MICCAI2024 (missing in this arxiv submission): Replaced T-Test with Wilcoxon\n  Signed Ranked Test, as DSC samples are not normally distributed => now only\n  significant improvements and no significant decreases in performance for\n  PGPS/PGPS+"},{"id":"http://arxiv.org/abs/2407.07788v2","updated":"2024-07-11T16:26:09Z","published":"2024-07-10T16:04:18Z","title":"BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark","summary":"  We introduce BiGym, a new benchmark and learning environment for mobile\nbi-manual demo-driven robotic manipulation. BiGym features 40 diverse tasks set\nin home environments, ranging from simple target reaching to complex kitchen\ncleaning. To capture the real-world performance accurately, we provide\nhuman-collected demonstrations for each task, reflecting the diverse modalities\nfound in real-world robot trajectories. BiGym supports a variety of\nobservations, including proprioceptive data and visual inputs such as RGB, and\ndepth from 3 camera views. To validate the usability of BiGym, we thoroughly\nbenchmark the state-of-the-art imitation learning algorithms and demo-driven\nreinforcement learning algorithms within the environment and discuss the future\nopportunities.\n","authors":["Nikita Chernyadev","Nicholas Backshall","Xiao Ma","Yunfan Lu","Younggyo Seo","Stephen James"],"pdf_url":"https://arxiv.org/pdf/2407.07788v2.pdf","comment":"Project webpage: https://chernyadev.github.io/bigym/"},{"id":"http://arxiv.org/abs/2407.07638v2","updated":"2024-07-11T04:46:24Z","published":"2024-07-10T13:19:31Z","title":"Tuning Vision-Language Models with Candidate Labels by Prompt Alignment","summary":"  Vision-language models (VLMs) can learn high-quality representations from a\nlarge-scale training dataset of image-text pairs. Prompt learning is a popular\napproach to fine-tuning VLM to adapt them to downstream tasks. Despite the\nsatisfying performance, a major limitation of prompt learning is the demand for\nlabelled data. In real-world scenarios, we may only obtain candidate labels\n(where the true label is included) instead of the true labels due to data\nprivacy or sensitivity issues. In this paper, we provide the first study on\nprompt learning with candidate labels for VLMs. We empirically demonstrate that\nprompt learning is more advantageous than other fine-tuning methods, for\nhandling candidate labels. Nonetheless, its performance drops when the label\nambiguity increases. In order to improve its robustness, we propose a simple\nyet effective framework that better leverages the prior knowledge of VLMs to\nguide the learning process with candidate labels. Specifically, our framework\ndisambiguates candidate labels by aligning the model output with the mixed\nclass posterior jointly predicted by both the learnable and the handcrafted\nprompt. Besides, our framework can be equipped with various off-the-shelf\ntraining objectives for learning with candidate labels to further improve their\nperformance. Extensive experiments demonstrate the effectiveness of our\nproposed framework.\n","authors":["Zhifang Zhang","Beibei Li"],"pdf_url":"https://arxiv.org/pdf/2407.07638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07614v2","updated":"2024-07-11T11:05:53Z","published":"2024-07-10T12:52:49Z","title":"MARS: Mixture of Auto-Regressive Models for Fine-grained Text-to-image\n  Synthesis","summary":"  Auto-regressive models have made significant progress in the realm of\nlanguage generation, yet they do not perform on par with diffusion models in\nthe domain of image synthesis. In this work, we introduce MARS, a novel\nframework for T2I generation that incorporates a specially designed Semantic\nVision-Language Integration Expert (SemVIE). This innovative component\nintegrates pre-trained LLMs by independently processing linguistic and visual\ninformation, freezing the textual component while fine-tuning the visual\ncomponent. This methodology preserves the NLP capabilities of LLMs while\nimbuing them with exceptional visual understanding. Building upon the powerful\nbase of the pre-trained Qwen-7B, MARS stands out with its bilingual generative\ncapabilities corresponding to both English and Chinese language prompts and the\ncapacity for joint image and text generation. The flexibility of this framework\nlends itself to migration towards any-to-any task adaptability. Furthermore,\nMARS employs a multi-stage training strategy that first establishes robust\nimage-text alignment through complementary bidirectional tasks and subsequently\nconcentrates on refining the T2I generation process, significantly augmenting\ntext-image synchrony and the granularity of image details. Notably, MARS\nrequires only 9% of the GPU days needed by SD1.5, yet it achieves remarkable\nresults across a variety of benchmarks, illustrating the training efficiency\nand the potential for swift deployment in various applications.\n","authors":["Wanggui He","Siming Fu","Mushui Liu","Xierui Wang","Wenyi Xiao","Fangxun Shu","Yi Wang","Lei Zhang","Zhelun Yu","Haoyuan Li","Ziwei Huang","LeiLei Gan","Hao Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.07614v2.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.07605v2","updated":"2024-07-11T08:01:58Z","published":"2024-07-10T12:44:22Z","title":"Early Explorations of Lightweight Models for Wound Segmentation on\n  Mobile Devices","summary":"  The aging population poses numerous challenges to healthcare, including the\nincrease in chronic wounds in the elderly. The current approach to wound\nassessment by therapists based on photographic documentation is subjective,\nhighlighting the need for computer-aided wound recognition from smartphone\nphotos. This offers objective and convenient therapy monitoring, while being\naccessible to patients from their home at any time. However, despite research\nin mobile image segmentation, there is a lack of focus on mobile wound\nsegmentation. To address this gap, we conduct initial research on three\nlightweight architectures to investigate their suitability for smartphone-based\nwound segmentation. Using public datasets and UNet as a baseline, our results\nare promising, with both ENet and TopFormer, as well as the larger UNeXt\nvariant, showing comparable performance to UNet. Furthermore, we deploy the\nmodels into a smartphone app for visual assessment of live segmentation, where\nresults demonstrate the effectiveness of TopFormer in distinguishing wounds\nfrom wound-coloured objects. While our study highlights the potential of\ntransformer models for mobile wound segmentation, future work should aim to\nfurther improve the mask contours.\n","authors":["Vanessa Borst","Timo Dittus","Konstantin MÃ¼ller","Samuel Kounev"],"pdf_url":"https://arxiv.org/pdf/2407.07605v2.pdf","comment":"To be published in the \"47th German Conference on Artificial\n  Intelligence (KI 2024)\""},{"id":"http://arxiv.org/abs/2407.07580v2","updated":"2024-07-11T03:19:08Z","published":"2024-07-10T12:13:39Z","title":"InstructLayout: Instruction-Driven 2D and 3D Layout Synthesis with\n  Semantic Graph Prior","summary":"  Comprehending natural language instructions is a charming property for both\n2D and 3D layout synthesis systems. Existing methods implicitly model object\njoint distributions and express object relations, hindering generation's\ncontrollability. We introduce InstructLayout, a novel generative framework that\nintegrates a semantic graph prior and a layout decoder to improve\ncontrollability and fidelity for 2D and 3D layout synthesis. The proposed\nsemantic graph prior learns layout appearances and object distributions\nsimultaneously, demonstrating versatility across various downstream tasks in a\nzero-shot manner. To facilitate the benchmarking for text-driven 2D and 3D\nscene synthesis, we respectively curate two high-quality datasets of\nlayout-instruction pairs from public Internet resources with large language and\nmultimodal models. Extensive experimental results reveal that the proposed\nmethod outperforms existing state-of-the-art approaches by a large margin in\nboth 2D and 3D layout synthesis tasks. Thorough ablation studies confirm the\nefficacy of crucial design components.\n","authors":["Chenguo Lin","Yuchen Lin","Panwang Pan","Xuanyang Zhang","Yadong Mu"],"pdf_url":"https://arxiv.org/pdf/2407.07580v2.pdf","comment":"This paper is an extension of ICLR 2024 \"InstructScene:\n  Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior\".\n  arXiv admin note: substantial text overlap with arXiv:2402.04717"},{"id":"http://arxiv.org/abs/2407.07564v2","updated":"2024-07-11T11:47:05Z","published":"2024-07-10T11:49:29Z","title":"Trainable Highly-expressive Activation Functions","summary":"  Nonlinear activation functions are pivotal to the success of deep neural\nnets, and choosing the appropriate activation function can significantly affect\ntheir performance. Most networks use fixed activation functions (e.g., ReLU,\nGELU, etc.), and this choice might limit their expressiveness. Furthermore,\ndifferent layers may benefit from diverse activation functions. Consequently,\nthere has been a growing interest in trainable activation functions. In this\npaper, we introduce DiTAC, a trainable highly-expressive activation function\nbased on an efficient diffeomorphic transformation (called CPAB). Despite\nintroducing only a negligible number of trainable parameters, DiTAC enhances\nmodel expressiveness and performance, often yielding substantial improvements.\nIt also outperforms existing activation functions (regardless whether the\nlatter are fixed or trainable) in tasks such as semantic segmentation, image\ngeneration, regression problems, and image classification. Our code is\navailable at https://github.com/BGU-CS-VIL/DiTAC.\n","authors":["Irit Chelly","Shahaf E. Finder","Shira Ifergane","Oren Freifeld"],"pdf_url":"https://arxiv.org/pdf/2407.07564v2.pdf","comment":"Accepted to ECCV 2024 on July 1st, 2024"},{"id":"http://arxiv.org/abs/2407.07503v2","updated":"2024-07-11T02:49:39Z","published":"2024-07-10T09:41:36Z","title":"Metasurface-based Snapshot Shortwave-Infrared Hyperspectral Image\n  Reconstruction with Inter and Intra Prior Learning Network","summary":"  Shortwave-infrared(SWIR) spectral information,ranging from 1 {\\mu}m to\n2.5{\\mu}m, breaks the limitations of traditional color cameras in acquiring\nscene information and has been used in many fields. However, conventional SWIR\nhyperspectral imaging systems face challenges due to their bulky setups and low\nacquisition speed. In this work, we introduce a snapshot SWIR hyperspectral\nimaging system based on a metasurface filter and a corresponding filter\nselection method to achieve the lowest correlation coefficient among these\nfilters.This systemhas the advantages of small size and snapshot imaging. We\npropose a novel inter and intra prior learning unfolding framework proposed to\nachieve high-quality SWIR hyperspectral image reconstruction, which bridges the\ngap between prior learning and cross-stage information interaction. We also\ndesign an adaptive feature transfer mechanism to adaptively the transfer\ncontextual correlation of multi-scale encoder features to prevent detailed\ninformation loss in the decoder. Experiment results demonstrate that our method\ncan reconstruct HSI with high speed and superior performance over existing\nmethods.\n","authors":["Linqiang Li","Jinglei Hao","Yongqiang Zhao","Pan Liu","Haofang Yan","Ziqin Zhang","Seong G. Kong"],"pdf_url":"https://arxiv.org/pdf/2407.07503v2.pdf","comment":"10 pages,5 figures"},{"id":"http://arxiv.org/abs/2403.10854v3","updated":"2024-07-11T04:11:53Z","published":"2024-03-16T08:30:45Z","title":"A Comprehensive Study of Multimodal Large Language Models for Image\n  Quality Assessment","summary":"  While Multimodal Large Language Models (MLLMs) have experienced significant\nadvancement in visual understanding and reasoning, their potential to serve as\npowerful, flexible, interpretable, and text-driven models for Image Quality\nAssessment (IQA) remains largely unexplored. In this paper, we conduct a\ncomprehensive and systematic study of prompting MLLMs for IQA. We first\ninvestigate nine prompting systems for MLLMs as the combinations of three\nstandardized testing procedures in psychophysics (i.e., the single-stimulus,\ndouble-stimulus, and multiple-stimulus methods) and three popular prompting\nstrategies in natural language processing (i.e., the standard, in-context, and\nchain-of-thought prompting). We then present a difficult sample selection\nprocedure, taking into account sample diversity and uncertainty, to further\nchallenge MLLMs equipped with the respective optimal prompting systems. We\nassess three open-source and one closed-source MLLMs on several visual\nattributes of image quality (e.g., structural and textural distortions,\ngeometric transformations, and color differences) in both full-reference and\nno-reference scenarios. Experimental results show that only the closed-source\nGPT-4V provides a reasonable account for human perception of image quality, but\nis weak at discriminating fine-grained quality variations (e.g., color\ndifferences) and at comparing visual quality of multiple images, tasks humans\ncan perform effortlessly.\n","authors":["Tianhe Wu","Kede Ma","Jie Liang","Yujiu Yang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.10854v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07441v2","updated":"2024-07-11T02:19:44Z","published":"2024-07-10T07:53:24Z","title":"HAFormer: Unleashing the Power of Hierarchy-Aware Features for\n  Lightweight Semantic Segmentation","summary":"  Both Convolutional Neural Networks (CNNs) and Transformers have shown great\nsuccess in semantic segmentation tasks. Efforts have been made to integrate\nCNNs with Transformer models to capture both local and global context\ninteractions. However, there is still room for enhancement, particularly when\nconsidering constraints on computational resources. In this paper, we introduce\nHAFormer, a model that combines the hierarchical features extraction ability of\nCNNs with the global dependency modeling capability of Transformers to tackle\nlightweight semantic segmentation challenges. Specifically, we design a\nHierarchy-Aware Pixel-Excitation (HAPE) module for adaptive multi-scale local\nfeature extraction. During the global perception modeling, we devise an\nEfficient Transformer (ET) module streamlining the quadratic calculations\nassociated with traditional Transformers. Moreover, a correlation-weighted\nFusion (cwF) module selectively merges diverse feature representations,\nsignificantly enhancing predictive accuracy. HAFormer achieves high performance\nwith minimal computational overhead and compact model size, achieving 74.2%\nmIoU on Cityscapes and 71.1% mIoU on CamVid test datasets, with frame rates of\n105FPS and 118FPS on a single 2080Ti GPU. The source codes are available at\nhttps://github.com/XU-GITHUB-curry/HAFormer.\n","authors":["Guoan Xu","Wenjing Jia","Tao Wu","Ligeng Chen","Guangwei Gao"],"pdf_url":"https://arxiv.org/pdf/2407.07441v2.pdf","comment":"13 pages, 10 figures, 8 tables, IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2406.18459v4","updated":"2024-07-11T15:03:44Z","published":"2024-06-26T16:10:31Z","title":"DiffuseHigh: Training-free Progressive High-Resolution Image Synthesis\n  through Structure Guidance","summary":"  Recent surge in large-scale generative models has spurred the development of\nvast fields in computer vision. In particular, text-to-image diffusion models\nhave garnered widespread adoption across diverse domain due to their potential\nfor high-fidelity image generation. Nonetheless, existing large-scale diffusion\nmodels are confined to generate images of up to 1K resolution, which is far\nfrom meeting the demands of contemporary commercial applications. Directly\nsampling higher-resolution images often yields results marred by artifacts such\nas object repetition and distorted shapes. Addressing the aforementioned issues\ntypically necessitates training or fine-tuning models on higher resolution\ndatasets. However, this undertaking poses a formidable challenge due to the\ndifficulty in collecting large-scale high-resolution contents and substantial\ncomputational resources. While several preceding works have proposed\nalternatives, they often fail to produce convincing results. In this work, we\nprobe the generative ability of diffusion models at higher resolution beyond\nits original capability and propose a novel progressive approach that fully\nutilizes generated low-resolution image to guide the generation of higher\nresolution image. Our method obviates the need for additional training or\nfine-tuning which significantly lowers the burden of computational costs.\nExtensive experiments and results validate the efficiency and efficacy of our\nmethod. Project page: https://yhyun225.github.io/DiffuseHigh/\n","authors":["Younghyun Kim","Geunmin Hwang","Junyu Zhang","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2406.18459v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06094v2","updated":"2024-07-11T17:59:53Z","published":"2023-06-09T17:57:01Z","title":"Leveraging Large Language Models for Scalable Vector Graphics-Driven\n  Image Understanding","summary":"  Large language models (LLMs) have made significant advancements in natural\nlanguage understanding. However, through that enormous semantic representation\nthat the LLM has learnt, is it somehow possible for it to understand images as\nwell? This work investigates this question. To enable the LLM to process\nimages, we convert them into a representation given by Scalable Vector Graphics\n(SVG). To study what the LLM can do with this XML-based textual description of\nimages, we test the LLM on three broad computer vision tasks: (i) visual\nreasoning and question answering, (ii) image classification under distribution\nshift, few-shot learning, and (iii) generating new images using visual\nprompting. Even though we do not naturally associate LLMs with any visual\nunderstanding capabilities, our results indicate that the LLM can often do a\ndecent job in many of these tasks, potentially opening new avenues for research\ninto LLMs' ability to understand image data. Our code, data, and models can be\nfound here https://github.com/mu-cai/svg-llm.\n","authors":["Mu Cai","Zeyi Huang","Yuheng Li","Utkarsh Ojha","Haohan Wang","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2306.06094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08739v1","updated":"2024-07-11T17:59:47Z","published":"2024-07-11T17:59:47Z","title":"MAVIS: Mathematical Visual Instruction Tuning","summary":"  Multi-modal Large Language Models (MLLMs) have recently emerged as a\nsignificant focus in academia and industry. Despite their proficiency in\ngeneral multi-modal scenarios, the mathematical problem-solving capabilities in\nvisual contexts remain insufficiently explored. We identify three key areas\nwithin MLLMs that need to be improved: visual encoding of math diagrams,\ndiagram-language alignment, and mathematical reasoning skills. This draws forth\nan urgent demand for large-scale, high-quality data and training pipelines in\nvisual mathematics. In this paper, we propose MAVIS, the first MAthematical\nVISual instruction tuning paradigm for MLLMs, involving a series of\nmathematical visual datasets and specialized MLLMs. Targeting the three issues,\nMAVIS contains three progressive training stages from scratch. First, we curate\nMAVIS-Caption, consisting of 558K diagram-caption pairs, to fine-tune a\nmath-specific vision encoder (CLIP-Math) through contrastive learning, tailored\nfor improved diagram visual encoding. Second, we utilize MAVIS-Caption to align\nthe CLIP-Math with a large language model (LLM) by a projection layer,\nenhancing vision-language alignment in mathematical domains. Third, we\nintroduce MAVIS-Instruct, including 900K meticulously collected and annotated\nvisual math problems, which is adopted to finally instruct-tune the MLLM for\nrobust mathematical reasoning skills. In MAVIS-Instruct, we incorporate\ncomplete chain-of-thought (CoT) rationales for each problem, and minimize\ntextual redundancy, thereby concentrating the model towards the visual\nelements. Data and Models are released at https://github.com/ZrrSkywalker/MAVIS\n","authors":["Renrui Zhang","Xinyu Wei","Dongzhi Jiang","Yichi Zhang","Ziyu Guo","Chengzhuo Tong","Jiaming Liu","Aojun Zhou","Bin Wei","Shanghang Zhang","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2407.08739v1.pdf","comment":"Work in progress. Data and Models are released at\n  https://github.com/ZrrSkywalker/MAVIS"},{"id":"http://arxiv.org/abs/2407.08737v1","updated":"2024-07-11T17:59:45Z","published":"2024-07-11T17:59:45Z","title":"Video Diffusion Alignment via Reward Gradients","summary":"  We have made significant progress towards building foundational video\ndiffusion models. As these models are trained using large-scale unsupervised\ndata, it has become crucial to adapt these models to specific downstream tasks.\nAdapting these models via supervised fine-tuning requires collecting target\ndatasets of videos, which is challenging and tedious. In this work, we utilize\npre-trained reward models that are learned via preferences on top of powerful\nvision discriminative models to adapt video diffusion models. These models\ncontain dense gradient information with respect to generated RGB pixels, which\nis critical to efficient learning in complex search spaces, such as videos. We\nshow that backpropagating gradients from these reward models to a video\ndiffusion model can allow for compute and sample efficient alignment of the\nvideo diffusion model. We show results across a variety of reward models and\nvideo diffusion models, demonstrating that our approach can learn much more\nefficiently in terms of reward queries and computation than prior gradient-free\napproaches. Our code, model weights,and more visualization are available at\nhttps://vader-vid.github.io.\n","authors":["Mihir Prabhudesai","Russell Mendonca","Zheyang Qin","Katerina Fragkiadaki","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2407.08737v1.pdf","comment":"Project Webpage: https://vader-vid.github.io; Code available at:\n  https://github.com/mihirp1998/VADER"},{"id":"http://arxiv.org/abs/2407.08729v1","updated":"2024-07-11T17:58:10Z","published":"2024-07-11T17:58:10Z","title":"BiEquiFormer: Bi-Equivariant Representations for Global Point Cloud\n  Registration","summary":"  The goal of this paper is to address the problem of \\textit{global} point\ncloud registration (PCR) i.e., finding the optimal alignment between point\nclouds irrespective of the initial poses of the scans. This problem is\nnotoriously challenging for classical optimization methods due to computational\nconstraints. First, we show that state-of-the-art deep learning methods suffer\nfrom huge performance degradation when the point clouds are arbitrarily placed\nin space. We propose that \\textit{equivariant deep learning} should be utilized\nfor solving this task and we characterize the specific type of bi-equivariance\nof PCR. Then, we design BiEquiformer a novel and scalable\n\\textit{bi-equivariant} pipeline i.e. equivariant to the independent\ntransformations of the input point clouds. While a naive approach would process\nthe point clouds independently we design expressive bi-equivariant layers that\nfuse the information from both point clouds. This allows us to extract\nhigh-quality superpoint correspondences and in turn, robust point-cloud\nregistration. Extensive comparisons against state-of-the-art methods show that\nour method achieves comparable performance in the canonical setting and\nsuperior performance in the robust setting in both the 3DMatch and the\nchallenging low-overlap 3DLoMatch dataset.\n","authors":["Stefanos Pertigkiozoglou","Evangelos Chatzipantazis","Kostas Daniilidis"],"pdf_url":"https://arxiv.org/pdf/2407.08729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06863v2","updated":"2024-07-11T17:57:37Z","published":"2024-07-09T13:50:43Z","title":"Beyond Aesthetics: Cultural Competence in Text-to-Image Models","summary":"  Text-to-Image (T2I) models are being increasingly adopted in diverse global\ncommunities where they create visual representations of their unique cultures.\nCurrent T2I benchmarks primarily focus on faithfulness, aesthetics, and realism\nof generated images, overlooking the critical dimension of cultural competence.\nIn this work, we introduce a framework to evaluate cultural competence of T2I\nmodels along two crucial dimensions: cultural awareness and cultural diversity,\nand present a scalable approach using a combination of structured knowledge\nbases and large language models to build a large dataset of cultural artifacts\nto enable this evaluation. In particular, we apply this approach to build CUBE\n(CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to\nevaluate cultural competence of T2I models. CUBE covers cultural artifacts\nassociated with 8 countries across different geo-cultural regions and along 3\nconcepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of\nhigh-quality prompts that enable the evaluation of cultural awareness, and 2)\nCUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to\nevaluate cultural diversity. We also introduce cultural diversity as a novel\nT2I evaluation component, leveraging quality-weighted Vendi score. Our\nevaluations reveal significant gaps in the cultural awareness of existing\nmodels across countries and provide valuable insights into the cultural\ndiversity of T2I outputs for under-specified prompts. Our methodology is\nextendable to other cultural regions and concepts, and can facilitate the\ndevelopment of T2I models that better cater to the global population.\n","authors":["Nithish Kannen","Arif Ahmad","Marco Andreetto","Vinodkumar Prabhakaran","Utsav Prabhu","Adji Bousso Dieng","Pushpak Bhattacharyya","Shachi Dave"],"pdf_url":"https://arxiv.org/pdf/2407.06863v2.pdf","comment":"30 pages, 10 figures, preprint"},{"id":"http://arxiv.org/abs/2407.08726v1","updated":"2024-07-11T17:57:22Z","published":"2024-07-11T17:57:22Z","title":"Map It Anywhere (MIA): Empowering Bird's Eye View Mapping using\n  Large-scale Public Data","summary":"  Top-down Bird's Eye View (BEV) maps are a popular representation for ground\nrobot navigation due to their richness and flexibility for downstream tasks.\nWhile recent methods have shown promise for predicting BEV maps from\nFirst-Person View (FPV) images, their generalizability is limited to small\nregions captured by current autonomous vehicle-based datasets. In this context,\nwe show that a more scalable approach towards generalizable map prediction can\nbe enabled by using two large-scale crowd-sourced mapping platforms, Mapillary\nfor FPV images and OpenStreetMap for BEV semantic maps. We introduce Map It\nAnywhere (MIA), a data engine that enables seamless curation and modeling of\nlabeled map prediction data from existing open-source map platforms. Using our\nMIA data engine, we display the ease of automatically collecting a dataset of\n1.2 million pairs of FPV images & BEV maps encompassing diverse geographies,\nlandscapes, environmental factors, camera models & capture scenarios. We\nfurther train a simple camera model-agnostic model on this data for BEV map\nprediction. Extensive evaluations using established benchmarks and our dataset\nshow that the data curated by MIA enables effective pretraining for\ngeneralizable BEV map prediction, with zero-shot performance far exceeding\nbaselines trained on existing datasets by 35%. Our analysis highlights the\npromise of using large-scale public maps for developing & testing generalizable\nBEV perception, paving the way for more robust autonomous navigation.\n","authors":["Cherie Ho","Jiaye Zou","Omar Alama","Sai Mitheran Jagadesh Kumar","Benjamin Chiang","Taneesh Gupta","Chen Wang","Nikhil Keetha","Katia Sycara","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2407.08726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08725v1","updated":"2024-07-11T17:56:49Z","published":"2024-07-11T17:56:49Z","title":"MetaUrban: A Simulation Platform for Embodied AI in Urban Spaces","summary":"  Public urban spaces like streetscapes and plazas serve residents and\naccommodate social life in all its vibrant variations. Recent advances in\nRobotics and Embodied AI make public urban spaces no longer exclusive to\nhumans. Food delivery bots and electric wheelchairs have started sharing\nsidewalks with pedestrians, while diverse robot dogs and humanoids have\nrecently emerged in the street. Ensuring the generalizability and safety of\nthese forthcoming mobile machines is crucial when navigating through the\nbustling streets in urban spaces. In this work, we present MetaUrban, a\ncompositional simulation platform for Embodied AI research in urban spaces.\nMetaUrban can construct an infinite number of interactive urban scenes from\ncompositional elements, covering a vast array of ground plans, object\nplacements, pedestrians, vulnerable road users, and other mobile agents'\nappearances and dynamics. We design point navigation and social navigation\ntasks as the pilot study using MetaUrban for embodied AI research and establish\nvarious baselines of Reinforcement Learning and Imitation Learning. Experiments\ndemonstrate that the compositional nature of the simulated environments can\nsubstantially improve the generalizability and safety of the trained mobile\nagents. MetaUrban will be made publicly available to provide more research\nopportunities and foster safe and trustworthy embodied AI in urban spaces.\n","authors":["Wayne Wu","Honglin He","Yiran Wang","Chenda Duan","Jack He","Zhizheng Liu","Quanyi Li","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.08725v1.pdf","comment":"Technical report. Project page:\n  https://metadriverse.github.io/metaurban/"},{"id":"http://arxiv.org/abs/2407.08722v1","updated":"2024-07-11T17:55:49Z","published":"2024-07-11T17:55:49Z","title":"Unifying 3D Representation and Control of Diverse Robots with a Single\n  Camera","summary":"  Mirroring the complex structures and diverse functions of natural organisms\nis a long-standing challenge in robotics. Modern fabrication techniques have\ndramatically expanded feasible hardware, yet deploying these systems requires\ncontrol software to translate desired motions into actuator commands. While\nconventional robots can easily be modeled as rigid links connected via joints,\nit remains an open challenge to model and control bio-inspired robots that are\noften multi-material or soft, lack sensing capabilities, and may change their\nmaterial properties with use. Here, we introduce Neural Jacobian Fields, an\narchitecture that autonomously learns to model and control robots from vision\nalone. Our approach makes no assumptions about the robot's materials,\nactuation, or sensing, requires only a single camera for control, and learns to\ncontrol the robot without expert intervention by observing the execution of\nrandom commands. We demonstrate our method on a diverse set of robot\nmanipulators, varying in actuation, materials, fabrication, and cost. Our\napproach achieves accurate closed-loop control and recovers the causal dynamic\nstructure of each robot. By enabling robot control with a generic camera as the\nonly sensor, we anticipate our work will dramatically broaden the design space\nof robotic systems and serve as a starting point for lowering the barrier to\nrobotic automation.\n","authors":["Sizhe Lester Li","Annan Zhang","Boyuan Chen","Hanna Matusik","Chao Liu","Daniela Rus","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2407.08722v1.pdf","comment":"Project Page:\n  https://sizhe-li.github.io/publication/neural_jacobian_field"},{"id":"http://arxiv.org/abs/2407.08717v1","updated":"2024-07-11T17:51:49Z","published":"2024-07-11T17:51:49Z","title":"WhisperNetV2: SlowFast Siamese Network For Lip-Based Biometrics","summary":"  Lip-based biometric authentication (LBBA) has attracted many researchers\nduring the last decade. The lip is specifically interesting for biometric\nresearchers because it is a twin biometric with the potential to function both\nas a physiological and a behavioral trait. Although much valuable research was\nconducted on LBBA, none of them considered the different emotions of the client\nduring the video acquisition step of LBBA, which can potentially affect the\nclient's facial expressions and speech tempo. We proposed a novel network\nstructure called WhisperNetV2, which extends our previously proposed network\ncalled WhisperNet. Our proposed network leverages a deep Siamese structure with\ntriplet loss having three identical SlowFast networks as embedding networks.\nThe SlowFast network is an excellent candidate for our task since the fast\npathway extracts motion-related features (behavioral lip movements) with a high\nframe rate and low channel capacity. The slow pathway extracts visual features\n(physiological lip appearance) with a low frame rate and high channel capacity.\nUsing an open-set protocol, we trained our network using the CREMA-D dataset\nand acquired an Equal Error Rate (EER) of 0.005 on the test set. Considering\nthat the acquired EER is less than most similar LBBA methods, our method can be\nconsidered as a state-of-the-art LBBA method.\n","authors":["Abdollah Zakeri","Hamid Hassanpour","Mohammad Hossein Khosravi","Amir Masoud Nourollah"],"pdf_url":"https://arxiv.org/pdf/2407.08717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12110v2","updated":"2024-07-11T17:50:47Z","published":"2024-05-20T15:25:47Z","title":"CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization","summary":"  3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D\nGaussians to represent a scene. With sparse training views, 3DGS easily suffers\nfrom overfitting, negatively impacting rendering. This paper introduces a new\nco-regularization perspective for improving sparse-view 3DGS. When training two\n3D Gaussian radiance fields, we observe that the two radiance fields exhibit\npoint disagreement and rendering disagreement that can unsupervisedly predict\nreconstruction quality, stemming from the randomness of densification\nimplementation. We further quantify the two disagreements and demonstrate the\nnegative correlation between them and accurate reconstruction, which allows us\nto identify inaccurate reconstruction without accessing ground-truth\ninformation. Based on the study, we propose CoR-GS, which identifies and\nsuppresses inaccurate reconstruction based on the two disagreements: (1)\nCo-pruning considers Gaussians that exhibit high point disagreement in\ninaccurate positions and prunes them. (2) Pseudo-view co-regularization\nconsiders pixels that exhibit high rendering disagreement are inaccurate and\nsuppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and Blender\ndemonstrate that CoR-GS effectively regularizes the scene geometry,\nreconstructs the compact representations, and achieves state-of-the-art novel\nview synthesis quality under sparse training views.\n","authors":["Jiawei Zhang","Jiahe Li","Xiaohan Yu","Lei Huang","Lin Gu","Jin Zheng","Xiao Bai"],"pdf_url":"https://arxiv.org/pdf/2405.12110v2.pdf","comment":"Accepted at ECCV 2024. Project page: https://jiaw-z.github.io/CoR-GS/"},{"id":"http://arxiv.org/abs/2310.17316v4","updated":"2024-07-11T17:50:40Z","published":"2023-10-26T11:23:24Z","title":"Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with\n  Rich Semantics","summary":"  Defect inspection is paramount within the closed-loop manufacturing system.\nHowever, existing datasets for defect inspection often lack precision and\nsemantic granularity required for practical applications. In this paper, we\nintroduce the Defect Spectrum, a comprehensive benchmark that offers precise,\nsemantic-abundant, and large-scale annotations for a wide range of industrial\ndefects. Building on four key industrial benchmarks, our dataset refines\nexisting annotations and introduces rich semantic details, distinguishing\nmultiple defect types within a single image. Furthermore, we introduce\nDefect-Gen, a two-stage diffusion-based generator designed to create\nhigh-quality and diverse defective images, even when working with limited\ndatasets. The synthetic images generated by Defect-Gen significantly enhance\nthe efficacy of defect inspection models. Overall, The Defect Spectrum dataset\ndemonstrates its potential in defect inspection research, offering a solid\nplatform for testing and refining advanced models.\n","authors":["Shuai Yang","Zhifei Chen","Pengguang Chen","Xi Fang","Shu Liu","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2310.17316v4.pdf","comment":"Accepted by ECCV2024. Please see our project page at\n  https://envision-research.github.io/Defect_Spectrum/"},{"id":"http://arxiv.org/abs/2407.08711v1","updated":"2024-07-11T17:49:05Z","published":"2024-07-11T17:49:05Z","title":"OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects","summary":"  We propose OmniNOCS, a large-scale monocular dataset with 3D Normalized\nObject Coordinate Space (NOCS) maps, object masks, and 3D bounding box\nannotations for indoor and outdoor scenes. OmniNOCS has 20 times more object\nclasses and 200 times more instances than existing NOCS datasets (NOCS-Real275,\nWild6D). We use OmniNOCS to train a novel, transformer-based monocular NOCS\nprediction model (NOCSformer) that can predict accurate NOCS, instance masks\nand poses from 2D object detections across diverse classes. It is the first\nNOCS model that can generalize to a broad range of classes when prompted with\n2D boxes. We evaluate our model on the task of 3D oriented bounding box\nprediction, where it achieves comparable results to state-of-the-art 3D\ndetection methods such as Cube R-CNN. Unlike other 3D detection methods, our\nmodel also provides detailed and accurate 3D object shape and segmentation. We\npropose a novel benchmark for the task of NOCS prediction based on OmniNOCS,\nwhich we hope will serve as a useful baseline for future work in this area. Our\ndataset and code will be at the project website: https://omninocs.github.io.\n","authors":["Akshay Krishnan","Abhijit Kundu","Kevis-Kokitsi Maninis","James Hays","Matthew Brown"],"pdf_url":"https://arxiv.org/pdf/2407.08711v1.pdf","comment":"Accepted to ECCV 2024, project website: https://omninocs.github.io"},{"id":"http://arxiv.org/abs/2407.08707v1","updated":"2024-07-11T17:44:41Z","published":"2024-07-11T17:44:41Z","title":"Extracting Training Data from Document-Based VQA Models","summary":"  Vision-Language Models (VLMs) have made remarkable progress in document-based\nVisual Question Answering (i.e., responding to queries about the contents of an\ninput document provided as an image). In this work, we show these models can\nmemorize responses for training samples and regurgitate them even when the\nrelevant visual information has been removed. This includes Personal\nIdentifiable Information (PII) repeated once in the training set, indicating\nthese models could divulge memorised sensitive information and therefore pose a\nprivacy risk. We quantitatively measure the extractability of information in\ncontrolled experiments and differentiate between cases where it arises from\ngeneralization capabilities or from memorization. We further investigate the\nfactors that influence memorization across multiple state-of-the-art models and\npropose an effective heuristic countermeasure that empirically prevents the\nextractability of PII.\n","authors":["Francesco Pinto","Nathalie Rauschmayr","Florian TramÃ¨r","Philip Torr","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2407.08707v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2407.08706v1","updated":"2024-07-11T17:42:17Z","published":"2024-07-11T17:42:17Z","title":"HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large\n  Vision-Language Models","summary":"  High-resolution inputs enable Large Vision-Language Models (LVLMs) to discern\nfiner visual details, enhancing their comprehension capabilities. To reduce the\ntraining and computation costs caused by high-resolution input, one promising\ndirection is to use sliding windows to slice the input into uniform patches,\neach matching the input size of the well-trained vision encoder. Although\nefficient, this slicing strategy leads to the fragmentation of original input,\ni.e., the continuity of contextual information and spatial geometry is lost\nacross patches, adversely affecting performance in cross-patch context\nperception and position-specific tasks. To overcome these shortcomings, we\nintroduce HiRes-LLaVA, a novel framework designed to efficiently process any\nsize of high-resolution input without altering the original contextual and\ngeometric information. HiRes-LLaVA comprises two innovative components: (i) a\nSliceRestore adapter that reconstructs sliced patches into their original form,\nefficiently extracting both global and local features via down-up-sampling and\nconvolution layers, and (ii) a Self-Mining Sampler to compresses the vision\ntokens based on themselves, preserving the original context and positional\ninformation while reducing training overhead. To assess the ability of handling\ncontext fragmentation, we construct a new benchmark, EntityGrid-QA, consisting\nof edge-related and position-related tasks. Our comprehensive experiments\ndemonstrate the superiority of HiRes-LLaVA on both existing public benchmarks\nand on EntityGrid-QA, particularly on document-oriented tasks, establishing new\nstandards for handling high-resolution inputs.\n","authors":["Runhui Huang","Xinpeng Ding","Chunwei Wang","Jianhua Han","Yulong Liu","Hengshuang Zhao","Hang Xu","Lu Hou","Wei Zhang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2407.08706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08704v1","updated":"2024-07-11T17:40:39Z","published":"2024-07-11T17:40:39Z","title":"Towards Efficient Deployment of Hybrid SNNs on Neuromorphic and Edge AI\n  Hardware","summary":"  This paper explores the synergistic potential of neuromorphic and edge\ncomputing to create a versatile machine learning (ML) system tailored for\nprocessing data captured by dynamic vision sensors. We construct and train\nhybrid models, blending spiking neural networks (SNNs) and artificial neural\nnetworks (ANNs) using PyTorch and Lava frameworks. Our hybrid architecture\nintegrates an SNN for temporal feature extraction and an ANN for\nclassification. We delve into the challenges of deploying such hybrid\nstructures on hardware. Specifically, we deploy individual components on\nIntel's Neuromorphic Processor Loihi (for SNN) and Jetson Nano (for ANN). We\nalso propose an accumulator circuit to transfer data from the spiking to the\nnon-spiking domain. Furthermore, we conduct comprehensive performance analyses\nof hybrid SNN-ANN models on a heterogeneous system of neuromorphic and edge AI\nhardware, evaluating accuracy, latency, power, and energy consumption. Our\nfindings demonstrate that the hybrid spiking networks surpass the baseline ANN\nmodel across all metrics and outperform the baseline SNN model in accuracy and\nlatency.\n","authors":["James Seekings","Peyton Chandarana","Mahsa Ardakani","MohammadReza Mohammadi","Ramtin Zand"],"pdf_url":"https://arxiv.org/pdf/2407.08704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08701v1","updated":"2024-07-11T17:34:51Z","published":"2024-07-11T17:34:51Z","title":"Live2Diff: Live Stream Translation via Uni-directional Attention in\n  Video Diffusion Models","summary":"  Large Language Models have shown remarkable efficacy in generating streaming\ndata such as text and audio, thanks to their temporally uni-directional\nattention mechanism, which models correlations between the current token and\nprevious tokens. However, video streaming remains much less explored, despite a\ngrowing need for live video processing. State-of-the-art video diffusion models\nleverage bi-directional temporal attention to model the correlations between\nthe current frame and all the surrounding (i.e. including future) frames, which\nhinders them from processing streaming videos. To address this problem, we\npresent Live2Diff, the first attempt at designing a video diffusion model with\nuni-directional temporal attention, specifically targeting live streaming video\ntranslation. Compared to previous works, our approach ensures temporal\nconsistency and smoothness by correlating the current frame with its\npredecessors and a few initial warmup frames, without any future frames.\nAdditionally, we use a highly efficient denoising scheme featuring a KV-cache\nmechanism and pipelining, to facilitate streaming video translation at\ninteractive framerates. Extensive experiments demonstrate the effectiveness of\nthe proposed attention mechanism and pipeline, outperforming previous methods\nin terms of temporal smoothness and/or efficiency.\n","authors":["Zhening Xing","Gereon Fox","Yanhong Zeng","Xingang Pan","Mohamed Elgharib","Christian Theobalt","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2407.08701v1.pdf","comment":"https://live2diff.github.io/"},{"id":"http://arxiv.org/abs/2403.17933v2","updated":"2024-07-11T17:27:49Z","published":"2024-03-26T17:58:29Z","title":"SLEDGE: Synthesizing Driving Environments with Generative Models and\n  Rule-Based Traffic","summary":"  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for rule-based traffic simulation. The unique properties of\nthe entities to be generated for SLEDGE, such as their connectivity and\nvariable count per scene, render the naive application of most modern\ngenerative models to this task non-trivial. Therefore, together with a\nsystematic study of existing lane graph representations, we introduce a novel\nraster-to-vector autoencoder. It encodes agents and the lane graph into\ndistinct channels in a rasterized latent map. This facilitates both\nlane-conditioned agent generation and combined generation of lanes and agents\nwith a Diffusion Transformer. Using generated entities in SLEDGE enables\ngreater control over the simulation, e.g. upsampling turns or increasing\ntraffic density. Further, SLEDGE can support 500m long routes, a capability not\nfound in existing data-driven simulators like nuPlan. It presents new\nchallenges for planning algorithms, evidenced by failure rates of over 40% for\nPDM, the winner of the 2023 nuPlan challenge, when tested on hard routes and\ndense traffic generated by our model. Compared to nuPlan, SLEDGE requires\n500$\\times$ less storage to set up (<4 GB), making it a more accessible option\nand helping with democratizing future research in this field.\n","authors":["Kashyap Chitta","Daniel Dauner","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.17933v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.08683v1","updated":"2024-07-11T17:21:03Z","published":"2024-07-11T17:21:03Z","title":"SEED-Story: Multimodal Long Story Generation with Large Language Model","summary":"  With the remarkable advancements in image generation and open-form text\ngeneration, the creation of interleaved image-text content has become an\nincreasingly intriguing field. Multimodal story generation, characterized by\nproducing narrative texts and vivid images in an interleaved manner, has\nemerged as a valuable and practical task with broad applications. However, this\ntask poses significant challenges, as it necessitates the comprehension of the\ncomplex interplay between texts and images, and the ability to generate long\nsequences of coherent, contextually relevant texts and visuals. In this work,\nwe propose SEED-Story, a novel method that leverages a Multimodal Large\nLanguage Model (MLLM) to generate extended multimodal stories. Our model, built\nupon the powerful comprehension capability of MLLM, predicts text tokens as\nwell as visual tokens, which are subsequently processed with an adapted visual\nde-tokenizer to produce images with consistent characters and styles. We\nfurther propose multimodal attention sink mechanism to enable the generation of\nstories with up to 25 sequences (only 10 for training) in a highly efficient\nautoregressive manner. Additionally, we present a large-scale and\nhigh-resolution dataset named StoryStream for training our model and\nquantitatively evaluating the task of multimodal story generation in various\naspects.\n","authors":["Shuai Yang","Yuying Ge","Yang Li","Yukang Chen","Yixiao Ge","Ying Shan","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.08683v1.pdf","comment":"Our models, codes and datasets are released in\n  https://github.com/TencentARC/SEED-Story"},{"id":"http://arxiv.org/abs/2407.08680v1","updated":"2024-07-11T17:13:15Z","published":"2024-07-11T17:13:15Z","title":"Generalizable Implicit Motion Modeling for Video Frame Interpolation","summary":"  Motion modeling is critical in flow-based Video Frame Interpolation (VFI).\nExisting paradigms either consider linear combinations of bidirectional flows\nor directly predict bilateral flows for given timestamps without exploring\nfavorable motion priors, thus lacking the capability of effectively modeling\nspatiotemporal dynamics in real-world videos. To address this limitation, in\nthis study, we introduce Generalizable Implicit Motion Modeling (GIMM), a novel\nand effective approach to motion modeling for VFI. Specifically, to enable GIMM\nas an effective motion modeling paradigm, we design a motion encoding pipeline\nto model spatiotemporal motion latent from bidirectional flows extracted from\npre-trained flow estimators, effectively representing input-specific motion\npriors. Then, we implicitly predict arbitrary-timestep optical flows within two\nadjacent input frames via an adaptive coordinate-based neural network, with\nspatiotemporal coordinates and motion latent as inputs. Our GIMM can be\nsmoothly integrated with existing flow-based VFI works without further\nmodifications. We show that GIMM performs better than the current state of the\nart on the VFI benchmarks.\n","authors":["Zujin Guo","Wei Li","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2407.08680v1.pdf","comment":"Project Page: https://gseancdat.github.io/projects/GIMMVFI"},{"id":"http://arxiv.org/abs/2407.08674v1","updated":"2024-07-11T17:06:53Z","published":"2024-07-11T17:06:53Z","title":"Still-Moving: Customized Video Generation without Customized Video Data","summary":"  Customizing text-to-image (T2I) models has seen tremendous progress recently,\nparticularly in areas such as personalization, stylization, and conditional\ngeneration. However, expanding this progress to video generation is still in\nits infancy, primarily due to the lack of customized video data. In this work,\nwe introduce Still-Moving, a novel generic framework for customizing a\ntext-to-video (T2V) model, without requiring any customized video data. The\nframework applies to the prominent T2V design where the video model is built\nover a text-to-image (T2I) model (e.g., via inflation). We assume access to a\ncustomized version of the T2I model, trained only on still image data (e.g.,\nusing DreamBooth or StyleDrop). Naively plugging in the weights of the\ncustomized T2I model into the T2V model often leads to significant artifacts or\ninsufficient adherence to the customization data. To overcome this issue, we\ntrain lightweight $\\textit{Spatial Adapters}$ that adjust the features produced\nby the injected T2I layers. Importantly, our adapters are trained on\n$\\textit{\"frozen videos\"}$ (i.e., repeated images), constructed from image\nsamples generated by the customized T2I model. This training is facilitated by\na novel $\\textit{Motion Adapter}$ module, which allows us to train on such\nstatic videos while preserving the motion prior of the video model. At test\ntime, we remove the Motion Adapter modules and leave in only the trained\nSpatial Adapters. This restores the motion prior of the T2V model while\nadhering to the spatial prior of the customized T2I model. We demonstrate the\neffectiveness of our approach on diverse tasks including personalized,\nstylized, and conditional generation. In all evaluated scenarios, our method\nseamlessly integrates the spatial prior of the customized T2I model with a\nmotion prior supplied by the T2V model.\n","authors":["Hila Chefer","Shiran Zada","Roni Paiss","Ariel Ephrat","Omer Tov","Michael Rubinstein","Lior Wolf","Tali Dekel","Tomer Michaeli","Inbar Mosseri"],"pdf_url":"https://arxiv.org/pdf/2407.08674v1.pdf","comment":"Webpage: https://still-moving.github.io/ | Video:\n  https://www.youtube.com/watch?v=U7UuV_VIjnA"},{"id":"http://arxiv.org/abs/2407.08672v1","updated":"2024-07-11T17:04:19Z","published":"2024-07-11T17:04:19Z","title":"NODE-Adapter: Neural Ordinary Differential Equations for Better\n  Vision-Language Reasoning","summary":"  In this paper, we consider the problem of prototype-based vision-language\nreasoning problem. We observe that existing methods encounter three major\nchallenges: 1) escalating resource demands and prolonging training times, 2)\ncontending with excessive learnable parameters, and 3) fine-tuning based only\non a single modality. These challenges will hinder their capability to adapt\nVision-Language Models (VLMs) to downstream tasks. Motivated by this critical\nobservation, we propose a novel method called NODE-Adapter, which utilizes\nNeural Ordinary Differential Equations for better vision-language reasoning. To\nfully leverage both visual and textual modalities and estimate class prototypes\nmore effectively and accurately, we divide our method into two stages:\ncross-modal prototype construction and cross-modal prototype optimization using\nneural ordinary differential equations. Specifically, we exploit VLM to encode\nhand-crafted prompts into textual features and few-shot support images into\nvisual features. Then, we estimate the textual prototype and visual prototype\nby averaging the textual features and visual features, respectively, and\nadaptively combine the textual prototype and visual prototype to construct the\ncross-modal prototype. To alleviate the prototype bias, we then model the\nprototype optimization process as an initial value problem with Neural ODEs to\nestimate the continuous gradient flow. Our extensive experimental results,\nwhich cover few-shot classification, domain generalization, and visual\nreasoning on human-object interaction, demonstrate that the proposed method\nsignificantly outperforms existing state-of-the-art approaches.\n","authors":["Yi Zhang","Chun-Wun Cheng","Ke Yu","Zhihai He","Carola-Bibiane SchÃ¶nlieb","Angelica I. Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2407.08672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16384v2","updated":"2024-07-11T17:03:29Z","published":"2024-06-24T07:53:46Z","title":"High-resolution open-vocabulary object 6D pose estimation","summary":"  The generalisation to unseen objects in the 6D pose estimation task is very\nchallenging. While Vision-Language Models (VLMs) enable using natural language\ndescriptions to support 6D pose estimation of unseen objects, these solutions\nunderperform compared to model-based methods. In this work we present Horyon,\nan open-vocabulary VLM-based architecture that addresses relative pose\nestimation between two scenes of an unseen object, described by a textual\nprompt only. We use the textual prompt to identify the unseen object in the\nscenes and then obtain high-resolution multi-scale features. These features are\nused to extract cross-scene matches for registration. We evaluate our model on\na benchmark with a large variety of unseen objects across four datasets, namely\nREAL275, Toyota-Light, Linemod, and YCB-Video. Our method achieves\nstate-of-the-art performance on all datasets, outperforming by 12.6 in Average\nRecall the previous best-performing approach.\n","authors":["Jaime Corsetti","Davide Boscaini","Francesco Giuliari","Changjae Oh","Andrea Cavallaro","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2406.16384v2.pdf","comment":"Technical report. Extension of CVPR paper \"Open-vocabulary object 6D\n  pose estimation\". Project page: https://jcorsetti.github.io/oryon"},{"id":"http://arxiv.org/abs/2407.08669v1","updated":"2024-07-11T16:59:32Z","published":"2024-07-11T16:59:32Z","title":"Segmentation-guided Attention for Visual Question Answering from Remote\n  Sensing Images","summary":"  Visual Question Answering for Remote Sensing (RSVQA) is a task that aims at\nanswering natural language questions about the content of a remote sensing\nimage. The visual features extraction is therefore an essential step in a VQA\npipeline. By incorporating attention mechanisms into this process, models gain\nthe ability to focus selectively on salient regions of the image, prioritizing\nthe most relevant visual information for a given question. In this work, we\npropose to embed an attention mechanism guided by segmentation into a RSVQA\npipeline. We argue that segmentation plays a crucial role in guiding attention\nby providing a contextual understanding of the visual information, underlying\nspecific objects or areas of interest. To evaluate this methodology, we provide\na new VQA dataset that exploits very high-resolution RGB orthophotos annotated\nwith 16 segmentation classes and question/answer pairs. Our study shows\npromising results of our new methodology, gaining almost 10% of overall\naccuracy compared to a classical method on the proposed dataset.\n","authors":["Lucrezia Tosato","Hichem Boussaid","Flora Weissgerber","Camille Kurtz","Laurent Wendling","Sylvain Lobry"],"pdf_url":"https://arxiv.org/pdf/2407.08669v1.pdf","comment":"Accepted to IGARSS 2024"},{"id":"http://arxiv.org/abs/2407.08659v1","updated":"2024-07-11T16:46:04Z","published":"2024-07-11T16:46:04Z","title":"Controlling the Fidelity and Diversity of Deep Generative Models via\n  Pseudo Density","summary":"  We introduce an approach to bias deep generative models, such as GANs and\ndiffusion models, towards generating data with either enhanced fidelity or\nincreased diversity. Our approach involves manipulating the distribution of\ntraining and generated data through a novel metric for individual samples,\nnamed pseudo density, which is based on the nearest-neighbor information from\nreal samples. Our approach offers three distinct techniques to adjust the\nfidelity and diversity of deep generative models: 1) Per-sample perturbation,\nenabling precise adjustments for individual samples towards either more common\nor more unique characteristics; 2) Importance sampling during model inference\nto enhance either fidelity or diversity in the generated data; 3) Fine-tuning\nwith importance sampling, which guides the generative model to learn an\nadjusted distribution, thus controlling fidelity and diversity. Furthermore,\nour fine-tuning method demonstrates the ability to improve the Frechet\nInception Distance (FID) for pre-trained generative models with minimal\niterations.\n","authors":["Shuangqi Li","Chen Liu","Tong Zhang","Hieu Le","Sabine SÃ¼sstrunk","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2407.08659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05210v2","updated":"2024-07-11T16:31:39Z","published":"2023-12-08T17:58:14Z","title":"IntrinsicAvatar: Physically Based Inverse Rendering of Dynamic Humans\n  from Monocular Videos via Explicit Ray Tracing","summary":"  We present IntrinsicAvatar, a novel approach to recovering the intrinsic\nproperties of clothed human avatars including geometry, albedo, material, and\nenvironment lighting from only monocular videos. Recent advancements in\nhuman-based neural rendering have enabled high-quality geometry and appearance\nreconstruction of clothed humans from just monocular videos. However, these\nmethods bake intrinsic properties such as albedo, material, and environment\nlighting into a single entangled neural representation. On the other hand, only\na handful of works tackle the problem of estimating geometry and disentangled\nappearance properties of clothed humans from monocular videos. They usually\nachieve limited quality and disentanglement due to approximations of secondary\nshading effects via learned MLPs. In this work, we propose to model secondary\nshading effects explicitly via Monte-Carlo ray tracing. We model the rendering\nprocess of clothed humans as a volumetric scattering process, and combine ray\ntracing with body articulation. Our approach can recover high-quality geometry,\nalbedo, material, and lighting properties of clothed humans from a single\nmonocular video, without requiring supervised pre-training using ground truth\nmaterials. Furthermore, since we explicitly model the volumetric scattering\nprocess and ray tracing, our model naturally generalizes to novel poses,\nenabling animation of the reconstructed avatar in novel lighting conditions.\n","authors":["Shaofei Wang","BoÅ¾idar AntiÄ","Andreas Geiger","Siyu Tang"],"pdf_url":"https://arxiv.org/pdf/2312.05210v2.pdf","comment":"CVPR camera-ready version. Project page:\n  https://neuralbodies.github.io/IntrinsicAvatar"},{"id":"http://arxiv.org/abs/2407.08650v1","updated":"2024-07-11T16:28:44Z","published":"2024-07-11T16:28:44Z","title":"Latent Spaces Enable Transformer-Based Dose Prediction in Complex\n  Radiotherapy Plans","summary":"  Evidence is accumulating in favour of using stereotactic ablative body\nradiotherapy (SABR) to treat multiple cancer lesions in the lung. Multi-lesion\nlung SABR plans are complex and require significant resources to create. In\nthis work, we propose a novel two-stage latent transformer framework (LDFormer)\nfor dose prediction of lung SABR plans with varying numbers of lesions. In the\nfirst stage, patient anatomical information and the dose distribution are\nencoded into a latent space. In the second stage, a transformer learns to\npredict the dose latent from the anatomical latents. Causal attention is\nmodified to adapt to different numbers of lesions. LDFormer outperforms a\nstate-of-the-art generative adversarial network on dose conformality in and\naround lesions, and the performance gap widens when considering overlapping\nlesions. LDFormer generates predictions of 3-D dose distributions in under 30s\non consumer hardware, and has the potential to assist physicians with clinical\ndecision making, reduce resource costs, and accelerate treatment planning.\n","authors":["Edward Wang","Ryan Au","Pencilla Lang","Sarah A. Mattonen"],"pdf_url":"https://arxiv.org/pdf/2407.08650v1.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.08648v1","updated":"2024-07-11T16:26:08Z","published":"2024-07-11T16:26:08Z","title":"CAR-MFL: Cross-Modal Augmentation by Retrieval for Multimodal Federated\n  Learning with Missing Modalities","summary":"  Multimodal AI has demonstrated superior performance over unimodal approaches\nby leveraging diverse data sources for more comprehensive analysis. However,\napplying this effectiveness in healthcare is challenging due to the limited\navailability of public datasets. Federated learning presents an exciting\nsolution, allowing the use of extensive databases from hospitals and health\ncenters without centralizing sensitive data, thus maintaining privacy and\nsecurity. Yet, research in multimodal federated learning, particularly in\nscenarios with missing modalities a common issue in healthcare datasets remains\nscarce, highlighting a critical area for future exploration. Toward this, we\npropose a novel method for multimodal federated learning with missing\nmodalities. Our contribution lies in a novel cross-modal data augmentation by\nretrieval, leveraging the small publicly available dataset to fill the missing\nmodalities in the clients. Our method learns the parameters in a federated\nmanner, ensuring privacy protection and improving performance in multiple\nchallenging multimodal benchmarks in the medical domain, surpassing several\ncompetitive baselines. Code Available: https://github.com/bhattarailab/CAR-MFL\n","authors":["Pranav Poudel","Prashant Shrestha","Sanskar Amgain","Yash Raj Shrestha","Prashnna Gyawali","Binod Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2407.08648v1.pdf","comment":"Accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2405.20222v3","updated":"2024-07-11T16:26:03Z","published":"2024-05-30T16:22:22Z","title":"MOFA-Video: Controllable Image Animation via Generative Motion Field\n  Adaptions in Frozen Image-to-Video Diffusion Model","summary":"  We present MOFA-Video, an advanced controllable image animation method that\ngenerates video from the given image using various additional controllable\nsignals (such as human landmarks reference, manual trajectories, and another\neven provided video) or their combinations. This is different from previous\nmethods which only can work on a specific motion domain or show weak control\nabilities with diffusion prior. To achieve our goal, we design several\ndomain-aware motion field adapters (\\ie, MOFA-Adapters) to control the\ngenerated motions in the video generation pipeline. For MOFA-Adapters, we\nconsider the temporal motion consistency of the video and generate the dense\nmotion flow from the given sparse control conditions first, and then, the\nmulti-scale features of the given image are wrapped as a guided feature for\nstable video diffusion generation. We naively train two motion adapters for the\nmanual trajectories and the human landmarks individually since they both\ncontain sparse information about the control. After training, the MOFA-Adapters\nin different domains can also work together for more controllable video\ngeneration. Project Page: https://myniuuu.github.io/MOFA_Video/\n","authors":["Muyao Niu","Xiaodong Cun","Xintao Wang","Yong Zhang","Ying Shan","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2405.20222v3.pdf","comment":"ECCV 2024 ; Project Page: https://myniuuu.github.io/MOFA_Video/ ;\n  Codes: https://github.com/MyNiuuu/MOFA-Video"},{"id":"http://arxiv.org/abs/2405.05241v2","updated":"2024-07-11T16:24:52Z","published":"2024-05-08T17:37:57Z","title":"BenthicNet: A global compilation of seafloor images for deep learning\n  applications","summary":"  Advances in underwater imaging enable the collection of extensive seafloor\nimage datasets that are necessary for monitoring important benthic ecosystems.\nThe ability to collect seafloor imagery has outpaced our capacity to analyze\nit, hindering expedient mobilization of this crucial environmental information.\nRecent machine learning approaches provide opportunities to increase the\nefficiency with which seafloor image datasets are analyzed, yet large and\nconsistent datasets necessary to support development of such approaches are\nscarce. Here we present BenthicNet: a global compilation of seafloor imagery\ndesigned to support the training and evaluation of large-scale image\nrecognition models. An initial set of over 11.4 million images was collected\nand curated to represent a diversity of seafloor environments using a\nrepresentative subset of 1.3 million images. These are accompanied by 2.6\nmillion annotations translated to the CATAMI scheme, which span 190,000 of the\nimages. A large deep learning model was trained on this compilation and\npreliminary results suggest it has utility for automating large and small-scale\nimage analysis tasks. The compilation and model are made openly available for\nuse by the scientific community at https://doi.org/10.20383/103.0614.\n","authors":["Scott C. Lowe","Benjamin Misiuk","Isaac Xu","Shakhboz Abdulazizov","Amit R. Baroi","Alex C. Bastos","Merlin Best","Vicki Ferrini","Ariell Friedman","Deborah Hart","Ove Hoegh-Guldberg","Daniel Ierodiaconou","Julia Mackin-McLaughlin","Kathryn Markey","Pedro S. Menandro","Jacquomo Monk","Shreya Nemani","John O'Brien","Elizabeth Oh","Luba Y. Reshitnyk","Katleen Robert","Chris M. Roelfsema","Jessica A. Sameoto","Alexandre C. G. Schimel","Jordan A. Thomson","Brittany R. Wilson","Melisa C. Wong","Craig J. Brown","Thomas Trappenberg"],"pdf_url":"https://arxiv.org/pdf/2405.05241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13972v3","updated":"2024-07-11T16:22:27Z","published":"2024-03-20T20:47:53Z","title":"UP-FacE: User-predictable Fine-grained Face Shape Editing","summary":"  We present User-predictable Face Editing (UP-FacE) -- a novel method for\npredictable face shape editing. In stark contrast to existing methods for face\nediting using trial and error, edits with UP-FacE are predictable by the human\nuser. That is, users can control the desired degree of change precisely and\ndeterministically and know upfront the amount of change required to achieve a\ncertain editing result. Our method leverages facial landmarks to precisely\nmeasure facial feature values, facilitating the training of UP-FacE without\nmanually annotated attribute labels. At the core of UP-FacE is a\ntransformer-based network that takes as input a latent vector from a\npre-trained generative model and a facial feature embedding, and predicts a\nsuitable manipulation vector. To enable user-predictable editing, a scaling\nlayer adjusts the manipulation vector to achieve the precise desired degree of\nchange. To ensure that the desired feature is manipulated towards the target\nvalue without altering uncorrelated features, we further introduce a novel\nsemantic face feature loss. Qualitative and quantitative results demonstrate\nthat UP-FacE enables precise and fine-grained control over 23 face shape\nfeatures.\n","authors":["Florian Strohm","Mihai BÃ¢ce","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2403.13972v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08640v1","updated":"2024-07-11T16:21:48Z","published":"2024-07-11T16:21:48Z","title":"Modality Agnostic Heterogeneous Face Recognition with Switch Style\n  Modulators","summary":"  Heterogeneous Face Recognition (HFR) systems aim to enhance the capability of\nface recognition in challenging cross-modal authentication scenarios. However,\nthe significant domain gap between the source and target modalities poses a\nconsiderable challenge for cross-domain matching. Existing literature primarily\nfocuses on developing HFR approaches for specific pairs of face modalities,\nnecessitating the explicit training of models for each source-target\ncombination. In this work, we introduce a novel framework designed to train a\nmodality-agnostic HFR method capable of handling multiple modalities during\ninference, all without explicit knowledge of the target modality labels. We\nachieve this by implementing a computationally efficient automatic routing\nmechanism called Switch Style Modulation Blocks (SSMB) that trains various\ndomain expert modulators which transform the feature maps adaptively reducing\nthe domain gap. Our proposed SSMB can be trained end-to-end and seamlessly\nintegrated into pre-trained face recognition models, transforming them into\nmodality-agnostic HFR models. We have performed extensive evaluations on HFR\nbenchmark datasets to demonstrate its effectiveness. The source code and\nprotocols will be made publicly available.\n","authors":["Anjith George","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2407.08640v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2407.08634v1","updated":"2024-07-11T16:15:47Z","published":"2024-07-11T16:15:47Z","title":"RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation","summary":"  Whole-body pose estimation is a challenging task that requires simultaneous\nprediction of keypoints for the body, hands, face, and feet. Whole-body pose\nestimation aims to predict fine-grained pose information for the human body,\nincluding the face, torso, hands, and feet, which plays an important role in\nthe study of human-centric perception and generation and in various\napplications. In this work, we present RTMW (Real-Time Multi-person Whole-body\npose estimation models), a series of high-performance models for 2D/3D\nwhole-body pose estimation. We incorporate RTMPose model architecture with FPN\nand HEM (Hierarchical Encoding Module) to better capture pose information from\ndifferent body parts with various scales. The model is trained with a rich\ncollection of open-source human keypoint datasets with manually aligned\nannotations and further enhanced via a two-stage distillation strategy. RTMW\ndemonstrates strong performance on multiple whole-body pose estimation\nbenchmarks while maintaining high inference efficiency and deployment\nfriendliness. We release three sizes: m/l/x, with RTMW-l achieving a 70.2 mAP\non the COCO-Wholebody benchmark, making it the first open-source model to\nexceed 70 mAP on this benchmark. Meanwhile, we explored the performance of RTMW\nin the task of 3D whole-body pose estimation, conducting image-based monocular\n3D whole-body pose estimation in a coordinate classification manner. We hope\nthis work can benefit both academic research and industrial applications. The\ncode and models have been made publicly available at:\nhttps://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose\n","authors":["Tao Jiang","Xinchen Xie","Yining Li"],"pdf_url":"https://arxiv.org/pdf/2407.08634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08625v1","updated":"2024-07-11T16:03:59Z","published":"2024-07-11T16:03:59Z","title":"Histopathological Image Classification with Cell Morphology Aware Deep\n  Neural Networks","summary":"  Histopathological images are widely used for the analysis of diseased (tumor)\ntissues and patient treatment selection. While the majority of microscopy image\nprocessing was previously done manually by pathologists, recent advances in\ncomputer vision allow for accurate recognition of lesion regions with deep\nlearning-based solutions. Such models, however, usually require extensive\nannotated datasets for training, which is often not the case in the considered\ntask, where the number of available patient data samples is very limited. To\ndeal with this problem, we propose a novel DeepCMorph model pre-trained to\nlearn cell morphology and identify a large number of different cancer types.\nThe model consists of two modules: the first one performs cell nuclei\nsegmentation and annotates each cell type, and is trained on a combination of 8\npublicly available datasets to ensure its high generalizability and robustness.\nThe second module combines the obtained segmentation map with the original\nmicroscopy image and is trained for the downstream task. We pre-trained this\nmodule on the Pan-Cancer TCGA dataset consisting of over 270K tissue patches\nextracted from 8736 diagnostic slides from 7175 patients. The proposed solution\nachieved a new state-of-the-art performance on the dataset under consideration,\ndetecting 32 cancer types with over 82% accuracy and outperforming all\npreviously proposed solutions by more than 4%. We demonstrate that the\nresulting pre-trained model can be easily fine-tuned on smaller microscopy\ndatasets, yielding superior results compared to the current top solutions and\nmodels initialized with ImageNet weights. The codes and pre-trained models\npresented in this paper are available at: https://github.com/aiff22/DeepCMorph\n","authors":["Andrey Ignatov","Josephine Yates","Valentina Boeva"],"pdf_url":"https://arxiv.org/pdf/2407.08625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03482v2","updated":"2024-07-11T16:00:05Z","published":"2024-04-04T14:35:49Z","title":"AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position\n  and Scale","summary":"  Active Visual Exploration (AVE) is a task that involves dynamically selecting\nobservations (glimpses), which is critical to facilitate comprehension and\nnavigation within an environment. While modern AVE methods have demonstrated\nimpressive performance, they are constrained to fixed-scale glimpses from rigid\ngrids. In contrast, existing mobile platforms equipped with optical zoom\ncapabilities can capture glimpses of arbitrary positions and scales. To address\nthis gap between software and hardware capabilities, we introduce AdaGlimpse.\nIt uses Soft Actor-Critic, a reinforcement learning algorithm tailored for\nexploration tasks, to select glimpses of arbitrary position and scale. This\napproach enables our model to rapidly establish a general awareness of the\nenvironment before zooming in for detailed analysis. Experimental results\ndemonstrate that AdaGlimpse surpasses previous methods across various visual\ntasks while maintaining greater applicability in realistic AVE scenarios.\n","authors":["Adam Pardyl","MichaÅ Wronka","Maciej WoÅczyk","Kamil Adamczewski","Tomasz TrzciÅski","Bartosz ZieliÅski"],"pdf_url":"https://arxiv.org/pdf/2404.03482v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.08609v1","updated":"2024-07-11T15:45:57Z","published":"2024-07-11T15:45:57Z","title":"BiasPruner: Debiased Continual Learning for Medical Image Classification","summary":"  Continual Learning (CL) is crucial for enabling networks to dynamically adapt\nas they learn new tasks sequentially, accommodating new data and classes\nwithout catastrophic forgetting. Diverging from conventional perspectives on\nCL, our paper introduces a new perspective wherein forgetting could actually\nbenefit the sequential learning paradigm. Specifically, we present BiasPruner,\na CL framework that intentionally forgets spurious correlations in the training\ndata that could lead to shortcut learning. Utilizing a new bias score that\nmeasures the contribution of each unit in the network to learning spurious\nfeatures, BiasPruner prunes those units with the highest bias scores to form a\ndebiased subnetwork preserved for a given task. As BiasPruner learns a new\ntask, it constructs a new debiased subnetwork, potentially incorporating units\nfrom previous subnetworks, which improves adaptation and performance on the new\ntask. During inference, BiasPruner employs a simple task-agnostic approach to\nselect the best debiased subnetwork for predictions. We conduct experiments on\nthree medical datasets for skin lesion classification and chest X-Ray\nclassification and demonstrate that BiasPruner consistently outperforms SOTA CL\nmethods in terms of classification performance and fairness. Our code is\navailable here.\n","authors":["Nourhan Bayasi","Jamil Fayyad","Alceu Bissoto","Ghassan Hamarneh","Rafeef Garbi"],"pdf_url":"https://arxiv.org/pdf/2407.08609v1.pdf","comment":"Accepted for publication in MICCAI 2024(Early Accept)"},{"id":"http://arxiv.org/abs/2407.06581v2","updated":"2024-07-11T15:33:10Z","published":"2024-07-09T06:20:17Z","title":"Vision language models are blind","summary":"  Large language models with vision capabilities (VLMs), e.g., GPT-4o and\nGemini 1.5 Pro are powering countless image-text applications and scoring high\non many vision-understanding benchmarks. We propose BlindTest, a suite of 7\nvisual tasks absurdly easy to humans such as identifying (a) whether two\ncircles overlap; (b) whether two lines intersect; (c) which letter is being\ncircled in a word; and (d) counting the number of circles in a Olympic-like\nlogo. Surprisingly, four state-of-the-art VLMs are, on average, only 56.20%\naccurate on our benchmark, with \\newsonnet being the best (73.77% accuracy). On\nBlindTest, VLMs struggle with tasks that requires precise spatial information\nand counting (from 0 to 10), sometimes providing an impression of a person with\nmyopia seeing fine details as blurry and making educated guesses. Code is\navailable at: https://vlmsareblind.github.io/\n","authors":["Pooyan Rahmanzadehgervi","Logan Bolton","Mohammad Reza Taesiri","Anh Totti Nguyen"],"pdf_url":"https://arxiv.org/pdf/2407.06581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17520v2","updated":"2024-07-11T15:13:05Z","published":"2024-05-27T11:41:48Z","title":"Advancing Medical Image Segmentation with Mini-Net: A Lightweight\n  Solution Tailored for Efficient Segmentation of Medical Images","summary":"  Accurate segmentation of anatomical structures and abnormalities in medical\nimages is crucial for computer-aided diagnosis and analysis. While deep\nlearning techniques excel at this task, their computational demands pose\nchallenges. Additionally, some cutting-edge segmentation methods, though\neffective for general object segmentation, may not be optimised for medical\nimages. To address these issues, we propose Mini-Net, a lightweight\nsegmentation network specifically designed for medical images. With fewer than\n38,000 parameters, Mini-Net efficiently captures both high- and low-frequency\nfeatures, enabling real-time applications in various medical imaging scenarios.\nWe evaluate Mini-Net on various datasets, including DRIVE, STARE, ISIC-2016,\nISIC-2018, and MoNuSeg, demonstrating its robustness and good performance\ncompared to state-of-the-art methods.\n","authors":["Syed Javed","Tariq M. Khan","Abdul Qayyum","Arcot Sowmya","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2405.17520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08583v1","updated":"2024-07-11T15:08:11Z","published":"2024-07-11T15:08:11Z","title":"The Synergy between Data and Multi-Modal Large Language Models: A Survey\n  from Co-Development Perspective","summary":"  The rapid development of large language models (LLMs) has been witnessed in\nrecent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the\nmodality from text to a broader spectrum of domains, attracting widespread\nattention due to the broader range of application scenarios. As LLMs and MLLMs\nrely on vast amounts of model parameters and data to achieve emergent\ncapabilities, the importance of data is receiving increasingly widespread\nattention and recognition. Tracing and analyzing recent data-oriented works for\nMLLMs, we find that the development of models and data is not two separate\npaths but rather interconnected. On the one hand, vaster and higher-quality\ndata contribute to better performance of MLLMs, on the other hand, MLLMs can\nfacilitate the development of data. The co-development of multi-modal data and\nMLLMs requires a clear view of 1) at which development stage of MLLMs can\nspecific data-centric approaches be employed to enhance which capabilities, and\n2) by utilizing which capabilities and acting as which roles can models\ncontribute to multi-modal data. To promote the data-model co-development for\nMLLM community, we systematically review existing works related to MLLMs from\nthe data-model co-development perspective. A regularly maintained project\nassociated with this survey is accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.\n","authors":["Zhen Qin","Daoyuan Chen","Wenhao Zhang","Liuyi Yao","Yilun Huang","Bolin Ding","Yaliang Li","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2407.08583v1.pdf","comment":"Ongoing work. 31 pages. Related materials are continually maintained\n  and available at\n  https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md"},{"id":"http://arxiv.org/abs/2406.01042v2","updated":"2024-07-11T15:02:38Z","published":"2024-06-03T06:52:35Z","title":"Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using\n  Gaussian Splatting","summary":"  Gaussian Splatting (GS) has significantly elevated scene reconstruction\nefficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance\nFields (NeRF), particularly for dynamic scenes. However, current 4D NVS\nmethods, whether based on GS or NeRF, primarily rely on camera parameters\nprovided by COLMAP and even utilize sparse point clouds generated by COLMAP for\ninitialization, which lack accuracy as well are time-consuming. This sometimes\nresults in poor dynamic scene representation, especially in scenes with large\nobject movements, or extreme camera conditions e.g. small translations combined\nwith large rotations. Some studies simultaneously optimize the estimation of\ncamera parameters and scenes, supervised by additional information like depth,\noptical flow, etc. obtained from off-the-shelf models. Using this unverified\ninformation as ground truth can reduce robustness and accuracy, which does\nfrequently occur for long monocular videos (with e.g. > hundreds of frames). We\npropose a novel approach that learns a high-fidelity 4D GS scene representation\nwith self-calibration of camera parameters. It includes the extraction of 2D\npoint features that robustly represent 3D structure, and their use for\nsubsequent joint optimization of camera parameters and 3D structure towards\noverall 4D scene optimization. We demonstrate the accuracy and time efficiency\nof our method through extensive quantitative and qualitative experimental\nresults on several standard benchmarks. The results show significant\nimprovements over state-of-the-art methods for 4D novel view synthesis. The\nsource code will be released soon at https://github.com/fangli333/SC-4DGS.\n","authors":["Fang Li","Hao Zhang","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2406.01042v2.pdf","comment":"GitHub Page: https://github.com/fangli333/SC-4DGS"},{"id":"http://arxiv.org/abs/2407.08572v1","updated":"2024-07-11T14:59:31Z","published":"2024-07-11T14:59:31Z","title":"Boosting Adversarial Transferability for Skeleton-based Action\n  Recognition via Exploring the Model Posterior Space","summary":"  Skeletal motion plays a pivotal role in human activity recognition (HAR).\nRecently, attack methods have been proposed to identify the universal\nvulnerability of skeleton-based HAR(S-HAR). However, the research of\nadversarial transferability on S-HAR is largely missing. More importantly,\nexisting attacks all struggle in transfer across unknown S-HAR models. We\nobserved that the key reason is that the loss landscape of the action\nrecognizers is rugged and sharp. Given the established correlation in prior\nstudies~\\cite{qin2022boosting,wu2020towards} between loss landscape and\nadversarial transferability, we assume and empirically validate that smoothing\nthe loss landscape could potentially improve adversarial transferability on\nS-HAR. This is achieved by proposing a new post-train Dual Bayesian strategy,\nwhich can effectively explore the model posterior space for a collection of\nsurrogates without the need for re-training. Furthermore, to craft adversarial\nexamples along the motion manifold, we incorporate the attack gradient with\ninformation of the motion dynamics in a Bayesian manner. Evaluated on benchmark\ndatasets, e.g. HDM05 and NTU 60, the average transfer success rate can reach as\nhigh as 35.9\\% and 45.5\\% respectively. In comparison, current state-of-the-art\nskeletal attacks achieve only 3.6\\% and 9.8\\%. The high adversarial\ntransferability remains consistent across various surrogate, victim, and even\ndefense models. Through a comprehensive analysis of the results, we provide\ninsights on what surrogates are more likely to exhibit transferability, to shed\nlight on future research.\n","authors":["Yunfeng Diao","Baiqi Wu","Ruixuan Zhang","Xun Yang","Meng Wang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08569v1","updated":"2024-07-11T14:58:49Z","published":"2024-07-11T14:58:49Z","title":"Approaching Outside: Scaling Unsupervised 3D Object Detection from 2D\n  Scene","summary":"  The unsupervised 3D object detection is to accurately detect objects in\nunstructured environments with no explicit supervisory signals. This task,\ngiven sparse LiDAR point clouds, often results in compromised performance for\ndetecting distant or small objects due to the inherent sparsity and limited\nspatial resolution. In this paper, we are among the early attempts to integrate\nLiDAR data with 2D images for unsupervised 3D detection and introduce a new\nmethod, dubbed LiDAR-2D Self-paced Learning (LiSe). We argue that RGB images\nserve as a valuable complement to LiDAR data, offering precise 2D localization\ncues, particularly when scarce LiDAR points are available for certain objects.\nConsidering the unique characteristics of both modalities, our framework\ndevises a self-paced learning pipeline that incorporates adaptive sampling and\nweak model aggregation strategies. The adaptive sampling strategy dynamically\ntunes the distribution of pseudo labels during training, countering the\ntendency of models to overfit easily detected samples, such as nearby and\nlarge-sized objects. By doing so, it ensures a balanced learning trajectory\nacross varying object scales and distances. The weak model aggregation\ncomponent consolidates the strengths of models trained under different pseudo\nlabel distributions, culminating in a robust and powerful final model.\nExperimental evaluations validate the efficacy of our proposed LiSe method,\nmanifesting significant improvements of +7.1% AP$_{BEV}$ and +3.4% AP$_{3D}$ on\nnuScenes, and +8.3% AP$_{BEV}$ and +7.4% AP$_{3D}$ on Lyft compared to existing\ntechniques.\n","authors":["Ruiyang Zhang","Hu Zhang","Hang Yu","Zhedong Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.08569v1.pdf","comment":"Accepted by ECCV'24, 18 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2407.08567v1","updated":"2024-07-11T14:57:27Z","published":"2024-07-11T14:57:27Z","title":"Adaptive Parametric Activation","summary":"  The activation function plays a crucial role in model optimisation, yet the\noptimal choice remains unclear. For example, the Sigmoid activation is the\nde-facto activation in balanced classification tasks, however, in imbalanced\nclassification, it proves inappropriate due to bias towards frequent classes.\nIn this work, we delve deeper in this phenomenon by performing a comprehensive\nstatistical analysis in the classification and intermediate layers of both\nbalanced and imbalanced networks and we empirically show that aligning the\nactivation function with the data distribution, enhances the performance in\nboth balanced and imbalanced tasks. To this end, we propose the Adaptive\nParametric Activation (APA) function, a novel and versatile activation function\nthat unifies most common activation functions under a single formula. APA can\nbe applied in both intermediate layers and attention layers, significantly\noutperforming the state-of-the-art on several imbalanced benchmarks such as\nImageNet-LT, iNaturalist2018, Places-LT, CIFAR100-LT and LVIS and balanced\nbenchmarks such as ImageNet1K, COCO and V3DET. The code is available at\nhttps://github.com/kostas1515/AGLU.\n","authors":["Konstantinos Panagiotis Alexandridis","Jiankang Deng","Anh Nguyen","Shan Luo"],"pdf_url":"https://arxiv.org/pdf/2407.08567v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2310.01324v2","updated":"2024-07-11T14:53:59Z","published":"2023-10-02T16:41:20Z","title":"ZeroI2V: Zero-Cost Adaptation of Pre-trained Transformers from Image to\n  Video","summary":"  Adapting image models to the video domain has emerged as an efficient\nparadigm for solving video recognition tasks. Due to the huge number of\nparameters and effective transferability of image models, performing full\nfine-tuning is less efficient and even unnecessary. Thus, recent research is\nshifting its focus toward parameter-efficient image-to-video adaptation.\nHowever, these adaptation strategies inevitably introduce extra computational\ncosts to deal with the domain gap and temporal modeling in videos. In this\npaper, we present a new adaptation paradigm (ZeroI2V) to transfer the image\ntransformers to video recognition tasks (i.e., introduce zero extra cost to the\noriginal models during inference). To achieve this goal, we present two core\ndesigns. First, to capture the dynamics in videos and reduce the difficulty of\nimage-to-video adaptation, we exploit the flexibility of self-attention and\nintroduce spatial-temporal dual-headed attention (STDHA). This approach\nefficiently endows the image transformers with temporal modeling capability at\nzero extra parameters and computation. Second, to handle the domain gap between\nimages and videos, we propose a linear adaption strategy that utilizes\nlightweight densely placed linear adapters to fully transfer the frozen image\nmodels to video recognition. Thanks to the customized linear design, all newly\nadded adapters could be easily merged with the original modules through\nstructural reparameterization after training, enabling zero extra cost during\ninference. Extensive experiments on representative fully-supervised and\nfew-shot video recognition benchmarks showcase that ZeroI2V can match or even\noutperform previous state-of-the-art methods while enjoying superior parameter\nand inference efficiency.\n","authors":["Xinhao Li","Yuhan Zhu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2310.01324v2.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.08561v1","updated":"2024-07-11T14:51:18Z","published":"2024-07-11T14:51:18Z","title":"MapLocNet: Coarse-to-Fine Feature Registration for Visual\n  Re-Localization in Navigation Maps","summary":"  Robust localization is the cornerstone of autonomous driving, especially in\nchallenging urban environments where GPS signals suffer from multipath errors.\nTraditional localization approaches rely on high-definition (HD) maps, which\nconsist of precisely annotated landmarks. However, building HD map is expensive\nand challenging to scale up. Given these limitations, leveraging navigation\nmaps has emerged as a promising low-cost alternative for localization. Current\napproaches based on navigation maps can achieve highly accurate localization,\nbut their complex matching strategies lead to unacceptable inference latency\nthat fails to meet the real-time demands. To address these limitations, we\npropose a novel transformer-based neural re-localization method. Inspired by\nimage registration, our approach performs a coarse-to-fine neural feature\nregistration between navigation map and visual bird's-eye view features. Our\nmethod significantly outperforms the current state-of-the-art OrienterNet on\nboth the nuScenes and Argoverse datasets, which is nearly 10%/20% localization\naccuracy and 30/16 FPS improvement on single-view and surround-view input\nsettings, separately. We highlight that our research presents an HD-map-free\nlocalization method for autonomous driving, offering cost-effective, reliable,\nand scalable performance in challenging driving environments.\n","authors":["Hang Wu","Zhenghao Zhang","Siyuan Lin","Xiangru Mu","Qiang Zhao","Ming Yang","Tong Qin"],"pdf_url":"https://arxiv.org/pdf/2407.08561v1.pdf","comment":"IROS 2024 (Oral)"},{"id":"http://arxiv.org/abs/2407.08555v1","updated":"2024-07-11T14:39:54Z","published":"2024-07-11T14:39:54Z","title":"SLoRD: Structural Low-Rank Descriptors for Shape Consistency in\n  Vertebrae Segmentation","summary":"  Automatic and precise segmentation of vertebrae from CT images is crucial for\nvarious clinical applications. However, due to a lack of explicit and strict\nconstraints, existing methods especially for single-stage methods, still suffer\nfrom the challenge of intra-vertebrae segmentation inconsistency, which refers\nto multiple label predictions inside a singular vertebra. For multi-stage\nmethods, vertebrae detection serving as the first step, is affected by the\npathology and mental implants. Thus, incorrect detections cause biased patches\nbefore segmentation, then lead to inconsistent labeling and segmentation. In\nour work, motivated by the perspective of instance segmentation, we try to\nlabel individual and complete binary masks to address this limitation.\nSpecifically, a contour-based network is proposed based on Structural Low-Rank\nDescriptors for shape consistency, termed SLoRD. These contour descriptors are\nacquired in a data-driven manner in advance. For a more precise representation\nof contour descriptors, we adopt the spherical coordinate system and devise the\nspherical centroid. Besides, the contour loss is designed to impose explicit\nconsistency constraints, facilitating regressed contour points close to\nvertebral boundaries. Quantitative and qualitative evaluations on VerSe 2019\ndemonstrate the superior performance of our framework over other single-stage\nand multi-stage state-of-the-art (SOTA) methods.\n","authors":["Xin You","Yixin Lou","Minghui Zhang","Chuyan Zhang","Jie Yang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2407.08555v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.08546v1","updated":"2024-07-11T14:30:49Z","published":"2024-07-11T14:30:49Z","title":"Quantitative Evaluation of the Saliency Map for Alzheimer's Disease\n  Classifier with Anatomical Segmentation","summary":"  Saliency maps have been widely used to interpret deep learning classifiers\nfor Alzheimer's disease (AD). However, since AD is heterogeneous and has\nmultiple subtypes, the pathological mechanism of AD remains not fully\nunderstood and may vary from patient to patient. Due to the lack of such\nunderstanding, it is difficult to comprehensively and effectively assess the\nsaliency map of AD classifier. In this paper, we utilize the anatomical\nsegmentation to allocate saliency values into different brain regions. By\nplotting the distributions of saliency maps corresponding to AD and NC (Normal\nControl), we can gain a comprehensive view of the model's decisions process. In\norder to leverage the fact that the brain volume shrinkage happens in AD\npatients during disease progression, we define a new evaluation metric, brain\nvolume change score (VCS), by computing the average Pearson correlation of the\nbrain volume changes and the saliency values of a model in different brain\nregions for each patient. Thus, the VCS metric can help us gain some knowledge\nof how saliency maps resulting from different models relate to the changes of\nthe volumes across different regions in the whole brain. We trained candidate\nmodels on the ADNI dataset and tested on three different datasets. Our results\nindicate: (i) models with higher VCSs tend to demonstrate saliency maps with\nmore details relevant to the AD pathology, (ii) using gradient-based\nadversarial training strategies such as FGSM and stochastic masking can improve\nthe VCSs of the models.\n","authors":["Yihan Zhang","Xuanshuo Zhang","Wei Wu","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08545v1","updated":"2024-07-11T14:30:46Z","published":"2024-07-11T14:30:46Z","title":"OMR-NET: a two-stage octave multi-scale residual network for screen\n  content image compression","summary":"  Screen content (SC) differs from natural scene (NS) with unique\ncharacteristics such as noise-free, repetitive patterns, and high contrast.\nAiming at addressing the inadequacies of current learned image compression\n(LIC) methods for SC, we propose an improved two-stage octave convolutional\nresidual blocks (IToRB) for high and low-frequency feature extraction and a\ncascaded two-stage multi-scale residual blocks (CTMSRB) for improved\nmulti-scale learning and nonlinearity in SC. Additionally, we employ a\nwindow-based attention module (WAM) to capture pixel correlations, especially\nfor high contrast regions in the image. We also construct a diverse SC image\ncompression dataset (SDU-SCICD2K) for training, including text, charts,\ngraphics, animation, movie, game and mixture of SC images and NS images.\nExperimental results show our method, more suited for SC than NS data,\noutperforms existing LIC methods in rate-distortion performance on SC images.\nThe code is publicly available at https://github.com/SunshineSki/OMR Net.git.\n","authors":["Shiqi Jiang","Ting Ren","Congrui Fu","Shuai Li","Hui Yuan"],"pdf_url":"https://arxiv.org/pdf/2407.08545v1.pdf","comment":"7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.08536v1","updated":"2024-07-11T14:23:08Z","published":"2024-07-11T14:23:08Z","title":"Exemplar-free Continual Representation Learning via Learnable Drift\n  Compensation","summary":"  Exemplar-free class-incremental learning using a backbone trained from\nscratch and starting from a small first task presents a significant challenge\nfor continual representation learning. Prototype-based approaches, when\ncontinually updated, face the critical issue of semantic drift due to which the\nold class prototypes drift to different positions in the new feature space.\nThrough an analysis of prototype-based continual learning, we show that\nforgetting is not due to diminished discriminative power of the feature\nextractor, and can potentially be corrected by drift compensation. To address\nthis, we propose Learnable Drift Compensation (LDC), which can effectively\nmitigate drift in any moving backbone, whether supervised or unsupervised. LDC\nis fast and straightforward to integrate on top of existing continual learning\napproaches. Furthermore, we showcase how LDC can be applied in combination with\nself-supervised CL methods, resulting in the first exemplar-free\nsemi-supervised continual learning approach. We achieve state-of-the-art\nperformance in both supervised and semi-supervised settings across multiple\ndatasets. Code is available at \\url{https://github.com/alviur/ldc}.\n","authors":["Alex Gomez-Villa","Dipam Goswami","Kai Wang","Andrew D. Bagdanov","Bartlomiej Twardowski","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2407.08536v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2308.15536v3","updated":"2024-07-11T14:19:11Z","published":"2023-08-29T18:00:22Z","title":"DebSDF: Delving into the Details and Bias of Neural Indoor Scene\n  Reconstruction","summary":"  In recent years, the neural implicit surface has emerged as a powerful\nrepresentation for multi-view surface reconstruction due to its simplicity and\nstate-of-the-art performance. However, reconstructing smooth and detailed\nsurfaces in indoor scenes from multi-view images presents unique challenges.\nIndoor scenes typically contain large texture-less regions, making the\nphotometric loss unreliable for optimizing the implicit surface. Previous work\nutilizes monocular geometry priors to improve the reconstruction in indoor\nscenes. However, monocular priors often contain substantial errors in thin\nstructure regions due to domain gaps and the inherent inconsistencies when\nderived independently from different views. This paper presents \\textbf{DebSDF}\nto address these challenges, focusing on the utilization of uncertainty in\nmonocular priors and the bias in SDF-based volume rendering. We propose an\nuncertainty modeling technique that associates larger uncertainties with larger\nerrors in the monocular priors. High-uncertainty priors are then excluded from\noptimization to prevent bias. This uncertainty measure also informs an\nimportance-guided ray sampling and adaptive smoothness regularization,\nenhancing the learning of fine structures. We further introduce a bias-aware\nsigned distance function to density transformation that takes into account the\ncurvature and the angle between the view direction and the SDF normals to\nreconstruct fine details better. Our approach has been validated through\nextensive experiments on several challenging datasets, demonstrating improved\nqualitative and quantitative results in reconstructing thin structures in\nindoor scenes, thereby outperforming previous work.\n","authors":["Yuting Xiao","Jingwei Xu","Zehao Yu","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2308.15536v3.pdf","comment":"14 pages, accepted by TPAMI"},{"id":"http://arxiv.org/abs/2306.03492v5","updated":"2024-07-11T14:17:34Z","published":"2023-06-06T08:19:30Z","title":"Industrial Anomaly Detection and Localization Using Weakly-Supervised\n  Residual Transformers","summary":"  Recent advancements in industrial Anomaly Detection (AD) have shown that\nincorporating a few anomalous samples during training can significantly boost\naccuracy. However, this performance improvement comes at a high cost: extensive\nannotation efforts, which are often impractical in real-world applications. In\nthis work, we propose a novel framework called \"Weakly-supervised RESidual\nTransformer\" (WeakREST), which aims to achieve high AD accuracy while\nminimizing the need for extensive annotations. First, we reformulate the\npixel-wise anomaly localization task into a block-wise classification problem.\nBy shifting the focus to block-wise level, we can drastically reduce the amount\nof required annotations without compromising on the accuracy of anomaly\ndetection Secondly, we design a residual-based transformer model, termed\n\"Positional Fast Anomaly Residuals\" (PosFAR), to classify the image blocks in\nreal time. We further propose to label the anomalous regions using only\nbounding boxes or image tags as weaker labels, leading to a semi-supervised\nlearning setting. On the benchmark dataset MVTec-AD, our proposed WeakREST\nframework achieves a remarkable Average Precision (AP) of 83.0%, significantly\noutperforming the previous best result of 75.8% in the unsupervised setting. In\nthe supervised AD setting, WeakREST further improves performance, attaining an\nAP of 87.6% compared to the previous best of 78.6%. Notably, even when\nutilizing weaker labels based on bounding boxes, WeakREST surpasses recent\nleading methods that rely on pixel-wise supervision, achieving an AP of 87.1%\nagainst the prior best of 78.6% on MVTec-AD. This precision advantage is also\nconsistently observed on other well-known AD datasets, such as BTAD and KSDD2.\n","authors":["Hanxi Li","Jingqi Wu","Lin Yuanbo Wu","Hao Chen","Deyin Liu","Mingwen Wang","Peng Wang"],"pdf_url":"https://arxiv.org/pdf/2306.03492v5.pdf","comment":"14 pages,7 figures"},{"id":"http://arxiv.org/abs/2407.08528v1","updated":"2024-07-11T14:16:41Z","published":"2024-07-11T14:16:41Z","title":"Enhancing octree-based context models for point cloud geometry\n  compression with attention-based child node number prediction","summary":"  In point cloud geometry compression, most octreebased context models use the\ncross-entropy between the onehot encoding of node occupancy and the probability\ndistribution predicted by the context model as the loss. This approach converts\nthe problem of predicting the number (a regression problem) and the position (a\nclassification problem) of occupied child nodes into a 255-dimensional\nclassification problem. As a result, it fails to accurately measure the\ndifference between the one-hot encoding and the predicted probability\ndistribution. We first analyze why the cross-entropy loss function fails to\naccurately measure the difference between the one-hot encoding and the\npredicted probability distribution. Then, we propose an attention-based child\nnode number prediction (ACNP) module to enhance the context models. The\nproposed module can predict the number of occupied child nodes and map it into\nan 8- dimensional vector to assist the context model in predicting the\nprobability distribution of the occupancy of the current node for efficient\nentropy coding. Experimental results demonstrate that the proposed module\nenhances the coding efficiency of octree-based context models.\n","authors":["Chang Sun","Hui Yuan","Xiaolong Mao","Xin Lu","Raouf Hamzaoui"],"pdf_url":"https://arxiv.org/pdf/2407.08528v1.pdf","comment":"2 figures and 2 tables"},{"id":"http://arxiv.org/abs/2407.08526v1","updated":"2024-07-11T14:15:48Z","published":"2024-07-11T14:15:48Z","title":"BLOS-BEV: Navigation Map Enhanced Lane Segmentation Network, Beyond Line\n  of Sight","summary":"  Bird's-eye-view (BEV) representation is crucial for the perception function\nin autonomous driving tasks. It is difficult to balance the accuracy,\nefficiency and range of BEV representation. The existing works are restricted\nto a limited perception range within 50 meters. Extending the BEV\nrepresentation range can greatly benefit downstream tasks such as topology\nreasoning, scene understanding, and planning by offering more comprehensive\ninformation and reaction time. The Standard-Definition (SD) navigation maps can\nprovide a lightweight representation of road structure topology, characterized\nby ease of acquisition and low maintenance costs. An intuitive idea is to\ncombine the close-range visual information from onboard cameras with the beyond\nline-of-sight (BLOS) environmental priors from SD maps to realize expanded\nperceptual capabilities. In this paper, we propose BLOS-BEV, a novel BEV\nsegmentation model that incorporates SD maps for accurate beyond line-of-sight\nperception, up to 200m. Our approach is applicable to common BEV architectures\nand can achieve excellent results by incorporating information derived from SD\nmaps. We explore various feature fusion schemes to effectively integrate the\nvisual BEV representations and semantic features from the SD map, aiming to\nleverage the complementary information from both sources optimally. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\nin BEV segmentation on nuScenes and Argoverse benchmark. Through multi-modal\ninputs, BEV segmentation is significantly enhanced at close ranges below 50m,\nwhile also demonstrating superior performance in long-range scenarios,\nsurpassing other methods by over 20% mIoU at distances ranging from 50-200m.\n","authors":["Hang Wu","Zhenghao Zhang","Siyuan Lin","Tong Qin","Jin Pan","Qiang Zhao","Chunjing Xu","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2407.08526v1.pdf","comment":"IEEE IV 2024"},{"id":"http://arxiv.org/abs/2407.03961v2","updated":"2024-07-11T14:14:22Z","published":"2024-07-04T14:28:52Z","title":"Leveraging Latent Diffusion Models for Training-Free In-Distribution\n  Data Augmentation for Surface Defect Detection","summary":"  Defect detection is the task of identifying defects in production samples.\nUsually, defect detection classifiers are trained on ground-truth data formed\nby normal samples (negative data) and samples with defects (positive data),\nwhere the latter are consistently fewer than normal samples. State-of-the-art\ndata augmentation procedures add synthetic defect data by superimposing\nartifacts to normal samples to mitigate problems related to unbalanced training\ndata. These techniques often produce out-of-distribution images, resulting in\nsystems that learn what is not a normal sample but cannot accurately identify\nwhat a defect looks like. In this work, we introduce DIAG, a training-free\nDiffusion-based In-distribution Anomaly Generation pipeline for data\naugmentation. Unlike conventional image generation techniques, we implement a\nhuman-in-the-loop pipeline, where domain experts provide multimodal guidance to\nthe model through text descriptions and region localization of the possible\nanomalies. This strategic shift enhances the interpretability of results and\nfosters a more robust human feedback loop, facilitating iterative improvements\nof the generated outputs. Remarkably, our approach operates in a zero-shot\nmanner, avoiding time-consuming fine-tuning procedures while achieving superior\nperformance. We demonstrate the efficacy and versatility of DIAG with respect\nto state-of-the-art data augmentation approaches on the challenging KSDD2\ndataset, with an improvement in AP of approximately 18% when positive samples\nare available and 28% when they are missing. The source code is available at\nhttps://github.com/intelligolabs/DIAG.\n","authors":["Federico Girella","Ziyue Liu","Franco Fummi","Francesco Setti","Marco Cristani","Luigi Capogrosso"],"pdf_url":"https://arxiv.org/pdf/2407.03961v2.pdf","comment":"Accepted at the 21st International Conference on Content-Based\n  Multimedia Indexing (CBMI 2024)"},{"id":"http://arxiv.org/abs/2407.08521v1","updated":"2024-07-11T14:09:42Z","published":"2024-07-11T14:09:42Z","title":"Emergent Visual-Semantic Hierarchies in Image-Text Representations","summary":"  While recent vision-and-language models (VLMs) like CLIP are a powerful tool\nfor analyzing text and images in a shared semantic space, they do not\nexplicitly model the hierarchical nature of the set of texts which may describe\nan image. Conversely, existing multimodal hierarchical representation learning\nmethods require costly training from scratch, failing to leverage the knowledge\nencoded by state-of-the-art multimodal foundation models. In this work, we\nstudy the knowledge of existing foundation models, finding that they exhibit\nemergent understanding of visual-semantic hierarchies despite not being\ndirectly trained for this purpose. We propose the Radial Embedding (RE)\nframework for probing and optimizing hierarchical understanding, and contribute\nthe HierarCaps dataset, a benchmark facilitating the study of hierarchical\nknowledge in image--text representations, constructed automatically via large\nlanguage models. Our results show that foundation VLMs exhibit zero-shot\nhierarchical understanding, surpassing the performance of prior models\nexplicitly designed for this purpose. Furthermore, we show that foundation\nmodels may be better aligned to hierarchical reasoning via a text-only\nfine-tuning phase, while retaining pretraining knowledge.\n","authors":["Morris Alper","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2407.08521v1.pdf","comment":"Accepted to ECCV 2024. Project page: https://hierarcaps.github.io/"},{"id":"http://arxiv.org/abs/2407.08520v1","updated":"2024-07-11T14:08:37Z","published":"2024-07-11T14:08:37Z","title":"Enhancing context models for point cloud geometry compression with\n  context feature residuals and multi-loss","summary":"  In point cloud geometry compression, context models usually use the one-hot\nencoding of node occupancy as the label, and the cross-entropy between the\none-hot encoding and the probability distribution predicted by the context\nmodel as the loss function. However, this approach has two main weaknesses.\nFirst, the differences between contexts of different nodes are not significant,\nmaking it difficult for the context model to accurately predict the probability\ndistribution of node occupancy. Second, as the one-hot encoding is not the\nactual probability distribution of node occupancy, the cross-entropy loss\nfunction is inaccurate. To address these problems, we propose a general\nstructure that can enhance existing context models. We introduce the context\nfeature residuals into the context model to amplify the differences between\ncontexts. We also add a multi-layer perception branch, that uses the mean\nsquared error between its output and node occupancy as a loss function to\nprovide accurate gradients in backpropagation. We validate our method by\nshowing that it can improve the performance of an octree-based model\n(OctAttention) and a voxel-based model (VoxelDNN) on the object point cloud\ndatasets MPEG 8i and MVUB, as well as the LiDAR point cloud dataset\nSemanticKITTI.\n","authors":["Chang Sun","Hui Yuan","Shuai Li","Xin Lu","Raouf Hamzaoui"],"pdf_url":"https://arxiv.org/pdf/2407.08520v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.08517v1","updated":"2024-07-11T14:01:57Z","published":"2024-07-11T14:01:57Z","title":"Generalized Low-Rank Matrix Completion Model with Overlapping Group\n  Error Representation","summary":"  The low-rank matrix completion (LRMC) technology has achieved remarkable\nresults in low-level visual tasks. There is an underlying assumption that the\nreal-world matrix data is low-rank in LRMC. However, the real matrix data does\nnot satisfy the strict low-rank property, which undoubtedly present serious\nchallenges for the above-mentioned matrix recovery methods. Fortunately, there\nare feasible schemes that devise appropriate and effective priori\nrepresentations for describing the intrinsic information of real data. In this\npaper, we firstly model the matrix data ${\\bf{Y}}$ as the sum of a low-rank\napproximation component $\\bf{X}$ and an approximation error component\n$\\cal{E}$. This finer-grained data decomposition architecture enables each\ncomponent of information to be portrayed more precisely. Further, we design an\noverlapping group error representation (OGER) function to characterize the\nabove error structure and propose a generalized low-rank matrix completion\nmodel based on OGER. Specifically, the low-rank component describes the global\nstructure information of matrix data, while the OGER component not only\ncompensates for the approximation error between the low-rank component and the\nreal data but also better captures the local block sparsity information of\nmatrix data. Finally, we develop an alternating direction method of multipliers\n(ADMM) that integrates the majorization-minimization (MM) algorithm, which\nenables the efficient solution of the proposed model. And we analyze the\nconvergence of the algorithm in detail both theoretically and experimentally.\nIn addition, the results of numerical experiments demonstrate that the proposed\nmodel outperforms existing competing models in performance.\n","authors":["Wenjing Lu","Zhuang Fang","Liang Wu","Liming Tang","Hanxin Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08515v1","updated":"2024-07-11T14:00:14Z","published":"2024-07-11T14:00:14Z","title":"15M Multimodal Facial Image-Text Dataset","summary":"  Currently, image-text-driven multi-modal deep learning models have\ndemonstrated their outstanding potential in many fields. In practice, tasks\ncentered around facial images have broad application prospects. This paper\npresents \\textbf{FaceCaption-15M}, a large-scale, diverse, and high-quality\ndataset of facial images accompanied by their natural language descriptions\n(facial image-to-text). This dataset aims to facilitate a study on\nface-centered tasks. FaceCaption-15M comprises over 15 million pairs of facial\nimages and their corresponding natural language descriptions of facial\nfeatures, making it the largest facial image-caption dataset to date. We\nconducted a comprehensive analysis of image quality, text naturalness, text\ncomplexity, and text-image relevance to demonstrate the superiority of\nFaceCaption-15M. To validate the effectiveness of FaceCaption-15M, we first\ntrained a facial language-image pre-training model (FLIP, similar to CLIP) to\nalign facial image with its corresponding captions in feature space.\nSubsequently, using both image and text encoders and fine-tuning only the\nlinear layer, our FLIP-based models achieved state-of-the-art results on two\nchallenging face-centered tasks. The purpose is to promote research in the\nfield of face-related tasks through the availability of the proposed\nFaceCaption-15M dataset. All data, codes, and models are publicly available.\nhttps://huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M\n","authors":["Dawei Dai","YuTang Li","YingGe Liu","Mingming Jia","Zhang YuanHui","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08515v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.08514v1","updated":"2024-07-11T13:58:09Z","published":"2024-07-11T13:58:09Z","title":"Rethinking the Threat and Accessibility of Adversarial Attacks against\n  Face Recognition Systems","summary":"  Face recognition pipelines have been widely deployed in various\nmission-critical systems in trust, equitable and responsible AI applications.\nHowever, the emergence of adversarial attacks has threatened the security of\nthe entire recognition pipeline. Despite the sheer number of attack methods\nproposed for crafting adversarial examples in both digital and physical forms,\nit is never an easy task to assess the real threat level of different attacks\nand obtain useful insight into the key risks confronted by face recognition\nsystems. Traditional attacks view imperceptibility as the most important\nmeasurement to keep perturbations stealthy, while we suspect that industry\nprofessionals may possess a different opinion. In this paper, we delve into\nmeasuring the threat brought about by adversarial attacks from the perspectives\nof the industry and the applications of face recognition. In contrast to widely\nstudied sophisticated attacks in the field, we propose an effective yet\neasy-to-launch physical adversarial attack, named AdvColor, against black-box\nface recognition pipelines in the physical world. AdvColor fools models in the\nrecognition pipeline via directly supplying printed photos of human faces to\nthe system under adversarial illuminations. Experimental results show that\nphysical AdvColor examples can achieve a fooling rate of more than 96% against\nthe anti-spoofing model and an overall attack success rate of 88% against the\nface recognition pipeline. We also conduct a survey on the threats of\nprevailing adversarial attacks, including AdvColor, to understand the gap\nbetween the machine-measured and human-assessed threat levels of different\nforms of adversarial attacks. The survey results surprisingly indicate that,\ncompared to deliberately launched imperceptible attacks, perceptible but\naccessible attacks pose more lethal threats to real-world commercial systems of\nface recognition.\n","authors":["Yuxin Cao","Yumeng Zhu","Derui Wang","Sheng Wen","Minhui Xue","Jin Lu","Hao Ge"],"pdf_url":"https://arxiv.org/pdf/2407.08514v1.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.08513v1","updated":"2024-07-11T13:55:20Z","published":"2024-07-11T13:55:20Z","title":"Fine-Tuning Stable Diffusion XL for Stylistic Icon Generation: A\n  Comparison of Caption Size","summary":"  In this paper, we show different fine-tuning methods for Stable Diffusion XL;\nthis includes inference steps, and caption customization for each image to\nalign with generating images in the style of a commercial 2D icon training set.\nWe also show how important it is to properly define what \"high-quality\" really\nis especially for a commercial-use environment. As generative AI models\ncontinue to gain widespread acceptance and usage, there emerge many different\nways to optimize and evaluate them for various applications. Specifically\ntext-to-image models, such as Stable Diffusion XL and DALL-E 3 require distinct\nevaluation practices to effectively generate high-quality icons according to a\nspecific style. Although some images that are generated based on a certain\nstyle may have a lower FID score (better), we show how this is not absolute in\nand of itself even for rasterized icons. While FID scores reflect the\nsimilarity of generated images to the overall training set, CLIP scores measure\nthe alignment between generated images and their textual descriptions. We show\nhow FID scores miss significant aspects, such as the minority of pixel\ndifferences that matter most in an icon, while CLIP scores result in misjudging\nthe quality of icons. The CLIP model's understanding of \"similarity\" is shaped\nby its own training data; which does not account for feature variation in our\nstyle of choice. Our findings highlight the need for specialized evaluation\nmetrics and fine-tuning approaches when generating high-quality commercial\nicons, potentially leading to more effective and tailored applications of\ntext-to-image models in professional design contexts.\n","authors":["Youssef Sultan","Jiangqin Ma","Yu-Ying Liao"],"pdf_url":"https://arxiv.org/pdf/2407.08513v1.pdf","comment":"11 pages, 22 figures"},{"id":"http://arxiv.org/abs/2407.08509v1","updated":"2024-07-11T13:46:47Z","published":"2024-07-11T13:46:47Z","title":"Haar Nuclear Norms with Applications to Remote Sensing Imagery\n  Restoration","summary":"  Remote sensing image restoration aims to reconstruct missing or corrupted\nareas within images. To date, low-rank based models have garnered significant\ninterest in this field. This paper proposes a novel low-rank regularization\nterm, named the Haar nuclear norm (HNN), for efficient and effective remote\nsensing image restoration.\n  It leverages the low-rank properties of wavelet coefficients derived from the\n2-D frontal slice-wise Haar discrete wavelet transform, effectively modeling\nthe low-rank prior for separated coarse-grained structure and fine-grained\ntextures in the image. Experimental evaluations conducted on hyperspectral\nimage inpainting, multi-temporal image cloud removal, and hyperspectral image\ndenoising have revealed the HNN's potential. Typically, HNN achieves a\nperformance improvement of 1-4 dB and a speedup of 10-28x compared to some\nstate-of-the-art methods (e.g., tensor correlated total variation, and\nfully-connected tensor network) for inpainting tasks.\n","authors":["Shuang Xu","Chang Yu","Jiangjun Peng","Xiangyong Cao"],"pdf_url":"https://arxiv.org/pdf/2407.08509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08507v1","updated":"2024-07-11T13:45:50Z","published":"2024-07-11T13:45:50Z","title":"Bootstrapping Vision-language Models for Self-supervised Remote\n  Physiological Measurement","summary":"  Facial video-based remote physiological measurement is a promising research\narea for detecting human vital signs (e.g., heart rate, respiration frequency)\nin a non-contact way. Conventional approaches are mostly supervised learning,\nrequiring extensive collections of facial videos and synchronously recorded\nphotoplethysmography (PPG) signals. To tackle it, self-supervised learning has\nrecently gained attentions; due to the lack of ground truth PPG signals, its\nperformance is however limited. In this paper, we propose a novel\nself-supervised framework that successfully integrates the popular\nvision-language models (VLMs) into the remote physiological measurement task.\nGiven a facial video, we first augment its positive and negative video samples\nwith varying rPPG signal frequencies. Next, we introduce a frequency-oriented\nvision-text pair generation method by carefully creating contrastive\nspatio-temporal maps from positive and negative samples and designing proper\ntext prompts to describe their relative ratios of signal frequencies. A\npre-trained VLM is employed to extract features for these formed vision-text\npairs and estimate rPPG signals thereafter. We develop a series of generative\nand contrastive learning mechanisms to optimize the VLM, including the\ntext-guided visual map reconstruction task, the vision-text contrastive\nlearning task, and the frequency contrastive and ranking task. Overall, our\nmethod for the first time adapts VLMs to digest and align the frequency-related\nknowledge in vision and text modalities. Extensive experiments on four\nbenchmark datasets demonstrate that it significantly outperforms state of the\nart self-supervised methods.\n","authors":["Zijie Yue","Miaojing Shi","Hanli Wang","Shuai Ding","Qijun Chen","Shanlin Yang"],"pdf_url":"https://arxiv.org/pdf/2407.08507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08498v1","updated":"2024-07-11T13:34:37Z","published":"2024-07-11T13:34:37Z","title":"ERD: Exponential Retinex decomposition based on weak space and hybrid\n  nonconvex regularization and its denoising application","summary":"  The Retinex theory models the image as a product of illumination and\nreflection components, which has received extensive attention and is widely\nused in image enhancement, segmentation and color restoration. However, it has\nbeen rarely used in additive noise removal due to the inclusion of both\nmultiplication and addition operations in the Retinex noisy image modeling. In\nthis paper, we propose an exponential Retinex decomposition model based on\nhybrid non-convex regularization and weak space oscillation-modeling for image\ndenoising. The proposed model utilizes non-convex first-order total variation\n(TV) and non-convex second-order TV to regularize the reflection component and\nthe illumination component, respectively, and employs weak $H^{-1}$ norm to\nmeasure the residual component. By utilizing different regularizers, the\nproposed model effectively decomposes the image into reflection, illumination,\nand noise components. An alternating direction multipliers method (ADMM)\ncombined with the Majorize-Minimization (MM) algorithm is developed to solve\nthe proposed model. Furthermore, we provide a detailed proof of the convergence\nproperty of the algorithm. Numerical experiments validate both the proposed\nmodel and algorithm. Compared with several state-of-the-art denoising models,\nthe proposed model exhibits superior performance in terms of peak\nsignal-to-noise ratio (PSNR) and mean structural similarity (MSSIM).\n","authors":["Wenjing Lu","Liang Wu","Liming Tang","Zhuang Fang"],"pdf_url":"https://arxiv.org/pdf/2407.08498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08489v1","updated":"2024-07-11T13:23:04Z","published":"2024-07-11T13:23:04Z","title":"Projecting Points to Axes: Oriented Object Detection via Point-Axis\n  Representation","summary":"  This paper introduces the point-axis representation for oriented object\ndetection, emphasizing its flexibility and geometrically intuitive nature with\ntwo key components: points and axes. 1) Points delineate the spatial extent and\ncontours of objects, providing detailed shape descriptions. 2) Axes define the\nprimary directionalities of objects, providing essential orientation cues\ncrucial for precise detection. The point-axis representation decouples location\nand rotation, addressing the loss discontinuity issues commonly encountered in\ntraditional bounding box-based approaches. For effective optimization without\nintroducing additional annotations, we propose the max-projection loss to\nsupervise point set learning and the cross-axis loss for robust axis\nrepresentation learning. Further, leveraging this representation, we present\nthe Oriented DETR model, seamlessly integrating the DETR framework for precise\npoint-axis prediction and end-to-end detection. Experimental results\ndemonstrate significant performance improvements in oriented object detection\ntasks.\n","authors":["Zeyang Zhao","Qilong Xue","Yuhang He","Yifan Bai","Xing Wei","Yihong Gong"],"pdf_url":"https://arxiv.org/pdf/2407.08489v1.pdf","comment":"19 pages,7 figures,accpeted by ECCV24!"},{"id":"http://arxiv.org/abs/2407.08484v1","updated":"2024-07-11T13:16:02Z","published":"2024-07-11T13:16:02Z","title":"Learning Localization of Body and Finger Animation Skeleton Joints on\n  Three-Dimensional Models of Human Bodies","summary":"  Contemporary approaches to solving various problems that require analyzing\nthree-dimensional (3D) meshes and point clouds have adopted the use of deep\nlearning algorithms that directly process 3D data such as point coordinates,\nnormal vectors and vertex connectivity information. Our work proposes one such\nsolution to the problem of positioning body and finger animation skeleton\njoints within 3D models of human bodies. Due to scarcity of annotated real\nhuman scans, we resort to generating synthetic samples while varying their\nshape and pose parameters. Similarly to the state-of-the-art approach, our\nmethod computes each joint location as a convex combination of input points.\nGiven only a list of point coordinates and normal vector estimates as input, a\ndynamic graph convolutional neural network is used to predict the coefficients\nof the convex combinations. By comparing our method with the state-of-the-art,\nwe show that it is possible to achieve significantly better results with a\nsimpler architecture, especially for finger joints. Since our solution requires\nfewer precomputed features, it also allows for shorter processing times.\n","authors":["Stefan NovakoviÄ","Vladimir RisojeviÄ"],"pdf_url":"https://arxiv.org/pdf/2407.08484v1.pdf","comment":"6 pages, 5 figures, final published version is available at:\n  https://ieeexplore.ieee.org/abstract/document/10579426, research code is\n  available at: https://github.com/sznov/joint-localization"},{"id":"http://arxiv.org/abs/2407.08481v1","updated":"2024-07-11T13:13:31Z","published":"2024-07-11T13:13:31Z","title":"SliceMamba for Medical Image Segmentation","summary":"  Despite the progress made in Mamba-based medical image segmentation models,\ncurrent methods utilizing unidirectional or multi-directional feature scanning\nmechanisms fail to well model dependencies between neighboring positions in the\nimage, hindering the effective modeling of local features. However, local\nfeatures are crucial for medical image segmentation as they provide vital\ninformation about lesions and tissue structures. To address this limitation, we\npropose a simple yet effective method named SliceMamba, a locally sensitive\npure Mamba medical image segmentation model. The proposed SliceMamba includes\nan efffcient Bidirectional Slice Scan module (BSS), which performs\nbidirectional feature segmentation while employing varied scanning mechanisms\nfor distinct features. This ensures that spatially adjacent features maintain\nproximity in the scanning sequence, thereby enhancing segmentation performance.\nExtensive experiments on skin lesion and polyp segmentation datasets validate\nthe effectiveness of our method.\n","authors":["Chao Fan","Hongyuan Yu","Luo Wang","Yan Huang","Liang Wang","Xibin Jia"],"pdf_url":"https://arxiv.org/pdf/2407.08481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08476v1","updated":"2024-07-11T13:11:21Z","published":"2024-07-11T13:11:21Z","title":"VideoMamba: Spatio-Temporal Selective State Space Model","summary":"  We introduce VideoMamba, a novel adaptation of the pure Mamba architecture,\nspecifically designed for video recognition. Unlike transformers that rely on\nself-attention mechanisms leading to high computational costs by quadratic\ncomplexity, VideoMamba leverages Mamba's linear complexity and selective SSM\nmechanism for more efficient processing. The proposed Spatio-Temporal Forward\nand Backward SSM allows the model to effectively capture the complex\nrelationship between non-sequential spatial and sequential temporal information\nin video. Consequently, VideoMamba is not only resource-efficient but also\neffective in capturing long-range dependency in videos, demonstrated by\ncompetitive performance and outstanding efficiency on a variety of video\nunderstanding benchmarks. Our work highlights the potential of VideoMamba as a\npowerful tool for video understanding, offering a simple yet effective baseline\nfor future research in video analysis.\n","authors":["Jinyoung Park","Hee-Seon Kim","Kangwook Ko","Minbeom Kim","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2407.08476v1.pdf","comment":"ECCV 2024. code available at http://github.com/jinyjelly/VideoMamba"},{"id":"http://arxiv.org/abs/2309.03774v2","updated":"2024-07-11T13:07:47Z","published":"2023-09-07T15:25:47Z","title":"Deep Learning Safety Concerns in Automated Driving Perception","summary":"  Recent advances in the field of deep learning and impressive performance of\ndeep neural networks (DNNs) for perception have resulted in an increased demand\nfor their use in automated driving (AD) systems. The safety of such systems is\nof utmost importance and thus requires to consider the unique properties of\nDNNs.\n  In order to achieve safety of AD systems with DNN-based perception components\nin a systematic and comprehensive approach, so-called safety concerns have been\nintroduced as a suitable structuring element. On the one hand, the concept of\nsafety concerns is -- by design -- well aligned to existing standards relevant\nfor safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has\nalready inspired several academic publications and upcoming standards on AI\nsafety such as ISO PAS 8800.\n  While the concept of safety concerns has been previously introduced, this\npaper extends and refines it, leveraging feedback from various domain and\nsafety experts in the field. In particular, this paper introduces an additional\ncategorization for a better understanding as well as enabling cross-functional\nteams to jointly address the concerns.\n","authors":["Stephanie Abrecht","Alexander Hirsch","Shervin Raafatnia","Matthias Woehrle"],"pdf_url":"https://arxiv.org/pdf/2309.03774v2.pdf","comment":"Added a note indicating that this work has been submitted to the IEEE\n  for possible publication"},{"id":"http://arxiv.org/abs/2407.08470v1","updated":"2024-07-11T13:04:20Z","published":"2024-07-11T13:04:20Z","title":"Brain Tumor Segmentation in MRI Images with 3D U-Net and Contextual\n  Transformer","summary":"  This research presents an enhanced approach for precise segmentation of brain\ntumor masses in magnetic resonance imaging (MRI) using an advanced 3D-UNet\nmodel combined with a Context Transformer (CoT). By architectural expansion\nCoT, the proposed model extends its architecture to a 3D format, integrates it\nsmoothly with the base model to utilize the complex contextual information\nfound in MRI scans, emphasizing how elements rely on each other across an\nextended spatial range. The proposed model synchronizes tumor mass\ncharacteristics from CoT, mutually reinforcing feature extraction, facilitating\nthe precise capture of detailed tumor mass structures, including location,\nsize, and boundaries. Several experimental results present the outstanding\nsegmentation performance of the proposed method in comparison to current\nstate-of-the-art approaches, achieving Dice score of 82.0%, 81.5%, 89.0% for\nEnhancing Tumor, Tumor Core and Whole Tumor, respectively, on BraTS2019.\n","authors":["Thien-Qua T. Nguyen","Hieu-Nghia Nguyen","Thanh-Hieu Bui","Thien B. Nguyen-Tat","Vuong M. Ngo"],"pdf_url":"https://arxiv.org/pdf/2407.08470v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.08466v1","updated":"2024-07-11T13:01:44Z","published":"2024-07-11T13:01:44Z","title":"Global Spatial-Temporal Information-based Residual ConvLSTM for Video\n  Space-Time Super-Resolution","summary":"  By converting low-frame-rate, low-resolution videos into high-frame-rate,\nhigh-resolution ones, space-time video super-resolution techniques can enhance\nvisual experiences and facilitate more efficient information dissemination. We\npropose a convolutional neural network (CNN) for space-time video\nsuper-resolution, namely GIRNet. To generate highly accurate features and thus\nimprove performance, the proposed network integrates a feature-level temporal\ninterpolation module with deformable convolutions and a global spatial-temporal\ninformation-based residual convolutional long short-term memory (convLSTM)\nmodule. In the feature-level temporal interpolation module, we leverage\ndeformable convolution, which adapts to deformations and scale variations of\nobjects across different scene locations. This presents a more efficient\nsolution than conventional convolution for extracting features from moving\nobjects. Our network effectively uses forward and backward feature information\nto determine inter-frame offsets, leading to the direct generation of\ninterpolated frame features. In the global spatial-temporal information-based\nresidual convLSTM module, the first convLSTM is used to derive global\nspatial-temporal information from the input features, and the second convLSTM\nuses the previously computed global spatial-temporal information feature as its\ninitial cell state. This second convLSTM adopts residual connections to\npreserve spatial information, thereby enhancing the output features.\nExperiments on the Vimeo90K dataset show that the proposed method outperforms\nstate-of-the-art techniques in peak signal-to-noise-ratio (by 1.45 dB, 1.14 dB,\nand 0.02 dB over STARnet, TMNet, and 3DAttGAN, respectively), structural\nsimilarity index(by 0.027, 0.023, and 0.006 over STARnet, TMNet, and 3DAttGAN,\nrespectively), and visually.\n","authors":["Congrui Fu","Hui Yuan","Shiqi Jiang","Guanghui Zhang","Liquan Shen","Raouf Hamzaoui"],"pdf_url":"https://arxiv.org/pdf/2407.08466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17891v2","updated":"2024-07-11T13:00:04Z","published":"2023-11-29T18:44:12Z","title":"A Graph-Based Approach for Category-Agnostic Pose Estimation","summary":"  Traditional 2D pose estimation models are limited by their category-specific\ndesign, making them suitable only for predefined object categories. This\nrestriction becomes particularly challenging when dealing with novel objects\ndue to the lack of relevant training data. To address this limitation,\ncategory-agnostic pose estimation (CAPE) was introduced. CAPE aims to enable\nkeypoint localization for arbitrary object categories using a few-shot single\nmodel, requiring minimal support images with annotated keypoints. We present a\nsignificant departure from conventional CAPE techniques, which treat keypoints\nas isolated entities, by treating the input pose data as a graph. We leverage\nthe inherent geometrical relations between keypoints through a graph-based\nnetwork to break symmetry, preserve structure, and better handle occlusions. We\nvalidate our approach on the MP-100 benchmark, a comprehensive dataset\ncomprising over 20,000 images spanning over 100 categories. Our solution boosts\nperformance by 0.98% under a 1-shot setting, achieving a new state-of-the-art\nfor CAPE. Additionally, we enhance the dataset with skeleton annotations. Our\ncode and data are publicly available.\n","authors":["Or Hirschorn","Shai Avidan"],"pdf_url":"https://arxiv.org/pdf/2311.17891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08460v1","updated":"2024-07-11T12:58:13Z","published":"2024-07-11T12:58:13Z","title":"Semi-Supervised Object Detection: A Survey on Progress from CNN to\n  Transformer","summary":"  The impressive advancements in semi-supervised learning have driven\nresearchers to explore its potential in object detection tasks within the field\nof computer vision. Semi-Supervised Object Detection (SSOD) leverages a\ncombination of a small labeled dataset and a larger, unlabeled dataset. This\napproach effectively reduces the dependence on large labeled datasets, which\nare often expensive and time-consuming to obtain. Initially, SSOD models\nencountered challenges in effectively leveraging unlabeled data and managing\nnoise in generated pseudo-labels for unlabeled data. However, numerous recent\nadvancements have addressed these issues, resulting in substantial improvements\nin SSOD performance. This paper presents a comprehensive review of 27\ncutting-edge developments in SSOD methodologies, from Convolutional Neural\nNetworks (CNNs) to Transformers. We delve into the core components of\nsemi-supervised learning and its integration into object detection frameworks,\ncovering data augmentation techniques, pseudo-labeling strategies, consistency\nregularization, and adversarial training methods. Furthermore, we conduct a\ncomparative analysis of various SSOD models, evaluating their performance and\narchitectural differences. We aim to ignite further research interest in\novercoming existing challenges and exploring new directions in semi-supervised\nlearning for object detection.\n","authors":["Tahira Shehzadi"," Ifza","Didier Stricker","Muhammad Zeshan Afzal"],"pdf_url":"https://arxiv.org/pdf/2407.08460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08457v1","updated":"2024-07-11T12:54:21Z","published":"2024-07-11T12:54:21Z","title":"Neural Poisson Solver: A Universal and Continuous Framework for Natural\n  Signal Blending","summary":"  Implicit Neural Representation (INR) has become a popular method for\nrepresenting visual signals (e.g., 2D images and 3D scenes), demonstrating\npromising results in various downstream applications. Given its potential as a\nmedium for visual signals, exploring the development of a neural blending\nmethod that utilizes INRs is a natural progression. Neural blending involves\nmerging two INRs to create a new INR that encapsulates information from both\noriginal representations. A direct approach involves applying traditional image\nediting methods to the INR rendering process. However, this method often\nresults in blending distortions, artifacts, and color shifts, primarily due to\nthe discretization of the underlying pixel grid and the introduction of\nboundary conditions for solving variational problems. To tackle this issue, we\nintroduce the Neural Poisson Solver, a plug-and-play and universally applicable\nframework across different signal dimensions for blending visual signals\nrepresented by INRs. Our Neural Poisson Solver offers a variational\nproblem-solving approach based on the continuous Poisson equation,\ndemonstrating exceptional performance across various domains. Specifically, we\npropose a gradient-guided neural solver to represent the solution process of\nthe variational problem, refining the target signal to achieve natural blending\nresults. We also develop a Poisson equation-based loss and optimization scheme\nto train our solver, ensuring it effectively blends the input INR scenes while\npreserving their inherent structure and semantic content. The lack of\ndependence on additional prior knowledge makes our method easily adaptable to\nvarious task categories, highlighting its versatility. Comprehensive\nexperimental results validate the robustness of our approach across multiple\ndimensions and blending tasks.\n","authors":["Delong Wu","Hao Zhu","Qi Zhang","You Li","Zhan Ma","Xun Cao"],"pdf_url":"https://arxiv.org/pdf/2407.08457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08448v1","updated":"2024-07-11T12:42:10Z","published":"2024-07-11T12:42:10Z","title":"Paving the way toward foundation models for irregular and unaligned\n  Satellite Image Time Series","summary":"  Although recently several foundation models for satellite remote sensing\nimagery have been proposed, they fail to address major challenges of\nreal/operational applications. Indeed, embeddings that don't take into account\nthe spectral, spatial and temporal dimensions of the data as well as the\nirregular or unaligned temporal sampling are of little use for most real world\nuses.As a consequence, we propose an ALIgned Sits Encoder (ALISE), a novel\napproach that leverages the spatial, spectral, and temporal dimensions of\nirregular and unaligned SITS while producing aligned latent representations.\nUnlike SSL models currently available for SITS, ALISE incorporates a flexible\nquery mechanism to project the SITS into a common and learned temporal\nprojection space. Additionally, thanks to a multi-view framework, we explore\nintegration of instance discrimination along a masked autoencoding task to\nSITS. The quality of the produced representation is assessed through three\ndownstream tasks: crop segmentation (PASTIS), land cover segmentation\n(MultiSenGE), and a novel crop change detection dataset. Furthermore, the\nchange detection task is performed without supervision. The results suggest\nthat the use of aligned representations is more effective than previous SSL\nmethods for linear probing segmentation tasks.\n","authors":["Iris Dumeur","Silvia Valero","Jordi Inglada"],"pdf_url":"https://arxiv.org/pdf/2407.08448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08447v1","updated":"2024-07-11T12:41:32Z","published":"2024-07-11T12:41:32Z","title":"WildGaussians: 3D Gaussian Splatting in the Wild","summary":"  While the field of 3D scene reconstruction is dominated by NeRFs due to their\nphotorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged,\noffering similar quality with real-time rendering speeds. However, both methods\nprimarily excel with well-controlled 3D scenes, while in-the-wild data -\ncharacterized by occlusions, dynamic objects, and varying illumination -\nremains challenging. NeRFs can adapt to such conditions easily through\nper-image embedding vectors, but 3DGS struggles due to its explicit\nrepresentation and lack of shared parameters. To address this, we introduce\nWildGaussians, a novel approach to handle occlusions and appearance changes\nwith 3DGS. By leveraging robust DINO features and integrating an appearance\nmodeling module within 3DGS, our method achieves state-of-the-art results. We\ndemonstrate that WildGaussians matches the real-time rendering speed of 3DGS\nwhile surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all\nwithin a simple architectural framework.\n","authors":["Jonas Kulhanek","Songyou Peng","Zuzana Kukelova","Marc Pollefeys","Torsten Sattler"],"pdf_url":"https://arxiv.org/pdf/2407.08447v1.pdf","comment":"https://wild-gaussians.github.io/"},{"id":"http://arxiv.org/abs/2407.08443v1","updated":"2024-07-11T12:33:56Z","published":"2024-07-11T12:33:56Z","title":"Infinite Motion: Extended Motion Generation via Long Text Instructions","summary":"  In the realm of motion generation, the creation of long-duration,\nhigh-quality motion sequences remains a significant challenge. This paper\npresents our groundbreaking work on \"Infinite Motion\", a novel approach that\nleverages long text to extended motion generation, effectively bridging the gap\nbetween short and long-duration motion synthesis. Our core insight is the\nstrategic extension and reassembly of existing high-quality text-motion\ndatasets, which has led to the creation of a novel benchmark dataset to\nfacilitate the training of models for extended motion sequences. A key\ninnovation of our model is its ability to accept arbitrary lengths of text as\ninput, enabling the generation of motion sequences tailored to specific\nnarratives or scenarios. Furthermore, we incorporate the timestamp design for\ntext which allows precise editing of local segments within the generated\nsequences, offering unparalleled control and flexibility in motion synthesis.\nWe further demonstrate the versatility and practical utility of \"Infinite\nMotion\" through three specific applications: natural language interactive\nediting, motion sequence editing within long sequences and splicing of\nindependent motion sequences. Each application highlights the adaptability of\nour approach and broadens the spectrum of possibilities for research and\ndevelopment in motion generation. Through extensive experiments, we demonstrate\nthe superior performance of our model in generating long sequence motions\ncompared to existing methods.Project page:\nhttps://shuochengzhai.github.io/Infinite-motion.github.io/\n","authors":["Mengtian Li","Chengshuo Zhai","Shengxiang Yao","Zhifeng Xie","Keyu Chen Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.08443v1.pdf","comment":"12 pages,13 figures"},{"id":"http://arxiv.org/abs/2407.05206v2","updated":"2024-07-11T12:33:53Z","published":"2024-07-06T23:16:41Z","title":"Helios: An extremely low power event-based gesture recognition for\n  always-on smart eyewear","summary":"  This paper introduces Helios, the first extremely low-power, real-time,\nevent-based hand gesture recognition system designed for all-day on smart\neyewear. As augmented reality (AR) evolves, current smart glasses like the Meta\nRay-Bans prioritize visual and wearable comfort at the expense of\nfunctionality. Existing human-machine interfaces (HMIs) in these devices, such\nas capacitive touch and voice controls, present limitations in ergonomics,\nprivacy and power consumption. Helios addresses these challenges by leveraging\nnatural hand interactions for a more intuitive and comfortable user experience.\nOur system utilizes a extremely low-power and compact 3mmx4mm/20mW event camera\nto perform natural hand-based gesture recognition for always-on smart eyewear.\nThe camera's output is processed by a convolutional neural network (CNN)\nrunning on a NXP Nano UltraLite compute platform, consuming less than 350mW.\nHelios can recognize seven classes of gestures, including subtle microgestures\nlike swipes and pinches, with 91% accuracy. We also demonstrate real-time\nperformance across 20 users at a remarkably low latency of 60ms. Our user\ntesting results align with the positive feedback we received during our recent\nsuccessful demo at AWE-USA-2024.\n","authors":["Prarthana Bhattacharyya","Joshua Mitton","Ryan Page","Owen Morgan","Ben Menzies","Gabriel Homewood","Kemi Jacobs","Paolo Baesso","Dave Trickett","Chris Mair","Taru Muhonen","Rory Clark","Louis Berridge","Richard Vigars","Iain Wallace"],"pdf_url":"https://arxiv.org/pdf/2407.05206v2.pdf","comment":"18 pages, 10 figures. First three authors contributed equally to this\n  paper"},{"id":"http://arxiv.org/abs/2407.08428v1","updated":"2024-07-11T12:09:05Z","published":"2024-07-11T12:09:05Z","title":"A Comprehensive Survey on Human Video Generation: Challenges, Methods,\n  and Insights","summary":"  Human video generation is a dynamic and rapidly evolving task that aims to\nsynthesize 2D human body video sequences with generative models given control\nconditions such as text, audio, and pose. With the potential for wide-ranging\napplications in film, gaming, and virtual communication, the ability to\ngenerate natural and realistic human video is critical. Recent advancements in\ngenerative models have laid a solid foundation for the growing interest in this\narea. Despite the significant progress, the task of human video generation\nremains challenging due to the consistency of characters, the complexity of\nhuman motion, and difficulties in their relationship with the environment. This\nsurvey provides a comprehensive review of the current state of human video\ngeneration, marking, to the best of our knowledge, the first extensive\nliterature review in this domain. We start with an introduction to the\nfundamentals of human video generation and the evolution of generative models\nthat have facilitated the field's growth. We then examine the main methods\nemployed for three key sub-tasks within human video generation: text-driven,\naudio-driven, and pose-driven motion generation. These areas are explored\nconcerning the conditions that guide the generation process. Furthermore, we\noffer a collection of the most commonly utilized datasets and the evaluation\nmetrics that are crucial in assessing the quality and realism of generated\nvideos. The survey concludes with a discussion of the current challenges in the\nfield and suggests possible directions for future research. The goal of this\nsurvey is to offer the research community a clear and holistic view of the\nadvancements in human video generation, highlighting the milestones achieved\nand the challenges that lie ahead.\n","authors":["Wentao Lei","Jinting Wang","Fengji Ma","Guanjie Huang","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11276v2","updated":"2024-07-11T12:07:06Z","published":"2024-05-18T12:22:26Z","title":"Visible and Clear: Finding Tiny Objects in Difference Map","summary":"  Tiny object detection is one of the key challenges in the field of object\ndetection. The performance of most generic detectors dramatically decreases in\ntiny object detection tasks. The main challenge lies in extracting effective\nfeatures of tiny objects. Existing methods usually perform generation-based\nfeature enhancement, which is seriously affected by spurious textures and\nartifacts, making it difficult to make the tiny-object-specific features\nvisible and clear for detection. To address this issue, we propose a\nself-reconstructed tiny object detection (SR-TOD) framework. We for the first\ntime introduce a self-reconstruction mechanism in the detection model, and\ndiscover the strong correlation between it and the tiny objects. Specifically,\nwe impose a reconstruction head in-between the neck of a detector, constructing\na difference map of the reconstructed image and the input, which shows high\nsensitivity to tiny objects. This inspires us to enhance the weak\nrepresentations of tiny objects under the guidance of the difference maps.\nThus, improving the visibility of tiny objects for the detectors. Building on\nthis, we further develop a Difference Map Guided Feature Enhancement (DGFE)\nmodule to make the tiny feature representation more clear. In addition, we\nfurther propose a new multi-instance anti-UAV dataset, which is called\nDroneSwarms dataset and contains a large number of tiny drones with the\nsmallest average size to date. Extensive experiments on the DroneSwarms dataset\nand other datasets demonstrate the effectiveness of the proposed method. The\ncode and dataset will be publicly available.\n","authors":["Bing Cao","Haiyu Yao","Pengfei Zhu","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2405.11276v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2404.04933v2","updated":"2024-07-11T12:03:35Z","published":"2024-04-07T12:14:42Z","title":"UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection","summary":"  Temporal Action Detection (TAD) focuses on detecting pre-defined actions,\nwhile Moment Retrieval (MR) aims to identify the events described by open-ended\nnatural language within untrimmed videos. Despite that they focus on different\nevents, we observe they have a significant connection. For instance, most\ndescriptions in MR involve multiple actions from TAD. In this paper, we aim to\ninvestigate the potential synergy between TAD and MR. Firstly, we propose a\nunified architecture, termed Unified Moment Detection (UniMD), for both TAD and\nMR. It transforms the inputs of the two tasks, namely actions for TAD or events\nfor MR, into a common embedding space, and utilizes two novel query-dependent\ndecoders to generate a uniform output of classification score and temporal\nsegments. Secondly, we explore the efficacy of two task fusion learning\napproaches, pre-training and co-training, in order to enhance the mutual\nbenefits between TAD and MR. Extensive experiments demonstrate that the\nproposed task fusion learning scheme enables the two tasks to help each other\nand outperform the separately trained counterparts. Impressively, UniMD\nachieves state-of-the-art results on three paired datasets Ego4D, Charades-STA,\nand ActivityNet. Our code is available at https://github.com/yingsen1/UniMD.\n","authors":["Yingsen Zeng","Yujie Zhong","Chengjian Feng","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2404.04933v2.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2407.08418v1","updated":"2024-07-11T11:51:36Z","published":"2024-07-11T11:51:36Z","title":"PredBench: Benchmarking Spatio-Temporal Prediction across Diverse\n  Disciplines","summary":"  In this paper, we introduce PredBench, a benchmark tailored for the holistic\nevaluation of spatio-temporal prediction networks. Despite significant progress\nin this field, there remains a lack of a standardized framework for a detailed\nand comparative analysis of various prediction network architectures. PredBench\naddresses this gap by conducting large-scale experiments, upholding\nstandardized and appropriate experimental settings, and implementing\nmulti-dimensional evaluations. This benchmark integrates 12 widely adopted\nmethods with 15 diverse datasets across multiple application domains, offering\nextensive evaluation of contemporary spatio-temporal prediction networks.\nThrough meticulous calibration of prediction settings across various\napplications, PredBench ensures evaluations relevant to their intended use and\nenables fair comparisons. Moreover, its multi-dimensional evaluation framework\nbroadens the analysis with a comprehensive set of metrics, providing deep\ninsights into the capabilities of models. The findings from our research offer\nstrategic directions for future developments in the field. Our codebase is\navailable at https://github.com/WZDTHU/PredBench.\n","authors":["ZiDong Wang","Zeyu Lu","Di Huang","Tong He","Xihui Liu","Wanli Ouyang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2407.08418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15613v5","updated":"2024-07-11T11:50:13Z","published":"2024-01-28T10:00:45Z","title":"Towards Arbitrary-Scale Histopathology Image Super-resolution: An\n  Efficient Dual-branch Framework via Implicit Self-texture Enhancement","summary":"  High-quality whole-slide scanners are expensive, complex, and time-consuming,\nthus limiting the acquisition and utilization of high-resolution pathology\nwhole-slide images in daily clinical work. Deep learning-based single-image\nsuper-resolution techniques are an effective way to solve this problem by\nsynthesizing high-resolution images from low-resolution ones. However, the\nexisting super-resolution models applied in pathology images can only work in\nfixed integer magnifications, significantly decreasing their applicability.\nThough methods based on implicit neural representation have shown promising\nresults in arbitrary-scale super-resolution of natural images, applying them\ndirectly to pathology images is inadequate because they have unique\nfine-grained image textures different from natural images. Thus, we propose an\nImplicit Self-Texture Enhancement-based dual-branch framework (ISTE) for\narbitrary-scale super-resolution of pathology images to address this challenge.\nISTE contains a pixel learning branch and a texture learning branch, which\nfirst learn pixel features and texture features, respectively. Then, we design\na two-stage texture enhancement strategy to fuse the features from the two\nbranches to obtain the super-resolution results, where the first stage is\nfeature-based texture enhancement, and the second stage is spatial-domain-based\ntexture enhancement. Extensive experiments on three public datasets show that\nISTE outperforms existing fixed-scale and arbitrary-scale algorithms at\nmultiple magnifications and helps to improve downstream task performance. To\nthe best of our knowledge, this is the first work to achieve arbitrary-scale\nsuper-resolution in pathology images. Codes will be available.\n","authors":["Minghong Duan","Linhao Qu","Zhiwei Yang","Manning Wang","Chenxi Zhang","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2401.15613v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14548v2","updated":"2024-07-11T11:47:26Z","published":"2024-03-21T16:49:20Z","title":"DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single\n  Video","summary":"  We present DINO-Tracker -- a new framework for long-term dense tracking in\nvideo. The pillar of our approach is combining test-time training on a single\nvideo, with the powerful localized semantic features learned by a pre-trained\nDINO-ViT model. Specifically, our framework simultaneously adopts DINO's\nfeatures to fit to the motion observations of the test video, while training a\ntracker that directly leverages the refined features. The entire framework is\ntrained end-to-end using a combination of self-supervised losses, and\nregularization that allows us to retain and benefit from DINO's semantic prior.\nExtensive evaluation demonstrates that our method achieves state-of-the-art\nresults on known benchmarks. DINO-tracker significantly outperforms\nself-supervised methods and is competitive with state-of-the-art supervised\ntrackers, while outperforming them in challenging cases of tracking under\nlong-term occlusions.\n","authors":["Narek Tumanyan","Assaf Singer","Shai Bagon","Tali Dekel"],"pdf_url":"https://arxiv.org/pdf/2403.14548v2.pdf","comment":"Accepted to ECCV 2024. Project page: https://dino-tracker.github.io/"},{"id":"http://arxiv.org/abs/2310.12904v2","updated":"2024-07-11T11:46:20Z","published":"2023-10-19T16:57:49Z","title":"Unsupervised Object Localization in the Era of Self-Supervised ViTs: A\n  Survey","summary":"  The recent enthusiasm for open-world vision systems show the high interest of\nthe community to perform perception tasks outside of the closed-vocabulary\nbenchmark setups which have been so popular until now. Being able to discover\nobjects in images/videos without knowing in advance what objects populate the\ndataset is an exciting prospect. But how to find objects without knowing\nanything about them? Recent works show that it is possible to perform\nclass-agnostic unsupervised object localization by exploiting self-supervised\npre-trained features. We propose here a survey of unsupervised object\nlocalization methods that discover objects in images without requiring any\nmanual annotation in the era of self-supervised ViTs. We gather links of\ndiscussed methods in the repository\nhttps://github.com/valeoai/Awesome-Unsupervised-Object-Localization.\n","authors":["Oriane SimÃ©oni","Ãloi Zablocki","Spyros Gidaris","Gilles Puy","Patrick PÃ©rez"],"pdf_url":"https://arxiv.org/pdf/2310.12904v2.pdf","comment":"IJCV 2024"},{"id":"http://arxiv.org/abs/2407.08414v1","updated":"2024-07-11T11:37:51Z","published":"2024-07-11T11:37:51Z","title":"MeshAvatar: Learning High-quality Triangular Human Avatars from\n  Multi-view Videos","summary":"  We present a novel pipeline for learning high-quality triangular human\navatars from multi-view videos. Recent methods for avatar learning are\ntypically based on neural radiance fields (NeRF), which is not compatible with\ntraditional graphics pipeline and poses great challenges for operations like\nediting or synthesizing under different environments. To overcome these\nlimitations, our method represents the avatar with an explicit triangular mesh\nextracted from an implicit SDF field, complemented by an implicit material\nfield conditioned on given poses. Leveraging this triangular avatar\nrepresentation, we incorporate physics-based rendering to accurately decompose\ngeometry and texture. To enhance both the geometric and appearance details, we\nfurther employ a 2D UNet as the network backbone and introduce pseudo normal\nground-truth as additional supervision. Experiments show that our method can\nlearn triangular avatars with high-quality geometry reconstruction and\nplausible material decomposition, inherently supporting editing, manipulation\nor relighting operations.\n","authors":["Yushuo Chen","Zerong Zheng","Zhe Li","Chao Xu","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08414v1.pdf","comment":"Project Page: https://shad0wta9.github.io/meshavatar-page/"},{"id":"http://arxiv.org/abs/2407.08411v1","updated":"2024-07-11T11:32:33Z","published":"2024-07-11T11:32:33Z","title":"CLEO: Continual Learning of Evolving Ontologies","summary":"  Continual learning (CL) addresses the problem of catastrophic forgetting in\nneural networks, which occurs when a trained model tends to overwrite\npreviously learned information, when presented with a new task. CL aims to\ninstill the lifelong learning characteristic of humans in intelligent systems,\nmaking them capable of learning continuously while retaining what was already\nlearned. Current CL problems involve either learning new domains\n(domain-incremental) or new and previously unseen classes (class-incremental).\nHowever, general learning processes are not just limited to learning\ninformation, but also refinement of existing information. In this paper, we\ndefine CLEO - Continual Learning of Evolving Ontologies, as a new incremental\nlearning setting under CL to tackle evolving classes. CLEO is motivated by the\nneed for intelligent systems to adapt to real-world ontologies that change over\ntime, such as those in autonomous driving. We use Cityscapes, PASCAL VOC, and\nMapillary Vistas to define the task settings and demonstrate the applicability\nof CLEO. We highlight the shortcomings of existing CIL methods in adapting to\nCLEO and propose a baseline solution, called Modelling Ontologies (MoOn). CLEO\nis a promising new approach to CL that addresses the challenge of evolving\nontologies in real-world applications. MoOn surpasses previous CL approaches in\nthe context of CLEO.\n","authors":["Shishir Muralidhara","Saqib Bukhari","Georg Schneider","Didier Stricker","RenÃ© Schuster"],"pdf_url":"https://arxiv.org/pdf/2407.08411v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.08403v1","updated":"2024-07-11T11:12:48Z","published":"2024-07-11T11:12:48Z","title":"Ethics of Generating Synthetic MRI Vocal Tract Views from the Face","summary":"  Forming oral models capable of understanding the complete dynamics of the\noral cavity is vital across research areas such as speech correction, designing\nfoods for the aging population, and dentistry. Magnetic resonance imaging (MRI)\ntechnologies, capable of capturing oral data essential for creating such\ndetailed representations, offer a powerful tool for illustrating articulatory\ndynamics. However, its real-time application is hindered by expense and\nexpertise requirements. Ever advancing generative AI approaches present\nthemselves as a way to address this barrier by leveraging multi-modal\napproaches for generating pseudo-MRI views. Nonetheless, this immediately\nsparks ethical concerns regarding the utilisation of a technology with the\ncapability to produce MRIs from facial observations.\n  This paper explores the ethical implications of external-to-internal\ncorrelation modeling (E2ICM). E2ICM utilises facial movements to infer internal\nconfigurations and provides a cost-effective supporting technology for MRI. In\nthis preliminary work, we employ Pix2PixGAN to generate pseudo-MRI views from\nexternal articulatory data, demonstrating the feasibility of this approach.\nEthical considerations concerning privacy, consent, and potential misuse, which\nare fundamental to our examination of this innovative methodology, are\ndiscussed as a result of this experimentation.\n","authors":["Muhammad Suhaib Shahid","Gleb E. Yakubov","Andrew P. French"],"pdf_url":"https://arxiv.org/pdf/2407.08403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06315v2","updated":"2024-07-11T11:11:03Z","published":"2024-07-08T18:31:19Z","title":"Shedding More Light on Robust Classifiers under the lens of Energy-based\n  Models","summary":"  By reinterpreting a robust discriminative classifier as Energy-based Model\n(EBM), we offer a new take on the dynamics of adversarial training (AT). Our\nanalysis of the energy landscape during AT reveals that untargeted attacks\ngenerate adversarial images much more in-distribution (lower energy) than the\noriginal data from the point of view of the model. Conversely, we observe the\nopposite for targeted attacks. On the ground of our thorough analysis, we\npresent new theoretical and practical results that show how interpreting AT\nenergy dynamics unlocks a better understanding: (1) AT dynamic is governed by\nthree phases and robust overfitting occurs in the third phase with a drastic\ndivergence between natural and adversarial energies (2) by rewriting the loss\nof TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization\n(TRADES) in terms of energies, we show that TRADES implicitly alleviates\noverfitting by means of aligning the natural energy with the adversarial one\n(3) we empirically show that all recent state-of-the-art robust classifiers are\nsmoothing the energy landscape and we reconcile a variety of studies about\nunderstanding AT and weighting the loss function under the umbrella of EBMs.\nMotivated by rigorous evidence, we propose Weighted Energy Adversarial Training\n(WEAT), a novel sample weighting scheme that yields robust accuracy matching\nthe state-of-the-art on multiple benchmarks such as CIFAR-10 and SVHN and going\nbeyond in CIFAR-100 and Tiny-ImageNet. We further show that robust classifiers\nvary in the intensity and quality of their generative capabilities, and offer a\nsimple method to push this capability, reaching a remarkable Inception Score\n(IS) and FID using a robust classifier without training for generative\nmodeling. The code to reproduce our results is available at\nhttp://github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/ .\n","authors":["Mujtaba Hussain Mirza","Maria Rosaria Briglia","Senad Beadini","Iacopo Masi"],"pdf_url":"https://arxiv.org/pdf/2407.06315v2.pdf","comment":"Accepted at European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2407.08395v1","updated":"2024-07-11T10:59:11Z","published":"2024-07-11T10:59:11Z","title":"Using deep neural networks to detect non-analytically defined expert\n  event labels in canoe sprint force sensor signals","summary":"  Assessing an athlete's performance in canoe sprint is often established by\nmeasuring a variety of kinematic parameters during training sessions. Many of\nthese parameters are related to single or multiple paddle stroke cycles.\nDetermining on- and offset of these cycles in force sensor signals is usually\nnot straightforward and requires human interaction. This paper explores\nconvolutional neural networks (CNNs) and recurrent neural networks (RNNs) in\nterms of their ability to automatically predict these events. In addition, our\nwork proposes an extension to the recently published SoftED metric for event\ndetection in order to properly assess the model performance on time windows. In\nour results, an RNN based on bidirectional gated recurrent units (BGRUs) turned\nout to be the most suitable model for paddle stroke detection.\n","authors":["Sarah Rockstrok","Patrick Frenzel","Daniel Matthes","Kay Schubert","David Wollburg","Mirco Fuchs"],"pdf_url":"https://arxiv.org/pdf/2407.08395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08394v1","updated":"2024-07-11T10:57:33Z","published":"2024-07-11T10:57:33Z","title":"Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers","summary":"  We introduce Diff-Tracker, a novel approach for the challenging unsupervised\nvisual tracking task leveraging the pre-trained text-to-image diffusion model.\nOur main idea is to leverage the rich knowledge encapsulated within the\npre-trained diffusion model, such as the understanding of image semantics and\nstructural information, to address unsupervised visual tracking. To this end,\nwe design an initial prompt learner to enable the diffusion model to recognize\nthe tracking target by learning a prompt representing the target. Furthermore,\nto facilitate dynamic adaptation of the prompt to the target's movements, we\npropose an online prompt updater. Extensive experiments on five benchmark\ndatasets demonstrate the effectiveness of our proposed method, which also\nachieves state-of-the-art performance.\n","authors":["Zhengbo Zhang","Li Xu","Duo Peng","Hossein Rahmani","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08394v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.08384v1","updated":"2024-07-11T10:44:42Z","published":"2024-07-11T10:44:42Z","title":"Accurate Cooperative Localization Utilizing LiDAR-equipped Roadside\n  Infrastructure for Autonomous Driving","summary":"  Recent advancements in LiDAR technology have significantly lowered costs and\nimproved both its precision and resolution, thereby solidifying its role as a\ncritical component in autonomous vehicle localization. Using sophisticated 3D\nregistration algorithms, LiDAR now facilitates vehicle localization with\ncentimeter-level accuracy. However, these high-precision techniques often face\nreliability challenges in environments devoid of identifiable map features. To\naddress this limitation, we propose a novel approach that utilizes road side\nunits (RSU) with vehicle-to-infrastructure (V2I) communications to assist\nvehicle self-localization. By using RSUs as stationary reference points and\nprocessing real-time LiDAR data, our method enhances localization accuracy\nthrough a cooperative localization framework. By placing RSUs in critical\nareas, our proposed method can improve the reliability and precision of vehicle\nlocalization when the traditional vehicle self-localization technique falls\nshort. Evaluation results in an end-to-end autonomous driving simulator AWSIM\nshow that the proposed method can improve localization accuracy by up to 80%\nunder vulnerable environments compared to traditional localization methods.\nAdditionally, our method also demonstrates robust resistance to network delays\nand packet loss in heterogeneous network environments.\n","authors":["Yuze Jiang","Ehsan Javanmardi","Manabu Tsukada","Hiroshi Esaki"],"pdf_url":"https://arxiv.org/pdf/2407.08384v1.pdf","comment":"Accepted by IEEE Intelligent Transportation Systems Conference (ITSC)\n  2024"},{"id":"http://arxiv.org/abs/2407.08380v1","updated":"2024-07-11T10:41:20Z","published":"2024-07-11T10:41:20Z","title":"Digital twins to alleviate the need for real field data in vision-based\n  vehicle speed detection systems","summary":"  Accurate vision-based speed estimation is much more cost-effective than\ntraditional methods based on radar or LiDAR. However, it is also challenging\ndue to the limitations of perspective projection on a discrete sensor, as well\nas the high sensitivity to calibration, lighting and weather conditions.\nInterestingly, deep learning approaches (which dominate the field of computer\nvision) are very limited in this context due to the lack of available data.\nIndeed, obtaining video sequences of real road traffic with accurate speed\nvalues associated with each vehicle is very complex and costly, and the number\nof available datasets is very limited. Recently, some approaches are focusing\non the use of synthetic data. However, it is still unclear how models trained\non synthetic data can be effectively applied to real world conditions. In this\nwork, we propose the use of digital-twins using CARLA simulator to generate a\nlarge dataset representative of a specific real-world camera. The synthetic\ndataset contains a large variability of vehicle types, colours, speeds,\nlighting and weather conditions. A 3D CNN model is trained on the digital twin\nand tested on the real sequences. Unlike previous approaches that generate\nmulti-camera sequences, we found that the gap between the the real and the\nvirtual conditions is a key factor in obtaining low speed estimation errors.\nEven with a preliminary approach, the mean absolute error obtained remains\nbelow 3km/h.\n","authors":["Antonio HernÃ¡ndez MartÃ­nez","IvÃ¡n GarcÃ­a Daza","Carlos FernÃ¡ndez LÃ³pez","David FernÃ¡ndez Llorca"],"pdf_url":"https://arxiv.org/pdf/2407.08380v1.pdf","comment":"Paper accepted at the 27th IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2024)"},{"id":"http://arxiv.org/abs/2407.08377v1","updated":"2024-07-11T10:38:02Z","published":"2024-07-11T10:38:02Z","title":"Long-range Turbulence Mitigation: A Large-scale Dataset and A\n  Coarse-to-fine Framework","summary":"  Long-range imaging inevitably suffers from atmospheric turbulence with severe\ngeometric distortions due to random refraction of light. The further the\ndistance, the more severe the disturbance. Despite existing research has\nachieved great progress in tackling short-range turbulence, there is less\nattention paid to long-range turbulence with significant distortions. To\naddress this dilemma and advance the field, we construct a large-scale real\nlong-range atmospheric turbulence dataset (RLR-AT), including 1500 turbulence\nsequences spanning distances from 1 Km to 13 Km. The advantages of RLR-AT\ncompared to existing ones: turbulence with longer-distances and\nhigher-diversity, scenes with greater-variety and larger-scale. Moreover, most\nexisting work adopts either registration-based or decomposition-based methods\nto address distortions through one-step mitigation. However, they fail to\neffectively handle long-range turbulence due to its significant pixel\ndisplacements. In this work, we propose a coarse-to-fine framework to handle\nsevere distortions, which cooperates dynamic turbulence and static background\npriors (CDSP). On the one hand, we discover the pixel motion statistical prior\nof turbulence, and propose a frequency-aware reference frame for better\nlarge-scale distortion registration, greatly reducing the burden of refinement.\nOn the other hand, we take advantage of the static prior of background, and\npropose a subspace-based low-rank tensor refinement model to eliminate the\nmisalignments inevitably left by registration while well preserving details.\nThe dynamic and static priors complement to each other, facilitating us to\nprogressively mitigate long-range turbulence with severe distortions. Extensive\nexperiments demonstrate that the proposed method outperforms SOTA methods on\ndifferent datasets.\n","authors":["Shengqi Xu","Run Sun","Yi Chang","Shuning Cao","Xueyao Xiao","Luxin Yan"],"pdf_url":"https://arxiv.org/pdf/2407.08377v1.pdf","comment":"This paper is accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.08374v1","updated":"2024-07-11T10:35:53Z","published":"2024-07-11T10:35:53Z","title":"Enhancing Robustness of Vision-Language Models through Orthogonality\n  Learning and Cross-Regularization","summary":"  Efficient finetuning of vision-language models (VLMs) like CLIP for specific\ndownstream tasks is gaining significant attention. Previous works primarily\nfocus on prompt learning to adapt the CLIP into a variety of downstream tasks,\nhowever, suffering from task overfitting when finetuned on a small data set. In\nthis paper, we introduce an orthogonal finetuning method for efficiently\nupdating pretrained weights which enhances robustness and generalization, while\na cross-regularization strategy is further exploited to maintain the stability\nin terms of zero-shot generalization of VLMs, dubbed \\textbf{\\textit{OrthCR}}.\nSpecifically, trainable orthogonal matrices are injected seamlessly into the\ntransformer architecture and enforced with orthogonality constraint using\nCayley parameterization, benefiting from the norm-preserving property and thus\nleading to stable and faster convergence. To alleviate deviation from\northogonal constraint during training, a cross-regularization strategy is\nfurther employed with initial pretrained weights within a bypass manner. In\naddition, to enrich the sample diversity for downstream tasks, we first explore\nCutout data augmentation to boost the efficient finetuning and comprehend how\nour approach improves the specific downstream performance and maintains the\ngeneralizability in the perspective of Orthogonality Learning. Beyond existing\nprompt learning techniques, we conduct extensive experiments to demonstrate\nthat our method explicitly steers pretrained weight space to represent the\ntask-specific knowledge and presents competitive generalizability under\n\\textit{base-to-base/base-to-new}, \\textit{cross-dataset transfer} and\n\\textit{domain generalization} evaluations.\n","authors":["Jinlong Li","Zequn Jie","Elisa Ricci","Lin Ma","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2407.08374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08366v1","updated":"2024-07-11T10:19:48Z","published":"2024-07-11T10:19:48Z","title":"An Economic Framework for 6-DoF Grasp Detection","summary":"  Robotic grasping in clutters is a fundamental task in robotic manipulation.\nIn this work, we propose an economic framework for 6-DoF grasp detection,\naiming to economize the resource cost in training and meanwhile maintain\neffective grasp performance. To begin with, we discover that the dense\nsupervision is the bottleneck of current SOTA methods that severely encumbers\nthe entire training overload, meanwhile making the training difficult to\nconverge. To solve the above problem, we first propose an economic supervision\nparadigm for efficient and effective grasping. This paradigm includes a\nwell-designed supervision selection strategy, selecting key labels basically\nwithout ambiguity, and an economic pipeline to enable the training after\nselection. Furthermore, benefit from the economic supervision, we can focus on\na specific grasp, and thus we devise a focal representation module, which\ncomprises an interactive grasp head and a composite score estimation to\ngenerate the specific grasp more accurately. Combining all together, the\nEconomicGrasp framework is proposed. Our extensive experiments show that\nEconomicGrasp surpasses the SOTA grasp method by about 3AP on average, and with\nextremely low resource cost, for about 1/4 training time cost, 1/8 memory cost\nand 1/30 storage cost. Our code is available at\nhttps://github.com/iSEE-Laboratory/EconomicGrasp.\n","authors":["Xiao-Ming Wu","Jia-Feng Cai","Jian-Jian Jiang","Dian Zheng","Yi-Lin Wei","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.08366v1.pdf","comment":"19 pages, 7 figures. Accepted in ECCV 2024!"},{"id":"http://arxiv.org/abs/2407.08364v1","updated":"2024-07-11T10:18:54Z","published":"2024-07-11T10:18:54Z","title":"Scalar Function Topology Divergence: Comparing Topology of 3D Objects","summary":"  We propose a new topological tool for computer vision - Scalar Function\nTopology Divergence (SFTD), which measures the dissimilarity of multi-scale\ntopology between sublevel sets of two functions having a common domain.\nFunctions can be defined on an undirected graph or Euclidean space of any\ndimensionality. Most of the existing methods for comparing topology are based\non Wasserstein distance between persistence barcodes and they don't take into\naccount the localization of topological features. On the other hand, the\nminimization of SFTD ensures that the corresponding topological features of\nscalar functions are located in the same places. The proposed tool provides\nuseful visualizations depicting areas where functions have topological\ndissimilarities. We provide applications of the proposed method to 3D computer\nvision. In particular, experiments demonstrate that SFTD improves the\nreconstruction of cellular 3D shapes from 2D fluorescence microscopy images,\nand helps to identify topological errors in 3D segmentation.\n","authors":["Ilya Trofimov","Daria Voronkova","Eduard Tulchinskii","Evgeny Burnaev","Serguei Barannikov"],"pdf_url":"https://arxiv.org/pdf/2407.08364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19632v2","updated":"2024-07-11T10:15:27Z","published":"2024-06-28T03:43:49Z","title":"PPTFormer: Pseudo Multi-Perspective Transformer for UAV Segmentation","summary":"  The ascension of Unmanned Aerial Vehicles (UAVs) in various fields\nnecessitates effective UAV image segmentation, which faces challenges due to\nthe dynamic perspectives of UAV-captured images. Traditional segmentation\nalgorithms falter as they cannot accurately mimic the complexity of UAV\nperspectives, and the cost of obtaining multi-perspective labeled datasets is\nprohibitive. To address these issues, we introduce the PPTFormer, a novel\n\\textbf{P}seudo Multi-\\textbf{P}erspective \\textbf{T}rans\\textbf{former}\nnetwork that revolutionizes UAV image segmentation. Our approach circumvents\nthe need for actual multi-perspective data by creating pseudo perspectives for\nenhanced multi-perspective learning. The PPTFormer network boasts Perspective\nRepresentation, novel Perspective Prototypes, and a specialized encoder and\ndecoder that together achieve superior segmentation results through Pseudo\nMulti-Perspective Attention (PMP Attention) and fusion. Our experiments\ndemonstrate that PPTFormer achieves state-of-the-art performance across five\nUAV segmentation datasets, confirming its capability to effectively simulate\nUAV flight perspectives and significantly advance segmentation precision. This\nwork presents a pioneering leap in UAV scene understanding and sets a new\nbenchmark for future developments in semantic segmentation.\n","authors":["Deyi Ji","Wenwei Jin","Hongtao Lu","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.19632v2.pdf","comment":"IJCAI 2024"},{"id":"http://arxiv.org/abs/2407.08356v1","updated":"2024-07-11T10:07:44Z","published":"2024-07-11T10:07:44Z","title":"Event-based vision on FPGAs -- a survey","summary":"  In recent years there has been a growing interest in event cameras, i.e.\nvision sensors that record changes in illumination independently for each\npixel. This type of operation ensures that acquisition is possible in very\nadverse lighting conditions, both in low light and high dynamic range, and\nreduces average power consumption. In addition, the independent operation of\neach pixel results in low latency, which is desirable for robotic solutions.\nNowadays, Field Programmable Gate Arrays (FPGAs), along with general-purpose\nprocessors (GPPs/CPUs) and programmable graphics processing units (GPUs), are\npopular architectures for implementing and accelerating computing tasks. In\nparticular, their usefulness in the embedded vision domain has been repeatedly\ndemonstrated over the past 30 years, where they have enabled fast data\nprocessing (even in real-time) and energy efficiency. Hence, the combination of\nevent cameras and reconfigurable devices seems to be a good solution,\nespecially in the context of energy-efficient real-time embedded systems. This\npaper gives an overview of the most important works, where FPGAs have been used\nin different contexts to process event data. It covers applications in the\nfollowing areas: filtering, stereovision, optical flow, acceleration of\nAI-based algorithms (including spiking neural networks) for object\nclassification, detection and tracking, and applications in robotics and\ninspection systems. Current trends and challenges for such systems are also\ndiscussed.\n","authors":["Tomasz Kryjak"],"pdf_url":"https://arxiv.org/pdf/2407.08356v1.pdf","comment":"Accepted for the 2024 27th Euromicro Conference on Digital System\n  Design (DSD)"},{"id":"http://arxiv.org/abs/2407.08349v1","updated":"2024-07-11T09:59:43Z","published":"2024-07-11T09:59:43Z","title":"Spine Vision X-Ray Image based GUI Planning of Pedicle Screws Using\n  Enhanced YOLOv5 for Vertebrae Segmentation","summary":"  In this paper, we propose an innovative Graphical User Interface (GUI) aimed\nat improving preoperative planning and intra-operative guidance for precise\nspinal screw placement through vertebrae segmentation. The methodology\nencompasses both front-end and back-end computations. The front end comprises a\nGUI that allows surgeons to precisely adjust the placement of screws on X-Ray\nimages, thereby improving the simulation of surgical screw insertion in the\npatient's spine. On the other hand, the back-end processing involves several\nsteps, including acquiring spinal X-ray images, performing pre-processing\ntechniques to reduce noise, and training a neural network model to achieve\nreal-time segmentation of the vertebrae. The integration of vertebral\nsegmentation in the GUI ensures precise screw placement, reducing complications\nlike nerve injury and ultimately improving surgical outcomes. The Spine-Vision\nprovides a comprehensive solution with innovative features like synchronous\nAP-LP planning, accurate screw positioning via vertebrae segmentation,\neffective screw visualization, and dynamic position adjustments. This X-ray\nimage-based GUI workflow emerges as a valuable tool, enhancing precision and\nsafety in spinal screw placement and planning procedures.\n","authors":["Yashwanth Rao","Gaurisankar S","Durga R","Aparna Purayath","Vivek Maik","Manojkumar Lakshmanan","Mohanasankar Sivaprakasm"],"pdf_url":"https://arxiv.org/pdf/2407.08349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05912v2","updated":"2024-07-11T09:58:51Z","published":"2024-03-09T13:37:02Z","title":"Mask-Enhanced Segment Anything Model for Tumor Lesion Semantic\n  Segmentation","summary":"  Tumor lesion segmentation on CT or MRI images plays a critical role in cancer\ndiagnosis and treatment planning. Considering the inherent differences in tumor\nlesion segmentation data across various medical imaging modalities and\nequipment, integrating medical knowledge into the Segment Anything Model (SAM)\npresents promising capability due to its versatility and generalization\npotential. Recent studies have attempted to enhance SAM with medical expertise\nby pre-training on large-scale medical segmentation datasets. However,\nchallenges still exist in 3D tumor lesion segmentation owing to tumor\ncomplexity and the imbalance in foreground and background regions. Therefore,\nwe introduce Mask-Enhanced SAM (M-SAM), an innovative architecture tailored for\n3D tumor lesion segmentation. We propose a novel Mask-Enhanced Adapter (MEA)\nwithin M-SAM that enriches the semantic information of medical images with\npositional data from coarse segmentation masks, facilitating the generation of\nmore precise segmentation masks. Furthermore, an iterative refinement scheme is\nimplemented in M-SAM to refine the segmentation masks progressively, leading to\nimproved performance. Extensive experiments on seven tumor lesion segmentation\ndatasets indicate that our M-SAM not only achieves high segmentation accuracy\nbut also exhibits robust generalization. The code is available at\nhttps://github.com/nanase1025/M-SAM.\n","authors":["Hairong Shi","Songhao Han","Shaofei Huang","Yue Liao","Guanbin Li","Xiangxing Kong","Hua Zhu","Xiaomu Wang","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05912v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08347v1","updated":"2024-07-11T09:55:42Z","published":"2024-07-11T09:55:42Z","title":"GUI-based Pedicle Screw Planning on Fluoroscopic Images Utilizing\n  Vertebral Segmentation","summary":"  The proposed work establishes a novel Graphical User Interface (GUI)\nframework, primarily designed for intraoperative pedicle screw planning.\nCurrent planning workflow in Image Guided Surgeries primarily relies on\npre-operative CT planning. Intraoperative CT planning can be time-consuming and\nexpensive and thus is not a common practice. In situations where efficiency and\ncost-effectiveness are paramount, planning to utilize fluoroscopic images\nacquired for image registration emerges as the optimal choice. The methodology\nproposed in this study employs a simulated 3D pedicle screw to calculate its\ncoronal and sagittal projections for pedicle screw planning using\nanterior-posterior (AP) and lateral (LP) images. The initialization and\nplacement of pedicle screw is computed by utilizing the bounding box of\nvertebral segmentation, which is obtained by the application of enhanced\nYOLOv5. The GUI front end includes functionality that allows surgeons or\nmedical practitioners to efficiently choose, set up, and dynamically maneuver\nthe pedicle screw on AP and LP images. This is based on a novel feature called\nsynchronous planning, which involves correlating pedicle screws from the\ncoronal and sagittal planes. This correlation utilizes projective\ncorrespondence to ensure that any movement of the pedicle screw in either the\nAP or LP image will be reflected in the other image. The proposed GUI framework\nis a time-efficient and cost-effective tool for synchronizing and planning the\nmovement of pedicle screws during intraoperative surgical procedures.\n","authors":["Vivek Maik","Aparna Purayath","Durga R","Manojkumar Lakshmanan","Mohanasankar Sivaprakasm"],"pdf_url":"https://arxiv.org/pdf/2407.08347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08341v1","updated":"2024-07-11T09:44:40Z","published":"2024-07-11T09:44:40Z","title":"Adaptive Deep Iris Feature Extractor at Arbitrary Resolutions","summary":"  This paper proposes a deep feature extractor for iris recognition at\narbitrary resolutions. Resolution degradation reduces the recognition\nperformance of deep learning models trained by high-resolution images. Using\nvarious-resolution images for training can improve the model's robustness while\nsacrificing recognition performance for high-resolution images. To achieve\nhigher recognition performance at various resolutions, we propose a method of\nresolution-adaptive feature extraction with automatically switching networks.\nOur framework includes resolution expert modules specialized for different\nresolution degradations, including down-sampling and out-of-focus blurring. The\nframework automatically switches them depending on the degradation condition of\nan input image. Lower-resolution experts are trained by knowledge-distillation\nfrom the high-resolution expert in such a manner that both experts can extract\ncommon identity features. We applied our framework to three conventional neural\nnetwork models. The experimental results show that our method enhances the\nrecognition performance at low-resolution in the conventional methods and also\nmaintains their performance at high-resolution.\n","authors":["Yuho Shoji","Yuka Ogino","Takahiro Toizumi","Atsushi Ito"],"pdf_url":"https://arxiv.org/pdf/2407.08341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04189v3","updated":"2024-07-11T09:39:02Z","published":"2023-10-06T12:08:15Z","title":"Bridging the Gap between Human Motion and Action Semantics via Kinematic\n  Phrases","summary":"  Motion understanding aims to establish a reliable mapping between motion and\naction semantics, while it is a challenging many-to-many problem. An abstract\naction semantic (i.e., walk forwards) could be conveyed by perceptually diverse\nmotions (walking with arms up or swinging). In contrast, a motion could carry\ndifferent semantics w.r.t. its context and intention. This makes an elegant\nmapping between them difficult. Previous attempts adopted direct-mapping\nparadigms with limited reliability. Also, current automatic metrics fail to\nprovide reliable assessments of the consistency between motions and action\nsemantics. We identify the source of these problems as the significant gap\nbetween the two modalities. To alleviate this gap, we propose Kinematic Phrases\n(KP) that take the objective kinematic facts of human motion with proper\nabstraction, interpretability, and generality. Based on KP, we can unify a\nmotion knowledge base and build a motion understanding system. Meanwhile, KP\ncan be automatically converted from motions to text descriptions with no\nsubjective bias, inspiring Kinematic Prompt Generation (KPG) as a novel\nwhite-box motion generation benchmark. In extensive experiments, our approach\nshows superiority over other methods. Our project is available at\nhttps://foruck.github.io/KP/.\n","authors":["Xinpeng Liu","Yong-Lu Li","Ailing Zeng","Zizheng Zhou","Yang You","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2310.04189v3.pdf","comment":"To appear in ECCV 2024. Yong-Lu Li and Cewu Lu are the corresponding\n  authors. Project page is available at https://foruck.github.io/KP/"},{"id":"http://arxiv.org/abs/2407.07340v2","updated":"2024-07-11T09:36:49Z","published":"2024-07-10T03:24:40Z","title":"FALFormer: Feature-aware Landmarks self-attention for Whole-slide Image\n  Classification","summary":"  Slide-level classification for whole-slide images (WSIs) has been widely\nrecognized as a crucial problem in digital and computational pathology. Current\napproaches commonly consider WSIs as a bag of cropped patches and process them\nvia multiple instance learning due to the large number of patches, which cannot\nfully explore the relationship among patches; in other words, the global\ninformation cannot be fully incorporated into decision making. Herein, we\npropose an efficient and effective slide-level classification model, named as\nFALFormer, that can process a WSI as a whole so as to fully exploit the\nrelationship among the entire patches and to improve the classification\nperformance. FALFormer is built based upon Transformers and self-attention\nmechanism. To lessen the computational burden of the original self-attention\nmechanism and to process the entire patches together in a WSI, FALFormer\nemploys Nystr\\\"om self-attention which approximates the computation by using a\nsmaller number of tokens or landmarks. For effective learning, FALFormer\nintroduces feature-aware landmarks to enhance the representation power of the\nlandmarks and the quality of the approximation. We systematically evaluate the\nperformance of FALFormer using two public datasets, including CAMELYON16 and\nTCGA-BRCA. The experimental results demonstrate that FALFormer achieves\nsuperior performance on both datasets, outperforming the state-of-the-art\nmethods for the slide-level classification. This suggests that FALFormer can\nfacilitate an accurate and precise analysis of WSIs, potentially leading to\nimproved diagnosis and prognosis on WSIs.\n","authors":["Doanh C. Bui","Trinh Thi Le Vuong","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.07340v2.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.08333v1","updated":"2024-07-11T09:34:31Z","published":"2024-07-11T09:34:31Z","title":"SR-Mamba: Effective Surgical Phase Recognition with State Space Model","summary":"  Surgical phase recognition is crucial for enhancing the efficiency and safety\nof computer-assisted interventions. One of the fundamental challenges involves\nmodeling the long-distance temporal relationships present in surgical videos.\nInspired by the recent success of Mamba, a state space model with linear\nscalability in sequence length, this paper presents SR-Mamba, a novel\nattention-free model specifically tailored to meet the challenges of surgical\nphase recognition. In SR-Mamba, we leverage a bidirectional Mamba decoder to\neffectively model the temporal context in overlong sequences. Moreover, the\nefficient optimization of the proposed Mamba decoder facilitates single-step\nneural network training, eliminating the need for separate training steps as in\nprevious works. This single-step training approach not only simplifies the\ntraining process but also ensures higher accuracy, even with a lighter spatial\nfeature extractor. Our SR-Mamba establishes a new benchmark in surgical video\nanalysis by demonstrating state-of-the-art performance on the Cholec80 and\nCATARACTS Challenge datasets. The code is accessible at\nhttps://github.com/rcao-hk/SR-Mamba.\n","authors":["Rui Cao","Jiangliu Wang","Yun-Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08333v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2303.05780v2","updated":"2024-07-11T08:52:14Z","published":"2023-03-10T08:29:35Z","title":"TAKT: Target-Aware Knowledge Transfer for Whole Slide Image\n  Classification","summary":"  Transferring knowledge from a source domain to a target domain can be crucial\nfor whole slide image classification, since the number of samples in a dataset\nis often limited due to high annotation costs. However, domain shift and task\ndiscrepancy between datasets can hinder effective knowledge transfer. In this\npaper, we propose a Target-Aware Knowledge Transfer framework, employing a\nteacher-student paradigm. Our framework enables the teacher model to learn\ncommon knowledge from the source and target domains by actively incorporating\nunlabelled target images into the training of the teacher model. The teacher\nbag features are subsequently adapted to supervise the training of the student\nmodel on the target domain. Despite incorporating the target features during\ntraining, the teacher model tends to overlook them under the inherent domain\nshift and task discrepancy. To alleviate this, we introduce a target-aware\nfeature alignment module to establish a transferable latent relationship\nbetween the source and target features by solving the optimal transport\nproblem. Experimental results show that models employing knowledge transfer\noutperform those trained from scratch, and our method achieves state-of-the-art\nperformance among other knowledge transfer methods on various datasets,\nincluding TCGA-RCC, TCGA-NSCLC, and Camelyon16.\n","authors":["Conghao Xiong","Yi Lin","Hao Chen","Hao Zheng","Dong Wei","Yefeng Zheng","Joseph J. Y. Sung","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2303.05780v2.pdf","comment":"Accepted by MICCAI2024"},{"id":"http://arxiv.org/abs/2407.08303v1","updated":"2024-07-11T08:48:06Z","published":"2024-07-11T08:48:06Z","title":"DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal\n  Perception","summary":"  Existing Multimodal Large Language Models (MLLMs) increasingly emphasize\ncomplex understanding of various visual elements, including multiple objects,\ntext information, and spatial relations. Their development for comprehensive\nvisual perception hinges on the availability of high-quality image-text\ndatasets that offer diverse visual elements and throughout image descriptions.\nHowever, the scarcity of such hyper-detailed datasets currently hinders\nprogress within the MLLM community. The bottleneck stems from the limited\nperceptual capabilities of current caption engines, which fall short in\nproviding complete and accurate annotations. To facilitate the cutting-edge\nresearch of MLLMs on comprehensive vision perception, we thereby propose\nPerceptual Fusion, using a low-budget but highly effective caption engine for\ncomplete and accurate image descriptions. Specifically, Perceptual Fusion\nintegrates diverse perception experts as image priors to provide explicit\ninformation on visual elements and adopts an efficient MLLM as a centric pivot\nto mimic advanced MLLMs' perception abilities. We carefully select 1M highly\nrepresentative images from uncurated LAION dataset and generate dense\ndescriptions using our engine, dubbed DenseFusion-1M. Extensive experiments\nvalidate that our engine outperforms its counterparts, where the resulting\ndataset significantly improves the perception and cognition abilities of\nexisting MLLMs across diverse vision-language benchmarks, especially with\nhigh-resolution images as inputs. The dataset and code are publicly available\nat https://github.com/baaivision/DenseFusion.\n","authors":["Xiaotong Li","Fan Zhang","Haiwen Diao","Yueze Wang","Xinlong Wang","Ling-Yu Duan"],"pdf_url":"https://arxiv.org/pdf/2407.08303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06154v2","updated":"2024-07-11T08:47:31Z","published":"2024-04-09T09:27:54Z","title":"Concise Plane Arrangements for Low-Poly Surface and Volume Modelling","summary":"  Plane arrangements are a useful tool for surface and volume modelling.\nHowever, their main drawback is poor scalability. We introduce two key\nnovelties that enable the construction of plane arrangements for complex\nobjects and entire scenes: (i) an ordering scheme for the plane insertion and\n(ii) the direct use of input points during arrangement construction. Both\ningredients reduce the number of unwanted splits, resulting in improved\nscalability of the construction mechanism by up to two orders of magnitude\ncompared to existing algorithms. We further introduce a remeshing and\nsimplification technique that allows us to extract low-polygon surface meshes\nand lightweight convex decompositions of volumes from the arrangement. We show\nthat our approach leads to state-of-the-art results for the aforementioned\ntasks by comparing it to learning-based and traditional approaches on various\ndifferent datasets. Our implementation is available at\nhttps://github.com/raphaelsulzer/compod .\n","authors":["Raphael Sulzer","Florent Lafarge"],"pdf_url":"https://arxiv.org/pdf/2404.06154v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08298v1","updated":"2024-07-11T08:44:43Z","published":"2024-07-11T08:44:43Z","title":"XAI-Guided Enhancement of Vegetation Indices for Crop Mapping","summary":"  Vegetation indices allow to efficiently monitor vegetation growth and\nagricultural activities. Previous generations of satellites were capturing a\nlimited number of spectral bands, and a few expert-designed vegetation indices\nwere sufficient to harness their potential. New generations of multi- and\nhyperspectral satellites can however capture additional bands, but are not yet\nefficiently exploited. In this work, we propose an explainable-AI-based method\nto select and design suitable vegetation indices. We first train a deep neural\nnetwork using multispectral satellite data, then extract feature importance to\nidentify the most influential bands. We subsequently select suitable existing\nvegetation indices or modify them to incorporate the identified bands and\nretrain our model. We validate our approach on a crop classification task. Our\nresults indicate that models trained on individual indices achieve comparable\nresults to the baseline model trained on all bands, while the combination of\ntwo indices surpasses the baseline in certain cases.\n","authors":["Hiba Najjar","Francisco Mena","Marlon Nuske","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2407.08298v1.pdf","comment":"Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2024"},{"id":"http://arxiv.org/abs/2407.08290v1","updated":"2024-07-11T08:36:40Z","published":"2024-07-11T08:36:40Z","title":"Gap Completion in Point Cloud Scene occluded by Vehicles using SGC-Net","summary":"  Recent advances in mobile mapping systems have greatly enhanced the\nefficiency and convenience of acquiring urban 3D data. These systems utilize\nLiDAR sensors mounted on vehicles to capture vast cityscapes. However, a\nsignificant challenge arises due to occlusions caused by roadside parked\nvehicles, leading to the loss of scene information, particularly on the roads,\nsidewalks, curbs, and the lower sections of buildings. In this study, we\npresent a novel approach that leverages deep neural networks to learn a model\ncapable of filling gaps in urban scenes that are obscured by vehicle occlusion.\nWe have developed an innovative technique where we place virtual vehicle models\nalong road boundaries in the gap-free scene and utilize a ray-casting algorithm\nto create a new scene with occluded gaps. This allows us to generate diverse\nand realistic urban point cloud scenes with and without vehicle occlusion,\nsurpassing the limitations of real-world training data collection and\nannotation. Furthermore, we introduce the Scene Gap Completion Network\n(SGC-Net), an end-to-end model that can generate well-defined shape boundaries\nand smooth surfaces within occluded gaps. The experiment results reveal that\n97.66% of the filled points fall within a range of 5 centimeters relative to\nthe high-density ground truth point cloud scene. These findings underscore the\nefficacy of our proposed model in gap completion and reconstructing urban\nscenes affected by vehicle occlusions.\n","authors":["Yu Feng","Yiming Xu","Yan Xia","Claus Brenner","Monika Sester"],"pdf_url":"https://arxiv.org/pdf/2407.08290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17081v3","updated":"2024-07-11T08:36:35Z","published":"2023-11-28T00:43:52Z","title":"I-MedSAM: Implicit Medical Image Segmentation with Segment Anything","summary":"  With the development of Deep Neural Networks (DNNs), many efforts have been\nmade to handle medical image segmentation. Traditional methods such as nnUNet\ntrain specific segmentation models on the individual datasets. Plenty of recent\nmethods have been proposed to adapt the foundational Segment Anything Model\n(SAM) to medical image segmentation. However, they still focus on discrete\nrepresentations to generate pixel-wise predictions, which are spatially\ninflexible and scale poorly to higher resolution. In contrast, implicit methods\nlearn continuous representations for segmentation, which is crucial for medical\nimage segmentation. In this paper, we propose I-MedSAM, which leverages the\nbenefits of both continuous representations and SAM, to obtain better\ncross-domain ability and accurate boundary delineation. Since medical image\nsegmentation needs to predict detailed segmentation boundaries, we designed a\nnovel adapter to enhance the SAM features with high-frequency information\nduring Parameter-Efficient Fine-Tuning (PEFT). To convert the SAM features and\ncoordinates into continuous segmentation output, we utilize Implicit Neural\nRepresentation (INR) to learn an implicit segmentation decoder. We also propose\nan uncertainty-guided sampling strategy for efficient learning of INR.\nExtensive evaluations on 2D medical image segmentation tasks have shown that\nour proposed method with only 1.6M trainable parameters outperforms existing\nmethods including discrete and implicit methods. The code will be available at:\nhttps://github.com/ucwxb/I-MedSAM.\n","authors":["Xiaobao Wei","Jiajun Cao","Yizhu Jin","Ming Lu","Guangyu Wang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.17081v3.pdf","comment":"accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.08280v1","updated":"2024-07-11T08:29:45Z","published":"2024-07-11T08:29:45Z","title":"WayveScenes101: A Dataset and Benchmark for Novel View Synthesis in\n  Autonomous Driving","summary":"  We present WayveScenes101, a dataset designed to help the community advance\nthe state of the art in novel view synthesis that focuses on challenging\ndriving scenes containing many dynamic and deformable elements with changing\ngeometry and texture. The dataset comprises 101 driving scenes across a wide\nrange of environmental conditions and driving scenarios. The dataset is\ndesigned for benchmarking reconstructions on in-the-wild driving scenes, with\nmany inherent challenges for scene reconstruction methods including image\nglare, rapid exposure changes, and highly dynamic scenes with significant\nocclusion. Along with the raw images, we include COLMAP-derived camera poses in\nstandard data formats. We propose an evaluation protocol for evaluating models\non held-out camera views that are off-axis from the training views,\nspecifically testing the generalisation capabilities of methods. Finally, we\nprovide detailed metadata for all scenes, including weather, time of day, and\ntraffic conditions, to allow for a detailed model performance breakdown across\nscene characteristics. Dataset and code are available at\nhttps://github.com/wayveai/wayve_scenes.\n","authors":["Jannik ZÃ¼rn","Paul Gladkov","SofÃ­a Dudas","Fergal Cotter","Sofi Toteva","Jamie Shotton","Vasiliki Simaiaki","Nikhil Mohan"],"pdf_url":"https://arxiv.org/pdf/2407.08280v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2407.08277v1","updated":"2024-07-11T08:25:51Z","published":"2024-07-11T08:25:51Z","title":"StixelNExT: Toward Monocular Low-Weight Perception for Object\n  Segmentation and Free Space Detection","summary":"  In this work, we present a novel approach for general object segmentation\nfrom a monocular image, eliminating the need for manually labeled training data\nand enabling rapid, straightforward training and adaptation with minimal data.\nOur model initially learns from LiDAR during the training process, which is\nsubsequently removed from the system, allowing it to function solely on\nmonocular imagery. This study leverages the concept of the Stixel-World to\nrecognize a medium level representation of its surroundings. Our network\ndirectly predicts a 2D multi-layer Stixel-World and is capable of recognizing\nand locating multiple, superimposed objects within an image. Due to the\nscarcity of comparable works, we have divided the capabilities into modules and\npresent a free space detection in our experiments section. Furthermore, we\nintroduce an improved method for generating Stixels from LiDAR data, which we\nuse as ground truth for our network.\n","authors":["Marcel Vosshans","Omar Ait-Aider","Youcef Mezouar","Markus Enzweiler"],"pdf_url":"https://arxiv.org/pdf/2407.08277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09079v2","updated":"2024-07-11T08:22:46Z","published":"2024-03-14T03:52:33Z","title":"PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF\n  Priors","summary":"  Autonomous vehicles rely extensively on perception systems to navigate and\ninterpret their surroundings. Despite significant advancements in these systems\nrecently, challenges persist under conditions like occlusion, extreme lighting,\nor in unfamiliar urban areas. Unlike these systems, humans do not solely depend\non immediate observations to perceive the environment. In navigating new\ncities, humans gradually develop a preliminary mental map to supplement\nreal-time perception during subsequent visits. Inspired by this human approach,\nwe introduce a novel framework, PreSight, that leverages past traversals to\nconstruct static prior memories, enhancing online perception in later\nnavigations. Our method involves optimizing a city-scale neural radiance field\nwith data from previous journeys to generate neural priors. These priors, rich\nin semantic and geometric details, are derived without manual annotations and\ncan seamlessly augment various state-of-the-art perception models, improving\ntheir efficacy with minimal additional computational cost. Experimental results\non the nuScenes dataset demonstrate the framework's high compatibility with\ndiverse online perception models. Specifically, it shows remarkable\nimprovements in HD-map construction and occupancy prediction tasks,\nhighlighting its potential as a new perception framework for autonomous driving\nsystems. Our code will be released at\nhttps://github.com/yuantianyuan01/PreSight.\n","authors":["Tianyuan Yuan","Yucheng Mao","Jiawei Yang","Yicheng Liu","Yue Wang","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.09079v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08272v1","updated":"2024-07-11T08:17:35Z","published":"2024-07-11T08:17:35Z","title":"PowerYOLO: Mixed Precision Model for Hardware Efficient Object Detection\n  with Event Data","summary":"  The performance of object detection systems in automotive solutions must be\nas high as possible, with minimal response time and, due to the often\nbattery-powered operation, low energy consumption. When designing such\nsolutions, we therefore face challenges typical for embedded vision systems:\nthe problem of fitting algorithms of high memory and computational complexity\ninto small low-power devices. In this paper we propose PowerYOLO - a mixed\nprecision solution, which targets three essential elements of such application.\nFirst, we propose a system based on a Dynamic Vision Sensor (DVS), a novel\nsensor, that offers low power requirements and operates well in conditions with\nvariable illumination. It is these features that may make event cameras a\npreferential choice over frame cameras in some applications. Second, to ensure\nhigh accuracy and low memory and computational complexity, we propose to use\n4-bit width Powers-of-Two (PoT) quantisation for convolution weights of the\nYOLO detector, with all other parameters quantised linearly. Finally, we\nembrace from PoT scheme and replace multiplication with bit-shifting to\nincrease the efficiency of hardware acceleration of such solution, with a\nspecial convolution-batch normalisation fusion scheme. The use of specific\nsensor with PoT quantisation and special batch normalisation fusion leads to a\nunique system with almost 8x reduction in memory complexity and vast\ncomputational simplifications, with relation to a standard approach. This\nefficient system achieves high accuracy of mAP 0.301 on the GEN1 DVS dataset,\nmarking the new state-of-the-art for such compressed model.\n","authors":["Dominika Przewlocka-Rus","Tomasz Kryjak","Marek Gorgon"],"pdf_url":"https://arxiv.org/pdf/2407.08272v1.pdf","comment":"The paper has been accepted for the 27th Euromicro Conference Series\n  on Digital System Design (DSD) 2024"},{"id":"http://arxiv.org/abs/2312.03661v2","updated":"2024-07-11T08:15:19Z","published":"2023-12-06T18:32:33Z","title":"Reason2Drive: Towards Interpretable and Chain-based Reasoning for\n  Autonomous Driving","summary":"  Large vision-language models (VLMs) have garnered increasing interest in\nautonomous driving areas, due to their advanced capabilities in complex\nreasoning tasks essential for highly autonomous vehicle behavior. Despite their\npotential, research in autonomous systems is hindered by the lack of datasets\nwith annotated reasoning chains that explain the decision-making processes in\ndriving. To bridge this gap, we present Reason2Drive, a benchmark dataset with\nover 600K video-text pairs, aimed at facilitating the study of interpretable\nreasoning in complex driving environments. We distinctly characterize the\nautonomous driving process as a sequential combination of perception,\nprediction, and reasoning steps, and the question-answer pairs are\nautomatically collected from a diverse range of open-source outdoor driving\ndatasets, including nuScenes, Waymo and ONCE. Moreover, we introduce a novel\naggregated evaluation metric to assess chain-based reasoning performance in\nautonomous systems, addressing the semantic ambiguities of existing metrics\nsuch as BLEU and CIDEr. Based on the proposed benchmark, we conduct experiments\nto assess various existing VLMs, revealing insights into their reasoning\ncapabilities. Additionally, we develop an efficient approach to empower VLMs to\nleverage object-level perceptual elements in both feature extraction and\nprediction, further enhancing their reasoning accuracy. The code and dataset\nwill be released.\n","authors":["Ming Nie","Renyuan Peng","Chunwei Wang","Xinyue Cai","Jianhua Han","Hang Xu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.03661v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08268v1","updated":"2024-07-11T08:12:16Z","published":"2024-07-11T08:12:16Z","title":"Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic\n  Segmentation","summary":"  CLIP, as a vision-language model, has significantly advanced Open-Vocabulary\nSemantic Segmentation (OVSS) with its zero-shot capabilities. Despite its\nsuccess, its application to OVSS faces challenges due to its initial\nimage-level alignment training, which affects its performance in tasks\nrequiring detailed local context. Our study delves into the impact of CLIP's\n[CLS] token on patch feature correlations, revealing a dominance of \"global\"\npatches that hinders local feature discrimination. To overcome this, we propose\nCLIPtrase, a novel training-free semantic segmentation strategy that enhances\nlocal feature awareness through recalibrated self-correlation among patches.\nThis approach demonstrates notable improvements in segmentation accuracy and\nthe ability to maintain semantic coherence across objects.Experiments show that\nwe are 22.3% ahead of CLIP on average on 9 segmentation benchmarks,\noutperforming existing state-of-the-art training-free methods.The code are made\npublicly available at: https://github.com/leaves162/CLIPtrase.\n","authors":["Tong Shao","Zhuotao Tian","Hang Zhao","Jingyong Su"],"pdf_url":"https://arxiv.org/pdf/2407.08268v1.pdf","comment":"ECCV24 accepted"},{"id":"http://arxiv.org/abs/2403.10911v3","updated":"2024-07-11T08:12:07Z","published":"2024-03-16T12:18:20Z","title":"Efficient Diffusion-Driven Corruption Editor for Test-Time Adaptation","summary":"  Test-time adaptation (TTA) addresses the unforeseen distribution shifts\noccurring during test time. In TTA, performance, memory consumption, and time\nconsumption are crucial considerations. A recent diffusion-based TTA approach\nfor restoring corrupted images involves image-level updates. However, using\npixel space diffusion significantly increases resource requirements compared to\nconventional model updating TTA approaches, revealing limitations as a TTA\nmethod. To address this, we propose a novel TTA method that leverages an image\nediting model based on a latent diffusion model (LDM) and fine-tunes it using\nour newly introduced corruption modeling scheme. This scheme enhances the\nrobustness of the diffusion model against distribution shifts by creating\n(clean, corrupted) image pairs and fine-tuning the model to edit corrupted\nimages into clean ones. Moreover, we introduce a distilled variant to\naccelerate the model for corruption editing using only 4 network function\nevaluations (NFEs). We extensively validated our method across various\narchitectures and datasets including image and video domains. Our model\nachieves the best performance with a 100 times faster runtime than that of a\ndiffusion-based baseline. Furthermore, it is three times faster than the\nprevious model updating TTA method that utilizes data augmentation, making an\nimage-level updating approach more feasible.\n","authors":["Yeongtak Oh","Jonghyun Lee","Jooyoung Choi","Dahuin Jung","Uiwon Hwang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2403.10911v3.pdf","comment":"ECCV 2024 Camera Ready"},{"id":"http://arxiv.org/abs/2403.09885v2","updated":"2024-07-11T08:09:49Z","published":"2024-03-14T21:38:00Z","title":"GazeMotion: Gaze-guided Human Motion Forecasting","summary":"  We present GazeMotion, a novel method for human motion forecasting that\ncombines information on past human poses with human eye gaze. Inspired by\nevidence from behavioural sciences showing that human eye and body movements\nare closely coordinated, GazeMotion first predicts future eye gaze from past\ngaze, then fuses predicted future gaze and past poses into a gaze-pose graph,\nand finally uses a residual graph convolutional network to forecast body\nmotion. We extensively evaluate our method on the MoGaze, ADT, and GIMO\nbenchmark datasets and show that it outperforms state-of-the-art methods by up\nto 7.4% improvement in mean per joint position error. Using head direction as a\nproxy to gaze, our method still achieves an average improvement of 5.5%. We\nfinally report an online user study showing that our method also outperforms\nprior methods in terms of perceived realism. These results show the significant\ninformation content available in eye gaze for human motion forecasting as well\nas the effectiveness of our method in exploiting this information.\n","authors":["Zhiming Hu","Syn Schmitt","Daniel Haeufle","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2403.09885v2.pdf","comment":"Accepted at IROS 2024 as Oral Presentation. Code available at\n  https://zhiminghu.net/hu24_gazemotion.html"},{"id":"http://arxiv.org/abs/2208.07174v4","updated":"2024-07-11T08:07:03Z","published":"2022-08-15T13:21:41Z","title":"A Human-in-the-Middle Attack against Object Detection Systems","summary":"  Object detection systems using deep learning models have become increasingly\npopular in robotics thanks to the rising power of CPUs and GPUs in embedded\nsystems. However, these models are susceptible to adversarial attacks. While\nsome attacks are limited by strict assumptions on access to the detection\nsystem, we propose a novel hardware attack inspired by Man-in-the-Middle\nattacks in cryptography. This attack generates a Universal Adversarial\nPerturbations (UAP) and injects the perturbation between the USB camera and the\ndetection system via a hardware attack. Besides, prior research is misled by an\nevaluation metric that measures the model accuracy rather than the attack\nperformance. In combination with our proposed evaluation metrics, we\nsignificantly increased the strength of adversarial perturbations. These\nfindings raise serious concerns for applications of deep learning models in\nsafety-critical systems, such as autonomous driving.\n","authors":["Han Wu","Sareh Rowlands","Johan Wahlstrom"],"pdf_url":"https://arxiv.org/pdf/2208.07174v4.pdf","comment":"Accepted by IEEE Transactions on Artificial Intelligence, 2024"},{"id":"http://arxiv.org/abs/2407.08265v1","updated":"2024-07-11T08:06:31Z","published":"2024-07-11T08:06:31Z","title":"Enhancing Thermal Infrared Tracking with Natural Language Modeling and\n  Coordinate Sequence Generation","summary":"  Thermal infrared tracking is an essential topic in computer vision tasks\nbecause of its advantage of all-weather imaging. However, most conventional\nmethods utilize only hand-crafted features, while deep learning-based\ncorrelation filtering methods are limited by simple correlation operations.\nTransformer-based methods ignore temporal and coordinate information, which is\ncritical for TIR tracking that lacks texture and color information. In this\npaper, to address these issues, we apply natural language modeling to TIR\ntracking and propose a novel model called NLMTrack, which enhances the\nutilization of coordinate and temporal information. NLMTrack applies an encoder\nthat unifies feature extraction and feature fusion, which simplifies the TIR\ntracking pipeline. To address the challenge of low detail and low contrast in\nTIR images, on the one hand, we design a multi-level progressive fusion module\nthat enhances the semantic representation and incorporates multi-scale\nfeatures. On the other hand, the decoder combines the TIR features and the\ncoordinate sequence features using a causal transformer to generate the target\nsequence step by step. Moreover, we explore an adaptive loss aimed at elevating\ntracking accuracy and a simple template update strategy to accommodate the\ntarget's appearance variations. Experiments show that NLMTrack achieves\nstate-of-the-art performance on multiple benchmarks. The Code is publicly\navailable at \\url{https://github.com/ELOESZHANG/NLMTrack}.\n","authors":["Miao Yan","Ping Zhang","Haofei Zhang","Ruqian Hao","Juanxiu Liu","Xiaoyang Wang","Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08260v1","updated":"2024-07-11T08:00:19Z","published":"2024-07-11T08:00:19Z","title":"SALSA: Swift Adaptive Lightweight Self-Attention for Enhanced LiDAR\n  Place Recognition","summary":"  Large-scale LiDAR mappings and localization leverage place recognition\ntechniques to mitigate odometry drifts, ensuring accurate mapping. These\ntechniques utilize scene representations from LiDAR point clouds to identify\npreviously visited sites within a database. Local descriptors, assigned to each\npoint within a point cloud, are aggregated to form a scene representation for\nthe point cloud. These descriptors are also used to re-rank the retrieved point\nclouds based on geometric fitness scores. We propose SALSA, a novel,\nlightweight, and efficient framework for LiDAR place recognition. It consists\nof a Sphereformer backbone that uses radial window attention to enable\ninformation aggregation for sparse distant points, an adaptive self-attention\nlayer to pool local descriptors into tokens, and a multi-layer-perceptron Mixer\nlayer for aggregating the tokens to generate a scene descriptor. The proposed\nframework outperforms existing methods on various LiDAR place recognition\ndatasets in terms of both retrieval and metric localization while operating in\nreal-time.\n","authors":["Raktim Gautam Goswami","Naman Patel","Prashanth Krishnamurthy","Farshad Khorrami"],"pdf_url":"https://arxiv.org/pdf/2407.08260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13570v2","updated":"2024-07-11T07:59:02Z","published":"2024-03-20T13:09:54Z","title":"Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer","summary":"  In this paper, we propose a novel learning approach for feed-forward one-shot\n4D head avatar synthesis. Different from existing methods that often learn from\nreconstructing monocular videos guided by 3DMM, we employ pseudo multi-view\nvideos to learn a 4D head synthesizer in a data-driven manner, avoiding\nreliance on inaccurate 3DMM reconstruction that could be detrimental to the\nsynthesis performance. The key idea is to first learn a 3D head synthesizer\nusing synthetic multi-view images to convert monocular real videos into\nmulti-view ones, and then utilize the pseudo multi-view videos to learn a 4D\nhead synthesizer via cross-view self-reenactment. By leveraging a simple vision\ntransformer backbone with motion-aware cross-attentions, our method exhibits\nsuperior performance compared to previous methods in terms of reconstruction\nfidelity, geometry consistency, and motion control accuracy. We hope our method\noffers novel insights into integrating 3D priors with 2D supervisions for\nimproved 4D head avatar creation.\n","authors":["Yu Deng","Duomin Wang","Baoyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13570v2.pdf","comment":"ECCV 24 camera ready version. Project page:\n  https://yudeng.github.io/Portrait4D-v2/"},{"id":"http://arxiv.org/abs/2407.08257v1","updated":"2024-07-11T07:57:33Z","published":"2024-07-11T07:57:33Z","title":"Knowledge distillation to effectively attain both region-of-interest and\n  global semantics from an image where multiple objects appear","summary":"  Models based on convolutional neural networks (CNN) and transformers have\nsteadily been improved. They also have been applied in various computer vision\ndownstream tasks. However, in object detection tasks, accurately localizing and\nclassifying almost infinite categories of foods in images remains challenging.\nTo address these problems, we first segmented the food as the\nregion-of-interest (ROI) by using the segment-anything model (SAM) and masked\nthe rest of the region except ROI as black pixels. This process simplified the\nproblems into a single classification for which annotation and training were\nmuch simpler than object detection. The images in which only the ROI was\npreserved were fed as inputs to fine-tune various off-the-shelf models that\nencoded their own inductive biases. Among them, Data-efficient image\nTransformers (DeiTs) had the best classification performance. Nonetheless, when\nfoods' shapes and textures were similar, the contextual features of the\nROI-only images were not enough for accurate classification. Therefore, we\nintroduced a novel type of combined architecture, RveRNet, which consisted of\nROI, extra-ROI, and integration modules that allowed it to account for both the\nROI's and global contexts. The RveRNet's F1 score was 10% better than other\nindividual models when classifying ambiguous food images. If the RveRNet's\nmodules were DeiT with the knowledge distillation from the CNN, performed the\nbest. We investigated how architectures can be made robust against input noise\ncaused by permutation and translocation. The results indicated that there was a\ntrade-off between how much the CNN teacher's knowledge could be distilled to\nDeiT and DeiT's innate strength. Code is publicly available at:\nhttps://github.com/Seonwhee-Genome/RveRNet.\n","authors":["Seonwhee Jin"],"pdf_url":"https://arxiv.org/pdf/2407.08257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08256v1","updated":"2024-07-11T07:56:17Z","published":"2024-07-11T07:56:17Z","title":"Adaptive Compressed Sensing with Diffusion-Based Posterior Sampling","summary":"  Compressed Sensing (CS) facilitates rapid image acquisition by selecting a\nsmall subset of measurements sufficient for high-fidelity reconstruction.\nAdaptive CS seeks to further enhance this process by dynamically choosing\nfuture measurements based on information gleaned from data that is already\nacquired. However, many existing frameworks are often tailored to specific\ntasks and require intricate training procedures. We propose AdaSense, a novel\nAdaptive CS approach that leverages zero-shot posterior sampling with\npre-trained diffusion models. By sequentially sampling from the posterior\ndistribution, we can quantify the uncertainty of each possible future linear\nmeasurement throughout the acquisition process. AdaSense eliminates the need\nfor additional training and boasts seamless adaptation to diverse domains with\nminimal tuning requirements. Our experiments demonstrate the effectiveness of\nAdaSense in reconstructing facial images from a small number of measurements.\nFurthermore, we apply AdaSense for active acquisition of medical images in the\ndomains of magnetic resonance imaging (MRI) and computed tomography (CT),\nhighlighting its potential for tangible real-world acceleration.\n","authors":["Noam Elata","Tomer Michaeli","Michael Elad"],"pdf_url":"https://arxiv.org/pdf/2407.08256v1.pdf","comment":"Published in European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2407.08255v1","updated":"2024-07-11T07:56:08Z","published":"2024-07-11T07:56:08Z","title":"GraphMamba: An Efficient Graph Structure Learning Vision Mamba for\n  Hyperspectral Image Classification","summary":"  Efficient extraction of spectral sequences and geospatial information has\nalways been a hot topic in hyperspectral image classification. In terms of\nspectral sequence feature capture, RNN and Transformer have become mainstream\nclassification frameworks due to their long-range feature capture capabilities.\nIn terms of spatial information aggregation, CNN enhances the receptive field\nto retain integrated spatial information as much as possible. However, the\nspectral feature-capturing architectures exhibit low computational efficiency,\nand CNNs lack the flexibility to perceive spatial contextual information. To\naddress these issues, this paper proposes GraphMamba--an efficient graph\nstructure learning vision Mamba classification framework that fully considers\nHSI characteristics to achieve deep spatial-spectral information mining.\nSpecifically, we propose a novel hyperspectral visual GraphMamba processing\nparadigm (HVGM) that preserves spatial-spectral features by constructing\nspatial-spectral cubes and utilizes linear spectral encoding to enhance the\noperability of subsequent tasks. The core components of GraphMamba include the\nHyperMamba module for improving computational efficiency and the SpectralGCN\nmodule for adaptive spatial context awareness. The HyperMamba mitigates clutter\ninterference by employing the global mask (GM) and introduces a parallel\ntraining inference architecture to alleviate computational bottlenecks. The\nSpatialGCN incorporates weighted multi-hop aggregation (WMA) spatial encoding\nto focus on highly correlated spatial structural features, thus flexibly\naggregating contextual information while mitigating spatial noise interference.\nExtensive experiments were conducted on three different scales of real HSI\ndatasets, and compared with the state-of-the-art classification frameworks,\nGraphMamba achieved optimal performance.\n","authors":["Aitao Yang","Min Li","Yao Ding","Leyuan Fang","Yaoming Cai","Yujie He"],"pdf_url":"https://arxiv.org/pdf/2407.08255v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.08252v1","updated":"2024-07-11T07:54:43Z","published":"2024-07-11T07:54:43Z","title":"Spatially-Variant Degradation Model for Dataset-free Super-resolution","summary":"  This paper focuses on the dataset-free Blind Image Super-Resolution (BISR).\nUnlike existing dataset-free BISR methods that focus on obtaining a degradation\nkernel for the entire image, we are the first to explicitly design a\nspatially-variant degradation model for each pixel. Our method also benefits\nfrom having a significantly smaller number of learnable parameters compared to\ndata-driven spatially-variant BISR methods. Concretely, each pixel's\ndegradation kernel is expressed as a linear combination of a learnable\ndictionary composed of a small number of spatially-variant atom kernels. The\ncoefficient matrices of the atom degradation kernels are derived using\nmembership functions of fuzzy set theory. We construct a novel Probabilistic\nBISR model with tailored likelihood function and prior terms. Subsequently, we\nemploy the Monte Carlo EM algorithm to infer the degradation kernels for each\npixel. Our method achieves a significant improvement over other\nstate-of-the-art BISR methods, with an average improvement of 1 dB (2x).Code\nwill be released at https://github.com/shaojieguoECNU/SVDSR.\n","authors":["Shaojie Guo","Haofei Song","Qingli Li","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08245v1","updated":"2024-07-11T07:45:10Z","published":"2024-07-11T07:45:10Z","title":"Feature Diversification and Adaptation for Federated Domain\n  Generalization","summary":"  Federated learning, a distributed learning paradigm, utilizes multiple\nclients to build a robust global model. In real-world applications, local\nclients often operate within their limited domains, leading to a `domain shift'\nacross clients. Privacy concerns limit each client's learning to its own domain\ndata, which increase the risk of overfitting. Moreover, the process of\naggregating models trained on own limited domain can be potentially lead to a\nsignificant degradation in the global model performance. To deal with these\nchallenges, we introduce the concept of federated feature diversification. Each\nclient diversifies the own limited domain data by leveraging global feature\nstatistics, i.e., the aggregated average statistics over all participating\nclients, shared through the global model's parameters. This data\ndiversification helps local models to learn client-invariant representations\nwhile preserving privacy. Our resultant global model shows robust performance\non unseen test domain data. To enhance performance further, we develop an\ninstance-adaptive inference approach tailored for test domain data. Our\nproposed instance feature adapter dynamically adjusts feature statistics to\nalign with the test input, thereby reducing the domain gap between the test and\ntraining domains. We show that our method achieves state-of-the-art performance\non several domain generalization benchmarks within a federated learning\nsetting.\n","authors":["Seunghan Yang","Seokeon Choi","Hyunsin Park","Sungha Choi","Simyung Chang","Sungrack Yun"],"pdf_url":"https://arxiv.org/pdf/2407.08245v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.08244v1","updated":"2024-07-11T07:45:06Z","published":"2024-07-11T07:45:06Z","title":"Synchronous Diffusion for Unsupervised Smooth Non-Rigid 3D Shape\n  Matching","summary":"  Most recent unsupervised non-rigid 3D shape matching methods are based on the\nfunctional map framework due to its efficiency and superior performance.\nNevertheless, respective methods struggle to obtain spatially smooth pointwise\ncorrespondences due to the lack of proper regularisation. In this work,\ninspired by the success of message passing on graphs, we propose a synchronous\ndiffusion process which we use as regularisation to achieve smoothness in\nnon-rigid 3D shape matching problems. The intuition of synchronous diffusion is\nthat diffusing the same input function on two different shapes results in\nconsistent outputs. Using different challenging datasets, we demonstrate that\nour novel regularisation can substantially improve the state-of-the-art in\nshape matching, especially in the presence of topological noise.\n","authors":["Dongliang Cao","Zorah Laehner","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2407.08244v1.pdf","comment":"accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07325v2","updated":"2024-07-11T07:44:28Z","published":"2024-07-10T02:43:18Z","title":"HiLight: Technical Report on the Motern AI Video Language Model","summary":"  This technical report presents the implementation of a state-of-the-art video\nencoder for video-text modal alignment and a video conversation framework\ncalled HiLight, which features dual visual towers. The work is divided into two\nmain parts: 1.alignment of video and text modalities; 2.convenient and\nefficient way to interact with users. Our goal is to address the task of video\ncomprehension in the context of billiards. The report includes a discussion of\nthe concepts and the final solution developed during the task's implementation.\n","authors":["Zhiting Wang","Qiangong Zhou","Kangjie Yang","Zongyang Liu","Xin Mao"],"pdf_url":"https://arxiv.org/pdf/2407.07325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10600v3","updated":"2024-07-11T07:43:36Z","published":"2024-06-15T11:26:10Z","title":"SparseRadNet: Sparse Perception Neural Network on Subsampled Radar Data","summary":"  Radar-based perception has gained increasing attention in autonomous driving,\nyet the inherent sparsity of radars poses challenges. Radar raw data often\ncontains excessive noise, whereas radar point clouds retain only limited\ninformation. In this work, we holistically treat the sparse nature of radar\ndata by introducing an adaptive subsampling method together with a tailored\nnetwork architecture that exploits the sparsity patterns to discover global and\nlocal dependencies in the radar signal. Our subsampling module selects a subset\nof pixels from range-doppler (RD) spectra that contribute most to the\ndownstream perception tasks. To improve the feature extraction on sparse\nsubsampled data, we propose a new way of applying graph neural networks on\nradar data and design a novel two-branch backbone to capture both global and\nlocal neighbor information. An attentive fusion module is applied to combine\nfeatures from both branches. Experiments on the RADIal dataset show that our\nSparseRadNet exceeds state-of-the-art (SOTA) performance in object detection\nand achieves close to SOTA accuracy in freespace segmentation, meanwhile using\nsparse subsampled input data.\n","authors":["Jialong Wu","Mirko Meuter","Markus Schoeler","Matthias Rottmann"],"pdf_url":"https://arxiv.org/pdf/2406.10600v3.pdf","comment":"18 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.08243v1","updated":"2024-07-11T07:39:58Z","published":"2024-07-11T07:39:58Z","title":"Generalized Face Anti-spoofing via Finer Domain Partition and\n  Disentangling Liveness-irrelevant Factors","summary":"  Face anti-spoofing techniques based on domain generalization have recently\nbeen studied widely. Adversarial learning and meta-learning techniques have\nbeen adopted to learn domain-invariant representations. However, prior\napproaches often consider the dataset gap as the primary factor behind domain\nshifts. This perspective is not fine-grained enough to reflect the intrinsic\ngap among the data accurately. In our work, we redefine domains based on\nidentities rather than datasets, aiming to disentangle liveness and identity\nattributes. We emphasize ignoring the adverse effect of identity shift,\nfocusing on learning identity-invariant liveness representations through\northogonalizing liveness and identity features. To cope with style shifts, we\npropose Style Cross module to expand the stylistic diversity and Channel-wise\nStyle Attention module to weaken the sensitivity to style shifts, aiming to\nlearn robust liveness representations. Furthermore, acknowledging the asymmetry\nbetween live and spoof samples, we introduce a novel contrastive loss,\nAsymmetric Augmented Instance Contrast. Extensive experiments on four public\ndatasets demonstrate that our method achieves state-of-the-art performance\nunder cross-dataset and limited source dataset scenarios. Additionally, our\nmethod has good scalability when expanding diversity of identities. The codes\nwill be released soon.\n","authors":["Jingyi Yang","Zitong Yu","Xiuming Ni","Jia He","Hui Li"],"pdf_url":"https://arxiv.org/pdf/2407.08243v1.pdf","comment":"Accepted by ECAI 2024"},{"id":"http://arxiv.org/abs/2303.10762v4","updated":"2024-07-11T07:24:11Z","published":"2023-03-19T20:31:38Z","title":"Deep Image Fingerprint: Towards Low Budget Synthetic Image Detection and\n  Model Lineage Analysis","summary":"  The generation of high-quality images has become widely accessible and is a\nrapidly evolving process. As a result, anyone can generate images that are\nindistinguishable from real ones. This leads to a wide range of applications,\nincluding malicious usage with deceptive intentions. Despite advances in\ndetection techniques for generated images, a robust detection method still\neludes us. Furthermore, model personalization techniques might affect the\ndetection capabilities of existing methods. In this work, we utilize the\narchitectural properties of convolutional neural networks (CNNs) to develop a\nnew detection method. Our method can detect images from a known generative\nmodel and enable us to establish relationships between fine-tuned generative\nmodels. We tested the method on images produced by both Generative Adversarial\nNetworks (GANs) and recent large text-to-image models (LTIMs) that rely on\nDiffusion Models. Our approach outperforms others trained under identical\nconditions and achieves comparable performance to state-of-the-art pre-trained\ndetection methods on images generated by Stable Diffusion and MidJourney, with\nsignificantly fewer required train samples.\n","authors":["Sergey Sinitsa","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2303.10762v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08231v1","updated":"2024-07-11T07:10:58Z","published":"2024-07-11T07:10:58Z","title":"E2VIDiff: Perceptual Events-to-Video Reconstruction using Diffusion\n  Priors","summary":"  Event cameras, mimicking the human retina, capture brightness changes with\nunparalleled temporal resolution and dynamic range. Integrating events into\nintensities poses a highly ill-posed challenge, marred by initial condition\nambiguities. Traditional regression-based deep learning methods fall short in\nperceptual quality, offering deterministic and often unrealistic\nreconstructions. In this paper, we introduce diffusion models to\nevents-to-video reconstruction, achieving colorful, realistic, and perceptually\nsuperior video generation from achromatic events. Powered by the image\ngeneration ability and knowledge of pretrained diffusion models, the proposed\nmethod can achieve a better trade-off between the perception and distortion of\nthe reconstructed frame compared to previous solutions. Extensive experiments\non benchmark datasets demonstrate that our approach can produce diverse,\nrealistic frames with faithfulness to the given events.\n","authors":["Jinxiu Liang","Bohan Yu","Yixin Yang","Yiming Han","Boxin Shi"],"pdf_url":"https://arxiv.org/pdf/2407.08231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09858v2","updated":"2024-07-11T07:09:00Z","published":"2024-05-16T07:25:15Z","title":"Towards Realistic Incremental Scenario in Class Incremental Semantic\n  Segmentation","summary":"  This paper addresses the unrealistic aspect of the commonly adopted\nContinuous Incremental Semantic Segmentation (CISS) scenario, termed\noverlapped. We point out that overlapped allows the same image to reappear in\nfuture tasks with different pixel labels, which is far from practical\nincremental learning scenarios. Moreover, we identified that this flawed\nscenario may lead to biased results for two commonly used techniques in CISS,\npseudo-labeling and exemplar memory, resulting in unintended advantages or\ndisadvantages for certain techniques. To mitigate this, a practical scenario\ncalled partitioned is proposed, in which the dataset is first divided into\ndistinct subsets representing each class, and then the subsets are assigned to\neach corresponding task. This efficiently addresses the issue above while\nmeeting the requirement of CISS scenario, such as capturing the background\nshifts. Furthermore, we identify and address the code implementation issues\nrelated to retrieving data from the exemplar memory, which was ignored in\nprevious works. Lastly, we introduce a simple yet competitive memory-based\nbaseline, MiB-AugM, that handles background shifts of current tasks in the\nexemplar memory. This baseline achieves state-of-the-art results across\nmultiple tasks involving learning numerous new classes.\n","authors":["Jihwan Kwak","Sungmin Cha","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2405.09858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03093v3","updated":"2024-07-11T06:50:50Z","published":"2024-02-05T15:24:13Z","title":"AI-Enhanced Virtual Reality in Medicine: A Comprehensive Survey","summary":"  With the rapid advance of computer graphics and artificial intelligence\ntechnologies, the ways we interact with the world have undergone a\ntransformative shift. Virtual Reality (VR) technology, aided by artificial\nintelligence (AI), has emerged as a dominant interaction media in multiple\napplication areas, thanks to its advantage of providing users with immersive\nexperiences. Among those applications, medicine is considered one of the most\npromising areas. In this paper, we present a comprehensive examination of the\nburgeoning field of AI-enhanced VR applications in medical care and services.\nBy introducing a systematic taxonomy, we meticulously classify the pertinent\ntechniques and applications into three well-defined categories based on\ndifferent phases of medical diagnosis and treatment: Visualization Enhancement,\nVR-related Medical Data Processing, and VR-assisted Intervention. This\ncategorization enables a structured exploration of the diverse roles that\nAI-powered VR plays in the medical domain, providing a framework for a more\ncomprehensive understanding and evaluation of these technologies. To our best\nknowledge, this is the first systematic survey of AI-powered VR systems in\nmedical settings, laying a foundation for future research in this\ninterdisciplinary domain.\n","authors":["Yixuan Wu","Kaiyuan Hu","Danny Z. Chen","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2402.03093v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16449v3","updated":"2024-07-11T06:48:39Z","published":"2024-06-24T08:42:42Z","title":"Evaluating and Analyzing Relationship Hallucinations in LVLMs","summary":"  The issue of hallucinations is a prevalent concern in existing Large\nVision-Language Models (LVLMs). Previous efforts have primarily focused on\ninvestigating object hallucinations, which can be easily alleviated by\nintroducing object detectors. However, these efforts neglect hallucinations in\ninter-object relationships, which is essential for visual comprehension. In\nthis work, we introduce R-Bench, a novel benchmark for evaluating Vision\nRelationship Hallucination. R-Bench features image-level questions that focus\non the existence of relationships and instance-level questions that assess\nlocal visual comprehension. We identify three types of relationship\nco-occurrences that lead to hallucinations: relationship-relationship,\nsubject-relationship, and relationship-object. The visual instruction tuning\ndataset's long-tail distribution significantly impacts LVLMs' understanding of\nvisual relationships. Furthermore, our analysis reveals that current LVLMs tend\nto disregard visual content and overly rely on the common sense knowledge of\nLarge Language Models. They also struggle with reasoning about spatial\nrelationships based on contextual information.\n","authors":["Mingrui Wu","Jiayi Ji","Oucheng Huang","Jiale Li","Yuhang Wu","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.16449v3.pdf","comment":"ICML2024; Project Page:https://github.com/mrwu-mac/R-Bench"},{"id":"http://arxiv.org/abs/2407.08221v1","updated":"2024-07-11T06:44:37Z","published":"2024-07-11T06:44:37Z","title":"GAURA: Generalizable Approach for Unified Restoration and Rendering of\n  Arbitrary Views","summary":"  Neural rendering methods can achieve near-photorealistic image synthesis of\nscenes from posed input images. However, when the images are imperfect, e.g.,\ncaptured in very low-light conditions, state-of-the-art methods fail to\nreconstruct high-quality 3D scenes. Recent approaches have tried to address\nthis limitation by modeling various degradation processes in the image\nformation model; however, this limits them to specific image degradations. In\nthis paper, we propose a generalizable neural rendering method that can perform\nhigh-fidelity novel view synthesis under several degradations. Our method,\nGAURA, is learning-based and does not require any test-time scene-specific\noptimization. It is trained on a synthetic dataset that includes several\ndegradation types. GAURA outperforms state-of-the-art methods on several\nbenchmarks for low-light enhancement, dehazing, deraining, and on-par for\nmotion deblurring. Further, our model can be efficiently fine-tuned to any new\nincoming degradation using minimal data. We thus demonstrate adaptation results\non two unseen degradations, desnowing and removing defocus blur. Code and video\nresults are available at vinayak-vg.github.io/GAURA.\n","authors":["Vinayak Gupta","Rongali Simhachala Venkata Girish","Mukund Varma T","Ayush Tewari","Kaushik Mitra"],"pdf_url":"https://arxiv.org/pdf/2407.08221v1.pdf","comment":"European Conference on Computer Vision(ECCV) 2024"},{"id":"http://arxiv.org/abs/2407.08216v1","updated":"2024-07-11T06:33:38Z","published":"2024-07-11T06:33:38Z","title":"Multimodal contrastive learning for spatial gene expression prediction\n  using histology images","summary":"  In recent years, the advent of spatial transcriptomics (ST) technology has\nunlocked unprecedented opportunities for delving into the complexities of gene\nexpression patterns within intricate biological systems. Despite its\ntransformative potential, the prohibitive cost of ST technology remains a\nsignificant barrier to its widespread adoption in large-scale studies. An\nalternative, more cost-effective strategy involves employing artificial\nintelligence to predict gene expression levels using readily accessible\nwhole-slide images (WSIs) stained with Hematoxylin and Eosin (H\\&E). However,\nexisting methods have yet to fully capitalize on multimodal information\nprovided by H&E images and ST data with spatial location. In this paper, we\npropose \\textbf{mclSTExp}, a multimodal contrastive learning with Transformer\nand Densenet-121 encoder for Spatial Transcriptomics Expression prediction. We\nconceptualize each spot as a \"word\", integrating its intrinsic features with\nspatial context through the self-attention mechanism of a Transformer encoder.\nThis integration is further enriched by incorporating image features via\ncontrastive learning, thereby enhancing the predictive capability of our model.\nOur extensive evaluation of \\textbf{mclSTExp} on two breast cancer datasets and\na skin squamous cell carcinoma dataset demonstrates its superior performance in\npredicting spatial gene expression. Moreover, mclSTExp has shown promise in\ninterpreting cancer-specific overexpressed genes, elucidating immune-related\ngenes, and identifying specialized spatial domains annotated by pathologists.\nOur source code is available at https://github.com/shizhiceng/mclSTExp.\n","authors":["Wenwen Min","Zhiceng Shi","Jun Zhang","Jun Wan","Changmiao Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08216v1.pdf","comment":"BIB, Code: https://github.com/shizhiceng/mclSTExp"},{"id":"http://arxiv.org/abs/2312.00826v2","updated":"2024-07-11T06:32:32Z","published":"2023-11-30T18:58:44Z","title":"DEVIAS: Learning Disentangled Video Representations of Action and Scene\n  for Holistic Video Understanding","summary":"  When watching a video, humans can naturally extract human actions from the\nsurrounding scene context, even when action-scene combinations are unusual.\nHowever, unlike humans, video action recognition models often learn\nscene-biased action representations from the spurious correlation in training\ndata, leading to poor performance in out-of-context scenarios. While\nscene-debiased models achieve improved performance in out-of-context scenarios,\nthey often overlook valuable scene information in the data. Addressing this\nchallenge, we propose Disentangled VIdeo representations of Action and Scene\n(DEVIAS), which aims to achieve holistic video understanding. Disentangled\naction and scene representations with our method could provide flexibility to\nadjust the emphasis on action or scene information depending on downstream task\nand dataset characteristics. Disentangled action and scene representations\ncould be beneficial for both in-context and out-of-context video understanding.\nTo this end, we employ slot attention to learn disentangled action and scene\nrepresentations with a single model, along with auxiliary tasks that further\nguide slot attention. We validate the proposed method on both in-context\ndatasets: UCF-101 and Kinetics-400, and out-of-context datasets: SCUBA and HAT.\nOur proposed method shows favorable performance across different datasets\ncompared to the baselines, demonstrating its effectiveness in diverse video\nunderstanding scenarios.\n","authors":["Kyungho Bae","Geo Ahn","Youngrae Kim","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2312.00826v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.03144v2","updated":"2024-07-11T06:29:06Z","published":"2024-07-03T14:22:51Z","title":"Venomancer: Towards Imperceptible and Target-on-Demand Backdoor Attacks\n  in Federated Learning","summary":"  Federated Learning (FL) is a distributed machine learning approach that\nmaintains data privacy by training on decentralized data sources. Similar to\ncentralized machine learning, FL is also susceptible to backdoor attacks, where\nan attacker can compromise some clients by injecting a backdoor trigger into\nlocal models of those clients, leading to the global model's behavior being\nmanipulated as desired by the attacker. Most backdoor attacks in FL assume a\npredefined target class and require control over a large number of clients or\nknowledge of benign clients' information. Furthermore, they are not\nimperceptible and are easily detected by human inspection due to clear\nartifacts left on the poison data. To overcome these challenges, we propose\nVenomancer, an effective backdoor attack that is imperceptible and allows\ntarget-on-demand. Specifically, imperceptibility is achieved by using a visual\nloss function to make the poison data visually indistinguishable from the\noriginal data. Target-on-demand property allows the attacker to choose\narbitrary target classes via conditional adversarial training. Additionally,\nexperiments showed that the method is robust against state-of-the-art defenses\nsuch as Norm Clipping, Weak DP, Krum, Multi-Krum, RLR, FedRAD, Deepsight, and\nRFLBAT. The source code is available at\nhttps://github.com/nguyenhongson1902/Venomancer.\n","authors":["Son Nguyen","Thinh Nguyen","Khoa D Doan","Kok-Seng Wong"],"pdf_url":"https://arxiv.org/pdf/2407.03144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08209v1","updated":"2024-07-11T06:25:26Z","published":"2024-07-11T06:25:26Z","title":"Enriching Information and Preserving Semantic Consistency in Expanding\n  Curvilinear Object Segmentation Datasets","summary":"  Curvilinear object segmentation plays a crucial role across various\napplications, yet datasets in this domain often suffer from small scale due to\nthe high costs associated with data acquisition and annotation. To address\nthese challenges, this paper introduces a novel approach for expanding\ncurvilinear object segmentation datasets, focusing on enhancing the\ninformativeness of generated data and the consistency between semantic maps and\ngenerated images.\n  Our method enriches synthetic data informativeness by generating curvilinear\nobjects through their multiple textual features. By combining textual features\nfrom each sample in original dataset, we obtain synthetic images that beyond\nthe original dataset's distribution. This initiative necessitated the creation\nof the Curvilinear Object Segmentation based on Text Generation (COSTG)\ndataset. Designed to surpass the limitations of conventional datasets, COSTG\nincorporates not only standard semantic maps but also some textual descriptions\nof curvilinear object features.\n  To ensure consistency between synthetic semantic maps and images, we\nintroduce the Semantic Consistency Preserving ControlNet (SCP ControlNet). This\ninvolves an adaptation of ControlNet with Spatially-Adaptive Normalization\n(SPADE), allowing it to preserve semantic information that would typically be\nwashed away in normalization layers. This modification facilitates more\naccurate semantic image synthesis.\n  Experimental results demonstrate the efficacy of our approach across three\ntypes of curvilinear objects (angiography, crack and retina) and six public\ndatasets (CHUAC, XCAD, DCA1, DRIVE, CHASEDB1 and Crack500). The synthetic data\ngenerated by our method not only expand the dataset, but also effectively\nimproves the performance of other curvilinear object segmentation models.\nSource code and dataset are available at\n\\url{https://github.com/tanlei0/COSTG}.\n","authors":["Qin Lei","Jiang Zhong","Qizhu Dai"],"pdf_url":"https://arxiv.org/pdf/2407.08209v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2403.00219v3","updated":"2024-07-11T06:10:51Z","published":"2024-03-01T01:28:10Z","title":"Multi-modal Attribute Prompting for Vision-Language Models","summary":"  Pre-trained Vision-Language Models (VLMs), like CLIP, exhibit strong\ngeneralization ability to downstream tasks but struggle in few-shot scenarios.\nExisting prompting techniques primarily focus on global text and image\nrepresentations, yet overlooking multi-modal attribute characteristics. This\nlimitation hinders the model's ability to perceive fine-grained visual details\nand restricts its generalization ability to a broader range of unseen classes.\nTo address this issue, we propose a Multi-modal Attribute Prompting method\n(MAP) by jointly exploring textual attribute prompting, visual attribute\nprompting, and attribute-level alignment. The proposed MAP enjoys several\nmerits. First, we introduce learnable visual attribute prompts enhanced by\ntextual attribute semantics to adaptively capture visual attributes for images\nfrom unknown categories, boosting fine-grained visual perception capabilities\nfor CLIP. Second, the proposed attribute-level alignment complements the global\nalignment to enhance the robustness of cross-modal alignment for\nopen-vocabulary objects. To our knowledge, this is the first work to establish\ncross-modal attribute-level alignment for CLIP-based few-shot adaptation.\nExtensive experimental results on 11 datasets demonstrate that our method\nperforms favorably against state-of-the-art approaches.\n","authors":["Xin Liu","Jiamin Wu","and Wenfei Yang","Xu Zhou","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.00219v3.pdf","comment":"Accepted for Publication in IEEE Transactions on Circuits and Systems\n  for Video Technology (TCSVT)"},{"id":"http://arxiv.org/abs/2312.17183v3","updated":"2024-07-11T06:09:39Z","published":"2023-12-28T18:16:00Z","title":"One Model to Rule them All: Towards Universal Segmentation for Medical\n  Images with Text Prompts","summary":"  In this study, we aim to build up a model that can Segment Anything in\nradiology scans, driven by Text prompts, termed as SAT. Our main contributions\nare three folds: (i) for dataset construction, we construct the first\nmulti-modal knowledge tree on human anatomy, including 6502 anatomical\nterminologies; Then we build up the largest and most comprehensive segmentation\ndataset for training, by collecting over 22K 3D medical image scans from 72\nsegmentation datasets, across 497 classes, with careful standardization on both\nimage scans and label space; (ii) for architecture design, we propose to inject\nmedical knowledge into a text encoder via contrastive learning, and then\nformulate a universal segmentation model, that can be prompted by feeding in\nmedical terminologies in text form; (iii) As a result, we have trained SAT-Nano\n(110M parameters) and SAT-Pro (447M parameters), demonstrating comparable\nperformance to 72 specialist nnU-Nets trained on each dataset/subsets. We\nvalidate SAT as a foundational segmentation model, with better generalization\nability on external (unseen) datasets, and can be further improved on specific\ntasks after fine-tuning adaptation. Comparing with interactive segmentation\nmodel, for example, MedSAM, segmentation model prompted by text enables\nsuperior performance, scalability and robustness. As a use case, we demonstrate\nthat SAT can act as a powerful out-of-the-box agent for large language models,\nenabling visual grounding in clinical procedures such as report generation. All\nthe data, codes, and models in this work have been released.\n","authors":["Ziheng Zhao","Yao Zhang","Chaoyi Wu","Xiaoman Zhang","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2312.17183v3.pdf","comment":"59 pages"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.07503v2","updated":"2024-07-11T02:49:39Z","published":"2024-07-10T09:41:36Z","title":"Metasurface-based Snapshot Shortwave-Infrared Hyperspectral Image\n  Reconstruction with Inter and Intra Prior Learning Network","summary":"  Shortwave-infrared(SWIR) spectral information,ranging from 1 {\\mu}m to\n2.5{\\mu}m, breaks the limitations of traditional color cameras in acquiring\nscene information and has been used in many fields. However, conventional SWIR\nhyperspectral imaging systems face challenges due to their bulky setups and low\nacquisition speed. In this work, we introduce a snapshot SWIR hyperspectral\nimaging system based on a metasurface filter and a corresponding filter\nselection method to achieve the lowest correlation coefficient among these\nfilters.This systemhas the advantages of small size and snapshot imaging. We\npropose a novel inter and intra prior learning unfolding framework proposed to\nachieve high-quality SWIR hyperspectral image reconstruction, which bridges the\ngap between prior learning and cross-stage information interaction. We also\ndesign an adaptive feature transfer mechanism to adaptively the transfer\ncontextual correlation of multi-scale encoder features to prevent detailed\ninformation loss in the decoder. Experiment results demonstrate that our method\ncan reconstruct HSI with high speed and superior performance over existing\nmethods.\n","authors":["Linqiang Li","Jinglei Hao","Yongqiang Zhao","Pan Liu","Haofang Yan","Ziqin Zhang","Seong G. Kong"],"pdf_url":"https://arxiv.org/pdf/2407.07503v2.pdf","comment":"10 pages,5 figures"},{"id":"http://arxiv.org/abs/2407.00072v3","updated":"2024-07-11T09:28:34Z","published":"2024-06-21T08:52:11Z","title":"Pistis-RAG: A Scalable Cascading Framework Towards Trustworthy\n  Retrieval-Augmented Generation","summary":"  In Greek mythology, Pistis symbolized good faith, trust, and reliability.\nDrawing inspiration from these principles, Pistis-RAG is a scalable multi-stage\nframework designed to address the challenges of large-scale retrieval-augmented\ngeneration (RAG) systems. This framework consists of distinct stages: matching,\npre-ranking, ranking, reasoning, and aggregating. Each stage contributes to\nnarrowing the search space, prioritizing semantically relevant documents,\naligning with the large language model's (LLM) preferences, supporting complex\nchain-of-thought (CoT) methods, and combining information from multiple\nsources.\n  Our ranking stage introduces a significant innovation by recognizing that\nsemantic relevance alone may not lead to improved generation quality, due to\nthe sensitivity of the few-shot prompt order, as noted in previous research.\nThis critical aspect is often overlooked in current RAG frameworks.\n  We argue that the alignment issue between LLMs and external knowledge ranking\nmethods is tied to the model-centric paradigm dominant in RAG systems. We\npropose a content-centric approach, emphasizing seamless integration between\nLLMs and external information sources to optimize content transformation for\nspecific tasks.\n  Our novel ranking stage is designed specifically for RAG systems,\nincorporating principles of information retrieval while considering the unique\nbusiness scenarios reflected in LLM preferences and user feedback. We simulated\nfeedback signals on the MMLU benchmark, resulting in a 9.3% performance\nimprovement. Our model and code will be open-sourced on GitHub. Additionally,\nexperiments on real-world, large-scale data validate the scalability of our\nframework.\n","authors":["Yu Bai","Yukai Miao","Li Chen","Dan Li","Yanyu Ren","Hongtao Xie","Ce Yang","Xuhui Cai"],"pdf_url":"https://arxiv.org/pdf/2407.00072v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08692v1","updated":"2024-07-11T17:30:04Z","published":"2024-07-11T17:30:04Z","title":"FAR-Trans: An Investment Dataset for Financial Asset Recommendation","summary":"  Financial asset recommendation (FAR) is a sub-domain of recommender systems\nwhich identifies useful financial securities for investors, with the\nexpectation that they will invest capital on the recommended assets. FAR\nsolutions analyse and learn from multiple data sources, including time series\npricing data, customer profile information and expectations, as well as past\ninvestments. However, most models have been developed over proprietary\ndatasets, making a comparison over a common benchmark impossible. In this\npaper, we aim to solve this problem by introducing FAR-Trans, the first public\ndataset for FAR, containing pricing information and retail investor\ntransactions acquired from a large European financial institution. We also\nprovide a bench-marking comparison between eleven FAR algorithms over the data\nfor use as future baselines. The dataset can be downloaded from\nhttps://doi.org/10.5525/gla.researchdata.1658 .\n","authors":["Javier Sanz-Cruzado","Nikolaos Droukas","Richard McCreadie"],"pdf_url":"https://arxiv.org/pdf/2407.08692v1.pdf","comment":"Accepted at the IJCAI-2024 Workshop on Recommender Systems in Finance\n  (Fin-RecSys)"},{"id":"http://arxiv.org/abs/2407.08647v1","updated":"2024-07-11T16:25:21Z","published":"2024-07-11T16:25:21Z","title":"From Real to Cloned Singer Identification","summary":"  Cloned voices of popular singers sound increasingly realistic and have gained\npopularity over the past few years. They however pose a threat to the industry\ndue to personality rights concerns. As such, methods to identify the original\nsinger in synthetic voices are needed. In this paper, we investigate how singer\nidentification methods could be used for such a task. We present three\nembedding models that are trained using a singer-level contrastive learning\nscheme, where positive pairs consist of segments with vocals from the same\nsingers. These segments can be mixtures for the first model, vocals for the\nsecond, and both for the third. We demonstrate that all three models are highly\ncapable of identifying real singers. However, their performance deteriorates\nwhen classifying cloned versions of singers in our evaluation set. This is\nespecially true for models that use mixtures as an input. These findings\nhighlight the need to understand the biases that exist within singer\nidentification systems, and how they can influence the identification of voice\ndeepfakes in music.\n","authors":["Dorian Desblancs","Gabriel Meseguer-Brocal","Romain Hennequin","Manuel Moussallam"],"pdf_url":"https://arxiv.org/pdf/2407.08647v1.pdf","comment":"To be published at ISMIR 2024"},{"id":"http://arxiv.org/abs/2210.05401v2","updated":"2024-07-11T15:13:14Z","published":"2022-10-11T12:25:26Z","title":"MiDe22: An Annotated Multi-Event Tweet Dataset for Misinformation\n  Detection","summary":"  The rapid dissemination of misinformation through online social networks\nposes a pressing issue with harmful consequences jeopardizing human health,\npublic safety, democracy, and the economy; therefore, urgent action is required\nto address this problem. In this study, we construct a new human-annotated\ndataset, called MiDe22, having 5,284 English and 5,064 Turkish tweets with\ntheir misinformation labels for several recent events between 2020 and 2022,\nincluding the Russia-Ukraine war, COVID-19 pandemic, and Refugees. The dataset\nincludes user engagements with the tweets in terms of likes, replies, retweets,\nand quotes. We also provide a detailed data analysis with descriptive\nstatistics and the experimental results of a benchmark evaluation for\nmisinformation detection.\n","authors":["Cagri Toraman","Oguzhan Ozcelik","Furkan ÅahinuÃ§","Fazli Can"],"pdf_url":"https://arxiv.org/pdf/2210.05401v2.pdf","comment":"Published at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2407.08571v1","updated":"2024-07-11T14:59:17Z","published":"2024-07-11T14:59:17Z","title":"Multi-Group Proportional Representation","summary":"  Image search and retrieval tasks can perpetuate harmful stereotypes, erase\ncultural identities, and amplify social disparities. Current approaches to\nmitigate these representational harms balance the number of retrieved items\nacross population groups defined by a small number of (often binary)\nattributes. However, most existing methods overlook intersectional groups\ndetermined by combinations of group attributes, such as gender, race, and\nethnicity. We introduce Multi-Group Proportional Representation (MPR), a novel\nmetric that measures representation across intersectional groups. We develop\npractical methods for estimating MPR, provide theoretical guarantees, and\npropose optimization algorithms to ensure MPR in retrieval. We demonstrate that\nexisting methods optimizing for equal and proportional representation metrics\nmay fail to promote MPR. Crucially, our work shows that optimizing MPR yields\nmore proportional representation across multiple intersectional groups\nspecified by a rich function class, often with minimal compromise in retrieval\naccuracy.\n","authors":["Alex Oesterling","Claudio Mayrink Verdun","Carol Xuan Long","Alex Glynn","Lucas Monteiro Paes","Sajani Vithana","Martina Cardone","Flavio P. Calmon"],"pdf_url":"https://arxiv.org/pdf/2407.08571v1.pdf","comment":"35 pages, 24 figures. Under review"},{"id":"http://arxiv.org/abs/2406.08229v2","updated":"2024-07-11T14:33:23Z","published":"2024-06-12T13:59:31Z","title":"GPT4Rec: Graph Prompt Tuning for Streaming Recommendation","summary":"  In the realm of personalized recommender systems, the challenge of adapting\nto evolving user preferences and the continuous influx of new users and items\nis paramount. Conventional models, typically reliant on a static training-test\napproach, struggle to keep pace with these dynamic demands. Streaming\nrecommendation, particularly through continual graph learning, has emerged as a\nnovel solution. However, existing methods in this area either rely on\nhistorical data replay, which is increasingly impractical due to stringent data\nprivacy regulations; or are inability to effectively address the over-stability\nissue; or depend on model-isolation and expansion strategies. To tackle these\ndifficulties, we present GPT4Rec, a Graph Prompt Tuning method for streaming\nRecommendation. Given the evolving user-item interaction graph, GPT4Rec first\ndisentangles the graph patterns into multiple views. After isolating specific\ninteraction patterns and relationships in different views, GPT4Rec utilizes\nlightweight graph prompts to efficiently guide the model across varying\ninteraction patterns within the user-item graph. Firstly, node-level prompts\nare employed to instruct the model to adapt to changes in the attributes or\nproperties of individual nodes within the graph. Secondly, structure-level\nprompts guide the model in adapting to broader patterns of connectivity and\nrelationships within the graph. Finally, view-level prompts are innovatively\ndesigned to facilitate the aggregation of information from multiple\ndisentangled views. These prompt designs allow GPT4Rec to synthesize a\ncomprehensive understanding of the graph, ensuring that all vital aspects of\nthe user-item interactions are considered and effectively integrated.\nExperiments on four diverse real-world datasets demonstrate the effectiveness\nand efficiency of our proposal.\n","authors":["Peiyan Zhang","Yuchen Yan","Xi Zhang","Liying Kang","Chaozhuo Li","Feiran Huang","Senzhang Wang","Sunghun Kim"],"pdf_url":"https://arxiv.org/pdf/2406.08229v2.pdf","comment":"Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2407.08334v1","updated":"2024-07-11T09:35:08Z","published":"2024-07-11T09:35:08Z","title":"ADMM Based Semi-Structured Pattern Pruning Framework For Transformer","summary":"  NLP(natural language processsing) has achieved great success through the\ntransformer model.However, the model has hundreds of millions or billions\nparameters,which is huge burden for its deployment on personal computer or\nsmall scale of server.To deal with it, we either make the model's weight matrix\nrelatively sparser, or compress attention layer. Pattern pruning ,one of the\nmost important pruning methods, permits selecting fixed number of parameters in\neach divided pattern block and prunes it. However, the effect of pattern\npruning is strictly limited by the sparsity within a region of weights in each\nlayer. In this paper,we first introduced Alternating Direction Method of\nMultipliers(ADMM) based pattern pruning framework to reshape the distribution\nof activation map. Specifically, we propose to formulate the pattern pruning on\ntransformer as a constrained optimization and use ADMM to optimize the problem.\nIn this way, the initial dense feature maps is transformed to rather regionally\nsparsified ones.Therefore, we can then achieve higher compression ratio with\nbetter performance based on pattern pruning method. Additionally, this paper\nprovides a theoretical derivations of the ADMM with local sparsity. Finally, we\nalso extend the proposed ADMM based framework on quantization to demonstrate\nits generalization and use SR-STE to avoid gradient vanishing problem. We\nconduct extensive experiments on classification tasks over GLUE datasets.\nSignificantly, we achieve 50% percent compression ratio while maintaining 55.4%\nMatthews correlation on COLA, 68.8% accuracy on RTE and overall score 80.1. Our\nframework also perform well on other tasks on GLUE datasets.\n","authors":["TianChen Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08334v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.08275v1","updated":"2024-07-11T08:24:16Z","published":"2024-07-11T08:24:16Z","title":"Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval\n  Augmented Generation Systems","summary":"  The choice of embedding model is a crucial step in the design of Retrieval\nAugmented Generation (RAG) systems. Given the sheer volume of available\noptions, identifying clusters of similar models streamlines this model\nselection process. Relying solely on benchmark performance scores only allows\nfor a weak assessment of model similarity. Thus, in this study, we evaluate the\nsimilarity of embedding models within the context of RAG systems. Our\nassessment is two-fold: We use Centered Kernel Alignment to compare embeddings\non a pair-wise level. Additionally, as it is especially pertinent to RAG\nsystems, we evaluate the similarity of retrieval results between these models\nusing Jaccard and rank similarity. We compare different families of embedding\nmodels, including proprietary ones, across five datasets from the popular\nBenchmark Information Retrieval (BEIR). Through our experiments we identify\nclusters of models corresponding to model families, but interestingly, also\nsome inter-family clusters. Furthermore, our analysis of top-k retrieval\nsimilarity reveals high-variance at low k values. We also identify possible\nopen-source alternatives to proprietary models, with Mistral exhibiting the\nhighest similarity to OpenAI models.\n","authors":["Laura Caspari","Kanishka Ghosh Dastidar","Saber Zerhoudi","Jelena Mitrovic","Michael Granitzer"],"pdf_url":"https://arxiv.org/pdf/2407.08275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08227v1","updated":"2024-07-11T07:01:50Z","published":"2024-07-11T07:01:50Z","title":"DALL-M: Context-Aware Clinical Data Augmentation with LLMs","summary":"  X-ray images are vital in medical diagnostics, but their effectiveness is\nlimited without clinical context. Radiologists often find chest X-rays\ninsufficient for diagnosing underlying diseases, necessitating comprehensive\nclinical features and data integration. We present a novel technique to enhance\nthe clinical context through augmentation techniques with clinical tabular\ndata, thereby improving its applicability and reliability in AI medical\ndiagnostics. To address this, we introduce a pioneering approach to clinical\ndata augmentation that employs large language models (LLMs) to generate patient\ncontextual synthetic data. This methodology is crucial for training more robust\ndeep learning models in healthcare. It preserves the integrity of real patient\ndata while enriching the dataset with contextually relevant synthetic features,\nsignificantly enhancing model performance. DALL-M uses a three-phase feature\ngeneration process: (i) clinical context storage, (ii) expert query generation,\nand (iii) context-aware feature augmentation. DALL-M generates new, clinically\nrelevant features by synthesizing chest X-ray images and reports. Applied to\n799 cases using nine features from the MIMIC-IV dataset, it created an\naugmented set of 91 features. This is the first work to generate contextual\nvalues for existing and new features based on patients' X-ray reports, gender,\nand age and to produce new contextual knowledge during data augmentation.\nEmpirical validation with machine learning models, including Decision Trees,\nRandom Forests, XGBoost, and TabNET, showed significant performance\nimprovements. Incorporating augmented features increased the F1 score by 16.5%\nand Precision and Recall by approximately 25%. DALL-M addresses a critical gap\nin clinical data augmentation, offering a robust framework for generating\ncontextually enriched datasets.\n","authors":["Chihcheng Hsieh","Catarina Moreira","Isabel Blanco Nobre","Sandra Costa Sousa","Chun Ouyang","Margot Brereton","Joaquim Jorge","Jacinto C. Nascimento"],"pdf_url":"https://arxiv.org/pdf/2407.08227v1.pdf","comment":"we introduce a pioneering approach to clinical data augmentation that\n  employs large language models (LLMs) to generate patient contextual synthetic\n  data. It preserves the integrity of real patient data while enriching the\n  dataset with contextually relevant synthetic features, significantly\n  enhancing model performance"},{"id":"http://arxiv.org/abs/2312.15490v3","updated":"2024-07-11T01:34:09Z","published":"2023-12-24T14:23:15Z","title":"Diffusion-EXR: Controllable Review Generation for Explainable\n  Recommendation via Diffusion Models","summary":"  Denoising Diffusion Probabilistic Model (DDPM) has shown great competence in\nimage and audio generation tasks. However, there exist few attempts to employ\nDDPM in the text generation, especially review generation under recommendation\nsystems. Fueled by the predicted reviews explainability that justifies\nrecommendations could assist users better understand the recommended items and\nincrease the transparency of recommendation system, we propose a Diffusion\nModel-based Review Generation towards EXplainable Recommendation named\nDiffusion-EXR. Diffusion-EXR corrupts the sequence of review embeddings by\nincrementally introducing varied levels of Gaussian noise to the sequence of\nword embeddings and learns to reconstruct the original word representations in\nthe reverse process. The nature of DDPM enables our lightweight Transformer\nbackbone to perform excellently in the recommendation review generation task.\nExtensive experimental results have demonstrated that Diffusion-EXR can achieve\nstate-of-the-art review generation for recommendation on two publicly available\nbenchmark datasets.\n","authors":["Ling Li","Shaohua Li","Winda Marantika","Alex C. Kot","Huijing Zhan"],"pdf_url":"https://arxiv.org/pdf/2312.15490v3.pdf","comment":"We request to withdraw our paper from the archive due to significant\n  errors identified in the analysis and conclusions. Upon further review, we\n  realized that these errors undermine the validity of our findings. We plan to\n  conduct additional research to correct these issues and resubmit a revised\n  version in the future"},{"id":"http://arxiv.org/abs/2407.08108v1","updated":"2024-07-11T00:54:56Z","published":"2024-07-11T00:54:56Z","title":"CADC: Encoding User-Item Interactions for Compressing Recommendation\n  Model Training Data","summary":"  Deep learning recommendation models (DLRMs) are at the heart of the current\ne-commerce industry. However, the amount of training data used to train these\nlarge models is growing exponentially, leading to substantial training hurdles.\nThe training dataset contains two primary types of information: content-based\ninformation (features of users and items) and collaborative information\n(interactions between users and items). One approach to reduce the training\ndataset is to remove user-item interactions. But that significantly diminishes\ncollaborative information, which is crucial for maintaining accuracy due to its\ninclusion of interaction histories. This loss profoundly impacts DLRM\nperformance.\n  This paper makes an important observation that if one can capture the\nuser-item interaction history to enrich the user and item embeddings, then the\ninteraction history can be compressed without losing model accuracy. Thus, this\nwork, Collaborative Aware Data Compression (CADC), takes a two-step approach to\ntraining dataset compression. In the first step, we use matrix factorization of\nthe user-item interaction matrix to create a novel embedding representation for\nboth the users and items. Once the user and item embeddings are enriched by the\ninteraction history information the approach then applies uniform random\nsampling of the training dataset to drastically reduce the training dataset\nsize while minimizing model accuracy drop. The source code of CADC is available\nat\n\\href{https://anonymous.4open.science/r/DSS-RM-8C1D/README.md}{https://anonymous.4open.science/r/DSS-RM-8C1D/README.md}.\n","authors":["Hossein Entezari Zarch","Abdulla Alshabanah","Chaoyi Jiang","Murali Annavaram"],"pdf_url":"https://arxiv.org/pdf/2407.08108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01610v3","updated":"2024-07-11T21:35:37Z","published":"2023-09-04T13:49:48Z","title":"Fairness in Ranking under Disparate Uncertainty","summary":"  Ranking is a ubiquitous method for focusing the attention of human evaluators\non a manageable subset of options. Its use as part of human decision-making\nprocesses ranges from surfacing potentially relevant products on an e-commerce\nsite to prioritizing college applications for human review. While ranking can\nmake human evaluation more effective by focusing attention on the most\npromising options, we argue that it can introduce unfairness if the uncertainty\nof the underlying relevance model differs between groups of options.\nUnfortunately, such disparity in uncertainty appears widespread, often to the\ndetriment of minority groups for which relevance estimates can have higher\nuncertainty due to a lack of data or appropriate features. To address this\nfairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness\ncriterion for ranking and show that it corresponds to a group-wise fair lottery\namong the relevant options even in the presence of disparate uncertainty. EOR\noptimizes for an even cost burden on all groups, unlike the conventional\nProbability Ranking Principle, and is fundamentally different from existing\nnotions of fairness in rankings, such as demographic parity and proportional\nRooney rule constraints that are motivated by proportional representation\nrelative to group size. To make EOR ranking practical, we present an efficient\nalgorithm for computing it in time $O(n \\log(n))$ and prove its close\napproximation guarantee to the globally optimal solution. In a comprehensive\nempirical evaluation on synthetic data, a US Census dataset, and a real-world\naudit of Amazon search queries, we find that the algorithm reliably guarantees\nEOR fairness while providing effective rankings.\n","authors":["Richa Rastogi","Thorsten Joachims"],"pdf_url":"https://arxiv.org/pdf/2309.01610v3.pdf","comment":"A version of this paper was accepted as Spotlight (Oral) at UAI\n  workshop on Epistemic AI, 2023"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.07874v2","updated":"2024-07-11T16:18:40Z","published":"2024-07-10T17:40:30Z","title":"Toto: Time Series Optimized Transformer for Observability","summary":"  This technical report describes the Time Series Optimized Transformer for\nObservability (Toto), a new state of the art foundation model for time series\nforecasting developed by Datadog. In addition to advancing the state of the art\non generalized time series benchmarks in domains such as electricity and\nweather, this model is the first general-purpose time series forecasting\nfoundation model to be specifically tuned for observability metrics.\n  Toto was trained on a dataset of one trillion time series data points, the\nlargest among all currently published time series foundation models. Alongside\npublicly available time series datasets, 75% of the data used to train Toto\nconsists of fully anonymous numerical metric data points from the Datadog\nplatform.\n  In our experiments, Toto outperforms existing time series foundation models\non observability data. It does this while also excelling at general-purpose\nforecasting tasks, achieving state-of-the-art zero-shot performance on multiple\nopen benchmark datasets.\n","authors":["Ben Cohen","Emaad Khwaja","Kan Wang","Charles Masson","Elise RamÃ©","Youssef Doubli","Othmane Abou-Amal"],"pdf_url":"https://arxiv.org/pdf/2407.07874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07801v2","updated":"2024-07-11T02:38:14Z","published":"2024-07-10T16:17:49Z","title":"AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning","summary":"  In recent years, advancements in representation learning and language models\nhave propelled Automated Captioning (AC) to new heights, enabling the\ngeneration of human-level descriptions. Leveraging these advancements, we\npropose AVCap, an Audio-Visual Captioning framework, a simple yet powerful\nbaseline approach applicable to audio-visual captioning. AVCap utilizes\naudio-visual features as text tokens, which has many advantages not only in\nperformance but also in the extensibility and scalability of the model. AVCap\nis designed around three pivotal dimensions: the exploration of optimal\naudio-visual encoder architectures, the adaptation of pre-trained models\naccording to the characteristics of generated text, and the investigation into\nthe efficacy of modality fusion in captioning. Our method outperforms existing\naudio-visual captioning methods across all metrics and the code is available on\nhttps://github.com/JongSuk1/AVCap\n","authors":["Jongsuk Kim","Jiwon Shin","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.07801v2.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2407.07796v2","updated":"2024-07-11T03:46:35Z","published":"2024-07-10T16:14:34Z","title":"Evaluating Large Language Models with Grid-Based Game Competitions: An\n  Extensible LLM Benchmark and Leaderboard","summary":"  We introduce a novel and extensible benchmark for large language models\n(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.\nThe open-source game simulation code, available on GitHub, allows LLMs to\ncompete and generates detailed data files in JSON, CSV, TXT, and PNG formats\nfor leaderboard rankings and further analysis. We present the results of games\namong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by\nAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and\nGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of\nresults from other LLMs. In total, we simulated 2,310 matches (5 sessions for\neach pair among 7 LLMs and a random player) across three types of games, using\nthree distinct prompt types: list, illustration, and image. The results\nrevealed significant variations in LLM performance across different games and\nprompt types, with analysis covering win and disqualification rates, missed\nopportunity analysis, and invalid move analysis. The details of the leaderboard\nand result matrix data are available as open-access data on GitHub. This study\nenhances our understanding of LLMs' capabilities in playing games they were not\nspecifically trained for, helping to assess their rule comprehension and\nstrategic thinking. On the path to Artificial General Intelligence (AGI), this\nstudy lays the groundwork for future exploration into their utility in complex\ndecision-making scenarios, illuminating their strategic thinking abilities and\noffering directions for further inquiry into the limits of LLMs within\ngame-based frameworks.\n","authors":["Oguzhan Topsakal","Colby Jacob Edell","Jackson Bailey Harper"],"pdf_url":"https://arxiv.org/pdf/2407.07796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07788v2","updated":"2024-07-11T16:26:09Z","published":"2024-07-10T16:04:18Z","title":"BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark","summary":"  We introduce BiGym, a new benchmark and learning environment for mobile\nbi-manual demo-driven robotic manipulation. BiGym features 40 diverse tasks set\nin home environments, ranging from simple target reaching to complex kitchen\ncleaning. To capture the real-world performance accurately, we provide\nhuman-collected demonstrations for each task, reflecting the diverse modalities\nfound in real-world robot trajectories. BiGym supports a variety of\nobservations, including proprioceptive data and visual inputs such as RGB, and\ndepth from 3 camera views. To validate the usability of BiGym, we thoroughly\nbenchmark the state-of-the-art imitation learning algorithms and demo-driven\nreinforcement learning algorithms within the environment and discuss the future\nopportunities.\n","authors":["Nikita Chernyadev","Nicholas Backshall","Xiao Ma","Yunfan Lu","Younggyo Seo","Stephen James"],"pdf_url":"https://arxiv.org/pdf/2407.07788v2.pdf","comment":"Project webpage: https://chernyadev.github.io/bigym/"},{"id":"http://arxiv.org/abs/2407.07457v2","updated":"2024-07-11T06:06:33Z","published":"2024-07-10T08:20:47Z","title":"GLBench: A Comprehensive Benchmark for Graph with Large Language Models","summary":"  The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench.\n","authors":["Yuhan Li","Peisong Wang","Xiao Zhu","Aochuan Chen","Haiyun Jiang","Deng Cai","Victor Wai Kin Chan","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2407.07457v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2306.10280 by other authors"},{"id":"http://arxiv.org/abs/2310.07338v4","updated":"2024-07-11T04:09:19Z","published":"2023-10-11T09:37:38Z","title":"From Supervised to Generative: A Novel Paradigm for Tabular Deep\n  Learning with Large Language Models","summary":"  Tabular data is foundational to predictive modeling in various crucial\nindustries, including healthcare, finance, retail, sustainability, etc. Despite\nthe progress made in specialized models, there is an increasing demand for\nuniversal models that can transfer knowledge, generalize from limited data, and\nfollow human instructions. These are challenges that current tabular deep\nlearning approaches have not fully tackled. Here we introduce Generative\nTabular Learning (GTL), a novel framework that integrates the advanced\nfunctionalities of large language models (LLMs)-such as prompt-based zero-shot\ngeneralization and in-context learning-into tabular deep learning. GTL\ncapitalizes on the pre-training of LLMs on diverse tabular data, enhancing\ntheir understanding of domain-specific knowledge, numerical sequences, and\nstatistical dependencies critical for accurate predictions. Our empirical study\nspans 384 public datasets, rigorously analyzing GTL's convergence and scaling\nbehaviors and assessing the impact of varied data templates. The GTL-enhanced\nLLaMA-2 model demonstrates superior zero-shot and in-context learning\ncapabilities across numerous classification and regression tasks. Notably, it\nachieves this without fine-tuning, outperforming traditional methods and\nrivaling state-of-the-art models like GPT-4 in certain cases. Through GTL, we\nnot only foster a deeper integration of LLMs' sophisticated abilities into\ntabular data comprehension and application but also offer a new training\nresource and a test bed for LLMs to enhance their ability to comprehend tabular\ndata. To facilitate reproducible research, we release our code, data, and model\ncheckpoints at https://github.com/microsoft/Industrial-Foundation-Models.\n","authors":["Xumeng Wen","Han Zhang","Shun Zheng","Wei Xu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.07338v4.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2407.06645v3","updated":"2024-07-11T03:06:45Z","published":"2024-07-09T08:14:29Z","title":"Entropy Law: The Story Behind Data Compression and LLM Performance","summary":"  Data is the cornerstone of large language models (LLMs), but not all data is\nuseful for model learning. Carefully selected data can better elicit the\ncapabilities of LLMs with much less computational overhead. Most methods\nconcentrate on evaluating the quality of individual samples in data selection,\nwhile the combinatorial effects among samples are neglected. Even if each\nsample is of perfect quality, their combinations may be suboptimal in teaching\nLLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim\nto uncover the underlying relationships between LLM performance and data\nselection. Inspired by the information compression nature of LLMs, we uncover\nan ``entropy law'' that connects LLM performance with data compression ratio\nand first-epoch training loss, which reflect the information redundancy of a\ndataset and the mastery of inherent knowledge encoded in this dataset,\nrespectively. Through both theoretical deduction and empirical evaluation, we\nfind that model performance is negatively correlated to the compression ratio\nof training data, which usually yields a lower training loss. Based on the\nfindings of the entropy law, we propose a quite efficient and universal data\nselection method named \\textbf{ZIP} for training LLMs, which aim to prioritize\ndata subsets exhibiting a low compression ratio. Based on a multi-stage\nalgorithm that selects diverse data in a greedy manner, we can obtain a good\ndata subset with satisfactory diversity. Extensive experiments have been\nconducted to validate the entropy law and the superiority of ZIP across\ndifferent LLM backbones and alignment stages. We also present an interesting\napplication of entropy law that can detect potential performance risks at the\nbeginning of model training.\n","authors":["Mingjia Yin","Chuhan Wu","Yufei Wang","Hao Wang","Wei Guo","Yasheng Wang","Yong Liu","Ruiming Tang","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.06645v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06094v2","updated":"2024-07-11T17:59:53Z","published":"2023-06-09T17:57:01Z","title":"Leveraging Large Language Models for Scalable Vector Graphics-Driven\n  Image Understanding","summary":"  Large language models (LLMs) have made significant advancements in natural\nlanguage understanding. However, through that enormous semantic representation\nthat the LLM has learnt, is it somehow possible for it to understand images as\nwell? This work investigates this question. To enable the LLM to process\nimages, we convert them into a representation given by Scalable Vector Graphics\n(SVG). To study what the LLM can do with this XML-based textual description of\nimages, we test the LLM on three broad computer vision tasks: (i) visual\nreasoning and question answering, (ii) image classification under distribution\nshift, few-shot learning, and (iii) generating new images using visual\nprompting. Even though we do not naturally associate LLMs with any visual\nunderstanding capabilities, our results indicate that the LLM can often do a\ndecent job in many of these tasks, potentially opening new avenues for research\ninto LLMs' ability to understand image data. Our code, data, and models can be\nfound here https://github.com/mu-cai/svg-llm.\n","authors":["Mu Cai","Zeyi Huang","Yuheng Li","Utkarsh Ojha","Haohan Wang","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2306.06094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08737v1","updated":"2024-07-11T17:59:45Z","published":"2024-07-11T17:59:45Z","title":"Video Diffusion Alignment via Reward Gradients","summary":"  We have made significant progress towards building foundational video\ndiffusion models. As these models are trained using large-scale unsupervised\ndata, it has become crucial to adapt these models to specific downstream tasks.\nAdapting these models via supervised fine-tuning requires collecting target\ndatasets of videos, which is challenging and tedious. In this work, we utilize\npre-trained reward models that are learned via preferences on top of powerful\nvision discriminative models to adapt video diffusion models. These models\ncontain dense gradient information with respect to generated RGB pixels, which\nis critical to efficient learning in complex search spaces, such as videos. We\nshow that backpropagating gradients from these reward models to a video\ndiffusion model can allow for compute and sample efficient alignment of the\nvideo diffusion model. We show results across a variety of reward models and\nvideo diffusion models, demonstrating that our approach can learn much more\nefficiently in terms of reward queries and computation than prior gradient-free\napproaches. Our code, model weights,and more visualization are available at\nhttps://vader-vid.github.io.\n","authors":["Mihir Prabhudesai","Russell Mendonca","Zheyang Qin","Katerina Fragkiadaki","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2407.08737v1.pdf","comment":"Project Webpage: https://vader-vid.github.io; Code available at:\n  https://github.com/mihirp1998/VADER"},{"id":"http://arxiv.org/abs/2407.08734v1","updated":"2024-07-11T17:59:00Z","published":"2024-07-11T17:59:00Z","title":"Transformer Circuit Faithfulness Metrics are not Robust","summary":"  Mechanistic interpretability work attempts to reverse engineer the learned\nalgorithms present inside neural networks. One focus of this work has been to\ndiscover 'circuits' -- subgraphs of the full model that explain behaviour on\nspecific tasks. But how do we measure the performance of such circuits? Prior\nwork has attempted to measure circuit 'faithfulness' -- the degree to which the\ncircuit replicates the performance of the full model. In this work, we survey\nmany considerations for designing experiments that measure circuit faithfulness\nby ablating portions of the model's computation. Concerningly, we find existing\nmethods are highly sensitive to seemingly insignificant changes in the ablation\nmethodology. We conclude that existing circuit faithfulness scores reflect both\nthe methodological choices of researchers as well as the actual components of\nthe circuit - the task a circuit is required to perform depends on the ablation\nused to test it. The ultimate goal of mechanistic interpretability work is to\nunderstand neural networks, so we emphasize the need for more clarity in the\nprecise claims being made about circuits. We open source a library at\nhttps://github.com/UFO-101/auto-circuit that includes highly efficient\nimplementations of a wide range of ablation methodologies and circuit discovery\nalgorithms.\n","authors":["Joseph Miller","Bilal Chughtai","William Saunders"],"pdf_url":"https://arxiv.org/pdf/2407.08734v1.pdf","comment":"CoLM 2024 Conference Paper. 11 page main body. 11 page appendix. 12\n  figures"},{"id":"http://arxiv.org/abs/2407.08729v1","updated":"2024-07-11T17:58:10Z","published":"2024-07-11T17:58:10Z","title":"BiEquiFormer: Bi-Equivariant Representations for Global Point Cloud\n  Registration","summary":"  The goal of this paper is to address the problem of \\textit{global} point\ncloud registration (PCR) i.e., finding the optimal alignment between point\nclouds irrespective of the initial poses of the scans. This problem is\nnotoriously challenging for classical optimization methods due to computational\nconstraints. First, we show that state-of-the-art deep learning methods suffer\nfrom huge performance degradation when the point clouds are arbitrarily placed\nin space. We propose that \\textit{equivariant deep learning} should be utilized\nfor solving this task and we characterize the specific type of bi-equivariance\nof PCR. Then, we design BiEquiformer a novel and scalable\n\\textit{bi-equivariant} pipeline i.e. equivariant to the independent\ntransformations of the input point clouds. While a naive approach would process\nthe point clouds independently we design expressive bi-equivariant layers that\nfuse the information from both point clouds. This allows us to extract\nhigh-quality superpoint correspondences and in turn, robust point-cloud\nregistration. Extensive comparisons against state-of-the-art methods show that\nour method achieves comparable performance in the canonical setting and\nsuperior performance in the robust setting in both the 3DMatch and the\nchallenging low-overlap 3DLoMatch dataset.\n","authors":["Stefanos Pertigkiozoglou","Evangelos Chatzipantazis","Kostas Daniilidis"],"pdf_url":"https://arxiv.org/pdf/2407.08729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08723v1","updated":"2024-07-11T17:56:03Z","published":"2024-07-11T17:56:03Z","title":"Topological Generalization Bounds for Discrete-Time Stochastic\n  Optimization Algorithms","summary":"  We present a novel set of rigorous and computationally efficient\ntopology-based complexity notions that exhibit a strong correlation with the\ngeneralization gap in modern deep neural networks (DNNs). DNNs show remarkable\ngeneralization properties, yet the source of these capabilities remains\nelusive, defying the established statistical learning theory. Recent studies\nhave revealed that properties of training trajectories can be indicative of\ngeneralization. Building on this insight, state-of-the-art methods have\nleveraged the topology of these trajectories, particularly their fractal\ndimension, to quantify generalization. Most existing works compute this\nquantity by assuming continuous- or infinite-time training dynamics,\ncomplicating the development of practical estimators capable of accurately\npredicting generalization without access to test data. In this paper, we\nrespect the discrete-time nature of training trajectories and investigate the\nunderlying topological quantities that can be amenable to topological data\nanalysis tools. This leads to a new family of reliable topological complexity\nmeasures that provably bound the generalization error, eliminating the need for\nrestrictive geometric assumptions. These measures are computationally friendly,\nenabling us to propose simple yet effective algorithms for computing\ngeneralization indices. Moreover, our flexible framework can be extended to\ndifferent domains, tasks, and architectures. Our experimental results\ndemonstrate that our new complexity measures correlate highly with\ngeneralization error in industry-standards architectures such as transformers\nand deep graph networks. Our approach consistently outperforms existing\ntopological bounds across a wide range of datasets, models, and optimizers,\nhighlighting the practical relevance and effectiveness of our complexity\nmeasures.\n","authors":["Rayna Andreeva","Benjamin Dupuis","Rik Sarkar","Tolga Birdal","Umut ÅimÅekli"],"pdf_url":"https://arxiv.org/pdf/2407.08723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08722v1","updated":"2024-07-11T17:55:49Z","published":"2024-07-11T17:55:49Z","title":"Unifying 3D Representation and Control of Diverse Robots with a Single\n  Camera","summary":"  Mirroring the complex structures and diverse functions of natural organisms\nis a long-standing challenge in robotics. Modern fabrication techniques have\ndramatically expanded feasible hardware, yet deploying these systems requires\ncontrol software to translate desired motions into actuator commands. While\nconventional robots can easily be modeled as rigid links connected via joints,\nit remains an open challenge to model and control bio-inspired robots that are\noften multi-material or soft, lack sensing capabilities, and may change their\nmaterial properties with use. Here, we introduce Neural Jacobian Fields, an\narchitecture that autonomously learns to model and control robots from vision\nalone. Our approach makes no assumptions about the robot's materials,\nactuation, or sensing, requires only a single camera for control, and learns to\ncontrol the robot without expert intervention by observing the execution of\nrandom commands. We demonstrate our method on a diverse set of robot\nmanipulators, varying in actuation, materials, fabrication, and cost. Our\napproach achieves accurate closed-loop control and recovers the causal dynamic\nstructure of each robot. By enabling robot control with a generic camera as the\nonly sensor, we anticipate our work will dramatically broaden the design space\nof robotic systems and serve as a starting point for lowering the barrier to\nrobotic automation.\n","authors":["Sizhe Lester Li","Annan Zhang","Boyuan Chen","Hanna Matusik","Chao Liu","Daniela Rus","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2407.08722v1.pdf","comment":"Project Page:\n  https://sizhe-li.github.io/publication/neural_jacobian_field"},{"id":"http://arxiv.org/abs/2407.08715v1","updated":"2024-07-11T17:50:31Z","published":"2024-07-11T17:50:31Z","title":"Sensor-Aware Classifiers for Energy-Efficient Time Series Applications\n  on IoT Devices","summary":"  Time-series data processing is an important component of many real-world\napplications, such as health monitoring, environmental monitoring, and digital\nagriculture. These applications collect distinct windows of sensor data (e.g.,\nfew seconds) and process them to assess the environment. Machine learning (ML)\nmodels are being employed in time-series applications due to their\ngeneralization abilities for classification. State-of-the-art time-series\napplications wait for entire sensor data window to become available before\nprocessing the data using ML algorithms, resulting in high sensor energy\nconsumption. However, not all situations require processing full sensor window\nto make accurate inference. For instance, in activity recognition, sitting and\nstanding activities can be inferred with partial windows. Using this insight,\nwe propose to employ early exit classifiers with partial sensor windows to\nminimize energy consumption while maintaining accuracy. Specifically, we first\nutilize multiple early exits with successively increasing amount of data as\nthey become available in a window. If early exits provide inference with high\nconfidence, we return the label and enter low power mode for sensors. The\nproposed approach has potential to enable significant energy savings in time\nseries applications. We utilize neural networks and random forest classifiers\nto evaluate our approach. Our evaluations with six datasets show that the\nproposed approach enables up to 50-60% energy savings on average without any\nimpact on accuracy. The energy savings can enable time-series applications in\nremote locations with limited energy availability.\n","authors":["Dina Hussein","Lubah Nelson","Ganapati Bhat"],"pdf_url":"https://arxiv.org/pdf/2407.08715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08708v1","updated":"2024-07-11T17:46:21Z","published":"2024-07-11T17:46:21Z","title":"eyeballvul: a future-proof benchmark for vulnerability detection in the\n  wild","summary":"  Long contexts of recent LLMs have enabled a new use case: asking models to\nfind security vulnerabilities in entire codebases. To evaluate model\nperformance on this task, we introduce eyeballvul: a benchmark designed to test\nthe vulnerability detection capabilities of language models at scale, that is\nsourced and updated weekly from the stream of published vulnerabilities in\nopen-source repositories. The benchmark consists of a list of revisions in\ndifferent repositories, each associated with the list of known vulnerabilities\npresent at that revision. An LLM-based scorer is used to compare the list of\npossible vulnerabilities returned by a model to the list of known\nvulnerabilities for each revision. As of July 2024, eyeballvul contains 24,000+\nvulnerabilities across 6,000+ revisions and 5,000+ repositories, and is around\n55GB in size.\n","authors":["Timothee Chauvin"],"pdf_url":"https://arxiv.org/pdf/2407.08708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08707v1","updated":"2024-07-11T17:44:41Z","published":"2024-07-11T17:44:41Z","title":"Extracting Training Data from Document-Based VQA Models","summary":"  Vision-Language Models (VLMs) have made remarkable progress in document-based\nVisual Question Answering (i.e., responding to queries about the contents of an\ninput document provided as an image). In this work, we show these models can\nmemorize responses for training samples and regurgitate them even when the\nrelevant visual information has been removed. This includes Personal\nIdentifiable Information (PII) repeated once in the training set, indicating\nthese models could divulge memorised sensitive information and therefore pose a\nprivacy risk. We quantitatively measure the extractability of information in\ncontrolled experiments and differentiate between cases where it arises from\ngeneralization capabilities or from memorization. We further investigate the\nfactors that influence memorization across multiple state-of-the-art models and\npropose an effective heuristic countermeasure that empirically prevents the\nextractability of PII.\n","authors":["Francesco Pinto","Nathalie Rauschmayr","Florian TramÃ¨r","Philip Torr","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2407.08707v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2407.08704v1","updated":"2024-07-11T17:40:39Z","published":"2024-07-11T17:40:39Z","title":"Towards Efficient Deployment of Hybrid SNNs on Neuromorphic and Edge AI\n  Hardware","summary":"  This paper explores the synergistic potential of neuromorphic and edge\ncomputing to create a versatile machine learning (ML) system tailored for\nprocessing data captured by dynamic vision sensors. We construct and train\nhybrid models, blending spiking neural networks (SNNs) and artificial neural\nnetworks (ANNs) using PyTorch and Lava frameworks. Our hybrid architecture\nintegrates an SNN for temporal feature extraction and an ANN for\nclassification. We delve into the challenges of deploying such hybrid\nstructures on hardware. Specifically, we deploy individual components on\nIntel's Neuromorphic Processor Loihi (for SNN) and Jetson Nano (for ANN). We\nalso propose an accumulator circuit to transfer data from the spiking to the\nnon-spiking domain. Furthermore, we conduct comprehensive performance analyses\nof hybrid SNN-ANN models on a heterogeneous system of neuromorphic and edge AI\nhardware, evaluating accuracy, latency, power, and energy consumption. Our\nfindings demonstrate that the hybrid spiking networks surpass the baseline ANN\nmodel across all metrics and outperform the baseline SNN model in accuracy and\nlatency.\n","authors":["James Seekings","Peyton Chandarana","Mahsa Ardakani","MohammadReza Mohammadi","Ramtin Zand"],"pdf_url":"https://arxiv.org/pdf/2407.08704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08700v1","updated":"2024-07-11T17:33:38Z","published":"2024-07-11T17:33:38Z","title":"Flex-TPU: A Flexible TPU with Runtime Reconfigurable Dataflow\n  Architecture","summary":"  Tensor processing units (TPUs) are one of the most well-known machine\nlearning (ML) accelerators utilized at large scale in data centers as well as\nin tiny ML applications. TPUs offer several improvements and advantages over\nconventional ML accelerators, like graphical processing units (GPUs), being\ndesigned specifically to perform the multiply-accumulate (MAC) operations\nrequired in the matrix-matrix and matrix-vector multiplies extensively present\nthroughout the execution of deep neural networks (DNNs). Such improvements\ninclude maximizing data reuse and minimizing data transfer by leveraging the\ntemporal dataflow paradigms provided by the systolic array architecture. While\nthis design provides a significant performance benefit, the current\nimplementations are restricted to a single dataflow consisting of either input,\noutput, or weight stationary architectures. This can limit the achievable\nperformance of DNN inference and reduce the utilization of compute units.\nTherefore, the work herein consists of developing a reconfigurable dataflow\nTPU, called the Flex-TPU, which can dynamically change the dataflow per layer\nduring run-time. Our experiments thoroughly test the viability of the Flex-TPU\ncomparing it to conventional TPU designs across multiple well-known ML\nworkloads. The results show that our Flex-TPU design achieves a significant\nperformance increase of up to 2.75x compared to conventional TPU, with only\nminor area and power overheads.\n","authors":["Mohammed Elbtity","Peyton Chandarana","Ramtin Zand"],"pdf_url":"https://arxiv.org/pdf/2407.08700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08699v1","updated":"2024-07-11T17:32:40Z","published":"2024-07-11T17:32:40Z","title":"Mitigating Catastrophic Forgetting in Language Transfer via Model\n  Merging","summary":"  As open-weight large language models (LLMs) achieve ever more impressive\nperformances across a wide range of tasks in English, practitioners aim to\nadapt these models to different languages. However, such language adaptation is\noften accompanied by catastrophic forgetting of the base model's capabilities,\nseverely limiting the usefulness of the resulting model. We address this issue\nby proposing Branch-and-Merge (BaM), a new adaptation method based on\niteratively merging multiple models, fine-tuned on a subset of the available\ntraining data. BaM is based on the insight that this yields lower magnitude but\nhigher quality weight changes, reducing forgetting of the source domain while\nmaintaining learning on the target domain. We demonstrate in an extensive\nempirical study on Bulgarian and German that BaM can significantly reduce\nforgetting while matching or even improving target domain performance compared\nto both standard continued pretraining and instruction finetuning across\ndifferent model architectures.\n","authors":["Anton Alexandrov","Veselin Raychev","Mark Niklas MÃ¼ller","Ce Zhang","Martin Vechev","Kristina Toutanova"],"pdf_url":"https://arxiv.org/pdf/2407.08699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08694v1","updated":"2024-07-11T17:31:12Z","published":"2024-07-11T17:31:12Z","title":"Cloud Atlas: Efficient Fault Localization for Cloud Systems using\n  Language Models and Causal Insight","summary":"  Runtime failure and performance degradation is commonplace in modern cloud\nsystems. For cloud providers, automatically determining the root cause of\nincidents is paramount to ensuring high reliability and availability as prompt\nfault localization can enable faster diagnosis and triage for timely\nresolution. A compelling solution explored in recent work is causal reasoning\nusing causal graphs to capture relationships between varied cloud system\nperformance metrics. To be effective, however, systems developers must\ncorrectly define the causal graph of their system, which is a time-consuming,\nbrittle, and challenging task that increases in difficulty for large and\ndynamic systems and requires domain expertise. Alternatively, automated\ndata-driven approaches have limited efficacy for cloud systems due to the\ninherent rarity of incidents. In this work, we present Atlas, a novel approach\nto automatically synthesizing causal graphs for cloud systems. Atlas leverages\nlarge language models (LLMs) to generate causal graphs using system\ndocumentation, telemetry, and deployment feedback. Atlas is complementary to\ndata-driven causal discovery techniques, and we further enhance Atlas with a\ndata-driven validation step. We evaluate Atlas across a range of fault\nlocalization scenarios and demonstrate that Atlas is capable of generating\ncausal graphs in a scalable and generalizable manner, with performance that far\nsurpasses that of data-driven algorithms and is commensurate to the\nground-truth baseline.\n","authors":["Zhiqiang Xie","Yujia Zheng","Lizi Ottens","Kun Zhang","Christos Kozyrakis","Jonathan Mace"],"pdf_url":"https://arxiv.org/pdf/2407.08694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08693v1","updated":"2024-07-11T17:31:01Z","published":"2024-07-11T17:31:01Z","title":"Robotic Control via Embodied Chain-of-Thought Reasoning","summary":"  A key limitation of learned robot control policies is their inability to\ngeneralize outside their training data. Recent works on vision-language-action\nmodels (VLAs) have shown that the use of large, internet pre-trained\nvision-language models as the backbone of learned robot policies can\nsubstantially improve their robustness and generalization ability. Yet, one of\nthe most exciting capabilities of large vision-language models in other domains\nis their ability to reason iteratively through complex problems. Can that same\ncapability be brought into robotics to allow policies to improve performance by\nreasoning about a given task before acting? Naive use of \"chain-of-thought\"\n(CoT) style prompting is significantly less effective with standard VLAs\nbecause of the relatively simple training examples that are available to them.\nAdditionally, purely semantic reasoning about sub-tasks, as is common in\nregular CoT, is insufficient for robot policies that need to ground their\nreasoning in sensory observations and the robot state. To this end, we\nintroduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which we\ntrain VLAs to perform multiple steps of reasoning about plans, sub-tasks,\nmotions, and visually grounded features like object bounding boxes and end\neffector positions, before predicting the robot action. We design a scalable\npipeline for generating synthetic training data for ECoT on large robot\ndatasets. We demonstrate, that ECoT increases the absolute success rate of\nOpenVLA, the current strongest open-source VLA policy, by 28% across\nchallenging generalization tasks, without any additional robot training data.\nAdditionally, ECoT makes it easier for humans to interpret a policy's failures\nand correct its behavior using natural language.\n","authors":["Zawalski MichaÅ","Chen William","Pertsch Karl","Mees Oier","Finn Chelsea","Levine Sergey"],"pdf_url":"https://arxiv.org/pdf/2407.08693v1.pdf","comment":"Project Website: https://embodied-cot.github.io"},{"id":"http://arxiv.org/abs/2407.08689v1","updated":"2024-07-11T17:28:07Z","published":"2024-07-11T17:28:07Z","title":"Operationalizing the Blueprint for an AI Bill of Rights: Recommendations\n  for Practitioners, Researchers, and Policy Makers","summary":"  As Artificial Intelligence (AI) tools are increasingly employed in diverse\nreal-world applications, there has been significant interest in regulating\nthese tools. To this end, several regulatory frameworks have been introduced by\ndifferent countries worldwide. For example, the European Union recently passed\nthe AI Act, the White House issued an Executive Order on safe, secure, and\ntrustworthy AI, and the White House Office of Science and Technology Policy\nissued the Blueprint for an AI Bill of Rights (AI BoR). Many of these\nframeworks emphasize the need for auditing and improving the trustworthiness of\nAI tools, underscoring the importance of safety, privacy, explainability,\nfairness, and human fallback options. Although these regulatory frameworks\nhighlight the necessity of enforcement, practitioners often lack detailed\nguidance on implementing them. Furthermore, the extensive research on\noperationalizing each of these aspects is frequently buried in technical papers\nthat are difficult for practitioners to parse. In this write-up, we address\nthis shortcoming by providing an accessible overview of existing literature\nrelated to operationalizing regulatory principles. We provide\neasy-to-understand summaries of state-of-the-art literature and highlight\nvarious gaps that exist between regulatory guidelines and existing AI research,\nincluding the trade-offs that emerge during operationalization. We hope that\nthis work not only serves as a starting point for practitioners interested in\nlearning more about operationalizing the regulatory guidelines outlined in the\nBlueprint for an AI BoR but also provides researchers with a list of critical\nopen problems and gaps between regulations and state-of-the-art AI research.\nFinally, we note that this is a working paper and we invite feedback in line\nwith the purpose of this document as described in the introduction.\n","authors":["Alex Oesterling","Usha Bhalla","Suresh Venkatasubramanian","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2407.08689v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2403.17933v2","updated":"2024-07-11T17:27:49Z","published":"2024-03-26T17:58:29Z","title":"SLEDGE: Synthesizing Driving Environments with Generative Models and\n  Rule-Based Traffic","summary":"  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for rule-based traffic simulation. The unique properties of\nthe entities to be generated for SLEDGE, such as their connectivity and\nvariable count per scene, render the naive application of most modern\ngenerative models to this task non-trivial. Therefore, together with a\nsystematic study of existing lane graph representations, we introduce a novel\nraster-to-vector autoencoder. It encodes agents and the lane graph into\ndistinct channels in a rasterized latent map. This facilitates both\nlane-conditioned agent generation and combined generation of lanes and agents\nwith a Diffusion Transformer. Using generated entities in SLEDGE enables\ngreater control over the simulation, e.g. upsampling turns or increasing\ntraffic density. Further, SLEDGE can support 500m long routes, a capability not\nfound in existing data-driven simulators like nuPlan. It presents new\nchallenges for planning algorithms, evidenced by failure rates of over 40% for\nPDM, the winner of the 2023 nuPlan challenge, when tested on hard routes and\ndense traffic generated by our model. Compared to nuPlan, SLEDGE requires\n500$\\times$ less storage to set up (<4 GB), making it a more accessible option\nand helping with democratizing future research in this field.\n","authors":["Kashyap Chitta","Daniel Dauner","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.17933v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2401.00003v2","updated":"2024-07-11T17:27:35Z","published":"2023-12-08T04:24:03Z","title":"Generative Inverse Design of Metamaterials with Functional Responses by\n  Interpretable Learning","summary":"  Metamaterials with functional responses, such as wave-based responses or\ndeformation-induced property variation under external stimuli, can exhibit\nvarying properties or functionalities under different conditions. Herein, we\naim at rapid inverse design of these metamaterials to meet target qualitative\nfunctional behaviors. This inverse problem is challenging due to its\nintractability and the existence of non-unique solutions. Past works mainly\nfocus on deep-learning-based methods that are data-demanding, require\ntime-consuming training and hyperparameter tuning, and are non-interpretable.\nTo overcome these limitations, we propose the Random-forest-based Interpretable\nGenerative Inverse Design (RIGID), an iteration-free, single-shot inverse\ndesign method to achieve the fast generation of metamaterial designs with\non-demand functional behaviors. Unlike most existing methods, by exploiting the\ninterpretability of the random forest, we eliminate the need to train an\ninverse model mapping responses to designs. Based on the likelihood of target\nsatisfaction derived from the trained forward model, one can sample design\nsolutions using Markov chain Monte Carlo methods. The RIGID method therefore\nfunctions as a generative model that captures the conditional distribution of\nsatisfying solutions given a design target. We demonstrate the effectiveness\nand efficiency of RIGID on both acoustic and optical metamaterial design\nproblems where only small datasets (less than 250 training samples) are\navailable. Synthetic design problems are created to further illustrate and\nvalidate the mechanism of likelihood estimation in RIGID. This work offers a\nnew perspective on solving on-demand inverse design problems, showcasing the\npotential for incorporating interpretable machine learning into generative\ndesign and eliminating its large data requirement.\n","authors":["Wei \"Wayne\" Chen","Rachel Sun","Doksoo Lee","Carlos M. Portela","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2401.00003v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08681v1","updated":"2024-07-11T17:14:19Z","published":"2024-07-11T17:14:19Z","title":"Hardware Neural Control of CartPole and F1TENTH Race Car","summary":"  Nonlinear model predictive control (NMPC) has proven to be an effective\ncontrol method, but it is expensive to compute. This work demonstrates the use\nof hardware FPGA neural network controllers trained to imitate NMPC with\nsupervised learning. We use these Neural Controllers (NCs) implemented on\ninexpensive embedded FPGA hardware for high frequency control on physical\ncartpole and F1TENTH race car. Our results show that the NCs match the control\nperformance of the NMPCs in simulation and outperform it in reality, due to the\nfaster control rate that is afforded by the quick FPGA NC inference. We\ndemonstrate kHz control rates for a physical cartpole and offloading control to\nthe FPGA hardware on the F1TENTH car. Code and hardware implementation for this\npaper are available at https:// github.com/SensorsINI/Neural-Control-Tools.\n","authors":["Marcin Paluch","Florian Bolli","Xiang Deng","Antonio Rios Navarro","Chang Gao","Tobi Delbruck"],"pdf_url":"https://arxiv.org/pdf/2407.08681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08678v1","updated":"2024-07-11T17:12:42Z","published":"2024-07-11T17:12:42Z","title":"How to beat a Bayesian adversary","summary":"  Deep neural networks and other modern machine learning models are often\nsusceptible to adversarial attacks. Indeed, an adversary may often be able to\nchange a model's prediction through a small, directed perturbation of the\nmodel's input - an issue in safety-critical applications. Adversarially robust\nmachine learning is usually based on a minmax optimisation problem that\nminimises the machine learning loss under maximisation-based adversarial\nattacks.\n  In this work, we study adversaries that determine their attack using a\nBayesian statistical approach rather than maximisation. The resulting Bayesian\nadversarial robustness problem is a relaxation of the usual minmax problem. To\nsolve this problem, we propose Abram - a continuous-time particle system that\nshall approximate the gradient flow corresponding to the underlying learning\nproblem. We show that Abram approximates a McKean-Vlasov process and justify\nthe use of Abram by giving assumptions under which the McKean-Vlasov process\nfinds the minimiser of the Bayesian adversarial robustness problem. We discuss\ntwo ways to discretise Abram and show its suitability in benchmark adversarial\ndeep learning experiments.\n","authors":["Zihan Ding","Kexin Jin","Jonas Latz","Chenguang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13283v2","updated":"2024-07-11T17:10:24Z","published":"2024-06-19T07:23:51Z","title":"Large-Scale Dataset Pruning in Adversarial Training through Data\n  Importance Extrapolation","summary":"  Their vulnerability to small, imperceptible attacks limits the adoption of\ndeep learning models to real-world systems. Adversarial training has proven to\nbe one of the most promising strategies against these attacks, at the expense\nof a substantial increase in training time. With the ongoing trend of\nintegrating large-scale synthetic data this is only expected to increase even\nfurther. Thus, the need for data-centric approaches that reduce the number of\ntraining samples while maintaining accuracy and robustness arises. While data\npruning and active learning are prominent research topics in deep learning,\nthey are as of now largely unexplored in the adversarial training literature.\nWe address this gap and propose a new data pruning strategy based on\nextrapolating data importance scores from a small set of data to a larger set.\nIn an empirical evaluation, we demonstrate that extrapolation-based pruning can\nefficiently reduce dataset size while maintaining robustness.\n","authors":["BjÃ¶rn Nieth","Thomas Altstidl","Leo Schwinn","BjÃ¶rn Eskofier"],"pdf_url":"https://arxiv.org/pdf/2406.13283v2.pdf","comment":"8 pages, 5 figures, 3 tables, to be published in ICML: DMLR workshop"},{"id":"http://arxiv.org/abs/2407.08668v1","updated":"2024-07-11T16:57:17Z","published":"2024-07-11T16:57:17Z","title":"Estimation of spatio-temporal extremes via generative neural networks","summary":"  Recent methods in modeling spatial extreme events have focused on utilizing\nparametric max-stable processes and their underlying dependence structure. In\nthis work, we provide a unified approach for analyzing spatial extremes with\nlittle available data by estimating the distribution of model parameters or the\nspatial dependence directly. By employing recent developments in generative\nneural networks we predict a full sample-based distribution, allowing for\ndirect assessment of uncertainty regarding model parameters or other parameter\ndependent functionals. We validate our method by fitting several simulated\nmax-stable processes, showing a high accuracy of the approach, regarding\nparameter estimation, as well as uncertainty quantification. Additional\nrobustness checks highlight the generalization and extrapolation capabilities\nof the model, while an application to precipitation extremes across Western\nGermany demonstrates the usability of our approach in real-world scenarios.\n","authors":["Christopher BÃ¼lte","Lisa Leimenstoll","Melanie Schienle"],"pdf_url":"https://arxiv.org/pdf/2407.08668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14194v2","updated":"2024-07-11T16:50:08Z","published":"2024-02-22T00:38:43Z","title":"BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human\n  Racing Gameplay","summary":"  Imitation learning learns a policy from demonstrations without requiring\nhand-designed reward functions. In many robotic tasks, such as autonomous\nracing, imitated policies must model complex environment dynamics and human\ndecision-making. Sequence modeling is highly effective in capturing intricate\npatterns of motion sequences but struggles to adapt to new environments or\ndistribution shifts that are common in real-world robotics tasks. In contrast,\nAdversarial Imitation Learning (AIL) can mitigate this effect, but struggles\nwith sample inefficiency and handling complex motion patterns. Thus, we propose\nBeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a\nBehavior Transformer (BeT) policy from human demonstrations with online AIL.\nBeTAIL adds an AIL residual policy to the BeT policy to model the sequential\ndecision-making process of human experts and correct for out-of-distribution\nstates or shifts in environment dynamics. We test BeTAIL on three challenges\nwith expert-level demonstrations of real human gameplay in Gran Turismo Sport.\nOur proposed residual BeTAIL reduces environment interactions and improves\nracing performance and stability, even when the BeT is pretrained on different\ntracks than downstream learning. Videos and code available at:\nhttps://sites.google.com/berkeley.edu/BeTAIL/home.\n","authors":["Catherine Weaver","Chen Tang","Ce Hao","Kenta Kawamoto","Masayoshi Tomizuka","Wei Zhan"],"pdf_url":"https://arxiv.org/pdf/2402.14194v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.08659v1","updated":"2024-07-11T16:46:04Z","published":"2024-07-11T16:46:04Z","title":"Controlling the Fidelity and Diversity of Deep Generative Models via\n  Pseudo Density","summary":"  We introduce an approach to bias deep generative models, such as GANs and\ndiffusion models, towards generating data with either enhanced fidelity or\nincreased diversity. Our approach involves manipulating the distribution of\ntraining and generated data through a novel metric for individual samples,\nnamed pseudo density, which is based on the nearest-neighbor information from\nreal samples. Our approach offers three distinct techniques to adjust the\nfidelity and diversity of deep generative models: 1) Per-sample perturbation,\nenabling precise adjustments for individual samples towards either more common\nor more unique characteristics; 2) Importance sampling during model inference\nto enhance either fidelity or diversity in the generated data; 3) Fine-tuning\nwith importance sampling, which guides the generative model to learn an\nadjusted distribution, thus controlling fidelity and diversity. Furthermore,\nour fine-tuning method demonstrates the ability to improve the Frechet\nInception Distance (FID) for pre-trained generative models with minimal\niterations.\n","authors":["Shuangqi Li","Chen Liu","Tong Zhang","Hieu Le","Sabine SÃ¼sstrunk","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2407.08659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05073v2","updated":"2024-07-11T16:43:38Z","published":"2023-12-08T14:46:50Z","title":"A Distributed ADMM-based Deep Learning Approach for Thermal Control in\n  Multi-Zone Buildings under Demand Response Events","summary":"  The increasing electricity use and reliance on intermittent renewable energy\nsources challenge power grid management during peak demand, making Demand\nResponse programs and energy conservation measures essential. This research\ncombines distributed optimization using ADMM with deep learning models to plan\nindoor temperature setpoints effectively. A two-layer hierarchical structure is\nused, with a central building coordinator at the upper layer and local\ncontrollers at the thermal zone layer. The coordinator must limit the\nbuilding's maximum power by translating the building's total power to local\npower targets for each zone. Local controllers can modify the temperature\nsetpoints to meet the local power targets. While most algorithms are either\ncentralized or require prior knowledge about the building's structure, our\napproach is distributed and fully data-driven. The proposed algorithm, called\nDistributed Planning Networks, is designed to be both adaptable and scalable to\nmany types of buildings, tackling two of the main challenges in the development\nof such systems. The proposed approach is tested on an 18-zone building modeled\nin EnergyPlus. The algorithm successfully manages Demand Response peak events.\n","authors":["Vincent Taboga","Hanane Dagdougui"],"pdf_url":"https://arxiv.org/pdf/2312.05073v2.pdf","comment":"15 pages, 9 figures, to be published in IEEE Transactions on\n  Automation Science and Engineering"},{"id":"http://arxiv.org/abs/2407.08655v1","updated":"2024-07-11T16:39:24Z","published":"2024-07-11T16:39:24Z","title":"SPOCKMIP: Segmentation of Vessels in MRAs with Enhanced Continuity using\n  Maximum Intensity Projection as Loss","summary":"  Identification of vessel structures of different sizes in biomedical images\nis crucial in the diagnosis of many neurodegenerative diseases. However, the\nsparsity of good-quality annotations of such images makes the task of vessel\nsegmentation challenging. Deep learning offers an efficient way to segment\nvessels of different sizes by learning their high-level feature representations\nand the spatial continuity of such features across dimensions. Semi-supervised\npatch-based approaches have been effective in identifying small vessels of one\nto two voxels in diameter. This study focuses on improving the segmentation\nquality by considering the spatial correlation of the features using the\nMaximum Intensity Projection~(MIP) as an additional loss criterion. Two methods\nare proposed with the incorporation of MIPs of label segmentation on the\nsingle~(z-axis) and multiple perceivable axes of the 3D volume. The proposed\nMIP-based methods produce segmentations with improved vessel continuity, which\nis evident in visual examinations of ROIs. Patch-based training is improved by\nintroducing an additional loss term, MIP loss, to penalise the predicted\ndiscontinuity of vessels. A training set of 14 volumes is selected from the\nStudyForrest dataset comprising of 18 7-Tesla 3D Time-of-Flight~(ToF) Magnetic\nResonance Angiography (MRA) images. The generalisation performance of the\nmethod is evaluated using the other unseen volumes in the dataset. It is\nobserved that the proposed method with multi-axes MIP loss produces better\nquality segmentations with a median Dice of $80.245 \\pm 0.129$. Also, the\nmethod with single-axis MIP loss produces segmentations with a median Dice of\n$79.749 \\pm 0.109$. Furthermore, a visual comparison of the ROIs in the\npredicted segmentation reveals a significant improvement in the continuity of\nthe vessels when MIP loss is incorporated into training.\n","authors":["Chethan Radhakrishna","Karthikesh Varma Chintalapati","Sri Chandana Hudukula Ram Kumar","Raviteja Sutrave","Hendrik Mattern","Oliver Speck","Andreas NÃ¼rnberger","Soumick Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2407.08655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08654v1","updated":"2024-07-11T16:37:15Z","published":"2024-07-11T16:37:15Z","title":"Adaptive Smooth Non-Stationary Bandits","summary":"  We study a $K$-armed non-stationary bandit model where rewards change\nsmoothly, as captured by H\\\"{o}lder class assumptions on rewards as functions\nof time. Such smooth changes are parametrized by a H\\\"{o}lder exponent $\\beta$\nand coefficient $\\lambda$. While various sub-cases of this general model have\nbeen studied in isolation, we first establish the minimax dynamic regret rate\ngenerally for all $K,\\beta,\\lambda$. Next, we show this optimal dynamic regret\ncan be attained adaptively, without knowledge of $\\beta,\\lambda$. To contrast,\neven with parameter knowledge, upper bounds were only previously known for\nlimited regimes $\\beta\\leq 1$ and $\\beta=2$ (Slivkins, 2014; Krishnamurthy and\nGopalan, 2021; Manegueu et al., 2021; Jia et al.,2023). Thus, our work resolves\nopen questions raised by these disparate threads of the literature.\n  We also study the problem of attaining faster gap-dependent regret rates in\nnon-stationary bandits. While such rates are long known to be impossible in\ngeneral (Garivier and Moulines, 2011), we show that environments admitting a\nsafe arm (Suk and Kpotufe, 2022) allow for much faster rates than the\nworst-case scaling with $\\sqrt{T}$. While previous works in this direction\nfocused on attaining the usual logarithmic regret bounds, as summed over\nstationary periods, our new gap-dependent rates reveal new optimistic regimes\nof non-stationarity where even the logarithmic bounds are pessimistic. We show\nour new gap-dependent rate is tight and that its achievability (i.e., as made\npossible by a safe arm) has a surprisingly simple and clean characterization\nwithin the smooth H\\\"{o}lder class model.\n","authors":["Joe Suk"],"pdf_url":"https://arxiv.org/pdf/2407.08654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11413v2","updated":"2024-07-11T16:32:12Z","published":"2023-11-19T20:16:16Z","title":"Large Pre-trained time series models for cross-domain Time series\n  analysis tasks","summary":"  Large pre-trained models have been vital in recent advancements in domains\nlike language and vision, making model training for individual downstream tasks\nmore efficient and provide superior performance. However, tackling time-series\nanalysis tasks usually involves designing and training a separate model from\nscratch leveraging training data and domain expertise specific to the task. We\ntackle a significant challenge for pre-training a foundational time-series\nmodel from multi-domain time-series datasets: extracting\n  semantically useful tokenized inputs to the model\n  across heterogenous time-series from different domains. We propose Large\nPre-trained Time-series Models (LPTM) that introduces a novel method of\n\\textit{adaptive segmentation} that automatically identifies optimal\ndataset-specific\n  segmentation strategy during pre-training. This enables\n  LPTM to perform similar to or better than domain-specific state-of-art model\n  when fine-tuned to different downstream time-series analysis tasks and under\nzero-shot settings.\n  LPTM achieves superior forecasting and time-series classification results\n  taking up to 40% less data and 50% less training time\n  compared to state-of-art baselines.\n","authors":["Harshavardhan Kamarthi","B. Aditya Prakash"],"pdf_url":"https://arxiv.org/pdf/2311.11413v2.pdf","comment":"16 pages, 5 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2407.08649v1","updated":"2024-07-11T16:28:31Z","published":"2024-07-11T16:28:31Z","title":"Confidence-based Estimators for Predictive Performance in Model\n  Monitoring","summary":"  After a machine learning model has been deployed into production, its\npredictive performance needs to be monitored. Ideally, such monitoring can be\ncarried out by comparing the model's predictions against ground truth labels.\nFor this to be possible, the ground truth labels must be available relatively\nsoon after inference. However, there are many use cases where ground truth\nlabels are available only after a significant delay, or in the worst case, not\nat all. In such cases, directly monitoring the model's predictive performance\nis impossible.\n  Recently, novel methods for estimating the predictive performance of a model\nwhen ground truth is unavailable have been developed. Many of these methods\nleverage model confidence or other uncertainty estimates and are experimentally\ncompared against a naive baseline method, namely Average Confidence (AC), which\nestimates model accuracy as the average of confidence scores for a given set of\npredictions. However, until now the theoretical properties of the AC method\nhave not been properly explored. In this paper, we try to fill this gap by\nreviewing the AC method and show that under certain general assumptions, it is\nan unbiased and consistent estimator of model accuracy with many desirable\nproperties. We also compare this baseline estimator against some more complex\nestimators empirically and show that in many cases the AC method is able to\nbeat the others, although the comparative quality of the different estimators\nis heavily case-dependent.\n","authors":["Juhani KivimÃ¤ki","Jakub BiaÅek","Jukka K. Nurminen","Wojtek Kuberski"],"pdf_url":"https://arxiv.org/pdf/2407.08649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08647v1","updated":"2024-07-11T16:25:21Z","published":"2024-07-11T16:25:21Z","title":"From Real to Cloned Singer Identification","summary":"  Cloned voices of popular singers sound increasingly realistic and have gained\npopularity over the past few years. They however pose a threat to the industry\ndue to personality rights concerns. As such, methods to identify the original\nsinger in synthetic voices are needed. In this paper, we investigate how singer\nidentification methods could be used for such a task. We present three\nembedding models that are trained using a singer-level contrastive learning\nscheme, where positive pairs consist of segments with vocals from the same\nsingers. These segments can be mixtures for the first model, vocals for the\nsecond, and both for the third. We demonstrate that all three models are highly\ncapable of identifying real singers. However, their performance deteriorates\nwhen classifying cloned versions of singers in our evaluation set. This is\nespecially true for models that use mixtures as an input. These findings\nhighlight the need to understand the biases that exist within singer\nidentification systems, and how they can influence the identification of voice\ndeepfakes in music.\n","authors":["Dorian Desblancs","Gabriel Meseguer-Brocal","Romain Hennequin","Manuel Moussallam"],"pdf_url":"https://arxiv.org/pdf/2407.08647v1.pdf","comment":"To be published at ISMIR 2024"},{"id":"http://arxiv.org/abs/2405.05241v2","updated":"2024-07-11T16:24:52Z","published":"2024-05-08T17:37:57Z","title":"BenthicNet: A global compilation of seafloor images for deep learning\n  applications","summary":"  Advances in underwater imaging enable the collection of extensive seafloor\nimage datasets that are necessary for monitoring important benthic ecosystems.\nThe ability to collect seafloor imagery has outpaced our capacity to analyze\nit, hindering expedient mobilization of this crucial environmental information.\nRecent machine learning approaches provide opportunities to increase the\nefficiency with which seafloor image datasets are analyzed, yet large and\nconsistent datasets necessary to support development of such approaches are\nscarce. Here we present BenthicNet: a global compilation of seafloor imagery\ndesigned to support the training and evaluation of large-scale image\nrecognition models. An initial set of over 11.4 million images was collected\nand curated to represent a diversity of seafloor environments using a\nrepresentative subset of 1.3 million images. These are accompanied by 2.6\nmillion annotations translated to the CATAMI scheme, which span 190,000 of the\nimages. A large deep learning model was trained on this compilation and\npreliminary results suggest it has utility for automating large and small-scale\nimage analysis tasks. The compilation and model are made openly available for\nuse by the scientific community at https://doi.org/10.20383/103.0614.\n","authors":["Scott C. Lowe","Benjamin Misiuk","Isaac Xu","Shakhboz Abdulazizov","Amit R. Baroi","Alex C. Bastos","Merlin Best","Vicki Ferrini","Ariell Friedman","Deborah Hart","Ove Hoegh-Guldberg","Daniel Ierodiaconou","Julia Mackin-McLaughlin","Kathryn Markey","Pedro S. Menandro","Jacquomo Monk","Shreya Nemani","John O'Brien","Elizabeth Oh","Luba Y. Reshitnyk","Katleen Robert","Chris M. Roelfsema","Jessica A. Sameoto","Alexandre C. G. Schimel","Jordan A. Thomson","Brittany R. Wilson","Melisa C. Wong","Craig J. Brown","Thomas Trappenberg"],"pdf_url":"https://arxiv.org/pdf/2405.05241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08641v1","updated":"2024-07-11T16:22:13Z","published":"2024-07-11T16:22:13Z","title":"How more data can hurt: Instability and regularization in\n  next-generation reservoir computing","summary":"  It has been found recently that more data can, counter-intuitively, hurt the\nperformance of deep neural networks. Here, we show that a more extreme version\nof the phenomenon occurs in data-driven models of dynamical systems. To\nelucidate the underlying mechanism, we focus on next-generation reservoir\ncomputing (NGRC) -- a popular framework for learning dynamics from data. We\nfind that, despite learning a better representation of the flow map with more\ntraining data, NGRC can adopt an ill-conditioned ``integrator'' and lose\nstability. We link this data-induced instability to the auxiliary dimensions\ncreated by the delayed states in NGRC. Based on these findings, we propose\nsimple strategies to mitigate the instability, either by increasing\nregularization strength in tandem with data size, or by carefully introducing\nnoise during training. Our results highlight the importance of proper\nregularization in data-driven modeling of dynamical systems.\n","authors":["Yuanzhao Zhang","Sean P. Cornelius"],"pdf_url":"https://arxiv.org/pdf/2407.08641v1.pdf","comment":"10 pages, 10 figures. Comments welcome"},{"id":"http://arxiv.org/abs/2407.08639v1","updated":"2024-07-11T16:21:18Z","published":"2024-07-11T16:21:18Z","title":"$Î²$-DPO: Direct Preference Optimization with Dynamic $Î²$","summary":"  Direct Preference Optimization (DPO) has emerged as a compelling approach for\ntraining Large Language Models (LLMs) to adhere to human preferences. However,\nthe performance of DPO is sensitive to the fine-tuning of its trade-off\nparameter $\\beta$, as well as to the quality of the preference data. We analyze\nthe impact of $\\beta$ and data quality on DPO, uncovering that optimal $\\beta$\nvalues vary with the informativeness of pairwise data. Addressing the\nlimitations of static $\\beta$ values, we introduce a novel framework that\ndynamically calibrates $\\beta$ at the batch level, informed by data quality\nconsiderations. Additionally, our method incorporates $\\beta$-guided data\nfiltering to safeguard against the influence of outliers. Through empirical\nevaluation, we demonstrate that our dynamic $\\beta$ adjustment technique\nsignificantly improves DPO's performance across a range of models and datasets,\noffering a more robust and adaptable training paradigm for aligning LLMs with\nhuman feedback. The code is available at\n\\url{https://github.com/junkangwu/beta-DPO}.\n","authors":["Junkang Wu","Yuexiang Xie","Zhengyi Yang","Jiancan Wu","Jinyang Gao","Bolin Ding","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2407.08639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08632v1","updated":"2024-07-11T16:12:53Z","published":"2024-07-11T16:12:53Z","title":"Generalization Error Matters in Decentralized Learning Under Byzantine\n  Attacks","summary":"  Recently, decentralized learning has emerged as a popular peer-to-peer signal\nand information processing paradigm that enables model training across\ngeographically distributed agents in a scalable manner, without the presence of\nany central server. When some of the agents are malicious (also termed as\nByzantine), resilient decentralized learning algorithms are able to limit the\nimpact of these Byzantine agents without knowing their number and identities,\nand have guaranteed optimization errors. However, analysis of the\ngeneralization errors, which are critical to implementations of the trained\nmodels, is still lacking. In this paper, we provide the first analysis of the\ngeneralization errors for a class of popular Byzantine-resilient decentralized\nstochastic gradient descent (DSGD) algorithms. Our theoretical results reveal\nthat the generalization errors cannot be entirely eliminated because of the\npresence of the Byzantine agents, even if the number of training samples are\ninfinitely large. Numerical experiments are conducted to confirm our\ntheoretical results.\n","authors":["Haoxiang Ye","Qing Ling"],"pdf_url":"https://arxiv.org/pdf/2407.08632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08626v1","updated":"2024-07-11T16:05:56Z","published":"2024-07-11T16:05:56Z","title":"RoboMorph: Evolving Robot Morphology using Large Language Models","summary":"  We introduce RoboMorph, an automated approach for generating and optimizing\nmodular robot designs using large language models (LLMs) and evolutionary\nalgorithms. In this framework, we represent each robot design as a grammar and\nleverage the capabilities of LLMs to navigate the extensive robot design space,\nwhich is traditionally time-consuming and computationally demanding. By\nintegrating automatic prompt design and a reinforcement learning based control\nalgorithm, RoboMorph iteratively improves robot designs through feedback loops.\nOur experimental results demonstrate that RoboMorph can successfully generate\nnontrivial robots that are optimized for a single terrain while showcasing\nimprovements in morphology over successive evolutions. Our approach\ndemonstrates the potential of using LLMs for data-driven and modular robot\ndesign, providing a promising methodology that can be extended to other domains\nwith similar design frameworks.\n","authors":["Kevin Qiu","Krzysztof Ciebiera","PaweÅ FijaÅkowski","Marek Cygan","Åukasz KuciÅski"],"pdf_url":"https://arxiv.org/pdf/2407.08626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19163v2","updated":"2024-07-11T16:04:27Z","published":"2023-10-29T21:47:24Z","title":"RAIFLE: Reconstruction Attacks on Interaction-based Federated Learning\n  with Adversarial Data Manipulation","summary":"  Federated learning has emerged as a promising privacy-preserving solution for\nmachine learning domains that rely on user interactions, particularly\nrecommender systems and online learning to rank. While there has been\nsubstantial research on the privacy of traditional federated learning, little\nattention has been paid to the privacy properties of these interaction-based\nsettings. In this work, we show that users face an elevated risk of having\ntheir private interactions reconstructed by the central server when the server\ncan control the training features of the items that users interact with. We\nintroduce RAIFLE, a novel optimization-based attack framework where the server\nactively manipulates the features of the items presented to users to increase\nthe success rate of reconstruction. Our experiments with federated\nrecommendation and online learning-to-rank scenarios demonstrate that RAIFLE is\nsignificantly more powerful than existing reconstruction attacks like gradient\ninversion, achieving high performance consistently in most settings. We discuss\nthe pros and cons of several possible countermeasures to defend against RAIFLE\nin the context of interaction-based federated learning. Our code is\nopen-sourced at https://github.com/dzungvpham/raifle.\n","authors":["Dzung Pham","Shreyas Kulkarni","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2310.19163v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08623v1","updated":"2024-07-11T16:00:22Z","published":"2024-07-11T16:00:22Z","title":"Surpassing Cosine Similarity for Multidimensional Comparisons: Dimension\n  Insensitive Euclidean Metric (DIEM)","summary":"  The advancement in computational power and hardware efficiency has enabled\nthe tackling of increasingly complex and high-dimensional problems. While\nartificial intelligence (AI) has achieved remarkable results in various\nscientific and technological fields, the interpretability of these\nhigh-dimensional solutions remains challenging. A critical issue in this\ncontext is the comparison of multidimensional quantities, which is essential in\ntechniques like Principal Component Analysis (PCA), Singular Value\nDecomposition (SVD), and k-means clustering. Common metrics such as cosine\nsimilarity, Euclidean distance, and Manhattan distance are often used for such\ncomparisons - for example in muscular synergies of the human motor control\nsystem. However, their applicability and interpretability diminish as\ndimensionality increases. This paper provides a comprehensive analysis of the\neffects of dimensionality on these three widely used metrics. Our results\nreveal significant limitations of cosine similarity, particularly its\ndependency on the dimensionality of the vectors, leading to biased and less\ninterpretable outcomes. To address this, we introduce the Dimension Insensitive\nEuclidean Metric (DIEM), derived from the Euclidean distance, which\ndemonstrates superior robustness and generalizability across varying\ndimensions. DIEM maintains consistent variability and eliminates the biases\nobserved in traditional metrics, making it a more reliable tool for\nhigh-dimensional comparisons. This novel metric has the potential to replace\ncosine similarity, providing a more accurate and insightful method to analyze\nmultidimensional data in fields ranging from neuromotor control to machine\nlearning and deep learning.\n","authors":["Federico Tessari","Neville Hogan"],"pdf_url":"https://arxiv.org/pdf/2407.08623v1.pdf","comment":"10 pages, 17 figures"},{"id":"http://arxiv.org/abs/2401.07844v5","updated":"2024-07-11T15:51:39Z","published":"2024-01-15T17:20:17Z","title":"The ODE Method for Stochastic Approximation and Reinforcement Learning\n  with Markovian Noise","summary":"  Stochastic approximation is a class of algorithms that update a vector\niteratively, incrementally, and stochastically, including, e.g., stochastic\ngradient descent and temporal difference learning. One fundamental challenge in\nanalyzing a stochastic approximation algorithm is to establish its stability,\ni.e., to show that the stochastic vector iterates are bounded almost surely. In\nthis paper, we extend the celebrated Borkar-Meyn theorem for stability from the\nMartingale difference noise setting to the Markovian noise setting, which\ngreatly improves its applicability in reinforcement learning, especially in\nthose off-policy reinforcement learning algorithms with linear function\napproximation and eligibility traces. Central to our analysis is the\ndiminishing asymptotic rate of change of a few functions, which is implied by\nboth a form of strong law of large numbers and a commonly used V4 Lyapunov\ndrift condition and trivially holds if the Markov chain is finite and\nirreducible.\n","authors":["Shuze Liu","Shuhang Chen","Shangtong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.07844v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08610v1","updated":"2024-07-11T15:48:36Z","published":"2024-07-11T15:48:36Z","title":"Semantic GUI Scene Learning and Video Alignment for Detecting Duplicate\n  Video-based Bug Reports","summary":"  Video-based bug reports are increasingly being used to document bugs for\nprograms centered around a graphical user interface (GUI). However, developing\nautomated techniques to manage video-based reports is challenging as it\nrequires identifying and understanding often nuanced visual patterns that\ncapture key information about a reported bug. In this paper, we aim to overcome\nthese challenges by advancing the bug report management task of duplicate\ndetection for video-based reports. To this end, we introduce a new approach,\ncalled JANUS, that adapts the scene-learning capabilities of vision\ntransformers to capture subtle visual and textual patterns that manifest on app\nUI screens - which is key to differentiating between similar screens for\naccurate duplicate report detection. JANUS also makes use of a video alignment\ntechnique capable of adaptive weighting of video frames to account for typical\nbug manifestation patterns. In a comprehensive evaluation on a benchmark\ncontaining 7,290 duplicate detection tasks derived from 270 video-based bug\nreports from 90 Android app bugs, the best configuration of our approach\nachieves an overall mRR/mAP of 89.8%/84.7%, and for the large majority of\nduplicate detection tasks, outperforms prior work by around 9% to a\nstatistically significant degree. Finally, we qualitatively illustrate how the\nscene-learning capabilities provided by Janus benefits its performance.\n","authors":["Yanfu Yan","Nathan Cooper","Oscar Chaparro","Kevin Moran","Denys Poshyvanyk"],"pdf_url":"https://arxiv.org/pdf/2407.08610v1.pdf","comment":"13 pages, accepted to 46th International Conference on Software\n  Engineering (ICSE 2024)"},{"id":"http://arxiv.org/abs/2407.08608v1","updated":"2024-07-11T15:44:48Z","published":"2024-07-11T15:44:48Z","title":"FlashAttention-3: Fast and Accurate Attention with Asynchrony and\n  Low-precision","summary":"  Attention, as a core layer of the ubiquitous Transformer architecture, is the\nbottleneck for large language models and long-context applications.\nFlashAttention elaborated an approach to speed up attention on GPUs through\nminimizing memory reads/writes. However, it has yet to take advantage of new\ncapabilities present in recent hardware, with FlashAttention-2 achieving only\n35% utilization on the H100 GPU. We develop three main techniques to speed up\nattention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to\n(1) overlap overall computation and data movement via warp-specialization and\n(2) interleave block-wise matmul and softmax operations, and (3) block\nquantization and incoherent processing that leverages hardware support for FP8\nlow-precision. We demonstrate that our method, FlashAttention-3, achieves\nspeedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s\n(75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate\nthat FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a\nbaseline FP8 attention.\n","authors":["Jay Shah","Ganesh Bikshandi","Ying Zhang","Vijay Thakkar","Pradeep Ramani","Tri Dao"],"pdf_url":"https://arxiv.org/pdf/2407.08608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04814v3","updated":"2024-07-11T15:33:35Z","published":"2024-04-07T05:47:41Z","title":"Inference-Time Rule Eraser: Fair Recognition via Distilling and Removing\n  Biased Rules","summary":"  Machine learning models often make predictions based on biased features such\nas gender, race, and other social attributes, posing significant fairness\nrisks, especially in societal applications, such as hiring, banking, and\ncriminal justice. Traditional approaches to addressing this issue involve\nretraining or fine-tuning neural networks with fairness-aware optimization\nobjectives. However, these methods can be impractical due to significant\ncomputational resources, complex industrial tests, and the associated CO2\nfootprint. Additionally, regular users often fail to fine-tune models because\nthey lack access to model parameters In this paper, we introduce the\nInference-Time Rule Eraser (Eraser), a novel method designed to address\nfairness concerns by removing biased decision-making rules from deployed models\nduring inference without altering model weights. We begin by establishing a\ntheoretical foundation for modifying model outputs to eliminate biased rules\nthrough Bayesian analysis. Next, we present a specific implementation of Eraser\nthat involves two stages: (1) distilling the biased rules from the deployed\nmodel into an additional patch model, and (2) removing these biased rules from\nthe output of the deployed model during inference. Extensive experiments\nvalidate the effectiveness of our approach, showcasing its superior performance\nin addressing fairness concerns in AI systems.\n","authors":["Yi Zhang","Dongyuan Lu","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2404.04814v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04272v3","updated":"2024-07-11T15:31:53Z","published":"2024-07-05T05:55:18Z","title":"Accelerating Communication in Deep Learning Recommendation Model\n  Training with Dual-Level Adaptive Lossy Compression","summary":"  DLRM is a state-of-the-art recommendation system model that has gained\nwidespread adoption across various industry applications. The large size of\nDLRM models, however, necessitates the use of multiple devices/GPUs for\nefficient training. A significant bottleneck in this process is the\ntime-consuming all-to-all communication required to collect embedding data from\nall devices. To mitigate this, we introduce a method that employs error-bounded\nlossy compression to reduce the communication data size and accelerate DLRM\ntraining. We develop a novel error-bounded lossy compression algorithm,\ninformed by an in-depth analysis of embedding data features, to achieve high\ncompression ratios. Moreover, we introduce a dual-level adaptive strategy for\nerror-bound adjustment, spanning both table-wise and iteration-wise aspects, to\nbalance the compression benefits with the potential impacts on accuracy. We\nfurther optimize our compressor for PyTorch tensors on GPUs, minimizing\ncompression overhead. Evaluation shows that our method achieves a 1.38$\\times$\ntraining speedup with a minimal accuracy impact.\n","authors":["Hao Feng","Boyuan Zhang","Fanjiang Ye","Min Si","Ching-Hsiang Chu","Jiannan Tian","Chunxing Yin","Summer Deng","Yuchen Hao","Pavan Balaji","Tong Geng","Dingwen Tao"],"pdf_url":"https://arxiv.org/pdf/2407.04272v3.pdf","comment":"accepted by SC '24"},{"id":"http://arxiv.org/abs/2407.08597v1","updated":"2024-07-11T15:25:02Z","published":"2024-07-11T15:25:02Z","title":"Learning Program Behavioral Models from Synthesized Input-Output Pairs","summary":"  We introduce Modelizer - a novel framework that, given a black-box program,\nlearns a _model from its input/output behavior_ using _neural machine\ntranslation_. The resulting model _mocks_ the original program: Given an input,\nthe model predicts the output that would have been produced by the program.\nHowever, the model is also _reversible_ - that is, the model can predict the\ninput that would have produced a given output. Finally, the model is\n_differentiable_ and can be efficiently restricted to predict only a certain\naspect of the program behavior.\n  Modelizer uses _grammars_ to synthesize inputs and to parse the resulting\noutputs, allowing it to learn sequence-to-sequence associations between token\nstreams. Other than input and output grammars, Modelizer only requires the\nability to execute the program.\n  The resulting models are _small_, requiring fewer than 6.3 million parameters\nfor languages such as Markdown or HTML; and they are _accurate_, achieving up\nto 95.4% accuracy and a BLEU score of 0.98 with standard error 0.04 in mocking\nreal-world applications. We foresee several _applications_ of these models,\nespecially as the output of the program can be any aspect of program behavior.\nBesides mocking and predicting program behavior, the model can also synthesize\ninputs that are likely to produce a particular behavior, such as failures or\ncoverage.\n","authors":["Tural Mammadov","Dietrich Klakow","Alexander Koller","Andreas Zeller"],"pdf_url":"https://arxiv.org/pdf/2407.08597v1.pdf","comment":"42 pages, 6 figures, 8 tables"},{"id":"http://arxiv.org/abs/2406.02061v3","updated":"2024-07-11T15:17:36Z","published":"2024-06-04T07:43:33Z","title":"Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown\n  in State-Of-the-Art Large Language Models","summary":"  Large Language Models (LLMs) are often described as being instances of\nfoundation models - that is, models that transfer strongly across various tasks\nand conditions in few-show or zero-shot manner, while exhibiting scaling laws\nthat predict function improvement when increasing the pre-training scale. These\nclaims of excelling in different functions and tasks rely on measurements taken\nacross various sets of standardized benchmarks showing high scores for such\nmodels. We demonstrate here a dramatic breakdown of function and reasoning\ncapabilities of state-of-the-art models trained at the largest available scales\nwhich claim strong function, using a simple, short, conventional common sense\nproblem (AIW problem) formulated in concise natural language, easily solvable\nby humans. The breakdown is dramatic, as models show strong fluctuations across\neven slight problem variations that should not affect problem solving, also\nexpressing strong overconfidence in the wrong solutions, often backed up by\nplausible sounding explanation-like confabulations. Various standard\ninterventions in an attempt to get the right solution, like various type of\nenhanced prompting, or urging the models to reconsider the wrong solutions\nagain by multi step re-evaluation, fail. We take these initial observations to\nthe scientific and technological community to stimulate urgent re-assessment of\nthe claimed capabilities of current generation of LLMs. Such re-assessment also\nrequires common action to create standardized benchmarks that would allow\nproper detection of such basic reasoning deficits that obviously manage to\nremain undiscovered by current state-of-the-art evaluation procedures and\nbenchmarks. Code for reproducing experiments in the paper and raw experiments\ndata can be found at https://github.com/LAION-AI/AIW\n","authors":["Marianna Nezhurina","Lucia Cipolina-Kun","Mehdi Cherti","Jenia Jitsev"],"pdf_url":"https://arxiv.org/pdf/2406.02061v3.pdf","comment":"v2.0. Adding further experiments on various AIW problem variations.\n  AIW \"Alice Female Power Boost\", AIW Extension (AIW Ext). Including recent\n  Claude 3.5 Sonnet and Qwen 2 72B Instruct results"},{"id":"http://arxiv.org/abs/2407.08590v1","updated":"2024-07-11T15:13:28Z","published":"2024-07-11T15:13:28Z","title":"A Review of Nine Physics Engines for Reinforcement Learning Research","summary":"  We present a review of popular simulation engines and frameworks used in\nreinforcement learning (RL) research, aiming to guide researchers in selecting\ntools for creating simulated physical environments for RL and training setups.\nIt evaluates nine frameworks (Brax, Chrono, Gazebo, MuJoCo, ODE, PhysX,\nPyBullet, Webots, and Unity) based on their popularity, feature range, quality,\nusability, and RL capabilities. We highlight the challenges in selecting and\nutilizing physics engines for RL research, including the need for detailed\ncomparisons and an understanding of each framework's capabilities. Key findings\nindicate MuJoCo as the leading framework due to its performance and\nflexibility, despite usability challenges. Unity is noted for its ease of use\nbut lacks scalability and simulation fidelity. The study calls for further\ndevelopment to improve simulation engines' usability and performance and\nstresses the importance of transparency and reproducibility in RL research.\nThis review contributes to the RL community by offering insights into the\nselection process for simulation engines, facilitating informed\ndecision-making.\n","authors":["Michael Kaup","Cornelius Wolff","Hyerim Hwang","Julius Mayer","Elia Bruni"],"pdf_url":"https://arxiv.org/pdf/2407.08590v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.08585v1","updated":"2024-07-11T15:10:14Z","published":"2024-07-11T15:10:14Z","title":"HACMan++: Spatially-Grounded Motion Primitives for Manipulation","summary":"  Although end-to-end robot learning has shown some success for robot\nmanipulation, the learned policies are often not sufficiently robust to\nvariations in object pose or geometry. To improve the policy generalization, we\nintroduce spatially-grounded parameterized motion primitives in our method\nHACMan++. Specifically, we propose an action representation consisting of three\ncomponents: what primitive type (such as grasp or push) to execute, where the\nprimitive will be grounded (e.g. where the gripper will make contact with the\nworld), and how the primitive motion is executed, such as parameters specifying\nthe push direction or grasp orientation. These three components define a novel\ndiscrete-continuous action space for reinforcement learning. Our framework\nenables robot agents to learn to chain diverse motion primitives together and\nselect appropriate primitive parameters to complete long-horizon manipulation\ntasks. By grounding the primitives on a spatial location in the environment,\nour method is able to effectively generalize across object shape and pose\nvariations. Our approach significantly outperforms existing methods,\nparticularly in complex scenarios demanding both high-level sequential\nreasoning and object generalization. With zero-shot sim-to-real transfer, our\npolicy succeeds in challenging real-world manipulation tasks, with\ngeneralization to unseen objects. Videos can be found on the project website:\nhttps://sgmp-rss2024.github.io.\n","authors":["Bowen Jiang","Yilin Wu","Wenxuan Zhou","Chris Paxton","David Held"],"pdf_url":"https://arxiv.org/pdf/2407.08585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08583v1","updated":"2024-07-11T15:08:11Z","published":"2024-07-11T15:08:11Z","title":"The Synergy between Data and Multi-Modal Large Language Models: A Survey\n  from Co-Development Perspective","summary":"  The rapid development of large language models (LLMs) has been witnessed in\nrecent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the\nmodality from text to a broader spectrum of domains, attracting widespread\nattention due to the broader range of application scenarios. As LLMs and MLLMs\nrely on vast amounts of model parameters and data to achieve emergent\ncapabilities, the importance of data is receiving increasingly widespread\nattention and recognition. Tracing and analyzing recent data-oriented works for\nMLLMs, we find that the development of models and data is not two separate\npaths but rather interconnected. On the one hand, vaster and higher-quality\ndata contribute to better performance of MLLMs, on the other hand, MLLMs can\nfacilitate the development of data. The co-development of multi-modal data and\nMLLMs requires a clear view of 1) at which development stage of MLLMs can\nspecific data-centric approaches be employed to enhance which capabilities, and\n2) by utilizing which capabilities and acting as which roles can models\ncontribute to multi-modal data. To promote the data-model co-development for\nMLLM community, we systematically review existing works related to MLLMs from\nthe data-model co-development perspective. A regularly maintained project\nassociated with this survey is accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.\n","authors":["Zhen Qin","Daoyuan Chen","Wenhao Zhang","Liuyi Yao","Yilun Huang","Bolin Ding","Yaliang Li","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2407.08583v1.pdf","comment":"Ongoing work. 31 pages. Related materials are continually maintained\n  and available at\n  https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md"},{"id":"http://arxiv.org/abs/2405.00311v2","updated":"2024-07-11T15:03:49Z","published":"2024-05-01T04:28:44Z","title":"Three-layer deep learning network random trees for fault detection in\n  chemical production process","summary":"  With the development of technology, the chemical production process is\nbecoming increasingly complex and large-scale, making fault detection\nparticularly important. However, current detective methods struggle to address\nthe complexities of large-scale production processes. In this paper, we\nintegrate the strengths of deep learning and machine learning technologies,\ncombining the advantages of bidirectional long and short-term memory neural\nnetworks, fully connected neural networks, and the extra trees algorithm to\npropose a novel fault detection model named three-layer deep learning network\nrandom trees (TDLN-trees). First, the deep learning component extracts temporal\nfeatures from industrial data, combining and transforming them into a\nhigher-level data representation. Second, the machine learning component\nprocesses and classifies the features extracted in the first step. An\nexperimental analysis based on the Tennessee Eastman process verifies the\nsuperiority of the proposed method.\n","authors":["Ming Lu","Zhen Gao","Ying Zou","Zuguo Chen","Pei Li"],"pdf_url":"https://arxiv.org/pdf/2405.00311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08571v1","updated":"2024-07-11T14:59:17Z","published":"2024-07-11T14:59:17Z","title":"Multi-Group Proportional Representation","summary":"  Image search and retrieval tasks can perpetuate harmful stereotypes, erase\ncultural identities, and amplify social disparities. Current approaches to\nmitigate these representational harms balance the number of retrieved items\nacross population groups defined by a small number of (often binary)\nattributes. However, most existing methods overlook intersectional groups\ndetermined by combinations of group attributes, such as gender, race, and\nethnicity. We introduce Multi-Group Proportional Representation (MPR), a novel\nmetric that measures representation across intersectional groups. We develop\npractical methods for estimating MPR, provide theoretical guarantees, and\npropose optimization algorithms to ensure MPR in retrieval. We demonstrate that\nexisting methods optimizing for equal and proportional representation metrics\nmay fail to promote MPR. Crucially, our work shows that optimizing MPR yields\nmore proportional representation across multiple intersectional groups\nspecified by a rich function class, often with minimal compromise in retrieval\naccuracy.\n","authors":["Alex Oesterling","Claudio Mayrink Verdun","Carol Xuan Long","Alex Glynn","Lucas Monteiro Paes","Sajani Vithana","Martina Cardone","Flavio P. Calmon"],"pdf_url":"https://arxiv.org/pdf/2407.08571v1.pdf","comment":"35 pages, 24 figures. Under review"},{"id":"http://arxiv.org/abs/2407.08567v1","updated":"2024-07-11T14:57:27Z","published":"2024-07-11T14:57:27Z","title":"Adaptive Parametric Activation","summary":"  The activation function plays a crucial role in model optimisation, yet the\noptimal choice remains unclear. For example, the Sigmoid activation is the\nde-facto activation in balanced classification tasks, however, in imbalanced\nclassification, it proves inappropriate due to bias towards frequent classes.\nIn this work, we delve deeper in this phenomenon by performing a comprehensive\nstatistical analysis in the classification and intermediate layers of both\nbalanced and imbalanced networks and we empirically show that aligning the\nactivation function with the data distribution, enhances the performance in\nboth balanced and imbalanced tasks. To this end, we propose the Adaptive\nParametric Activation (APA) function, a novel and versatile activation function\nthat unifies most common activation functions under a single formula. APA can\nbe applied in both intermediate layers and attention layers, significantly\noutperforming the state-of-the-art on several imbalanced benchmarks such as\nImageNet-LT, iNaturalist2018, Places-LT, CIFAR100-LT and LVIS and balanced\nbenchmarks such as ImageNet1K, COCO and V3DET. The code is available at\nhttps://github.com/kostas1515/AGLU.\n","authors":["Konstantinos Panagiotis Alexandridis","Jiankang Deng","Anh Nguyen","Shan Luo"],"pdf_url":"https://arxiv.org/pdf/2407.08567v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2407.08560v1","updated":"2024-07-11T14:47:44Z","published":"2024-07-11T14:47:44Z","title":"Causal inference through multi-stage learning and doubly robust deep\n  neural networks","summary":"  Deep neural networks (DNNs) have demonstrated remarkable empirical\nperformance in large-scale supervised learning problems, particularly in\nscenarios where both the sample size $n$ and the dimension of covariates $p$\nare large. This study delves into the application of DNNs across a wide\nspectrum of intricate causal inference tasks, where direct estimation falls\nshort and necessitates multi-stage learning. Examples include estimating the\nconditional average treatment effect and dynamic treatment effect. In this\nframework, DNNs are constructed sequentially, with subsequent stages building\nupon preceding ones. To mitigate the impact of estimation errors from early\nstages on subsequent ones, we integrate DNNs in a doubly robust manner. In\ncontrast to previous research, our study offers theoretical assurances\nregarding the effectiveness of DNNs in settings where the dimensionality $p$\nexpands with the sample size. These findings are significant independently and\nextend to degenerate single-stage learning problems.\n","authors":["Yuqian Zhang","Jelena Bradic"],"pdf_url":"https://arxiv.org/pdf/2407.08560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06837v4","updated":"2024-07-11T14:35:23Z","published":"2023-12-11T21:00:28Z","title":"Spectral State Space Models","summary":"  This paper studies sequence modeling for prediction tasks with long range\ndependencies. We propose a new formulation for state space models (SSMs) based\non learning linear dynamical systems with the spectral filtering algorithm\n(Hazan et al. (2017)). This gives rise to a novel sequence prediction\narchitecture we call a spectral state space model.\n  Spectral state space models have two primary advantages. First, they have\nprovable robustness properties as their performance depends on neither the\nspectrum of the underlying dynamics nor the dimensionality of the problem.\nSecond, these models are constructed with fixed convolutional filters that do\nnot require learning while still outperforming SSMs in both theory and\npractice.\n  The resulting models are evaluated on synthetic dynamical systems and\nlong-range prediction tasks of various modalities. These evaluations support\nthe theoretical benefits of spectral filtering for tasks requiring very long\nrange memory.\n","authors":["Naman Agarwal","Daniel Suo","Xinyi Chen","Elad Hazan"],"pdf_url":"https://arxiv.org/pdf/2312.06837v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08229v2","updated":"2024-07-11T14:33:23Z","published":"2024-06-12T13:59:31Z","title":"GPT4Rec: Graph Prompt Tuning for Streaming Recommendation","summary":"  In the realm of personalized recommender systems, the challenge of adapting\nto evolving user preferences and the continuous influx of new users and items\nis paramount. Conventional models, typically reliant on a static training-test\napproach, struggle to keep pace with these dynamic demands. Streaming\nrecommendation, particularly through continual graph learning, has emerged as a\nnovel solution. However, existing methods in this area either rely on\nhistorical data replay, which is increasingly impractical due to stringent data\nprivacy regulations; or are inability to effectively address the over-stability\nissue; or depend on model-isolation and expansion strategies. To tackle these\ndifficulties, we present GPT4Rec, a Graph Prompt Tuning method for streaming\nRecommendation. Given the evolving user-item interaction graph, GPT4Rec first\ndisentangles the graph patterns into multiple views. After isolating specific\ninteraction patterns and relationships in different views, GPT4Rec utilizes\nlightweight graph prompts to efficiently guide the model across varying\ninteraction patterns within the user-item graph. Firstly, node-level prompts\nare employed to instruct the model to adapt to changes in the attributes or\nproperties of individual nodes within the graph. Secondly, structure-level\nprompts guide the model in adapting to broader patterns of connectivity and\nrelationships within the graph. Finally, view-level prompts are innovatively\ndesigned to facilitate the aggregation of information from multiple\ndisentangled views. These prompt designs allow GPT4Rec to synthesize a\ncomprehensive understanding of the graph, ensuring that all vital aspects of\nthe user-item interactions are considered and effectively integrated.\nExperiments on four diverse real-world datasets demonstrate the effectiveness\nand efficiency of our proposal.\n","authors":["Peiyan Zhang","Yuchen Yan","Xi Zhang","Liying Kang","Chaozhuo Li","Feiran Huang","Senzhang Wang","Sunghun Kim"],"pdf_url":"https://arxiv.org/pdf/2406.08229v2.pdf","comment":"Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2407.08546v1","updated":"2024-07-11T14:30:49Z","published":"2024-07-11T14:30:49Z","title":"Quantitative Evaluation of the Saliency Map for Alzheimer's Disease\n  Classifier with Anatomical Segmentation","summary":"  Saliency maps have been widely used to interpret deep learning classifiers\nfor Alzheimer's disease (AD). However, since AD is heterogeneous and has\nmultiple subtypes, the pathological mechanism of AD remains not fully\nunderstood and may vary from patient to patient. Due to the lack of such\nunderstanding, it is difficult to comprehensively and effectively assess the\nsaliency map of AD classifier. In this paper, we utilize the anatomical\nsegmentation to allocate saliency values into different brain regions. By\nplotting the distributions of saliency maps corresponding to AD and NC (Normal\nControl), we can gain a comprehensive view of the model's decisions process. In\norder to leverage the fact that the brain volume shrinkage happens in AD\npatients during disease progression, we define a new evaluation metric, brain\nvolume change score (VCS), by computing the average Pearson correlation of the\nbrain volume changes and the saliency values of a model in different brain\nregions for each patient. Thus, the VCS metric can help us gain some knowledge\nof how saliency maps resulting from different models relate to the changes of\nthe volumes across different regions in the whole brain. We trained candidate\nmodels on the ADNI dataset and tested on three different datasets. Our results\nindicate: (i) models with higher VCSs tend to demonstrate saliency maps with\nmore details relevant to the AD pathology, (ii) using gradient-based\nadversarial training strategies such as FGSM and stochastic masking can improve\nthe VCSs of the models.\n","authors":["Yihan Zhang","Xuanshuo Zhang","Wei Wu","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03961v2","updated":"2024-07-11T14:14:22Z","published":"2024-07-04T14:28:52Z","title":"Leveraging Latent Diffusion Models for Training-Free In-Distribution\n  Data Augmentation for Surface Defect Detection","summary":"  Defect detection is the task of identifying defects in production samples.\nUsually, defect detection classifiers are trained on ground-truth data formed\nby normal samples (negative data) and samples with defects (positive data),\nwhere the latter are consistently fewer than normal samples. State-of-the-art\ndata augmentation procedures add synthetic defect data by superimposing\nartifacts to normal samples to mitigate problems related to unbalanced training\ndata. These techniques often produce out-of-distribution images, resulting in\nsystems that learn what is not a normal sample but cannot accurately identify\nwhat a defect looks like. In this work, we introduce DIAG, a training-free\nDiffusion-based In-distribution Anomaly Generation pipeline for data\naugmentation. Unlike conventional image generation techniques, we implement a\nhuman-in-the-loop pipeline, where domain experts provide multimodal guidance to\nthe model through text descriptions and region localization of the possible\nanomalies. This strategic shift enhances the interpretability of results and\nfosters a more robust human feedback loop, facilitating iterative improvements\nof the generated outputs. Remarkably, our approach operates in a zero-shot\nmanner, avoiding time-consuming fine-tuning procedures while achieving superior\nperformance. We demonstrate the efficacy and versatility of DIAG with respect\nto state-of-the-art data augmentation approaches on the challenging KSDD2\ndataset, with an improvement in AP of approximately 18% when positive samples\nare available and 28% when they are missing. The source code is available at\nhttps://github.com/intelligolabs/DIAG.\n","authors":["Federico Girella","Ziyue Liu","Franco Fummi","Francesco Setti","Marco Cristani","Luigi Capogrosso"],"pdf_url":"https://arxiv.org/pdf/2407.03961v2.pdf","comment":"Accepted at the 21st International Conference on Content-Based\n  Multimedia Indexing (CBMI 2024)"},{"id":"http://arxiv.org/abs/2212.02075v3","updated":"2024-07-11T14:11:23Z","published":"2022-12-05T07:40:29Z","title":"Differentiated Federated Reinforcement Learning Based Traffic Offloading\n  on Space-Air-Ground Integrated Networks","summary":"  The Space-Air-Ground Integrated Network (SAGIN) plays a pivotal role as a\ncomprehensive foundational network communication infrastructure, presenting\nopportunities for highly efficient global data transmission. Nonetheless, given\nSAGIN's unique characteristics as a dynamically heterogeneous network,\nconventional network optimization methodologies encounter challenges in\nsatisfying the stringent requirements for network latency and stability\ninherent to data transmission within this network environment. Therefore, this\npaper proposes the use of differentiated federated reinforcement learning\n(DFRL) to solve the traffic offloading problem in SAGIN, i.e., using multiple\nagents to generate differentiated traffic offloading policies. Considering the\ndifferentiated characteristics of each region of SAGIN, DFRL models the traffic\noffloading policy optimization process as the process of solving the\nDecentralized Partially Observable Markov Decision Process (DEC-POMDP) problem.\nThe paper proposes a novel Differentiated Federated Soft Actor-Critic (DFSAC)\nalgorithm to solve the problem. The DFSAC algorithm takes the network packet\ndelay as the joint reward value and introduces the global trend model as the\njoint target action-value function of each agent to guide the update of each\nagent's policy. The simulation results demonstrate that the traffic offloading\npolicy based on the DFSAC algorithm achieves better performance in terms of\nnetwork throughput, packet loss rate, and packet delay compared to the\ntraditional federated reinforcement learning approach and other baseline\napproaches.\n","authors":["Yeguang Qin","Yilin Yang","Fengxiao Tang","Xin Yao","Ming Zhao","Nei Kato"],"pdf_url":"https://arxiv.org/pdf/2212.02075v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06590v3","updated":"2024-07-11T14:02:37Z","published":"2024-02-09T18:10:38Z","title":"Predictive representations: building blocks of intelligence","summary":"  Adaptive behavior often requires predicting future events. The theory of\nreinforcement learning prescribes what kinds of predictive representations are\nuseful and how to compute them. This paper integrates these theoretical ideas\nwith work on cognition and neuroscience. We pay special attention to the\nsuccessor representation (SR) and its generalizations, which have been widely\napplied both as engineering tools and models of brain function. This\nconvergence suggests that particular kinds of predictive representations may\nfunction as versatile building blocks of intelligence.\n","authors":["Wilka Carvalho","Momchil S. Tomov","William de Cothi","Caswell Barry","Samuel J. Gershman"],"pdf_url":"https://arxiv.org/pdf/2402.06590v3.pdf","comment":"accepted to Neural Computation"},{"id":"http://arxiv.org/abs/2403.00942v2","updated":"2024-07-11T13:51:56Z","published":"2024-03-01T19:39:19Z","title":"Resilience of Entropy Model in Distributed Neural Networks","summary":"  Distributed deep neural networks (DNNs) have emerged as a key technique to\nreduce communication overhead without sacrificing performance in edge computing\nsystems. Recently, entropy coding has been introduced to further reduce the\ncommunication overhead. The key idea is to train the distributed DNN jointly\nwith an entropy model, which is used as side information during inference time\nto adaptively encode latent representations into bit streams with variable\nlength. To the best of our knowledge, the resilience of entropy models is yet\nto be investigated. As such, in this paper we formulate and investigate the\nresilience of entropy models to intentional interference (e.g., adversarial\nattacks) and unintentional interference (e.g., weather changes and motion\nblur). Through an extensive experimental campaign with 3 different DNN\narchitectures, 2 entropy models and 4 rate-distortion trade-off factors, we\ndemonstrate that the entropy attacks can increase the communication overhead by\nup to 95%. By separating compression features in frequency and spatial domain,\nwe propose a new defense mechanism that can reduce the transmission overhead of\nthe attacked input by about 9% compared to unperturbed data, with only about 2%\naccuracy loss. Importantly, the proposed defense mechanism is a standalone\napproach which can be applied in conjunction with approaches such as\nadversarial training to further improve robustness. Code will be shared for\nreproducibility.\n","authors":["Milin Zhang","Mohammad Abdi","Shahriar Rifat","Francesco Restuccia"],"pdf_url":"https://arxiv.org/pdf/2403.00942v2.pdf","comment":"accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2402.08616v2","updated":"2024-07-11T13:45:33Z","published":"2024-02-13T17:32:59Z","title":"Adjustment Identification Distance: A gadjid for Causal Structure\n  Learning","summary":"  Evaluating graphs learned by causal discovery algorithms is difficult: The\nnumber of edges that differ between two graphs does not reflect how the graphs\ndiffer with respect to the identifying formulas they suggest for causal\neffects. We introduce a framework for developing causal distances between\ngraphs which includes the structural intervention distance for directed acyclic\ngraphs as a special case. We use this framework to develop improved\nadjustment-based distances as well as extensions to completed partially\ndirected acyclic graphs and causal orders. We develop new reachability\nalgorithms to compute the distances efficiently and to prove their low\npolynomial time complexity. In our package gadjid (open source at\nhttps://github.com/CausalDisco/gadjid), we provide implementations of our\ndistances; they are orders of magnitude faster with proven lower time\ncomplexity than the structural intervention distance and thereby provide a\nsuccess metric for causal discovery that scales to graph sizes that were\npreviously prohibitive.\n","authors":["Leonard Henckel","Theo WÃ¼rtzen","Sebastian Weichwald"],"pdf_url":"https://arxiv.org/pdf/2402.08616v2.pdf","comment":"accepted at UAI 2024 (Conference on Uncertainty in Artificial\n  Intelligence)"},{"id":"http://arxiv.org/abs/2407.08500v1","updated":"2024-07-11T13:35:22Z","published":"2024-07-11T13:35:22Z","title":"Latent Conditional Diffusion-based Data Augmentation for Continuous-Time\n  Dynamic Graph Mode","summary":"  Continuous-Time Dynamic Graph (CTDG) precisely models evolving real-world\nrelationships, drawing heightened interest in dynamic graph learning across\nacademia and industry. However, existing CTDG models encounter challenges\nstemming from noise and limited historical data. Graph Data Augmentation (GDA)\nemerges as a critical solution, yet current approaches primarily focus on\nstatic graphs and struggle to effectively address the dynamics inherent in\nCTDGs. Moreover, these methods often demand substantial domain expertise for\nparameter tuning and lack theoretical guarantees for augmentation efficacy. To\naddress these issues, we propose Conda, a novel latent diffusion-based GDA\nmethod tailored for CTDGs. Conda features a sandwich-like architecture,\nincorporating a Variational Auto-Encoder (VAE) and a conditional diffusion\nmodel, aimed at generating enhanced historical neighbor embeddings for target\nnodes. Unlike conventional diffusion models trained on entire graphs via\npre-training, Conda requires historical neighbor sequence embeddings of target\nnodes for training, thus facilitating more targeted augmentation. We integrate\nConda into the CTDG model and adopt an alternating training strategy to\noptimize performance. Extensive experimentation across six widely used\nreal-world datasets showcases the consistent performance improvement of our\napproach, particularly in scenarios with limited historical data.\n","authors":["Yuxing Tian","Yiyan Qi","Aiwen Jiang","Qi Huang","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2407.08500v1.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2406.12008v3","updated":"2024-07-11T13:32:59Z","published":"2024-06-17T18:21:03Z","title":"QC-Forest: a Classical-Quantum Algorithm to Provably Speedup Retraining\n  of Random Forest","summary":"  Random Forest (RF) is a popular tree-ensemble method for supervised learning,\nprized for its ease of use and flexibility. Online RF models require to account\nfor new training data to maintain model accuracy. This is particularly\nimportant in applications where data is periodically and sequentially generated\nover time in data streams, such as auto-driving systems, and credit card\npayments. In this setting, performing periodic model retraining with the old\nand new data accumulated is beneficial as it fully captures possible drifts in\nthe data distribution over time. However, this is unpractical with\nstate-of-the-art classical algorithms for RF as they scale linearly with the\naccumulated number of samples. We propose QC-Forest, a classical-quantum\nalgorithm designed to time-efficiently retrain RF models in the streaming\nsetting for multi-class classification and regression, achieving a runtime\npoly-logarithmic in the total number of accumulated samples. QC-Forest\nleverages Des-q, a quantum algorithm for single tree construction and\nretraining proposed by Kumar et al. by expanding to multi-class classification,\nas the original proposal was limited to binary classes, and introducing an\nexact classical method to replace an underlying quantum subroutine incurring a\nfinite error, while maintaining the same poly-logarithmic dependence. Finally,\nwe showcase that QC-Forest achieves competitive accuracy in comparison to\nstate-of-the-art RF methods on widely used benchmark datasets with up to 80,000\nsamples, while significantly speeding up the model retrain.\n","authors":["Romina Yalovetzky","Niraj Kumar","Changhao Li","Marco Pistoia"],"pdf_url":"https://arxiv.org/pdf/2406.12008v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08752v3","updated":"2024-07-11T13:23:32Z","published":"2023-05-15T16:02:56Z","title":"A Matter of Annotation: An Empirical Study on In Situ and Self-Recall\n  Activity Annotations from Wearable Sensors","summary":"  Research into the detection of human activities from wearable sensors is a\nhighly active field, benefiting numerous applications, from ambulatory\nmonitoring of healthcare patients via fitness coaching to streamlining manual\nwork processes.\n  We present an empirical study that evaluates and contrasts four commonly\nemployed annotation methods in user studies focused on in-the-wild data\ncollection. For both the user-driven, in situ annotations, where participants\nannotate their activities during the actual recording process, and the recall\nmethods, where participants retrospectively annotate their data at the end of\neach day, the participants had the flexibility to select their own set of\nactivity classes and corresponding labels.\n  Our study illustrates that different labeling methodologies directly impact\nthe annotations' quality, as well as the capabilities of a deep learning\nclassifier trained with the data. We noticed that in situ methods produce less\nbut more precise labels than recall methods. Furthermore, we combined an\nactivity diary with a visualization tool that enables the participant to\ninspect and label their activity data. Due to the introduction of such a tool\nwere able to decrease missing annotations and increase the annotation\nconsistency, and therefore the F1-Score of the deep learning model by up to 8%\n(ranging between 82.1 and 90.4% F1-Score). Furthermore, we discuss the\nadvantages and disadvantages of the methods compared in our study, the biases\nthey could introduce, and the consequences of their usage on human activity\nrecognition studies as well as possible solutions.\n","authors":["Alexander Hoelzemann","Kristof Van Laerhoven"],"pdf_url":"https://arxiv.org/pdf/2305.08752v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08479v1","updated":"2024-07-11T13:13:24Z","published":"2024-07-11T13:13:24Z","title":"Robust Generalization of Graph Neural Networks for Carrier Scheduling","summary":"  Battery-free sensor tags are devices that leverage backscatter techniques to\ncommunicate with standard IoT devices, thereby augmenting a network's sensing\ncapabilities in a scalable way. For communicating, a sensor tag relies on an\nunmodulated carrier provided by a neighboring IoT device, with a schedule\ncoordinating this provisioning across the network. Carrier\nscheduling--computing schedules to interrogate all sensor tags while minimizing\nenergy, spectrum utilization, and latency--is an NP-Hard optimization problem.\nRecent work introduces learning-based schedulers that achieve resource savings\nover a carefully-crafted heuristic, generalizing to networks of up to 60 nodes.\nHowever, we find that their advantage diminishes in networks with hundreds of\nnodes, and degrades further in larger setups. This paper introduces\nRobustGANTT, a GNN-based scheduler that improves generalization (without\nre-training) to networks up to 1000 nodes (100x training topology sizes).\nRobustGANTT not only achieves better and more consistent generalization, but\nalso computes schedules requiring up to 2x less resources than existing\nsystems. Our scheduler exhibits average runtimes of hundreds of milliseconds,\nallowing it to react fast to changing network conditions. Our work not only\nimproves resource utilization in large-scale backscatter networks, but also\noffers valuable insights in learning-based scheduling.\n","authors":["Daniel F. Perez-Ramirez","Carlos PÃ©rez-Penichet","Nicolas Tsiftes","Dejan Kostic","Magnus Boman","Thiemo Voigt"],"pdf_url":"https://arxiv.org/pdf/2407.08479v1.pdf","comment":"15 Pages, 12 Figures. Pre-print, under review"},{"id":"http://arxiv.org/abs/2309.03774v2","updated":"2024-07-11T13:07:47Z","published":"2023-09-07T15:25:47Z","title":"Deep Learning Safety Concerns in Automated Driving Perception","summary":"  Recent advances in the field of deep learning and impressive performance of\ndeep neural networks (DNNs) for perception have resulted in an increased demand\nfor their use in automated driving (AD) systems. The safety of such systems is\nof utmost importance and thus requires to consider the unique properties of\nDNNs.\n  In order to achieve safety of AD systems with DNN-based perception components\nin a systematic and comprehensive approach, so-called safety concerns have been\nintroduced as a suitable structuring element. On the one hand, the concept of\nsafety concerns is -- by design -- well aligned to existing standards relevant\nfor safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has\nalready inspired several academic publications and upcoming standards on AI\nsafety such as ISO PAS 8800.\n  While the concept of safety concerns has been previously introduced, this\npaper extends and refines it, leveraging feedback from various domain and\nsafety experts in the field. In particular, this paper introduces an additional\ncategorization for a better understanding as well as enabling cross-functional\nteams to jointly address the concerns.\n","authors":["Stephanie Abrecht","Alexander Hirsch","Shervin Raafatnia","Matthias Woehrle"],"pdf_url":"https://arxiv.org/pdf/2309.03774v2.pdf","comment":"Added a note indicating that this work has been submitted to the IEEE\n  for possible publication"},{"id":"http://arxiv.org/abs/2406.06566v2","updated":"2024-07-11T13:05:37Z","published":"2024-06-03T07:44:32Z","title":"Natural Language Interaction with a Household Electricity\n  Knowledge-based Digital Twin","summary":"  Domain specific digital twins, representing a digital replica of various\nsegments of the smart grid, are foreseen as able to model, simulate, and\ncontrol the respective segments. At the same time, knowledge-based digital\ntwins, coupled with AI, may also empower humans to understand aspects of the\nsystem through natural language interaction in view of planning and policy\nmaking. This paper is the first to assess and report on the potential of\nRetrieval Augmented Generation (RAG) question answers related to household\nelectrical energy measurement aspects leveraging a knowledge-based energy\ndigital twin. Relying on the recently published electricity consumption\nknowledge graph that actually represents a knowledge-based digital twin, we\nstudy the capabilities of ChatGPT, Gemini and Llama in answering electricity\nrelated questions. Furthermore, we compare the answers with the ones generated\nthrough a RAG techniques that leverages an existing electricity knowledge-based\ndigital twin. Our findings illustrate that the RAG approach not only reduces\nthe incidence of incorrect information typically generated by LLMs but also\nsignificantly improves the quality of the output by grounding responses in\nverifiable data. This paper details our methodology, presents a comparative\nanalysis of responses with and without RAG, and discusses the implications of\nour findings for future applications of AI in specialized sectors like energy\ndata analysis.\n","authors":["Carolina Fortuna","Vid HanÅ¾el","BlaÅ¾ BertalaniÄ"],"pdf_url":"https://arxiv.org/pdf/2406.06566v2.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2407.08464v1","updated":"2024-07-11T13:01:18Z","published":"2024-07-11T13:01:18Z","title":"TLDR: Unsupervised Goal-Conditioned RL via Temporal Distance-Aware\n  Representations","summary":"  Unsupervised goal-conditioned reinforcement learning (GCRL) is a promising\nparadigm for developing diverse robotic skills without external supervision.\nHowever, existing unsupervised GCRL methods often struggle to cover a wide\nrange of states in complex environments due to their limited exploration and\nsparse or noisy rewards for GCRL. To overcome these challenges, we propose a\nnovel unsupervised GCRL method that leverages TemporaL Distance-aware\nRepresentations (TLDR). TLDR selects faraway goals to initiate exploration and\ncomputes intrinsic exploration rewards and goal-reaching rewards, based on\ntemporal distance. Specifically, our exploration policy seeks states with large\ntemporal distances (i.e. covering a large state space), while the\ngoal-conditioned policy learns to minimize the temporal distance to the goal\n(i.e. reaching the goal). Our experimental results in six simulated robotic\nlocomotion environments demonstrate that our method significantly outperforms\nprevious unsupervised GCRL methods in achieving a wide variety of states.\n","authors":["Junik Bae","Kwanyoung Park","Youngwoon Lee"],"pdf_url":"https://arxiv.org/pdf/2407.08464v1.pdf","comment":"Website: https://heatz123.github.io/tldr"},{"id":"http://arxiv.org/abs/2407.08462v1","updated":"2024-07-11T12:58:47Z","published":"2024-07-11T12:58:47Z","title":"Distributed Deep Reinforcement Learning Based Gradient Quantization for\n  Federated Learning Enabled Vehicle Edge Computing","summary":"  Federated Learning (FL) can protect the privacy of the vehicles in vehicle\nedge computing (VEC) to a certain extent through sharing the gradients of\nvehicles' local models instead of local data. The gradients of vehicles' local\nmodels are usually large for the vehicular artificial intelligence (AI)\napplications, thus transmitting such large gradients would cause large\nper-round latency. Gradient quantization has been proposed as one effective\napproach to reduce the per-round latency in FL enabled VEC through compressing\ngradients and reducing the number of bits, i.e., the quantization level, to\ntransmit gradients. The selection of quantization level and thresholds\ndetermines the quantization error, which further affects the model accuracy and\ntraining time. To do so, the total training time and quantization error (QE)\nbecome two key metrics for the FL enabled VEC. It is critical to jointly\noptimize the total training time and QE for the FL enabled VEC. However, the\ntime-varying channel condition causes more challenges to solve this problem. In\nthis paper, we propose a distributed deep reinforcement learning (DRL)-based\nquantization level allocation scheme to optimize the long-term reward in terms\nof the total training time and QE. Extensive simulations identify the optimal\nweighted factors between the total training time and QE, and demonstrate the\nfeasibility and effectiveness of the proposed scheme.\n","authors":["Cui Zhang","Wenjun Zhang","Qiong Wu","Pingyi Fan","Qiang Fan","Jiangzhou Wang","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2407.08462v1.pdf","comment":"This paper has been submitted to IEEE Journal. The source code has\n  been released at:\n  https://github.com/qiongwu86/Distributed-Deep-Reinforcement-Learning-Based-Gradient\n  Quantization-for-Federated-Learning-Enabled-Vehicle-Edge-Computing"},{"id":"http://arxiv.org/abs/2407.08459v1","updated":"2024-07-11T12:58:07Z","published":"2024-07-11T12:58:07Z","title":"Graph Expansions of Deep Neural Networks and their Universal Scaling\n  Limits","summary":"  We present a unified approach to obtain scaling limits of neural networks\nusing the genus expansion technique from random matrix theory. This approach\nbegins with a novel expansion of neural networks which is reminiscent of\nButcher series for ODEs, and is obtained through a generalisation of Fa\\`a di\nBruno's formula to an arbitrary number of compositions. In this expansion, the\nrole of monomials is played by random multilinear maps indexed by directed\ngraphs whose edges correspond to random matrices, which we call operator\ngraphs. This expansion linearises the effect of the activation functions,\nallowing for the direct application of Wick's principle to compute the\nexpectation of each of its terms. We then determine the leading contribution to\neach term by embedding the corresponding graphs onto surfaces, and computing\ntheir Euler characteristic. Furthermore, by developing a correspondence between\nanalytic and graphical operations, we obtain similar graph expansions for the\nneural tangent kernel as well as the input-output Jacobian of the original\nneural network, and derive their infinite-width limits with relative ease.\nNotably, we find explicit formulae for the moments of the limiting singular\nvalue distribution of the Jacobian. We then show that all of these results hold\nfor networks with more general weights, such as general matrices with i.i.d.\nentries satisfying moment assumptions, complex matrices and sparse matrices.\n","authors":["Nicola Muca Cirone","Jad Hamdan","Cristopher Salvi"],"pdf_url":"https://arxiv.org/pdf/2407.08459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08458v1","updated":"2024-07-11T12:54:38Z","published":"2024-07-11T12:54:38Z","title":"Joint Optimization of Age of Information and Energy Consumption in\n  NR-V2X System based on Deep Reinforcement Learning","summary":"  Autonomous driving may be the most important application scenario of next\ngeneration, the development of wireless access technologies enabling reliable\nand low-latency vehicle communication becomes crucial. To address this, 3GPP\nhas developed Vehicle-to-Everything (V2X) specifications based on 5G New Radio\n(NR) technology, where Mode 2 Side-Link (SL) communication resembles Mode 4 in\nLTE-V2X, allowing direct communication between vehicles. This supplements SL\ncommunication in LTE-V2X and represents the latest advancement in cellular V2X\n(C-V2X) with improved performance of NR-V2X. However, in NR-V2X Mode 2,\nresource collisions still occur, and thus degrade the age of information (AOI).\nTherefore, a interference cancellation method is employed to mitigate this\nimpact by combining NR-V2X with Non-Orthogonal multiple access (NOMA)\ntechnology. In NR-V2X, when vehicles select smaller resource reservation\ninterval (RRI), higher-frequency transmissions take ore energy to reduce AoI.\nHence, it is important to jointly consider AoI and communication energy\nconsumption based on NR-V2X communication. Then, we formulate such an\noptimization problem and employ the Deep Reinforcement Learning (DRL) algorithm\nto compute the optimal transmission RRI and transmission power for each\ntransmitting vehicle to reduce the energy consumption of each transmitting\nvehicle and the AoI of each receiving vehicle. Extensive simulations have\ndemonstrated the performance of our proposed algorithm.\n","authors":["Shulin Song","Zheng Zhang","Qiong Wu","Qiang Fan","Pingyi Fan"],"pdf_url":"https://arxiv.org/pdf/2407.08458v1.pdf","comment":"This paper has been accepted by sensors. The source code has been\n  released at:\n  https://github.com/qiongwu86/Joint-Optimization-of-AoI-and-Energy-Consumption-in-NR-V2X-System-based-on-DRL"},{"id":"http://arxiv.org/abs/2404.08108v2","updated":"2024-07-11T12:41:51Z","published":"2024-04-11T20:14:14Z","title":"Protein intrinsic disorder prediction using Attention U-Net and\n  ProtTrans protein language model","summary":"  The prediction of intrinsic disorder regions has significant implications for\nunderstanding protein function, structure, and dynamics. It can help to\ndiscover novel functions or protein-protein interactions essential to designing\nnew drugs, therapies, or enzymes. Recently, a new generation of predictors\nbased on protein language models is emerging. These algorithms reach\nstate-of-the-art accuracy without calculating time-consuming multiple sequence\nalignments (MSAs). The article pre-sents a new protein intrinsic disorder\npredictor DisorderUnetLM based on the Attention U-Net convolutional neural\nnetwork using features from the protein language model ProtTrans.\nDisorderUnetLM shows top results in the direct comparison with flDPnn and\nIDP-CRF predictors using MSAs and with the SETH predictor using features from\nthe same ProtTrans model. Moreover, among 41 predictors from the latest\nCritical Assessment of Protein Intrinsic Disorder Prediction (CAID-2)\nbenchmark, it ranks 9th for the Disorder-PDB subset (with ROC-AUC of 0.924) and\n1st for the Disorder-NOX subset (with ROC-AUC of 0.844) which confirms its\npotential to perform well in the upcoming CAID-3 challenge for which\nDisor-derUnetLM was submitted.\n","authors":["Krzysztof Kotowski","Irena Roterman","Katarzyna Stapor"],"pdf_url":"https://arxiv.org/pdf/2404.08108v2.pdf","comment":"11 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.05206v2","updated":"2024-07-11T12:33:53Z","published":"2024-07-06T23:16:41Z","title":"Helios: An extremely low power event-based gesture recognition for\n  always-on smart eyewear","summary":"  This paper introduces Helios, the first extremely low-power, real-time,\nevent-based hand gesture recognition system designed for all-day on smart\neyewear. As augmented reality (AR) evolves, current smart glasses like the Meta\nRay-Bans prioritize visual and wearable comfort at the expense of\nfunctionality. Existing human-machine interfaces (HMIs) in these devices, such\nas capacitive touch and voice controls, present limitations in ergonomics,\nprivacy and power consumption. Helios addresses these challenges by leveraging\nnatural hand interactions for a more intuitive and comfortable user experience.\nOur system utilizes a extremely low-power and compact 3mmx4mm/20mW event camera\nto perform natural hand-based gesture recognition for always-on smart eyewear.\nThe camera's output is processed by a convolutional neural network (CNN)\nrunning on a NXP Nano UltraLite compute platform, consuming less than 350mW.\nHelios can recognize seven classes of gestures, including subtle microgestures\nlike swipes and pinches, with 91% accuracy. We also demonstrate real-time\nperformance across 20 users at a remarkably low latency of 60ms. Our user\ntesting results align with the positive feedback we received during our recent\nsuccessful demo at AWE-USA-2024.\n","authors":["Prarthana Bhattacharyya","Joshua Mitton","Ryan Page","Owen Morgan","Ben Menzies","Gabriel Homewood","Kemi Jacobs","Paolo Baesso","Dave Trickett","Chris Mair","Taru Muhonen","Rory Clark","Louis Berridge","Richard Vigars","Iain Wallace"],"pdf_url":"https://arxiv.org/pdf/2407.05206v2.pdf","comment":"18 pages, 10 figures. First three authors contributed equally to this\n  paper"},{"id":"http://arxiv.org/abs/2407.08442v1","updated":"2024-07-11T12:33:28Z","published":"2024-07-11T12:33:28Z","title":"How Deep is your Guess? A Fresh Perspective on Deep Learning for Medical\n  Time-Series Imputation","summary":"  We introduce a novel classification framework for time-series imputation\nusing deep learning, with a particular focus on clinical data. By identifying\nconceptual gaps in the literature and existing reviews, we devise a taxonomy\ngrounded on the inductive bias of neural imputation frameworks, resulting in a\nclassification of existing deep imputation strategies based on their\nsuitability for specific imputation scenarios and data-specific properties. Our\nreview further examines the existing methodologies employed to benchmark deep\nimputation models, evaluating their effectiveness in capturing the missingness\nscenarios found in clinical data and emphasising the importance of reconciling\nmathematical abstraction with clinical insights. Our classification aims to\nserve as a guide for researchers to facilitate the selection of appropriate\ndeep learning imputation techniques tailored to their specific clinical data.\nOur novel perspective also highlights the significance of bridging the gap\nbetween computational methodologies and medical insights to achieve clinically\nsound imputation models.\n","authors":["Linglong Qian","Tao Wang","Jun Wang","Hugh Logan Ellis","Robin Mitra","Richard Dobson","Zina Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2407.08442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08434v1","updated":"2024-07-11T12:17:31Z","published":"2024-07-11T12:17:31Z","title":"Improve Load Forecasting in Energy Communities through Transfer Learning\n  using Open-Access Synthetic Profiles","summary":"  According to a conservative estimate, a 1% reduction in forecast error for a\n10 GW energy utility can save up to $ 1.6 million annually. In our context,\nachieving precise forecasts of future power consumption is crucial for\noperating flexible energy assets using model predictive control approaches.\nSpecifically, this work focuses on the load profile forecast of a first-year\nenergy community with the common practical challenge of limited historical data\navailability. We propose to pre-train the load prediction models with\nopen-access synthetic load profiles using transfer learning techniques to\ntackle this challenge. Results show that this approach improves both, the\ntraining stability and prediction error. In a test case with 74 households, the\nprediction mean squared error (MSE) decreased from 0.34 to 0.13, showing\ntransfer learning based on synthetic load profiles to be a viable approach to\ncompensate for a lack of historic data.\n","authors":["Lukas Moosbrugger","Valentin Seiler","Gerhard Huber","Peter Kepplinger"],"pdf_url":"https://arxiv.org/pdf/2407.08434v1.pdf","comment":"The paper has been accepted for the IEEE RTSI 2024 conference"},{"id":"http://arxiv.org/abs/2310.02206v2","updated":"2024-07-11T12:13:38Z","published":"2023-10-03T17:04:33Z","title":"Chunking: Continual Learning is not just about Distribution Shift","summary":"  Work on continual learning (CL) has thus far largely focused on the problems\narising from shifts in the data distribution. However, CL can be decomposed\ninto two sub-problems: (a) shifts in the data distribution, and (b) dealing\nwith the fact that the data is split into chunks and so only a part of the data\nis available to be trained on at any point in time. In this work, we look at\nthe latter sub-problem, the chunking of data. We show that chunking is an\nimportant part of CL, accounting for around half of the performance drop from\noffline learning in our experiments. Furthermore, our results reveal that\ncurrent CL algorithms do not address the chunking sub-problem, only performing\nas well as plain SGD training when there is no shift in the data distribution.\nTherefore, we show that chunking is both an important and currently unaddressed\nsub-problem and until it is addressed CL methods will be capped in performance.\nAdditionally, we analyse why performance drops when learning occurs on\nidentically distributed chunks of data, and find that forgetting, which is\noften seen to be a problem due to distribution shift, still arises and is a\nsignificant problem. We also show that performance on the chunking sub-problem\ncan be increased and that this performance transfers to the full CL setting,\nwhere there is distribution shift. Hence, we argue that work on chunking can\nhelp advance CL in general.\n","authors":["Thomas L. Lee","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2310.02206v2.pdf","comment":"Published at the 3rd Conference on Lifelong Learning Agents (CoLLAs),\n  2024"},{"id":"http://arxiv.org/abs/2302.03460v3","updated":"2024-07-11T12:13:04Z","published":"2023-02-07T13:31:02Z","title":"Mind the Gap! Bridging Explainable Artificial Intelligence and Human\n  Understanding with Luhmann's Functional Theory of Communication","summary":"  Over the past decade explainable artificial intelligence has evolved from a\npredominantly technical discipline into a field that is deeply intertwined with\nsocial sciences. Insights such as human preference for contrastive -- more\nprecisely, counterfactual -- explanations have played a major role in this\ntransition, inspiring and guiding the research in computer science. Other\nobservations, while equally important, have nevertheless received much less\nconsideration. The desire of human explainees to communicate with artificial\nintelligence explainers through a dialogue-like interaction has been mostly\nneglected by the community. This poses many challenges for the effectiveness\nand widespread adoption of such technologies as delivering a single explanation\noptimised according to some predefined objectives may fail to engender\nunderstanding in its recipients and satisfy their unique needs given the\ndiversity of human knowledge and intention. Using insights elaborated by Niklas\nLuhmann and, more recently, Elena Esposito we apply social systems theory to\nhighlight challenges in explainable artificial intelligence and offer a path\nforward, striving to reinvigorate the technical research in the direction of\ninteractive and iterative explainers. Specifically, this paper demonstrates the\npotential of systems theoretical approaches to communication in elucidating and\naddressing the problems and limitations of human-centred explainable artificial\nintelligence.\n","authors":["Bernard Keenan","Kacper Sokol"],"pdf_url":"https://arxiv.org/pdf/2302.03460v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08432v1","updated":"2024-07-11T12:12:55Z","published":"2024-07-11T12:12:55Z","title":"Subgroup-Specific Risk-Controlled Dose Estimation in Radiotherapy","summary":"  Cancer remains a leading cause of death, highlighting the importance of\neffective radiotherapy (RT). Magnetic resonance-guided linear accelerators\n(MR-Linacs) enable imaging during RT, allowing for inter-fraction, and perhaps\neven intra-fraction, adjustments of treatment plans. However, achieving this\nrequires fast and accurate dose calculations. While Monte Carlo simulations\noffer accuracy, they are computationally intensive. Deep learning frameworks\nshow promise, yet lack uncertainty quantification crucial for high-risk\napplications like RT. Risk-controlling prediction sets (RCPS) offer\nmodel-agnostic uncertainty quantification with mathematical guarantees.\nHowever, we show that naive application of RCPS may lead to only certain\nsubgroups such as the image background being risk-controlled. In this work, we\nextend RCPS to provide prediction intervals with coverage guarantees for\nmultiple subgroups with unknown subgroup membership at test time. We evaluate\nour algorithm on real clinical planing volumes from five different anatomical\nregions and show that our novel subgroup RCPS (SG-RCPS) algorithm leads to\nprediction intervals that jointly control the risk for multiple subgroups. In\nparticular, our method controls the risk of the crucial voxels along the\nradiation beam significantly better than conventional RCPS.\n","authors":["Paul Fischer","Hannah Willms","Moritz Schneider","Daniela Thorwarth","Michael Muehlebach","Christian F. Baumgartner"],"pdf_url":"https://arxiv.org/pdf/2407.08432v1.pdf","comment":"This work was accepted as a full paper at MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.00107v2","updated":"2024-07-11T12:12:48Z","published":"2024-06-27T11:11:19Z","title":"WineGraph: A Graph Representation For Food-Wine Pairing","summary":"  We present WineGraph, an extended version of FlavorGraph, a heterogeneous\ngraph incorporating wine data into its structure. This integration enables\nfood-wine pairing based on taste and sommelier-defined rules. Leveraging a food\ndataset comprising 500,000 reviews and a wine reviews dataset with over 130,000\nentries, we computed taste descriptors for both food and wine. This information\nwas then utilised to pair food items with wine and augment FlavorGraph with\nadditional data. The results demonstrate the potential of heterogeneous graphs\nto acquire supplementary information, proving beneficial for wine pairing.\n","authors":["Zuzanna Gawrysiak","Agata Å»ywot","Agnieszka Åawrynowicz"],"pdf_url":"https://arxiv.org/pdf/2407.00107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11269v2","updated":"2024-07-11T12:04:13Z","published":"2024-04-17T11:20:14Z","title":"DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in\n  Multivariate Time Series","summary":"  In time series anomaly detection (TSAD), the scarcity of labeled data poses a\nchallenge to the development of accurate models. Unsupervised domain adaptation\n(UDA) offers a solution by leveraging labeled data from a related domain to\ndetect anomalies in an unlabeled target domain. However, existing UDA methods\nassume consistent anomalous classes across domains. To address this limitation,\nwe propose a novel Domain Adaptation Contrastive learning model for Anomaly\nDetection in multivariate time series (DACAD), combining UDA with contrastive\nlearning. DACAD utilizes an anomaly injection mechanism that enhances\ngeneralization across unseen anomalous classes, improving adaptability and\nrobustness. Additionally, our model employs supervised contrastive loss for the\nsource domain and self-supervised contrastive triplet loss for the target\ndomain, ensuring comprehensive feature representation learning and\ndomain-invariant feature extraction. Finally, an effective Centre-based Entropy\nClassifier (CEC) accurately learns normal boundaries in the source domain.\nExtensive evaluations on multiple real-world datasets and a synthetic dataset\nhighlight DACAD's superior performance in transferring knowledge across domains\nand mitigating the challenge of limited labeled data in TSAD.\n","authors":["Zahra Zamanzadeh Darban","Yiyuan Yang","Geoffrey I. Webb","Charu C. Aggarwal","Qingsong Wen","Mahsa Salehi"],"pdf_url":"https://arxiv.org/pdf/2404.11269v2.pdf","comment":"11 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2401.13537v3","updated":"2024-07-11T11:55:25Z","published":"2024-01-24T15:46:32Z","title":"Masked Particle Modeling on Sets: Towards Self-Supervised High Energy\n  Physics Foundation Models","summary":"  We propose masked particle modeling (MPM) as a self-supervised method for\nlearning generic, transferable, and reusable representations on unordered sets\nof inputs for use in high energy physics (HEP) scientific data. This work\nprovides a novel scheme to perform masked modeling based pre-training to learn\npermutation invariant functions on sets. More generally, this work provides a\nstep towards building large foundation models for HEP that can be generically\npre-trained with self-supervised learning and later fine-tuned for a variety of\ndown-stream tasks. In MPM, particles in a set are masked and the training\nobjective is to recover their identity, as defined by a discretized token\nrepresentation of a pre-trained vector quantized variational autoencoder. We\nstudy the efficacy of the method in samples of high energy jets at collider\nphysics experiments, including studies on the impact of discretization,\npermutation invariance, and ordering. We also study the fine-tuning capability\nof the model, showing that it can be adapted to tasks such as supervised and\nweakly supervised jet classification, and that the model can transfer\nefficiently with small fine-tuning data sets to new classes and new data\ndomains.\n","authors":["Tobias Golling","Lukas Heinrich","Michael Kagan","Samuel Klein","Matthew Leigh","Margarita Osadchy","John Andrew Raine"],"pdf_url":"https://arxiv.org/pdf/2401.13537v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10879v3","updated":"2024-07-11T11:53:21Z","published":"2023-01-26T00:17:10Z","title":"SuperFedNAS: Cost-Efficient Federated Neural Architecture Search for\n  On-Device Inference","summary":"  Neural Architecture Search (NAS) for Federated Learning (FL) is an emerging\nfield. It automates the design and training of Deep Neural Networks (DNNs) when\ndata cannot be centralized due to privacy, communication costs, or regulatory\nrestrictions. Recent federated NAS methods not only reduce manual effort but\nalso help achieve higher accuracy than traditional FL methods like FedAvg.\nDespite the success, existing federated NAS methods still fall short in\nsatisfying diverse deployment targets common in on-device inference like\nhardware, latency budgets, or variable battery levels. Most federated NAS\nmethods search for only a limited range of neuro-architectural patterns, repeat\nthem in a DNN, thereby restricting achievable performance. Moreover, these\nmethods incur prohibitive training costs to satisfy deployment targets. They\nperform the training and search of DNN architectures repeatedly for each case.\nSuperFedNAS addresses these challenges by decoupling the training and search in\nfederated NAS. SuperFedNAS co-trains a large number of diverse DNN\narchitectures contained inside one supernet in the FL setting. Post-training,\nclients perform NAS locally to find specialized DNNs by extracting different\nparts of the trained supernet with no additional training. SuperFedNAS takes\nO(1) (instead of O(N)) cost to find specialized DNN architectures in FL for any\nN deployment targets. As part of SuperFedNAS, we introduce MaxNet - a novel FL\ntraining algorithm that performs multi-objective federated optimization of a\nlarge number of DNN architectures ($\\approx 5*10^8$) under different client\ndata distributions. Overall, SuperFedNAS achieves upto 37.7% higher accuracy\nfor the same MACs or upto 8.13x reduction in MACs for the same accuracy than\nexisting federated NAS methods.\n","authors":["Alind Khare","Animesh Agrawal","Aditya Annavajjala","Payman Behnam","Myungjin Lee","Hugo Latapie","Alexey Tumanov"],"pdf_url":"https://arxiv.org/pdf/2301.10879v3.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.08418v1","updated":"2024-07-11T11:51:36Z","published":"2024-07-11T11:51:36Z","title":"PredBench: Benchmarking Spatio-Temporal Prediction across Diverse\n  Disciplines","summary":"  In this paper, we introduce PredBench, a benchmark tailored for the holistic\nevaluation of spatio-temporal prediction networks. Despite significant progress\nin this field, there remains a lack of a standardized framework for a detailed\nand comparative analysis of various prediction network architectures. PredBench\naddresses this gap by conducting large-scale experiments, upholding\nstandardized and appropriate experimental settings, and implementing\nmulti-dimensional evaluations. This benchmark integrates 12 widely adopted\nmethods with 15 diverse datasets across multiple application domains, offering\nextensive evaluation of contemporary spatio-temporal prediction networks.\nThrough meticulous calibration of prediction settings across various\napplications, PredBench ensures evaluations relevant to their intended use and\nenables fair comparisons. Moreover, its multi-dimensional evaluation framework\nbroadens the analysis with a comprehensive set of metrics, providing deep\ninsights into the capabilities of models. The findings from our research offer\nstrategic directions for future developments in the field. Our codebase is\navailable at https://github.com/WZDTHU/PredBench.\n","authors":["ZiDong Wang","Zeyu Lu","Di Huang","Tong He","Xihui Liu","Wanli Ouyang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2407.08418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08417v1","updated":"2024-07-11T11:47:43Z","published":"2024-07-11T11:47:43Z","title":"Unveiling the Potential of BERTopic for Multilingual Fake News Analysis\n  -- Use Case: Covid-19","summary":"  Topic modeling is frequently being used for analysing large text corpora such\nas news articles or social media data. BERTopic, consisting of sentence\nembedding, dimension reduction, clustering, and topic extraction, is the newest\nand currently the SOTA topic modeling method. However, current topic modeling\nmethods have room for improvement because, as unsupervised methods, they\nrequire careful tuning and selection of hyperparameters, e.g., for dimension\nreduction and clustering. This paper aims to analyse the technical application\nof BERTopic in practice. For this purpose, it compares and selects different\nmethods and hyperparameters for each stage of BERTopic through density based\nclustering validation and six different topic coherence measures. Moreover, it\nalso aims to analyse the results of topic modeling on real world data as a use\ncase. For this purpose, the German fake news dataset (GermanFakeNCovid) on\nCovid-19 was created by us and in order to experiment with topic modeling in a\nmultilingual (English and German) setting combined with the FakeCovid dataset.\nWith the final results, we were able to determine thematic similarities between\nthe United States and Germany. Whereas, distinguishing the topics of fake news\nfrom India proved to be more challenging.\n","authors":["Karla SchÃ¤fer","Jeong-Eun Choi","Inna Vogel","Martin Steinebach"],"pdf_url":"https://arxiv.org/pdf/2407.08417v1.pdf","comment":"Accepted at the Workshop on Representation Learning and Clustering\n  (RLC) at the 17th ACM International WSDM Conference in 2024"},{"id":"http://arxiv.org/abs/2407.08415v1","updated":"2024-07-11T11:41:29Z","published":"2024-07-11T11:41:29Z","title":"Parallelizing Autoregressive Generation with Variational State Space\n  Models","summary":"  Attention-based models such as Transformers and recurrent models like state\nspace models (SSMs) have emerged as successful methods for autoregressive\nsequence modeling. Although both enable parallel training, none enable parallel\ngeneration due to their autoregressiveness. We propose the variational SSM\n(VSSM), a variational autoencoder (VAE) where both the encoder and decoder are\nSSMs. Since sampling the latent variables and decoding them with the SSM can be\nparallelized, both training and generation can be conducted in parallel.\nMoreover, the decoder recurrence allows generation to be resumed without\nreprocessing the whole sequence. Finally, we propose the autoregressive VSSM\nthat can be conditioned on a partial realization of the sequence, as is common\nin language generation tasks. Interestingly, the autoregressive VSSM still\nenables parallel generation. We highlight on toy problems (MNIST, CIFAR) the\nempirical gains in speed-up and show that it competes with traditional models\nin terms of generation quality (Transformer, Mamba SSM).\n","authors":["Gaspard Lambrechts","Yann Claes","Pierre Geurts","Damien Ernst"],"pdf_url":"https://arxiv.org/pdf/2407.08415v1.pdf","comment":"4 pages, 11 pages total, 3 figures"},{"id":"http://arxiv.org/abs/2407.06315v2","updated":"2024-07-11T11:11:03Z","published":"2024-07-08T18:31:19Z","title":"Shedding More Light on Robust Classifiers under the lens of Energy-based\n  Models","summary":"  By reinterpreting a robust discriminative classifier as Energy-based Model\n(EBM), we offer a new take on the dynamics of adversarial training (AT). Our\nanalysis of the energy landscape during AT reveals that untargeted attacks\ngenerate adversarial images much more in-distribution (lower energy) than the\noriginal data from the point of view of the model. Conversely, we observe the\nopposite for targeted attacks. On the ground of our thorough analysis, we\npresent new theoretical and practical results that show how interpreting AT\nenergy dynamics unlocks a better understanding: (1) AT dynamic is governed by\nthree phases and robust overfitting occurs in the third phase with a drastic\ndivergence between natural and adversarial energies (2) by rewriting the loss\nof TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization\n(TRADES) in terms of energies, we show that TRADES implicitly alleviates\noverfitting by means of aligning the natural energy with the adversarial one\n(3) we empirically show that all recent state-of-the-art robust classifiers are\nsmoothing the energy landscape and we reconcile a variety of studies about\nunderstanding AT and weighting the loss function under the umbrella of EBMs.\nMotivated by rigorous evidence, we propose Weighted Energy Adversarial Training\n(WEAT), a novel sample weighting scheme that yields robust accuracy matching\nthe state-of-the-art on multiple benchmarks such as CIFAR-10 and SVHN and going\nbeyond in CIFAR-100 and Tiny-ImageNet. We further show that robust classifiers\nvary in the intensity and quality of their generative capabilities, and offer a\nsimple method to push this capability, reaching a remarkable Inception Score\n(IS) and FID using a robust classifier without training for generative\nmodeling. The code to reproduce our results is available at\nhttp://github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/ .\n","authors":["Mujtaba Hussain Mirza","Maria Rosaria Briglia","Senad Beadini","Iacopo Masi"],"pdf_url":"https://arxiv.org/pdf/2407.06315v2.pdf","comment":"Accepted at European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2407.08364v1","updated":"2024-07-11T10:18:54Z","published":"2024-07-11T10:18:54Z","title":"Scalar Function Topology Divergence: Comparing Topology of 3D Objects","summary":"  We propose a new topological tool for computer vision - Scalar Function\nTopology Divergence (SFTD), which measures the dissimilarity of multi-scale\ntopology between sublevel sets of two functions having a common domain.\nFunctions can be defined on an undirected graph or Euclidean space of any\ndimensionality. Most of the existing methods for comparing topology are based\non Wasserstein distance between persistence barcodes and they don't take into\naccount the localization of topological features. On the other hand, the\nminimization of SFTD ensures that the corresponding topological features of\nscalar functions are located in the same places. The proposed tool provides\nuseful visualizations depicting areas where functions have topological\ndissimilarities. We provide applications of the proposed method to 3D computer\nvision. In particular, experiments demonstrate that SFTD improves the\nreconstruction of cellular 3D shapes from 2D fluorescence microscopy images,\nand helps to identify topological errors in 3D segmentation.\n","authors":["Ilya Trofimov","Daria Voronkova","Eduard Tulchinskii","Evgeny Burnaev","Serguei Barannikov"],"pdf_url":"https://arxiv.org/pdf/2407.08364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02271v2","updated":"2024-07-11T10:16:41Z","published":"2023-06-04T06:30:13Z","title":"SubspaceNet: Deep Learning-Aided Subspace Methods for DoA Estimation","summary":"  Direction of arrival (DoA) estimation is a fundamental task in array\nprocessing. A popular family of DoA estimation algorithms are subspace methods,\nwhich operate by dividing the measurements into distinct signal and noise\nsubspaces. Subspace methods, such as Multiple Signal Classification (MUSIC) and\nRoot-MUSIC, rely on several restrictive assumptions, including narrowband\nnon-coherent sources and fully calibrated arrays, and their performance is\nconsiderably degraded when these do not hold. In this work we propose\nSubspaceNet; a data-driven DoA estimator which learns how to divide the\nobservations into distinguishable subspaces. This is achieved by utilizing a\ndedicated deep neural network to learn the empirical autocorrelation of the\ninput, by training it as part of the Root-MUSIC method, leveraging the inherent\ndifferentiability of this specific DoA estimator, while removing the need to\nprovide a ground-truth decomposable autocorrelation matrix. Once trained, the\nresulting SubspaceNet serves as a universal surrogate covariance estimator that\ncan be applied in combination with any subspace-based DoA estimation method,\nallowing its successful application in challenging setups. SubspaceNet is shown\nto enable various DoA estimation algorithms to cope with coherent sources,\nwideband signals, low SNR, array mismatches, and limited snapshots, while\npreserving the interpretability and the suitability of classic subspace\nmethods.\n","authors":["Dor H. Shmuel","Julian P. Merkofer","Guy Revach","Ruud J. G. van Sloun","Nir Shlezinger"],"pdf_url":"https://arxiv.org/pdf/2306.02271v2.pdf","comment":"Under review for publication in the IEEE"},{"id":"http://arxiv.org/abs/2302.13939v5","updated":"2024-07-11T10:16:12Z","published":"2023-02-27T16:43:04Z","title":"SpikeGPT: Generative Pre-trained Language Model with Spiking Neural\n  Networks","summary":"  As the size of large language models continue to scale, so does the\ncomputational resources required to run it. Spiking Neural Networks (SNNs) have\nemerged as an energy-efficient approach to deep learning that leverage sparse\nand event-driven activations to reduce the computational overhead associated\nwith model inference. While they have become competitive with non-spiking\nmodels on many computer vision tasks, SNNs have also proven to be more\nchallenging to train. As a result, their performance lags behind modern deep\nlearning, and we are yet to see the effectiveness of SNNs in language\ngeneration. In this paper, inspired by the Receptance Weighted Key Value (RWKV)\nlanguage model, we successfully implement `SpikeGPT', a generative language\nmodel with binary, event-driven spiking activation units. We train the proposed\nmodel on two model variants: 45M and 216M parameters. To the best of our\nknowledge, SpikeGPT is the largest backpropagation-trained SNN model to date,\nrendering it suitable for both the generation and comprehension of natural\nlanguage. We achieve this by modifying the transformer block to replace\nmulti-head self attention to reduce quadratic computational complexity O(N^2)\nto linear complexity O(N) with increasing sequence length. Input tokens are\ninstead streamed in sequentially to our attention mechanism (as with typical\nSNNs). Our preliminary experiments show that SpikeGPT remains competitive with\nnon-spiking models on tested benchmarks, while maintaining 20x fewer operations\nwhen processed on neuromorphic hardware that can leverage sparse, event-driven\nactivations. Our code implementation is available at\nhttps://github.com/ridgerchu/SpikeGPT.\n","authors":["Rui-Jie Zhu","Qihang Zhao","Guoqi Li","Jason K. Eshraghian"],"pdf_url":"https://arxiv.org/pdf/2302.13939v5.pdf","comment":"Accepted by TMLR"},{"id":"http://arxiv.org/abs/2407.08362v1","updated":"2024-07-11T10:15:52Z","published":"2024-07-11T10:15:52Z","title":"STAL: Spike Threshold Adaptive Learning Encoder for Classification of\n  Pain-Related Biosignal Data","summary":"  This paper presents the first application of spiking neural networks (SNNs)\nfor the classification of chronic lower back pain (CLBP) using the EmoPain\ndataset. Our work has two main contributions. We introduce Spike Threshold\nAdaptive Learning (STAL), a trainable encoder that effectively converts\ncontinuous biosignals into spike trains. Additionally, we propose an ensemble\nof Spiking Recurrent Neural Network (SRNN) classifiers for the multi-stream\nprocessing of sEMG and IMU data. To tackle the challenges of small sample size\nand class imbalance, we implement minority over-sampling with weighted sample\nreplacement during batch creation. Our method achieves outstanding performance\nwith an accuracy of 80.43%, AUC of 67.90%, F1 score of 52.60%, and Matthews\nCorrelation Coefficient (MCC) of 0.437, surpassing traditional rate-based and\nlatency-based encoding methods. The STAL encoder shows superior performance in\npreserving temporal dynamics and adapting to signal characteristics.\nImportantly, our approach (STAL-SRNN) outperforms the best deep learning method\nin terms of MCC, indicating better balanced class prediction. This research\ncontributes to the development of neuromorphic computing for biosignal\nanalysis. It holds promise for energy-efficient, wearable solutions in chronic\npain management.\n","authors":["Freek Hens","Mohammad Mahdi Dehshibi","Leila Bagheriye","Mahyar Shahsavari","Ana Tajadura-JimÃ©nez"],"pdf_url":"https://arxiv.org/pdf/2407.08362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08351v1","updated":"2024-07-11T10:03:47Z","published":"2024-07-11T10:03:47Z","title":"AutoBencher: Creating Salient, Novel, Difficult Datasets for Language\n  Models","summary":"  Evaluation is critical for assessing capabilities, tracking scientific\nprogress, and informing model selection. In this paper, we present three\ndesiderata for a good benchmark for language models: (i) salience (e.g.,\nknowledge about World War II is more salient than a random day in history),\n(ii) novelty (i.e., the benchmark reveals new trends in model rankings not\nshown by previous benchmarks), and (iii) difficulty (i.e., the benchmark should\nbe difficult for existing models, leaving headroom for future improvement). We\noperationalize these three desiderata and cast benchmark creation as a search\nproblem, that of finding benchmarks that that satisfy all three desiderata. To\ntackle this search problem, we present AutoBencher, which uses a language model\nto automatically search for datasets that meet the three desiderata.\nAutoBencher uses privileged information (e.g. relevant documents) to construct\nreliable datasets, and adaptivity with reranking to optimize for the search\nobjective. We use AutoBencher to create datasets for math, multilingual, and\nknowledge-intensive question answering. The scalability of AutoBencher allows\nit to test fine-grained categories and tail knowledge, creating datasets that\nare on average 27% more novel and 22% more difficult than existing benchmarks.\nA closer investigation of our constructed datasets shows that we can identify\nspecific gaps in LM knowledge in language models that are not captured by\nexisting benchmarks, such as Gemini Pro performing much worse on question\nanswering about the Permian Extinction and Fordism, while OpenAGI-7B performing\nsurprisingly well on QA about COVID-19.\n","authors":["Xiang Lisa Li","Evan Zheran Liu","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2407.08351v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2407.08348v1","updated":"2024-07-11T09:56:51Z","published":"2024-07-11T09:56:51Z","title":"Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large\n  Language Models -- The Story Goes On","summary":"  In this paper, we investigate the underlying factors that potentially enhance\nthe mathematical reasoning capabilities of large language models (LLMs). We\nargue that the data scaling law for math reasoning capabilities in modern LLMs\nis far from being saturated, highlighting how the model's quality improves with\nincreases in data quantity. To support this claim, we introduce the\nSkywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using\nour proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved\nimpressive accuracies of 51.2% on the competition-level MATH benchmark and\n83.9% on the GSM8K benchmark using only SFT data, outperforming an early\nversion of GPT-4 on MATH. The superior performance of Skywork-Math models\ncontributes to our novel two-stage data synthesis and model SFT pipelines,\nwhich include three different augmentation methods and a diverse seed problem\nset, ensuring both the quantity and quality of Skywork-MathQA dataset across\nvarying difficulty levels. Most importantly, we provide several practical\ntakeaways to enhance math reasoning abilities in LLMs for both research and\nindustry applications.\n","authors":["Liang Zeng","Liangjun Zhong","Liang Zhao","Tianwen Wei","Liu Yang","Jujie He","Cheng Cheng","Rui Hu","Yang Liu","Shuicheng Yan","Han Fang","Yahui Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.08348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08340v1","updated":"2024-07-11T09:43:57Z","published":"2024-07-11T09:43:57Z","title":"SLRL: Structured Latent Representation Learning for Multi-view\n  Clustering","summary":"  In recent years, Multi-View Clustering (MVC) has attracted increasing\nattention for its potential to reduce the annotation burden associated with\nlarge datasets. The aim of MVC is to exploit the inherent consistency and\ncomplementarity among different views, thereby integrating information from\nmultiple perspectives to improve clustering outcomes.\n  Despite extensive research in MVC, most existing methods focus predominantly\non harnessing complementary information across views to enhance clustering\neffectiveness, often neglecting the structural information among samples, which\nis crucial for exploring sample correlations. To address this gap, we introduce\na novel framework, termed Structured Latent Representation Learning based\nMulti-View Clustering method (SLRL). SLRL leverages both the complementary and\nstructural information. Initially, it learns a common latent representation for\nall views. Subsequently, to exploit the structural information among samples, a\nk-nearest neighbor graph is constructed from this common latent representation.\nThis graph facilitates enhanced sample interaction through graph learning\ntechniques, leading to a structured latent representation optimized for\nclustering. Extensive experiments demonstrate that SLRL not only competes well\nwith existing methods but also sets new benchmarks in various multi-view\ndatasets.\n","authors":["Zhangci Xiong","Meng Cao"],"pdf_url":"https://arxiv.org/pdf/2407.08340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08337v1","updated":"2024-07-11T09:40:29Z","published":"2024-07-11T09:40:29Z","title":"FedLog: Personalized Federated Classification with Less Communication\n  and More Flexibility","summary":"  In federated learning (FL), the common paradigm that FedAvg proposes and most\nalgorithms follow is that clients train local models with their private data,\nand the model parameters are shared for central aggregation, mostly averaging.\nIn this paradigm, the communication cost is often a challenge, as modern\nmassive neural networks can contain millions to billions parameters. We suggest\nthat clients do not share model parameters but local data summaries, to\ndecrease the cost of sharing. We develop a new algorithm FedLog with Bayesian\ninference, which shares only sufficient statistics of local data. FedLog\ntransmits messages as small as the last layer of the original model. We\nconducted comprehensive experiments to show we outperform other FL algorithms\nthat aim at decreasing the communication cost. To provide formal privacy\nguarantees, we further extend FedLog with differential privacy and show the\ntrade-off between privacy budget and accuracy.\n","authors":["Haolin Yu","Guojun Zhang","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2407.08337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01648v3","updated":"2024-07-11T09:32:19Z","published":"2023-12-04T06:01:32Z","title":"Characterizing Large Language Model Geometry Helps Solve Toxicity\n  Detection and Generation","summary":"  Large Language Models (LLMs) drive current AI breakthroughs despite very\nlittle being known about their internal representations. In this work, we\npropose to shed the light on LLMs inner mechanisms through the lens of\ngeometry. In particular, we develop in closed form $(i)$ the intrinsic\ndimension in which the Multi-Head Attention embeddings are constrained to exist\nand $(ii)$ the partition and per-region affine mappings of the feedforward\n(MLP) network of LLMs' layers. Our theoretical findings further enable the\ndesign of novel principled solutions applicable to state-of-the-art LLMs.\nFirst, we show that, through our geometric understanding, we can bypass LLMs'\nRLHF protection by controlling the embedding's intrinsic dimension through\ninformed prompt manipulation. Second, we derive interpretable geometrical\nfeatures that can be extracted from any (pre-trained) LLM, providing a rich\nabstract representation of their inputs. We observe that these features are\nsufficient to help solve toxicity detection, and even allow the identification\nof various types of toxicity. Our results demonstrate how, even in large-scale\nregimes, exact theoretical results can answer practical questions in LLMs.\nCode: https://github.com/RandallBalestriero/SplineLLM\n","authors":["Randall Balestriero","Romain Cosentino","Sarath Shekkizhar"],"pdf_url":"https://arxiv.org/pdf/2312.01648v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08330v1","updated":"2024-07-11T09:28:04Z","published":"2024-07-11T09:28:04Z","title":"HDT: Hierarchical Document Transformer","summary":"  In this paper, we propose the Hierarchical Document Transformer (HDT), a\nnovel sparse Transformer architecture tailored for structured hierarchical\ndocuments. Such documents are extremely important in numerous domains,\nincluding science, law or medicine. However, most existing solutions are\ninefficient and fail to make use of the structure inherent to documents. HDT\nexploits document structure by introducing auxiliary anchor tokens and\nredesigning the attention mechanism into a sparse multi-level hierarchy. This\napproach facilitates information exchange between tokens at different levels\nwhile maintaining sparsity, thereby enhancing computational and memory\nefficiency while exploiting the document structure as an inductive bias. We\naddress the technical challenge of implementing HDT's sample-dependent\nhierarchical attention pattern by developing a novel sparse attention kernel\nthat considers the hierarchical structure of documents. As demonstrated by our\nexperiments, utilizing structural information present in documents leads to\nfaster convergence, higher sample efficiency and better performance on\ndownstream tasks.\n","authors":["Haoyu He","Markus Flicke","Jan Buchmann","Iryna Gurevych","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2407.08330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08328v1","updated":"2024-07-11T09:26:05Z","published":"2024-07-11T09:26:05Z","title":"Unveiling Disparities in Maternity Care: A Topic Modelling Approach to\n  Analysing Maternity Incident Investigation Reports","summary":"  This study applies Natural Language Processing techniques, including Latent\nDirichlet Allocation, to analyse anonymised maternity incident investigation\nreports from the Healthcare Safety Investigation Branch. The reports underwent\npreprocessing, annotation using the Safety Intelligence Research taxonomy, and\ntopic modelling to uncover prevalent topics and detect differences in maternity\ncare across ethnic groups. A combination of offline and online methods was\nutilised to ensure data protection whilst enabling advanced analysis, with\noffline processing for sensitive data and online processing for non-sensitive\ndata using the `Claude 3 Opus' language model. Interactive topic analysis and\nsemantic network visualisation were employed to extract and display thematic\ntopics and visualise semantic relationships among keywords. The analysis\nrevealed disparities in care among different ethnic groups, with distinct focus\nareas for the Black, Asian, and White British ethnic groups. The study\ndemonstrates the effectiveness of topic modelling and NLP techniques in\nanalysing maternity incident investigation reports and highlighting disparities\nin care. The findings emphasise the crucial role of advanced data analysis in\nimproving maternity care quality and equity.\n","authors":["Georgina Cosma","Mohit Kumar Singh","Patrick Waterson","Gyuchan Thomas Jun","Jonathan Back"],"pdf_url":"https://arxiv.org/pdf/2407.08328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03133v2","updated":"2024-07-11T09:19:11Z","published":"2024-05-24T08:10:31Z","title":"Quantifying the Cross-sectoral Intersecting Discrepancies within\n  Multiple Groups Using Latent Class Analysis Towards Fairness","summary":"  The growing interest in fair AI development is evident. The ''Leave No One\nBehind'' initiative urges us to address multiple and intersecting forms of\ninequality in accessing services, resources, and opportunities, emphasising the\nsignificance of fairness in AI. This is particularly relevant as an increasing\nnumber of AI tools are applied to decision-making processes, such as resource\nallocation and service scheme development, across various sectors such as\nhealth, energy, and housing. Therefore, exploring joint inequalities in these\nsectors is significant and valuable for thoroughly understanding overall\ninequality and unfairness. This research introduces an innovative approach to\nquantify cross-sectoral intersecting discrepancies among user-defined groups\nusing latent class analysis. These discrepancies can be used to approximate\ninequality and provide valuable insights to fairness issues. We validate our\napproach using both proprietary and public datasets, including EVENS and Census\n2021 (England & Wales) datasets, to examine cross-sectoral intersecting\ndiscrepancies among different ethnic groups. We also verify the reliability of\nthe quantified discrepancy by conducting a correlation analysis with a\ngovernment public metric. Our findings reveal significant discrepancies between\nminority ethnic groups, highlighting the need for targeted interventions in\nreal-world AI applications. Additionally, we demonstrate how the proposed\napproach can be used to provide insights into the fairness of machine learning.\n","authors":["Yingfang Yuan","Kefan Chen","Mehdi Rizvi","Lynne Baillie","Wei Pang"],"pdf_url":"https://arxiv.org/pdf/2407.03133v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08324v1","updated":"2024-07-11T09:13:11Z","published":"2024-07-11T09:13:11Z","title":"A Cantor-Kantorovich Metric Between Markov Decision Processes with\n  Application to Transfer Learning","summary":"  We extend the notion of Cantor-Kantorovich distance between Markov chains\nintroduced by (Banse et al., 2023) in the context of Markov Decision Processes\n(MDPs). The proposed metric is well-defined and can be efficiently approximated\ngiven a finite horizon. Then, we provide numerical evidences that the latter\nmetric can lead to interesting applications in the field of reinforcement\nlearning. In particular, we show that it could be used for forecasting the\nperformance of transfer learning algorithms.\n","authors":["Adrien Banse","Venkatraman Renganathan","RaphaÃ«l M. Jungers"],"pdf_url":"https://arxiv.org/pdf/2407.08324v1.pdf","comment":"Presented at the 26th International Symposium on Mathematical Theory\n  of Networks and Systems (Cambridge, UK)"},{"id":"http://arxiv.org/abs/2407.05832v2","updated":"2024-07-11T09:10:09Z","published":"2024-07-08T11:25:30Z","title":"A Machine Learning Approach to Detecting Albedo Anomalies on the Lunar\n  Surface","summary":"  This study introduces a data-driven approach using machine learning (ML)\ntechniques to explore and predict albedo anomalies on the Moon's surface. The\nresearch leverages diverse planetary datasets, including\nhigh-spatial-resolution albedo maps and element maps (LPFe, LPK, LPTh, LPTi)\nderived from laser and gamma-ray measurements. The primary objective is to\nidentify relationships between chemical elements and albedo, thereby expanding\nour understanding of planetary surfaces and offering predictive capabilities\nfor areas with incomplete datasets. To bridge the gap in resolution between the\nalbedo and element maps, we employ Gaussian blurring techniques, including an\ninnovative adaptive Gaussian blur. Our methodology culminates in the deployment\nof an Extreme Gradient Boosting Regression Model, optimized to predict full\nalbedo based on elemental composition. Furthermore, we present an interactive\nanalytical tool to visualize prediction errors, delineating their spatial and\nchemical characteristics. The findings not only pave the way for a more\ncomprehensive understanding of the Moon's surface but also provide a framework\nfor similar studies on other celestial bodies.\n","authors":["Sofia Strukova","Sergei Gleyzer","Patrick Peplowski","Jason P. Terry"],"pdf_url":"https://arxiv.org/pdf/2407.05832v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08316v1","updated":"2024-07-11T09:07:22Z","published":"2024-07-11T09:07:22Z","title":"Enhancing ADHD Diagnosis with EEG: The Critical Role of Preprocessing\n  and Key Features","summary":"  Background: Attention-Deficit/Hyperactivity Disorder (ADHD) is a prevalent\nneurodevelopmental disorder that significantly impacts various key aspects of\nlife, requiring accurate diagnostic methods. Electroencephalogram (EEG) signals\nare used in diagnosing ADHD, but proper preprocessing is crucial to avoid noise\nand artifacts that could lead to unreliable results.\n  Method: This study utilized a public EEG dataset from children diagnosed with\nADHD and typically developing (TD) children. Four preprocessing techniques were\napplied: no preprocessing (Raw), Finite Impulse Response (FIR) filtering,\nArtifact Subspace Reconstruction (ASR), and Independent Component Analysis\n(ICA). EEG recordings were segmented, and features were extracted and selected\nbased on statistical significance. Classification was performed using Machine\nLearning models, as XGBoost, Support Vector Machine, and K-Nearest Neighbors.\n  Results: The absence of preprocessing leads to artificially high\nclassification accuracy due to noise. In contrast, ASR and ICA preprocessing\ntechniques significantly improved the reliability of results. Segmenting EEG\nrecordings revealed that later segments provided better classification\naccuracy, likely due to the manifestation of ADHD symptoms over time. The most\nrelevant EEG channels were P3, P4, and C3. The top features for classification\nincluded Kurtosis, Katz fractal dimension, and power spectral density of Delta,\nTheta, and Alpha bands.\n  Conclusions: Effective preprocessing is essential in EEG-based ADHD diagnosis\nto prevent noise-induced biases. This study identifies crucial EEG channels and\nfeatures, providing a foundation for further research and improving ADHD\ndiagnostic accuracy. Future work should focus on expanding datasets, refining\npreprocessing methods, and enhancing feature interpretability to improve\ndiagnostic accuracy and model robustness for clinical use.\n","authors":["Sandra GarcÃ­a-Ponsoda","Alejandro MatÃ©","Juan Trujillo"],"pdf_url":"https://arxiv.org/pdf/2407.08316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08313v1","updated":"2024-07-11T09:04:12Z","published":"2024-07-11T09:04:12Z","title":"Improving Molecular Modeling with Geometric GNNs: an Empirical Study","summary":"  Rapid advancements in machine learning (ML) are transforming materials\nscience by significantly speeding up material property calculations. However,\nthe proliferation of ML approaches has made it challenging for scientists to\nkeep up with the most promising techniques. This paper presents an empirical\nstudy on Geometric Graph Neural Networks for 3D atomic systems, focusing on the\nimpact of different (1) canonicalization methods, (2) graph creation\nstrategies, and (3) auxiliary tasks, on performance, scalability and symmetry\nenforcement. Our findings and insights aim to guide researchers in selecting\noptimal modeling components for molecular modeling tasks.\n","authors":["Ali Ramlaoui","ThÃ©o Saulus","Basile Terver","Victor Schmidt","David Rolnick","Fragkiskos D. Malliaros","Alexandre Duval"],"pdf_url":"https://arxiv.org/pdf/2407.08313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14908v2","updated":"2024-07-11T08:44:45Z","published":"2024-05-23T09:44:02Z","title":"Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model\n  Pretraining","summary":"  Large language models exhibit exceptional generalization capabilities,\nprimarily attributed to the utilization of diversely sourced data. However,\nconventional practices in integrating this diverse data heavily rely on\nheuristic schemes, lacking theoretical guidance. This research tackles these\nlimitations by investigating strategies based on low-cost proxies for data\nmixtures, with the aim of streamlining data curation to enhance training\nefficiency. Specifically, we propose a unified scaling law, termed\n$\\textbf{BiMix}$, which accurately models the bivariate scaling behaviors of\nboth data quantity and mixing proportions. We conduct systematic experiments\nand provide empirical evidence for the predictive power and fundamental\nprinciples of $\\textbf{BiMix}$. Notably, our findings reveal that\nentropy-driven training-free data mixtures can achieve comparable or even\nbetter performance than more resource-intensive methods. We hope that our\nquantitative insights can shed light on further judicious research and\ndevelopment in cost-effective language modeling.\n","authors":["Ce Ge","Zhijian Ma","Daoyuan Chen","Yaliang Li","Bolin Ding"],"pdf_url":"https://arxiv.org/pdf/2405.14908v2.pdf","comment":"typos corrected"},{"id":"http://arxiv.org/abs/2407.08298v1","updated":"2024-07-11T08:44:43Z","published":"2024-07-11T08:44:43Z","title":"XAI-Guided Enhancement of Vegetation Indices for Crop Mapping","summary":"  Vegetation indices allow to efficiently monitor vegetation growth and\nagricultural activities. Previous generations of satellites were capturing a\nlimited number of spectral bands, and a few expert-designed vegetation indices\nwere sufficient to harness their potential. New generations of multi- and\nhyperspectral satellites can however capture additional bands, but are not yet\nefficiently exploited. In this work, we propose an explainable-AI-based method\nto select and design suitable vegetation indices. We first train a deep neural\nnetwork using multispectral satellite data, then extract feature importance to\nidentify the most influential bands. We subsequently select suitable existing\nvegetation indices or modify them to incorporate the identified bands and\nretrain our model. We validate our approach on a crop classification task. Our\nresults indicate that models trained on individual indices achieve comparable\nresults to the baseline model trained on all bands, while the combination of\ntwo indices surpasses the baseline in certain cases.\n","authors":["Hiba Najjar","Francisco Mena","Marlon Nuske","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2407.08298v1.pdf","comment":"Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2024"},{"id":"http://arxiv.org/abs/2407.08296v1","updated":"2024-07-11T08:42:58Z","published":"2024-07-11T08:42:58Z","title":"Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive\n  Low-Rank Gradients","summary":"  Training Large Language Models (LLMs) is memory-intensive due to the large\nnumber of parameters and associated optimization states. GaLore, a recent\nmethod, reduces memory usage by projecting weight gradients into a low-rank\nsubspace without compromising performance. However, GaLore relies on\ntime-consuming Singular Value Decomposition (SVD) operations to identify the\nsubspace, and the frequent subspace updates lead to significant training time\noverhead. Moreover, GaLore offers minimal improvements in accuracy and\nefficiency compared to LoRA in more accessible fine-tuning scenarios. To\naddress these limitations, we introduce Q-Galore, a novel approach that\nsubstantially reduces memory usage by combining quantization and low-rank\nprojection, surpassing the benefits of GaLore. Our method is based on two key\nobservations: (i) the gradient subspace exhibits diverse properties, with some\nlayers converging early in training while others are subject to frequent\nchanges; (ii) the projection matrices are highly resilient to low-bit\nquantization. Leveraging these insights, Q-GaLore adaptively updates the\ngradient subspace based on its convergence statistics, achieving comparable\nperformance while significantly reducing the number of SVD operations. We\nmaintain the projection matrices in INT4 format and weights in INT8 format,\nincorporating stochastic rounding to capture accumulated gradient information.\nThis approach enables a high-precision training trajectory using only\nlow-precision weights. We demonstrate that Q-GaLore achieves highly competitive\nperformance with exceptional memory efficiency. At pre-training, Q-GaLore\nfacilitates training a LLaMA-7B model from scratch on a single NVIDIA RTX 4060\nTi with only 16 GB memory. At fine-tuning, it reduces memory consumption by up\nto 50% compared to LoRA and GaLore, while consistently outperforming QLoRA at\nthe same memory cost.\n","authors":["Zhenyu Zhang","Ajay Jaiswal","Lu Yin","Shiwei Liu","Jiawei Zhao","Yuandong Tian","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08289v1","updated":"2024-07-11T08:33:42Z","published":"2024-07-11T08:33:42Z","title":"Predicting Heart Failure with Attention Learning Techniques Utilizing\n  Cardiovascular Data","summary":"  Cardiovascular diseases (CVDs) encompass a group of disorders affecting the\nheart and blood vessels, including conditions such as coronary artery disease,\nheart failure, stroke, and hypertension. In cardiovascular diseases, heart\nfailure is one of the main causes of death and also long-term suffering in\npatients worldwide. Prediction is one of the risk factors that is highly\nvaluable for treatment and intervention to minimize heart failure. In this\nwork, an attention learning-based heart failure prediction approach is proposed\non EHR(electronic health record) cardiovascular data such as ejection fraction\nand serum creatinine. Moreover, different optimizers with various learning rate\napproaches are applied to fine-tune the proposed approach. Serum creatinine and\nejection fraction are the two most important features to predict the patient's\nheart failure. The computational result shows that the RMSProp optimizer with\n0.001 learning rate has a better prediction based on serum creatinine. On the\nother hand, the combination of SGD optimizer with 0.01 learning rate exhibits\noptimum performance based on ejection fraction features. Overall, the proposed\nattention learning-based approach performs very efficiently in predicting heart\nfailure compared to the existing state-of-the-art such as LSTM approach.\n","authors":["Ershadul Haque","Manoranjan Paul","Faranak Tohidi"],"pdf_url":"https://arxiv.org/pdf/2407.08289v1.pdf","comment":"11 pages, 37 figures"},{"id":"http://arxiv.org/abs/2407.08282v1","updated":"2024-07-11T08:30:04Z","published":"2024-07-11T08:30:04Z","title":"AoA-Based Physical Layer Authentication in Analog Arrays under\n  Impersonation Attacks","summary":"  We discuss the use of angle of arrival (AoA) as an authentication measure in\nanalog array multiple-input multiple-output (MIMO) systems. A base station\nequipped with an analog array authenticates users based on the AoA estimated\nfrom certified pilot transmissions, while active attackers manipulate their\ntransmitted signals to mount impersonation attacks. We study several attacks of\nincreasing intensity (captured through the availability of side information at\nthe attackers) and assess the performance of AoA-based authentication using\none-class classifiers. Our results show that some attack techniques with\nknowledge of the combiners at the verifier are effective in falsifying the AoA\nand compromising the security of the considered type of physical layer\nauthentication.\n","authors":["Muralikrishnan Srinivasan","Linda Senigagliesi","Hui Chen","Arsenia Chorti","Marco Baldi","Henk Wymeersch"],"pdf_url":"https://arxiv.org/pdf/2407.08282v1.pdf","comment":"25th IEEE International Workshop on Signal Processing Advances in\n  Wireless Communications (SPAWC 2024)"},{"id":"http://arxiv.org/abs/2407.08274v1","updated":"2024-07-11T08:23:46Z","published":"2024-07-11T08:23:46Z","title":"Explainability of Sub-Field Level Crop Yield Prediction using Remote\n  Sensing","summary":"  Crop yield forecasting plays a significant role in addressing growing\nconcerns about food security and guiding decision-making for policymakers and\nfarmers. When deep learning is employed, understanding the learning and\ndecision-making processes of the models, as well as their interaction with the\ninput data, is crucial for establishing trust in the models and gaining insight\ninto their reliability. In this study, we focus on the task of crop yield\nprediction, specifically for soybean, wheat, and rapeseed crops in Argentina,\nUruguay, and Germany. Our goal is to develop and explain predictive models for\nthese crops, using a large dataset of satellite images, additional data\nmodalities, and crop yield maps. We employ a long short-term memory network and\ninvestigate the impact of using different temporal samplings of the satellite\ndata and the benefit of adding more relevant modalities. For model\nexplainability, we utilize feature attribution methods to quantify input\nfeature contributions, identify critical growth stages, analyze yield\nvariability at the field level, and explain less accurate predictions.\n  The modeling results show an improvement when adding more modalities or using\nall available instances of satellite data. The explainability results reveal\ndistinct feature importance patterns for each crop and region. We further found\nthat the most influential growth stages on the prediction are dependent on the\ntemporal sampling of the input data. We demonstrated how these critical growth\nstages, which hold significant agronomic value, closely align with the existing\nliterature in agronomy and crop development biology.\n","authors":["Hiba Najjar","Miro Miranda","Marlon Nuske","Ribana Roscher","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2407.08274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08271v1","updated":"2024-07-11T08:15:57Z","published":"2024-07-11T08:15:57Z","title":"Gaussian process interpolation with conformal prediction: methods and\n  comparative analysis","summary":"  This article advocates the use of conformal prediction (CP) methods for\nGaussian process (GP) interpolation to enhance the calibration of prediction\nintervals. We begin by illustrating that using a GP model with parameters\nselected by maximum likelihood often results in predictions that are not\noptimally calibrated. CP methods can adjust the prediction intervals, leading\nto better uncertainty quantification while maintaining the accuracy of the\nunderlying GP model. We compare different CP variants and introduce a novel\nvariant based on an asymmetric score. Our numerical experiments demonstrate the\neffectiveness of CP methods in improving calibration without compromising\naccuracy. This work aims to facilitate the adoption of CP methods in the GP\ncommunity.\n","authors":["AurÃ©lien Pion","Emmanuel Vazquez"],"pdf_url":"https://arxiv.org/pdf/2407.08271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08270v1","updated":"2024-07-11T08:12:46Z","published":"2024-07-11T08:12:46Z","title":"SciQu: Accelerating Materials Properties Prediction with Automated\n  Literature Mining for Self-Driving Laboratories","summary":"  Assessing different material properties to predict specific attributes, such\nas band gap, resistivity, young modulus, work function, and refractive index,\nis a fundamental requirement for materials science-based applications. However,\nthe process is time-consuming and often requires extensive literature reviews\nand numerous experiments. Our study addresses these challenges by leveraging\nmachine learning to analyze material properties with greater precision and\nefficiency. By automating the data extraction process and using the extracted\ninformation to train machine learning models, our developed model, SciQu,\noptimizes material properties. As a proof of concept, we predicted the\nrefractive index of materials using data extracted from numerous research\narticles with SciQu, considering input descriptors such as space group, volume,\nand bandgap with Root Mean Square Error (RMSE) 0.068 and R2 0.94. Thus, SciQu\nnot only predicts the properties of materials but also plays a key role in\nself-driving laboratories by optimizing the synthesis parameters to achieve\nprecise shape, size, and phase of the materials subjected to the input\nparameters.\n","authors":["Anand Babu"],"pdf_url":"https://arxiv.org/pdf/2407.08270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08257v1","updated":"2024-07-11T07:57:33Z","published":"2024-07-11T07:57:33Z","title":"Knowledge distillation to effectively attain both region-of-interest and\n  global semantics from an image where multiple objects appear","summary":"  Models based on convolutional neural networks (CNN) and transformers have\nsteadily been improved. They also have been applied in various computer vision\ndownstream tasks. However, in object detection tasks, accurately localizing and\nclassifying almost infinite categories of foods in images remains challenging.\nTo address these problems, we first segmented the food as the\nregion-of-interest (ROI) by using the segment-anything model (SAM) and masked\nthe rest of the region except ROI as black pixels. This process simplified the\nproblems into a single classification for which annotation and training were\nmuch simpler than object detection. The images in which only the ROI was\npreserved were fed as inputs to fine-tune various off-the-shelf models that\nencoded their own inductive biases. Among them, Data-efficient image\nTransformers (DeiTs) had the best classification performance. Nonetheless, when\nfoods' shapes and textures were similar, the contextual features of the\nROI-only images were not enough for accurate classification. Therefore, we\nintroduced a novel type of combined architecture, RveRNet, which consisted of\nROI, extra-ROI, and integration modules that allowed it to account for both the\nROI's and global contexts. The RveRNet's F1 score was 10% better than other\nindividual models when classifying ambiguous food images. If the RveRNet's\nmodules were DeiT with the knowledge distillation from the CNN, performed the\nbest. We investigated how architectures can be made robust against input noise\ncaused by permutation and translocation. The results indicated that there was a\ntrade-off between how much the CNN teacher's knowledge could be distilled to\nDeiT and DeiT's innate strength. Code is publicly available at:\nhttps://github.com/Seonwhee-Genome/RveRNet.\n","authors":["Seonwhee Jin"],"pdf_url":"https://arxiv.org/pdf/2407.08257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08256v1","updated":"2024-07-11T07:56:17Z","published":"2024-07-11T07:56:17Z","title":"Adaptive Compressed Sensing with Diffusion-Based Posterior Sampling","summary":"  Compressed Sensing (CS) facilitates rapid image acquisition by selecting a\nsmall subset of measurements sufficient for high-fidelity reconstruction.\nAdaptive CS seeks to further enhance this process by dynamically choosing\nfuture measurements based on information gleaned from data that is already\nacquired. However, many existing frameworks are often tailored to specific\ntasks and require intricate training procedures. We propose AdaSense, a novel\nAdaptive CS approach that leverages zero-shot posterior sampling with\npre-trained diffusion models. By sequentially sampling from the posterior\ndistribution, we can quantify the uncertainty of each possible future linear\nmeasurement throughout the acquisition process. AdaSense eliminates the need\nfor additional training and boasts seamless adaptation to diverse domains with\nminimal tuning requirements. Our experiments demonstrate the effectiveness of\nAdaSense in reconstructing facial images from a small number of measurements.\nFurthermore, we apply AdaSense for active acquisition of medical images in the\ndomains of magnetic resonance imaging (MRI) and computed tomography (CT),\nhighlighting its potential for tangible real-world acceleration.\n","authors":["Noam Elata","Tomer Michaeli","Michael Elad"],"pdf_url":"https://arxiv.org/pdf/2407.08256v1.pdf","comment":"Published in European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2407.08255v1","updated":"2024-07-11T07:56:08Z","published":"2024-07-11T07:56:08Z","title":"GraphMamba: An Efficient Graph Structure Learning Vision Mamba for\n  Hyperspectral Image Classification","summary":"  Efficient extraction of spectral sequences and geospatial information has\nalways been a hot topic in hyperspectral image classification. In terms of\nspectral sequence feature capture, RNN and Transformer have become mainstream\nclassification frameworks due to their long-range feature capture capabilities.\nIn terms of spatial information aggregation, CNN enhances the receptive field\nto retain integrated spatial information as much as possible. However, the\nspectral feature-capturing architectures exhibit low computational efficiency,\nand CNNs lack the flexibility to perceive spatial contextual information. To\naddress these issues, this paper proposes GraphMamba--an efficient graph\nstructure learning vision Mamba classification framework that fully considers\nHSI characteristics to achieve deep spatial-spectral information mining.\nSpecifically, we propose a novel hyperspectral visual GraphMamba processing\nparadigm (HVGM) that preserves spatial-spectral features by constructing\nspatial-spectral cubes and utilizes linear spectral encoding to enhance the\noperability of subsequent tasks. The core components of GraphMamba include the\nHyperMamba module for improving computational efficiency and the SpectralGCN\nmodule for adaptive spatial context awareness. The HyperMamba mitigates clutter\ninterference by employing the global mask (GM) and introduces a parallel\ntraining inference architecture to alleviate computational bottlenecks. The\nSpatialGCN incorporates weighted multi-hop aggregation (WMA) spatial encoding\nto focus on highly correlated spatial structural features, thus flexibly\naggregating contextual information while mitigating spatial noise interference.\nExtensive experiments were conducted on three different scales of real HSI\ndatasets, and compared with the state-of-the-art classification frameworks,\nGraphMamba achieved optimal performance.\n","authors":["Aitao Yang","Min Li","Yao Ding","Leyuan Fang","Yaoming Cai","Yujie He"],"pdf_url":"https://arxiv.org/pdf/2407.08255v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2405.20278v2","updated":"2024-07-11T07:55:14Z","published":"2024-05-30T17:32:46Z","title":"Length independent generalization bounds for deep SSM architectures","summary":"  Many state-of-the-art models trained on long-range sequences, for example S4,\nS5 or LRU, are made of sequential blocks combining State-Space Models (SSMs)\nwith neural networks. In this paper we provide a PAC bound that holds for these\nkind of architectures with stable SSM blocks and does not depend on the length\nof the input sequence. Imposing stability of the SSM blocks is a standard\npractice in the literature, and it is known to help performance. Our results\nprovide a theoretical justification for the use of stable SSM blocks as the\nproposed PAC bound decreases as the degree of stability of the SSM blocks\nincreases.\n","authors":["DÃ¡niel RÃ¡cz","MihÃ¡ly Petreczky","BÃ¡lint DarÃ³czy"],"pdf_url":"https://arxiv.org/pdf/2405.20278v2.pdf","comment":"20 pages, no figures, accepted at ICML 2024 Next Generation of\n  Sequence Modeling Architectures Workshop"},{"id":"http://arxiv.org/abs/2407.08250v1","updated":"2024-07-11T07:52:33Z","published":"2024-07-11T07:52:33Z","title":"Gradient Boosting Reinforcement Learning","summary":"  Neural networks (NN) achieve remarkable results in various tasks, but lack\nkey characteristics: interpretability, support for categorical features, and\nlightweight implementations suitable for edge devices. While ongoing efforts\naim to address these challenges, Gradient Boosting Trees (GBT) inherently meet\nthese requirements. As a result, GBTs have become the go-to method for\nsupervised learning tasks in many real-world applications and competitions.\nHowever, their application in online learning scenarios, notably in\nreinforcement learning (RL), has been limited. In this work, we bridge this gap\nby introducing Gradient-Boosting RL (GBRL), a framework that extends the\nadvantages of GBT to the RL domain. Using the GBRL framework, we implement\nvarious actor-critic algorithms and compare their performance with their NN\ncounterparts. Inspired by shared backbones in NN we introduce a tree-sharing\napproach for policy and value functions with distinct learning rates, enhancing\nlearning efficiency over millions of interactions. GBRL achieves competitive\nperformance across a diverse array of tasks, excelling in domains with\nstructured or categorical features. Additionally, we present a\nhigh-performance, GPU-accelerated implementation that integrates seamlessly\nwith widely-used RL libraries (available at https://github.com/NVlabs/gbrl).\nGBRL expands the toolkit for RL practitioners, demonstrating the viability and\npromise of GBT within the RL paradigm, particularly in domains characterized by\nstructured or categorical features.\n","authors":["Benjamin Fuhrer","Chen Tessler","Gal Dalal"],"pdf_url":"https://arxiv.org/pdf/2407.08250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08245v1","updated":"2024-07-11T07:45:10Z","published":"2024-07-11T07:45:10Z","title":"Feature Diversification and Adaptation for Federated Domain\n  Generalization","summary":"  Federated learning, a distributed learning paradigm, utilizes multiple\nclients to build a robust global model. In real-world applications, local\nclients often operate within their limited domains, leading to a `domain shift'\nacross clients. Privacy concerns limit each client's learning to its own domain\ndata, which increase the risk of overfitting. Moreover, the process of\naggregating models trained on own limited domain can be potentially lead to a\nsignificant degradation in the global model performance. To deal with these\nchallenges, we introduce the concept of federated feature diversification. Each\nclient diversifies the own limited domain data by leveraging global feature\nstatistics, i.e., the aggregated average statistics over all participating\nclients, shared through the global model's parameters. This data\ndiversification helps local models to learn client-invariant representations\nwhile preserving privacy. Our resultant global model shows robust performance\non unseen test domain data. To enhance performance further, we develop an\ninstance-adaptive inference approach tailored for test domain data. Our\nproposed instance feature adapter dynamically adjusts feature statistics to\nalign with the test input, thereby reducing the domain gap between the test and\ntraining domains. We show that our method achieves state-of-the-art performance\non several domain generalization benchmarks within a federated learning\nsetting.\n","authors":["Seunghan Yang","Seokeon Choi","Hyunsin Park","Sungha Choi","Simyung Chang","Sungrack Yun"],"pdf_url":"https://arxiv.org/pdf/2407.08245v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2406.18664v3","updated":"2024-07-11T07:45:04Z","published":"2024-06-26T18:09:46Z","title":"Evaluating Copyright Takedown Methods for Language Models","summary":"  Language models (LMs) derive their capabilities from extensive training on\ndiverse data, including potentially copyrighted material. These models can\nmemorize and generate content similar to their training data, posing potential\nconcerns. Therefore, model creators are motivated to develop mitigation methods\nthat prevent generating protected content. We term this procedure as copyright\ntakedowns for LMs, noting the conceptual similarity to (but legal distinction\nfrom) the DMCA takedown This paper introduces the first evaluation of the\nfeasibility and side effects of copyright takedowns for LMs. We propose\nCoTaEval, an evaluation framework to assess the effectiveness of copyright\ntakedown methods, the impact on the model's ability to retain uncopyrightable\nfactual knowledge from the training data whose recitation is embargoed, and how\nwell the model maintains its general utility and efficiency. We examine several\nstrategies, including adding system prompts, decoding-time filtering\ninterventions, and unlearning approaches. Our findings indicate that no tested\nmethod excels across all metrics, showing significant room for research in this\nunique problem setting and indicating potential unresolved challenges for live\npolicy proposals.\n","authors":["Boyi Wei","Weijia Shi","Yangsibo Huang","Noah A. Smith","Chiyuan Zhang","Luke Zettlemoyer","Kai Li","Peter Henderson"],"pdf_url":"https://arxiv.org/pdf/2406.18664v3.pdf","comment":"31 pages, 9 figures, 14 tables"},{"id":"http://arxiv.org/abs/2401.05363v4","updated":"2024-07-11T07:38:32Z","published":"2023-12-13T14:26:37Z","title":"Generalizable Sleep Staging via Multi-Level Domain Alignment","summary":"  Automatic sleep staging is essential for sleep assessment and disorder\ndiagnosis. Most existing methods depend on one specific dataset and are limited\nto be generalized to other unseen datasets, for which the training data and\ntesting data are from the same dataset. In this paper, we introduce domain\ngeneralization into automatic sleep staging and propose the task of\ngeneralizable sleep staging which aims to improve the model generalization\nability to unseen datasets. Inspired by existing domain generalization methods,\nwe adopt the feature alignment idea and propose a framework called SleepDG to\nsolve it. Considering both of local salient features and sequential features\nare important for sleep staging, we propose a Multi-level Feature Alignment\ncombining epoch-level and sequence-level feature alignment to learn\ndomain-invariant feature representations. Specifically, we design an\nEpoch-level Feature Alignment to align the feature distribution of each single\nsleep epoch among different domains, and a Sequence-level Feature Alignment to\nminimize the discrepancy of sequential features among different domains.\nSleepDG is validated on five public datasets, achieving the state-of-the-art\nperformance.\n","authors":["Jiquan Wang","Sha Zhao","Haiteng Jiang","Shijian Li","Tao Li","Gang Pan"],"pdf_url":"https://arxiv.org/pdf/2401.05363v4.pdf","comment":"Accepted by the Thirty-Eighth AAAI Conference on Artificial\n  Intelligence (AAAI-24)"},{"id":"http://arxiv.org/abs/2407.08239v1","updated":"2024-07-11T07:32:16Z","published":"2024-07-11T07:32:16Z","title":"An Unsupervised Domain Adaptation Method for Locating Manipulated Region\n  in partially fake Audio","summary":"  When the task of locating manipulation regions in partially-fake audio (PFA)\ninvolves cross-domain datasets, the performance of deep learning models drops\nsignificantly due to the shift between the source and target domains. To\naddress this issue, existing approaches often employ data augmentation before\ntraining. However, they overlook the characteristics in target domain that are\nabsent in source domain. Inspired by the mixture-of-experts model, we propose\nan unsupervised method named Samples mining with Diversity and Entropy (SDE).\nOur method first learns from a collection of diverse experts that achieve great\nperformance from different perspectives in the source domain, but with\nambiguity on target samples. We leverage these diverse experts to select the\nmost informative samples by calculating their entropy. Furthermore, we\nintroduced a label generation method tailored for these selected samples that\nare incorporated in the training process in source domain integrating the\ntarget domain information. We applied our method to a cross-domain partially\nfake audio detection dataset, ADD2023Track2. By introducing 10% of unknown\nsamples from the target domain, we achieved an F1 score of 43.84%, which\nrepresents a relative increase of 77.2% compared to the second-best method.\n","authors":["Siding Zeng","Jiangyan Yi","Jianhua Tao","Yujie Chen","Shan Liang","Yong Ren","Xiaohui Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.08239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08233v1","updated":"2024-07-11T07:14:40Z","published":"2024-07-11T07:14:40Z","title":"Differentially Private Neural Network Training under Hidden State\n  Assumption","summary":"  We present a novel approach called differentially private stochastic block\ncoordinate descent (DP-SBCD) for training neural networks with provable\nguarantees of differential privacy under the hidden state assumption. Our\nmethodology incorporates Lipschitz neural networks and decomposes the training\nprocess of the neural network into sub-problems, each corresponding to the\ntraining of a specific layer. By doing so, we extend the analysis of\ndifferential privacy under the hidden state assumption to encompass non-convex\nproblems and algorithms employing proximal gradient descent. Furthermore, in\ncontrast to existing methods, we adopt a novel approach by utilizing calibrated\nnoise sampled from adaptive distributions, yielding improved empirical\ntrade-offs between utility and privacy.\n","authors":["Ding Chen","Chen Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08232v1","updated":"2024-07-11T07:14:34Z","published":"2024-07-11T07:14:34Z","title":"SwishReLU: A Unified Approach to Activation Functions for Enhanced Deep\n  Neural Networks Performance","summary":"  ReLU, a commonly used activation function in deep neural networks, is prone\nto the issue of \"Dying ReLU\". Several enhanced versions, such as ELU, SeLU, and\nSwish, have been introduced and are considered to be less commonly utilized.\nHowever, replacing ReLU can be somewhat challenging due to its inconsistent\nadvantages. While Swish offers a smoother transition similar to ReLU, its\nutilization generally incurs a greater computational burden compared to ReLU.\nThis paper proposes SwishReLU, a novel activation function combining elements\nof ReLU and Swish. Our findings reveal that SwishReLU outperforms ReLU in\nperformance with a lower computational cost than Swish. This paper undertakes\nan examination and comparison of different types of ReLU variants with\nSwishReLU. Specifically, we compare ELU and SeLU along with Tanh on three\ndatasets: CIFAR-10, CIFAR-100 and MNIST. Notably, applying SwishReLU in the\nVGG16 model described in Algorithm 2 yields a 6% accuracy improvement on the\nCIFAR-10 dataset.\n","authors":["Jamshaid Ul Rahman","Rubiqa Zulfiqar","Asad Khan"," Nimra"],"pdf_url":"https://arxiv.org/pdf/2407.08232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09858v2","updated":"2024-07-11T07:09:00Z","published":"2024-05-16T07:25:15Z","title":"Towards Realistic Incremental Scenario in Class Incremental Semantic\n  Segmentation","summary":"  This paper addresses the unrealistic aspect of the commonly adopted\nContinuous Incremental Semantic Segmentation (CISS) scenario, termed\noverlapped. We point out that overlapped allows the same image to reappear in\nfuture tasks with different pixel labels, which is far from practical\nincremental learning scenarios. Moreover, we identified that this flawed\nscenario may lead to biased results for two commonly used techniques in CISS,\npseudo-labeling and exemplar memory, resulting in unintended advantages or\ndisadvantages for certain techniques. To mitigate this, a practical scenario\ncalled partitioned is proposed, in which the dataset is first divided into\ndistinct subsets representing each class, and then the subsets are assigned to\neach corresponding task. This efficiently addresses the issue above while\nmeeting the requirement of CISS scenario, such as capturing the background\nshifts. Furthermore, we identify and address the code implementation issues\nrelated to retrieving data from the exemplar memory, which was ignored in\nprevious works. Lastly, we introduce a simple yet competitive memory-based\nbaseline, MiB-AugM, that handles background shifts of current tasks in the\nexemplar memory. This baseline achieves state-of-the-art results across\nmultiple tasks involving learning numerous new classes.\n","authors":["Jihwan Kwak","Sungmin Cha","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2405.09858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08227v1","updated":"2024-07-11T07:01:50Z","published":"2024-07-11T07:01:50Z","title":"DALL-M: Context-Aware Clinical Data Augmentation with LLMs","summary":"  X-ray images are vital in medical diagnostics, but their effectiveness is\nlimited without clinical context. Radiologists often find chest X-rays\ninsufficient for diagnosing underlying diseases, necessitating comprehensive\nclinical features and data integration. We present a novel technique to enhance\nthe clinical context through augmentation techniques with clinical tabular\ndata, thereby improving its applicability and reliability in AI medical\ndiagnostics. To address this, we introduce a pioneering approach to clinical\ndata augmentation that employs large language models (LLMs) to generate patient\ncontextual synthetic data. This methodology is crucial for training more robust\ndeep learning models in healthcare. It preserves the integrity of real patient\ndata while enriching the dataset with contextually relevant synthetic features,\nsignificantly enhancing model performance. DALL-M uses a three-phase feature\ngeneration process: (i) clinical context storage, (ii) expert query generation,\nand (iii) context-aware feature augmentation. DALL-M generates new, clinically\nrelevant features by synthesizing chest X-ray images and reports. Applied to\n799 cases using nine features from the MIMIC-IV dataset, it created an\naugmented set of 91 features. This is the first work to generate contextual\nvalues for existing and new features based on patients' X-ray reports, gender,\nand age and to produce new contextual knowledge during data augmentation.\nEmpirical validation with machine learning models, including Decision Trees,\nRandom Forests, XGBoost, and TabNET, showed significant performance\nimprovements. Incorporating augmented features increased the F1 score by 16.5%\nand Precision and Recall by approximately 25%. DALL-M addresses a critical gap\nin clinical data augmentation, offering a robust framework for generating\ncontextually enriched datasets.\n","authors":["Chihcheng Hsieh","Catarina Moreira","Isabel Blanco Nobre","Sandra Costa Sousa","Chun Ouyang","Margot Brereton","Joaquim Jorge","Jacinto C. Nascimento"],"pdf_url":"https://arxiv.org/pdf/2407.08227v1.pdf","comment":"we introduce a pioneering approach to clinical data augmentation that\n  employs large language models (LLMs) to generate patient contextual synthetic\n  data. It preserves the integrity of real patient data while enriching the\n  dataset with contextually relevant synthetic features, significantly\n  enhancing model performance"},{"id":"http://arxiv.org/abs/2407.08215v1","updated":"2024-07-11T06:33:11Z","published":"2024-07-11T06:33:11Z","title":"Enhancing Performance and User Engagement in Everyday Stress Monitoring:\n  A Context-Aware Active Reinforcement Learning Approach","summary":"  In today's fast-paced world, accurately monitoring stress levels is crucial.\nSensor-based stress monitoring systems often need large datasets for training\neffective models. However, individual-specific models are necessary for\npersonalized and interactive scenarios. Traditional methods like Ecological\nMomentary Assessments (EMAs) assess stress but struggle with efficient data\ncollection without burdening users. The challenge is to timely send EMAs,\nespecially during stress, balancing monitoring efficiency and user convenience.\nThis paper introduces a novel context-aware active reinforcement learning (RL)\nalgorithm for enhanced stress detection using Photoplethysmography (PPG) data\nfrom smartwatches and contextual data from smartphones. Our approach\ndynamically selects optimal times for deploying EMAs, utilizing the user's\nimmediate context to maximize label accuracy and minimize intrusiveness.\nInitially, the study was executed in an offline environment to refine the label\ncollection process, aiming to increase accuracy while reducing user burden.\nLater, we integrated a real-time label collection mechanism, transitioning to\nan online methodology. This shift resulted in an 11% improvement in stress\ndetection efficiency. Incorporating contextual data improved model accuracy by\n4%. Personalization studies indicated a 10% enhancement in AUC-ROC scores,\ndemonstrating better stress level differentiation. This research marks a\nsignificant move towards personalized, context-driven real-time stress\nmonitoring methods.\n","authors":["Seyed Amir Hossein Aqajari","Ziyu Wang","Ali Tazarv","Sina Labbaf","Salar Jafarlou","Brenda Nguyen","Nikil Dutt","Marco Levorato","Amir M. Rahmani"],"pdf_url":"https://arxiv.org/pdf/2407.08215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08214v1","updated":"2024-07-11T06:31:04Z","published":"2024-07-11T06:31:04Z","title":"Towards stable training of parallel continual learning","summary":"  Parallel Continual Learning (PCL) tasks investigate the training methods for\ncontinual learning with multi-source input, where data from different tasks are\nlearned as they arrive. PCL offers high training efficiency and is well-suited\nfor complex multi-source data systems, such as autonomous vehicles equipped\nwith multiple sensors. However, at any time, multiple tasks need to be trained\nsimultaneously, leading to severe training instability in PCL. This instability\nmanifests during both forward and backward propagation, where features are\nentangled and gradients are conflict. This paper introduces Stable Parallel\nContinual Learning (SPCL), a novel approach that enhances the training\nstability of PCL for both forward and backward propagation. For the forward\npropagation, we apply Doubly-block Toeplit (DBT) Matrix based orthogonality\nconstraints to network parameters to ensure stable and consistent propagation.\nFor the backward propagation, we employ orthogonal decomposition for gradient\nmanagement stabilizes backpropagation and mitigates gradient conflicts across\ntasks. By optimizing gradients by ensuring orthogonality and minimizing the\ncondition number, SPCL effectively stabilizing the gradient descent in complex\noptimization tasks. Experimental results demonstrate that SPCL outperforms\nstate-of-the-art methjods and achieve better training stability.\n","authors":["Li Yuepan","Fan Lyu","Yuyang Li","Wei Feng","Guangcan Liu","Fanhua Shang"],"pdf_url":"https://arxiv.org/pdf/2407.08214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.15408v5","updated":"2024-07-11T06:18:12Z","published":"2020-12-31T02:42:27Z","title":"Gated Ensemble of Spatio-temporal Mixture of Experts for Multi-task\n  Learning in Ride-hailing System","summary":"  Ride-hailing system requires efficient management of dynamic demand and\nsupply to ensure optimal service delivery, pricing strategies, and operational\nefficiency. Designing spatio-temporal forecasting models separately in a\ntask-wise and city-wise manner to forecast demand and supply-demand gap in a\nride-hailing system poses a burden for the expanding transportation network\ncompanies. Therefore, a multi-task learning architecture is proposed in this\nstudy by developing gated ensemble of spatio-temporal mixture of experts\nnetwork (GESME-Net) with convolutional recurrent neural network (CRNN),\nconvolutional neural network (CNN), and recurrent neural network (RNN) for\nsimultaneously forecasting these spatio-temporal tasks in a city as well as\nacross different cities. Furthermore, a task adaptation layer is integrated\nwith the architecture for learning joint representation in multi-task learning\nand revealing the contribution of the input features utilized in prediction.\nThe proposed architecture is tested with data from Didi Chuxing for: (i)\nsimultaneously forecasting demand and supply-demand gap in Beijing, and (ii)\nsimultaneously forecasting demand across Chengdu and Xian. In both scenarios,\nmodels from our proposed architecture outperformed the single-task and\nmulti-task deep learning benchmarks and ensemble-based machine learning\nalgorithms.\n","authors":["M. H. Rahman","S. M. Rifaat","S. N. Sadeek","M. Abrar","D. Wang"],"pdf_url":"https://arxiv.org/pdf/2012.15408v5.pdf","comment":"arXiv admin note: text overlap with arXiv:2012.08868"},{"id":"http://arxiv.org/abs/2407.08205v1","updated":"2024-07-11T06:12:04Z","published":"2024-07-11T06:12:04Z","title":"OPIMA: Optical Processing-In-Memory for Convolutional Neural Network\n  Acceleration","summary":"  Recent advances in machine learning (ML) have spotlighted the pressing need\nfor computing architectures that bridge the gap between memory bandwidth and\nprocessing power. The advent of deep neural networks has pushed traditional Von\nNeumann architectures to their limits due to the high latency and energy\nconsumption costs associated with data movement between the processor and\nmemory for these workloads. One of the solutions to overcome this bottleneck is\nto perform computation within the main memory through processing-in-memory\n(PIM), thereby limiting data movement and the costs associated with it.\nHowever, DRAM-based PIM struggles to achieve high throughput and energy\nefficiency due to internal data movement bottlenecks and the need for frequent\nrefresh operations. In this work, we introduce OPIMA, a PIM-based ML\naccelerator, architected within an optical main memory. OPIMA has been designed\nto leverage the inherent massive parallelism within main memory while\nperforming high-speed, low-energy optical computation to accelerate ML models\nbased on convolutional neural networks. We present a comprehensive analysis of\nOPIMA to guide design choices and operational mechanisms. Additionally, we\nevaluate the performance and energy consumption of OPIMA, comparing it with\nconventional electronic computing systems and emerging photonic PIM\narchitectures. The experimental results show that OPIMA can achieve 2.98x\nhigher throughput and 137x better energy efficiency than the best-known prior\nwork.\n","authors":["Febin Sunny","Amin Shafiee","Abhishek Balasubramaniam","Mahdi Nikdast","Sudeep Pasricha"],"pdf_url":"https://arxiv.org/pdf/2407.08205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01325v3","updated":"2024-07-11T06:11:46Z","published":"2024-01-02T18:30:51Z","title":"LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning","summary":"  It is well known that LLMs cannot generalize well to long contexts whose\nlengths are larger than the training sequence length. This poses challenges\nwhen employing LLMs for processing long input sequences during inference. In\nthis work, we argue that LLMs themselves have inherent capabilities to handle\nlong contexts without fine-tuning. To achieve this goal, we propose SelfExtend\nto extend the context window of LLMs by constructing bi-level attention\ninformation: the grouped attention and the neighbor attention. The grouped\nattention captures the dependencies among tokens that are far apart, while\nneighbor attention captures dependencies among adjacent tokens within a\nspecified range. The two-level attentions are computed based on the original\nmodel's self-attention mechanism during inference. With minor code\nmodification, our SelfExtend can effortlessly extend existing LLMs' context\nwindow without any fine-tuning. We conduct comprehensive experiments on\nmultiple benchmarks and the results show that our SelfExtend can effectively\nextend existing LLMs' context window length. The code can be found at\n\\url{https://github.com/datamllab/LongLM}.\n","authors":["Hongye Jin","Xiaotian Han","Jingfeng Yang","Zhimeng Jiang","Zirui Liu","Chia-Yuan Chang","Huiyuan Chen","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2401.01325v3.pdf","comment":"ICML2024 Spotlight"},{"id":"http://arxiv.org/abs/2401.07039v3","updated":"2024-07-11T05:46:04Z","published":"2024-01-13T10:56:34Z","title":"Quantum Generative Diffusion Model: A Fully Quantum-Mechanical Model for\n  Generating Quantum State Ensemble","summary":"  Classical diffusion models have shown superior generative results. Exploring\nthem in the quantum domain can advance the field of quantum generative\nlearning. This work introduces Quantum Generative Diffusion Model (QGDM) as\ntheir simple and elegant quantum counterpart. Through a non-unitary forward\nprocess, any target quantum state can be transformed into a completely mixed\nstate that has the highest entropy and maximum uncertainty about the system. A\ntrainable backward process is used to recover the former from the latter. The\ndesign requirements for its backward process includes non-unitarity and small\nparameter count. We introduce partial trace operations to enforce non-unitary\nand reduce the number of trainable parameters by using a parameter-sharing\nstrategy and incorporating temporal information as an input in the backward\nprocess. We present QGDM's resource-efficient version to reduce auxiliary\nqubits while preserving generative capabilities. QGDM exhibits faster\nconvergence than Quantum Generative Adversarial Network (QGAN) because its\nadopted convex-based optimization can result in faster convergence. The results\nof comparing it with QGAN demonstrate its effectiveness in generating both pure\nand mixed quantum states. It can achieve 53.02% higher fidelity in mixed-state\ngeneration than QGAN. The results highlight its great potential to tackle\nchallenging quantum generation tasks.\n","authors":["Chuangtao Chen","Qinglin Zhao","MengChu Zhou","Zhimin He","Zhili Sun","Haozhen Situ"],"pdf_url":"https://arxiv.org/pdf/2401.07039v3.pdf","comment":"Comments are welcome"},{"id":"http://arxiv.org/abs/2407.02419v2","updated":"2024-07-11T05:42:23Z","published":"2024-07-02T16:44:14Z","title":"Quantum Curriculum Learning","summary":"  Quantum machine learning (QML) requires significant quantum resources to\nachieve quantum advantage. Research should prioritize both the efficient design\nof quantum architectures and the development of learning strategies to optimize\nresource usage. We propose a framework called quantum curriculum learning\n(Q-CurL) for quantum data, where the curriculum introduces simpler tasks or\ndata to the learning model before progressing to more challenging ones. We\ndefine the curriculum criteria based on the data density ratio between tasks to\ndetermine the curriculum order. We also implement a dynamic learning schedule\nto emphasize the significance of quantum data in optimizing the loss function.\nEmpirical evidence shows that Q-CurL significantly enhances the training\nconvergence and the generalization for unitary learning tasks and improves the\nrobustness of quantum phase recognition tasks. Our framework provides a general\nlearning strategy, bringing QML closer to realizing practical advantages.\n","authors":["Quoc Hoan Tran","Yasuhiro Endo","Hirotaka Oshima"],"pdf_url":"https://arxiv.org/pdf/2407.02419v2.pdf","comment":"main 6 pages, supplementary materials 8 pages (update the\n  supplementary materials)"},{"id":"http://arxiv.org/abs/2403.14067v2","updated":"2024-07-11T05:22:42Z","published":"2024-03-21T01:30:24Z","title":"Automatic Outlier Rectification via Optimal Transport","summary":"  In this paper, we propose a novel conceptual framework to detect outliers\nusing optimal transport with a concave cost function. Conventional outlier\ndetection approaches typically use a two-stage procedure: first, outliers are\ndetected and removed, and then estimation is performed on the cleaned data.\nHowever, this approach does not inform outlier removal with the estimation\ntask, leaving room for improvement. To address this limitation, we propose an\nautomatic outlier rectification mechanism that integrates rectification and\nestimation within a joint optimization framework. We take the first step to\nutilize the optimal transport distance with a concave cost function to\nconstruct a rectification set in the space of probability distributions. Then,\nwe select the best distribution within the rectification set to perform the\nestimation task. Notably, the concave cost function we introduced in this paper\nis the key to making our estimator effectively identify the outlier during the\noptimization process. We demonstrate the effectiveness of our approach over\nconventional approaches in simulations and empirical analyses for mean\nestimation, least absolute regression, and the fitting of option implied\nvolatility surfaces.\n","authors":["Jose Blanchet","Jiajin Li","Markus Pelger","Greg Zanotti"],"pdf_url":"https://arxiv.org/pdf/2403.14067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08192v1","updated":"2024-07-11T05:22:04Z","published":"2024-07-11T05:22:04Z","title":"ARCO:Adaptive Multi-Agent Reinforcement Learning-Based Hardware/Software\n  Co-Optimization Compiler for Improved Performance in DNN Accelerator Design","summary":"  This paper presents ARCO, an adaptive Multi-Agent Reinforcement Learning\n(MARL)-based co-optimizing compilation framework designed to enhance the\nefficiency of mapping machine learning (ML) models - such as Deep Neural\nNetworks (DNNs) - onto diverse hardware platforms. The framework incorporates\nthree specialized actor-critic agents within MARL, each dedicated to a distinct\naspect of compilation/optimization at an abstract level: one agent focuses on\nhardware, while two agents focus on software optimizations. This integration\nresults in a collaborative hardware/software co-optimization strategy that\nimproves the precision and speed of DNN deployments. Concentrating on\nhigh-confidence configurations simplifies the search space and delivers\nsuperior performance compared to current optimization methods. The ARCO\nframework surpasses existing leading frameworks, achieving a throughput\nincrease of up to 37.95% while reducing the optimization time by up to 42.2%\nacross various DNNs.\n","authors":["Arya Fayyazi","Mehdi Kamal","Massoud Pedram"],"pdf_url":"https://arxiv.org/pdf/2407.08192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08188v1","updated":"2024-07-11T05:13:27Z","published":"2024-07-11T05:13:27Z","title":"Position: Measure Dataset Diversity, Don't Just Claim It","summary":"  Machine learning (ML) datasets, often perceived as neutral, inherently\nencapsulate abstract and disputed social constructs. Dataset curators\nfrequently employ value-laden terms such as diversity, bias, and quality to\ncharacterize datasets. Despite their prevalence, these terms lack clear\ndefinitions and validation. Our research explores the implications of this\nissue by analyzing \"diversity\" across 135 image and text datasets. Drawing from\nsocial sciences, we apply principles from measurement theory to identify\nconsiderations and offer recommendations for conceptualizing, operationalizing,\nand evaluating diversity in datasets. Our findings have broader implications\nfor ML research, advocating for a more nuanced and precise approach to handling\nvalue-laden properties in dataset construction.\n","authors":["Dora Zhao","Jerone T. A. Andrews","Orestis Papakyriakopoulos","Alice Xiang"],"pdf_url":"https://arxiv.org/pdf/2407.08188v1.pdf","comment":"ICML 2024 (Position Paper Track)"},{"id":"http://arxiv.org/abs/2407.08179v1","updated":"2024-07-11T04:50:51Z","published":"2024-07-11T04:50:51Z","title":"CoGS: Causality Constrained Counterfactual Explanations using\n  goal-directed ASP","summary":"  Machine learning models are increasingly used in areas such as loan approvals\nand hiring, yet they often function as black boxes, obscuring their\ndecision-making processes. Transparency is crucial, and individuals need\nexplanations to understand decisions, especially for the ones not desired by\nthe user. Ethical and legal considerations require informing individuals of\nchanges in input attribute values (features) that could lead to a desired\noutcome for the user. Our work aims to generate counterfactual explanations by\nconsidering causal dependencies between features. We present the CoGS\n(Counterfactual Generation with s(CASP)) framework that utilizes the\ngoal-directed Answer Set Programming system s(CASP) to generate counterfactuals\nfrom rule-based machine learning models, specifically the FOLD-SE algorithm.\nCoGS computes realistic and causally consistent changes to attribute values\ntaking causal dependencies between them into account. It finds a path from an\nundesired outcome to a desired one using counterfactuals. We present details of\nthe CoGS framework along with its evaluation.\n","authors":["Sopam Dasgupta","JoaquÃ­n Arias","Elmer Salazar","Gopal Gupta"],"pdf_url":"https://arxiv.org/pdf/2407.08179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12550v4","updated":"2024-07-11T04:45:41Z","published":"2023-11-21T11:59:16Z","title":"Explainable Time Series Anomaly Detection using Masked Latent Generative\n  Modeling","summary":"  We present a novel time series anomaly detection method that achieves\nexcellent detection accuracy while offering a superior level of explainability.\nOur proposed method, TimeVQVAE-AD, leverages masked generative modeling adapted\nfrom the cutting-edge time series generation method known as TimeVQVAE. The\nprior model is trained on the discrete latent space of a time-frequency domain.\nNotably, the dimensional semantics of the time-frequency domain are preserved\nin the latent space, enabling us to compute anomaly scores across different\nfrequency bands, which provides a better insight into the detected anomalies.\nAdditionally, the generative nature of the prior model allows for sampling\nlikely normal states for detected anomalies, enhancing the explainability of\nthe detected anomalies through counterfactuals. Our experimental evaluation on\nthe UCR Time Series Anomaly archive demonstrates that TimeVQVAE-AD\nsignificantly surpasses the existing methods in terms of detection accuracy and\nexplainability. We provide our implementation on GitHub:\n\\url{https://github.com/ML4ITS/TimeVQVAE-AnomalyDetection}.\n","authors":["Daesoo Lee","Sara Malacarne","Erlend Aune"],"pdf_url":"https://arxiv.org/pdf/2311.12550v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08176v1","updated":"2024-07-11T04:40:02Z","published":"2024-07-11T04:40:02Z","title":"Foundation Model Engineering: Engineering Foundation Models Just as\n  Engineering Software","summary":"  By treating data and models as the source code, Foundation Models (FMs)\nbecome a new type of software. Mirroring the concept of software crisis, the\nincreasing complexity of FMs making FM crisis a tangible concern in the coming\ndecade, appealing for new theories and methodologies from the field of software\nengineering. In this paper, we outline our vision of introducing Foundation\nModel (FM) engineering, a strategic response to the anticipated FM crisis with\nprincipled engineering methodologies. FM engineering aims to mitigate potential\nissues in FM development and application through the introduction of\ndeclarative, automated, and unified programming interfaces for both data and\nmodel management, reducing the complexities involved in working with FMs by\nproviding a more structured and intuitive process for developers. Through the\nestablishment of FM engineering, we aim to provide a robust, automated, and\nextensible framework that addresses the imminent challenges, and discovering\nnew research opportunities for the software engineering field.\n","authors":["Dezhi Ran","Mengzhou Wu","Wei Yang","Tao Xie"],"pdf_url":"https://arxiv.org/pdf/2407.08176v1.pdf","comment":"Accepted by 2030 Software Engineering Workshop, co-located with\n  FSE24; Invited to ACM TOSEM 2030 Roadmap for Software Engineering"},{"id":"http://arxiv.org/abs/2407.08169v1","updated":"2024-07-11T04:19:28Z","published":"2024-07-11T04:19:28Z","title":"Faster Machine Unlearning via Natural Gradient Descent","summary":"  We address the challenge of efficiently and reliably deleting data from\nmachine learning models trained using Empirical Risk Minimization (ERM), a\nprocess known as machine unlearning. To avoid retraining models from scratch,\nwe propose a novel algorithm leveraging Natural Gradient Descent (NGD). Our\ntheoretical framework ensures strong privacy guarantees for convex models,\nwhile a practical Min/Max optimization algorithm is developed for non-convex\nmodels. Comprehensive evaluations show significant improvements in privacy,\ncomputational efficiency, and generalization compared to state-of-the-art\nmethods, advancing both the theoretical and practical aspects of machine\nunlearning.\n","authors":["Omri Lev","Ashia Wilson"],"pdf_url":"https://arxiv.org/pdf/2407.08169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08166v1","updated":"2024-07-11T04:11:52Z","published":"2024-07-11T04:11:52Z","title":"Synthetic Electroretinogram Signal Generation Using Conditional\n  Generative Adversarial Network for Enhancing Classification of Autism\n  Spectrum Disorder","summary":"  The electroretinogram (ERG) is a clinical test that records the retina's\nelectrical response to light. The ERG is a promising way to study different\nneurodevelopmental and neurodegenerative disorders, including autism spectrum\ndisorder (ASD) - a neurodevelopmental condition that impacts language,\ncommunication, and reciprocal social interactions. However, in heterogeneous\npopulations, such as ASD, where the ability to collect large datasets is\nlimited, the application of artificial intelligence (AI) is complicated.\nSynthetic ERG signals generated from real ERG recordings carry similar\ninformation as natural ERGs and, therefore, could be used as an extension for\nnatural data to increase datasets so that AI applications can be fully\nutilized. As proof of principle, this study presents a Generative Adversarial\nNetwork capable of generating synthetic ERG signals of children with ASD and\ntypically developing control individuals. We applied a Time Series Transformer\nand Visual Transformer with Continuous Wavelet Transform to enhance\nclassification results on the extended synthetic signals dataset. This approach\nmay support classification models in related psychiatric conditions where the\nERG may help classify disorders.\n","authors":["Mikhail Kulyabin","Paul A. Constable","Aleksei Zhdanov","Irene O. Lee","David H. Skuse","Dorothy A. Thompson","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2407.08166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.09326v2","updated":"2024-07-11T03:45:01Z","published":"2021-10-18T13:50:43Z","title":"Graph convolutional network for predicting abnormal grain growth in\n  Monte Carlo simulations of microstructural evolution","summary":"  Recent developments in graph neural networks show promise for predicting the\noccurrence of abnormal grain growth, which has been a particularly challenging\narea of research due to its apparent stochastic nature. In this study, we\ngenerate a large dataset of Monte Carlo simulations of abnormal grain growth.\nWe train simple graph convolution networks to predict which initial\nmicrostructures will exhibit abnormal grain growth, and compare the results to\na standard computer vision approach for the same task. The graph neural network\noutperformed the computer vision method and achieved 73% prediction accuracy\nand fewer false positives. It also provided some physical insight into feature\nimportance and the relevant length scale required to maximize predictive\nperformance. Analysis of the uncertainty in the Monte Carlo simulations\nprovides additional insights for ongoing work in this area.\n","authors":["Ryan Cohn","Elizabeth Holm"],"pdf_url":"https://arxiv.org/pdf/2110.09326v2.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2310.05797v4","updated":"2024-07-11T03:42:12Z","published":"2023-10-09T15:31:03Z","title":"In-Context Explainers: Harnessing LLMs for Explaining Black Box Models","summary":"  Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional capabilities in complex tasks like machine translation, commonsense\nreasoning, and language understanding. One of the primary reasons for the\nadaptability of LLMs in such diverse tasks is their in-context learning (ICL)\ncapability, which allows them to perform well on new tasks by simply using a\nfew task samples in the prompt. Despite their effectiveness in enhancing the\nperformance of LLMs on diverse language and tabular tasks, these methods have\nnot been thoroughly explored for their potential to generate post hoc\nexplanations. In this work, we carry out one of the first explorations to\nanalyze the effectiveness of LLMs in explaining other complex predictive models\nusing ICL. To this end, we propose a novel framework, In-Context Explainers,\ncomprising of three novel approaches that exploit the ICL capabilities of LLMs\nto explain the predictions made by other predictive models. We conduct\nextensive analysis with these approaches on real-world tabular and text\ndatasets and demonstrate that LLMs are capable of explaining other predictive\nmodels similar to state-of-the-art post hoc explainers, opening up promising\navenues for future research into LLM-based post hoc explanations of complex\npredictive models.\n","authors":["Nicholas Kroeger","Dan Ley","Satyapriya Krishna","Chirag Agarwal","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2310.05797v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16075v4","updated":"2024-07-11T03:41:42Z","published":"2024-02-25T12:19:21Z","title":"Don't Start from Scratch: Behavioral Refinement via Interpolant-based\n  Policy Diffusion","summary":"  Imitation learning empowers artificial agents to mimic behavior by learning\nfrom demonstrations. Recently, diffusion models, which have the ability to\nmodel high-dimensional and multimodal distributions, have shown impressive\nperformance on imitation learning tasks. These models learn to shape a policy\nby diffusing actions (or states) from standard Gaussian noise. However, the\ntarget policy to be learned is often significantly different from Gaussian and\nthis mismatch can result in poor performance when using a small number of\ndiffusion steps (to improve inference speed) and under limited data. The key\nidea in this work is that initiating from a more informative source than\nGaussian enables diffusion methods to mitigate the above limitations. We\ncontribute both theoretical results, a new method, and empirical findings that\nshow the benefits of using an informative source policy. Our method, which we\ncall BRIDGER, leverages the stochastic interpolants framework to bridge\narbitrary policies, thus enabling a flexible approach towards imitation\nlearning. It generalizes prior work in that standard Gaussians can still be\napplied, but other source policies can be used if available. In experiments on\nchallenging simulation benchmarks and on real robots, BRIDGER outperforms\nstate-of-the-art diffusion policies. We provide further analysis on design\nconsiderations when applying BRIDGER. Code for BRIDGER is available at\nhttps://github.com/clear-nus/bridger.\n","authors":["Kaiqi Chen","Eugene Lim","Kelvin Lin","Yiyang Chen","Harold Soh"],"pdf_url":"https://arxiv.org/pdf/2402.16075v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08159v1","updated":"2024-07-11T03:25:40Z","published":"2024-07-11T03:25:40Z","title":"Model-agnostic clean-label backdoor mitigation in cybersecurity\n  environments","summary":"  The training phase of machine learning models is a delicate step, especially\nin cybersecurity contexts. Recent research has surfaced a series of insidious\ntraining-time attacks that inject backdoors in models designed for security\nclassification tasks without altering the training labels. With this work, we\npropose new techniques that leverage insights in cybersecurity threat models to\neffectively mitigate these clean-label poisoning attacks, while preserving the\nmodel utility. By performing density-based clustering on a carefully chosen\nfeature subspace, and progressively isolating the suspicious clusters through a\nnovel iterative scoring procedure, our defensive mechanism can mitigate the\nattacks without requiring many of the common assumptions in the existing\nbackdoor defense literature. To show the generality of our proposed mitigation,\nwe evaluate it on two clean-label model-agnostic attacks on two different\nclassic cybersecurity data modalities: network flows classification and malware\nclassification, using gradient boosting and neural network models.\n","authors":["Giorgio Severi","Simona Boboila","John Holodnak","Kendra Kratkiewicz","Rauf Izmailov","Alina Oprea"],"pdf_url":"https://arxiv.org/pdf/2407.08159v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.08152v1","updated":"2024-07-11T03:10:27Z","published":"2024-07-11T03:10:27Z","title":"Privacy-Preserving Data Deduplication for Enhancing Federated Learning\n  of Language Models","summary":"  Deduplication is a vital preprocessing step that enhances machine learning\nmodel performance and saves training time and energy. However, enhancing\nfederated learning through deduplication poses challenges, especially regarding\nscalability and potential privacy violations if deduplication involves sharing\nall clients' data. In this paper, we address the problem of deduplication in a\nfederated setup by introducing a pioneering protocol, Efficient\nPrivacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes\nduplicates from multiple clients' datasets without compromising data privacy.\nEP-MPD is constructed in a modular fashion, utilizing two novel variants of the\nPrivate Set Intersection protocol. Our extensive experiments demonstrate the\nsignificant benefits of deduplication in federated learning of large language\nmodels. For instance, we observe up to 19.61% improvement in perplexity and up\nto 27.95% reduction in running time. EP-MPD effectively balances privacy and\nperformance in federated learning, making it a valuable solution for\nlarge-scale applications.\n","authors":["Aydin Abadi","Vishnu Asutosh Dasu","Sumanta Sarkar"],"pdf_url":"https://arxiv.org/pdf/2407.08152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08134v1","updated":"2024-07-11T02:15:21Z","published":"2024-07-11T02:15:21Z","title":"Highway Networks for Improved Surface Reconstruction: The Role of\n  Residuals and Weight Updates","summary":"  Surface reconstruction from point clouds is a fundamental challenge in\ncomputer graphics and medical imaging. In this paper, we explore the\napplication of advanced neural network architectures for the accurate and\nefficient reconstruction of surfaces from data points. We introduce a novel\nvariant of the Highway network (Hw) called Square-Highway (SqrHw) within the\ncontext of multilayer perceptrons and investigate its performance alongside\nplain neural networks and a simplified Hw in various numerical examples. These\nexamples include the reconstruction of simple and complex surfaces, such as\nspheres, human hands, and intricate models like the Stanford Bunny. We analyze\nthe impact of factors such as the number of hidden layers, interior and\nexterior points, and data distribution on surface reconstruction quality. Our\nresults show that the proposed SqrHw architecture outperforms other neural\nnetwork configurations, achieving faster convergence and higher-quality surface\nreconstructions. Additionally, we demonstrate the SqrHw's ability to predict\nsurfaces over missing data, a valuable feature for challenging applications\nlike medical imaging. Furthermore, our study delves into further details,\ndemonstrating that the proposed method based on highway networks yields more\nstable weight norms and backpropagation gradients compared to the Plain Network\narchitecture. This research not only advances the field of computer graphics\nbut also holds utility for other purposes such as function interpolation and\nphysics-informed neural networks, which integrate multilayer perceptrons into\ntheir algorithms.\n","authors":["A. Noorizadegan","Y. C. Hon","D. L. Young","C. S. Chen"],"pdf_url":"https://arxiv.org/pdf/2407.08134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13803v3","updated":"2024-07-11T02:11:49Z","published":"2023-01-31T17:44:59Z","title":"Fairness-aware Vision Transformer via Debiased Self-Attention","summary":"  Vision Transformer (ViT) has recently gained significant attention in solving\ncomputer vision (CV) problems due to its capability of extracting informative\nfeatures and modeling long-range dependencies through the attention mechanism.\nWhereas recent works have explored the trustworthiness of ViT, including its\nrobustness and explainability, the issue of fairness has not yet been\nadequately addressed. We establish that the existing fairness-aware algorithms\ndesigned for CNNs do not perform well on ViT, which highlights the need to\ndevelop our novel framework via Debiased Self-Attention (DSA). DSA is a\nfairness-through-blindness approach that enforces ViT to eliminate spurious\nfeatures correlated with the sensitive label for bias mitigation and\nsimultaneously retain real features for target prediction. Notably, DSA\nleverages adversarial examples to locate and mask the spurious features in the\ninput image patches with an additional attention weights alignment regularizer\nin the training objective to encourage learning real features for target\nprediction. Importantly, our DSA framework leads to improved fairness\nguarantees over prior works on multiple prediction tasks without compromising\ntarget prediction performance. Code is available at\n\\href{https://github.com/qiangyao1988/DSA}{https://github.com/qiangyao1988/DSA}.\n","authors":["Yao Qiang","Chengyin Li","Prashant Khanduri","Dongxiao Zhu"],"pdf_url":"https://arxiv.org/pdf/2301.13803v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08125v1","updated":"2024-07-11T01:56:31Z","published":"2024-07-11T01:56:31Z","title":"Real-Time Summarization of Twitter","summary":"  In this paper, we describe our approaches to TREC Real-Time Summarization of\nTwitter. We focus on real time push notification scenario, which requires a\nsystem monitors the stream of sampled tweets and returns the tweets relevant\nand novel to given interest profiles. Dirichlet score with and with very little\nsmoothing (baseline) are employed to classify whether a tweet is relevant to a\ngiven interest profile. Using metrics including Mean Average Precision (MAP,\ncumulative gain (CG) and discount cumulative gain (DCG), the experiment\nindicates that our approach has a good performance. It is also desired to\nremove the redundant tweets from the pushing queue. Due to the precision limit,\nwe only describe the algorithm in this paper.\n","authors":["Yixin Jin","Meiqi Wang","Meng Li","Wenjing Zhou","Yi Shen","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08125v1.pdf","comment":"This paper was accepted to International Conference on Artificial\n  Intelligence and Electromechanical Automation 2024"},{"id":"http://arxiv.org/abs/2005.11304v4","updated":"2024-07-11T15:06:35Z","published":"2020-05-22T17:50:38Z","title":"Neural Bipartite Matching","summary":"  Graph neural networks (GNNs) have found application for learning in the space\nof algorithms. However, the algorithms chosen by existing research (sorting,\nBreadth-First search, shortest path finding, etc.) usually align perfectly with\na standard GNN architecture. This report describes how neural execution is\napplied to a complex algorithm, such as finding maximum bipartite matching by\nreducing it to a flow problem and using Ford-Fulkerson to find the maximum\nflow. This is achieved via neural execution based only on features generated\nfrom a single GNN. The evaluation shows strongly generalising results with the\nnetwork achieving optimal matching almost 100% of the time.\n","authors":["Dobrik Georgiev","Pietro LiÃ²"],"pdf_url":"https://arxiv.org/pdf/2005.11304v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2005.14105v3","updated":"2024-07-11T09:39:30Z","published":"2020-05-28T15:55:37Z","title":"Provably Good Solutions to the Knapsack Problem via Neural Networks of\n  Bounded Size","summary":"  The development of a satisfying and rigorous mathematical understanding of\nthe performance of neural networks is a major challenge in artificial\nintelligence. Against this background, we study the expressive power of neural\nnetworks through the example of the classical NP-hard Knapsack Problem. Our\nmain contribution is a class of recurrent neural networks (RNNs) with rectified\nlinear units that are iteratively applied to each item of a Knapsack instance\nand thereby compute optimal or provably good solution values. We show that an\nRNN of depth four and width depending quadratically on the profit of an optimum\nKnapsack solution is sufficient to find optimum Knapsack solutions. We also\nprove the following tradeoff between the size of an RNN and the quality of the\ncomputed Knapsack solution: for Knapsack instances consisting of $n$ items, an\nRNN of depth five and width $w$ computes a solution of value at least\n$1-\\mathcal{O}(n^2/\\sqrt{w})$ times the optimum solution value. Our results\nbuild upon a classical dynamic programming formulation of the Knapsack Problem\nas well as a careful rounding of profit values that are also at the core of the\nwell-known fully polynomial-time approximation scheme for the Knapsack Problem.\nA carefully conducted computational study qualitatively supports our\ntheoretical size bounds. Finally, we point out that our results can be\ngeneralized to many other combinatorial optimization problems that admit\ndynamic programming solution methods, such as various Shortest Path Problems,\nthe Longest Common Subsequence Problem, and the Traveling Salesperson Problem.\n","authors":["Christoph Hertrich","Martin Skutella"],"pdf_url":"https://arxiv.org/pdf/2005.14105v3.pdf","comment":"Authors' accepted manuscript for the INFORMS Journal on Computing. A\n  short version of this paper appeared in the proceedings of AAAI 2021"},{"id":"http://arxiv.org/abs/2402.09373v2","updated":"2024-07-11T23:43:18Z","published":"2024-02-14T18:20:44Z","title":"Loss Shaping Constraints for Long-Term Time Series Forecasting","summary":"  Several applications in time series forecasting require predicting multiple\nsteps ahead. Despite the vast amount of literature in the topic, both classical\nand recent deep learning based approaches have mostly focused on minimising\nperformance averaged over the predicted window. We observe that this can lead\nto disparate distributions of errors across forecasting steps, especially for\nrecent transformer architectures trained on popular forecasting benchmarks.\nThat is, optimising performance on average can lead to undesirably large errors\nat specific time-steps. In this work, we present a Constrained Learning\napproach for long-term time series forecasting that aims to find the best model\nin terms of average performance that respects a user-defined upper bound on the\nloss at each time-step. We call our approach loss shaping constraints because\nit imposes constraints on the loss at each time step, and leverage recent\nduality results to show that despite its non-convexity, the resulting problem\nhas a bounded duality gap. We propose a practical Primal-Dual algorithm to\ntackle it, and demonstrate that the proposed approach exhibits competitive\naverage performance in time series forecasting benchmarks, while shaping the\ndistribution of errors across the predicted window.\n","authors":["Ignacio Hounie","Javier Porras-Valenzuela","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2402.09373v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08892v1","updated":"2024-07-11T23:34:32Z","published":"2024-07-11T23:34:32Z","title":"Characterizing Prompt Compression Methods for Long Context Inference","summary":"  Long context inference presents challenges at the system level with increased\ncompute and memory requirements, as well as from an accuracy perspective in\nbeing able to reason over long contexts. Recently, several methods have been\nproposed to compress the prompt to reduce the context length. However, there\nhas been little work on comparing the different proposed methods across\ndifferent tasks through a standardized analysis. This has led to conflicting\nresults. To address this, here we perform a comprehensive characterization and\nevaluation of different prompt compression methods. In particular, we analyze\nextractive compression, summarization-based abstractive compression, and token\npruning methods. Surprisingly, we find that extractive compression often\noutperforms all the other approaches, and enables up to 10x compression with\nminimal accuracy degradation. Interestingly, we also find that despite several\nrecent claims, token pruning methods often lag behind extractive compression.\nWe only found marginal improvements on summarization tasks.\n","authors":["Siddharth Jha","Lutfi Eren Erdogan","Sehoon Kim","Kurt Keutzer","Amir Gholami"],"pdf_url":"https://arxiv.org/pdf/2407.08892v1.pdf","comment":"Es-FoMo @ ICML 2024"},{"id":"http://arxiv.org/abs/2407.08890v1","updated":"2024-07-11T23:16:44Z","published":"2024-07-11T23:16:44Z","title":"DeepCodeProbe: Towards Understanding What Models Trained on Code Learn","summary":"  Machine learning models trained on code and related artifacts offer valuable\nsupport for software maintenance but suffer from interpretability issues due to\ntheir complex internal variables. These concerns are particularly significant\nin safety-critical applications where the models' decision-making processes\nmust be reliable. The specific features and representations learned by these\nmodels remain unclear, adding to the hesitancy in adopting them widely. To\naddress these challenges, we introduce DeepCodeProbe, a probing approach that\nexamines the syntax and representation learning abilities of ML models designed\nfor software maintenance tasks. Our study applies DeepCodeProbe to\nstate-of-the-art models for code clone detection, code summarization, and\ncomment generation. Findings reveal that while small models capture abstract\nsyntactic representations, their ability to fully grasp programming language\nsyntax is limited. Increasing model capacity improves syntax learning but\nintroduces trade-offs such as increased training time and overfitting.\nDeepCodeProbe also identifies specific code patterns the models learn from\ntheir training data. Additionally, we provide best practices for training\nmodels on code to enhance performance and interpretability, supported by an\nopen-source replication package for broader application of DeepCodeProbe in\ninterpreting other code-related models.\n","authors":["Vahid Majdinasab","Amin Nikanjam","Foutse Khomh"],"pdf_url":"https://arxiv.org/pdf/2407.08890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08888v1","updated":"2024-07-11T23:04:16Z","published":"2024-07-11T23:04:16Z","title":"Uncovering Semantics and Topics Utilized by Threat Actors to Deliver\n  Malicious Attachments and URLs","summary":"  Recent threat reports highlight that email remains the top vector for\ndelivering malware to endpoints. Despite these statistics, detecting malicious\nemail attachments and URLs often neglects semantic cues linguistic features and\ncontextual clues. Our study employs BERTopic unsupervised topic modeling to\nidentify common semantics and themes embedded in email to deliver malicious\nattachments and call-to-action URLs. We preprocess emails by extracting and\nsanitizing content and employ multilingual embedding models like BGE-M3 for\ndense representations, which clustering algorithms(HDBSCAN and OPTICS) use to\ngroup emails by semantic similarity. Phi3-Mini-4K-Instruct facilitates semantic\nand hLDA aid in thematic analysis to understand threat actor patterns. Our\nresearch will evaluate and compare different clustering algorithms on topic\nquantity, coherence, and diversity metrics, concluding with insights into the\nsemantics and topics commonly used by threat actors to deliver malicious\nattachments and URLs, a significant contribution to the field of threat\ndetection.\n","authors":["Andrey Yakymovych","Abhishek Singh"],"pdf_url":"https://arxiv.org/pdf/2407.08888v1.pdf","comment":"6 Pages, 7 Figures"},{"id":"http://arxiv.org/abs/2310.16228v2","updated":"2024-07-11T23:03:09Z","published":"2023-10-24T22:54:05Z","title":"On the Foundations of Shortcut Learning","summary":"  Deep-learning models can extract a rich assortment of features from data.\nWhich features a model uses depends not only on \\emph{predictivity} -- how\nreliably a feature indicates training-set labels -- but also on\n\\emph{availability} -- how easily the feature can be extracted from inputs. The\nliterature on shortcut learning has noted examples in which models privilege\none feature over another, for example texture over shape and image backgrounds\nover foreground objects. Here, we test hypotheses about which input properties\nare more available to a model, and systematically study how predictivity and\navailability interact to shape models' feature use. We construct a minimal,\nexplicit generative framework for synthesizing classification datasets with two\nlatent features that vary in predictivity and in factors we hypothesize to\nrelate to availability, and we quantify a model's shortcut bias -- its\nover-reliance on the shortcut (more available, less predictive) feature at the\nexpense of the core (less available, more predictive) feature. We find that\nlinear models are relatively unbiased, but introducing a single hidden layer\nwith ReLU or Tanh units yields a bias. Our empirical findings are consistent\nwith a theoretical account based on Neural Tangent Kernels. Finally, we study\nhow models used in practice trade off predictivity and availability in\nnaturalistic datasets, discovering availability manipulations which increase\nmodels' degree of shortcut bias. Taken together, these findings suggest that\nthe propensity to learn shortcut features is a fundamental characteristic of\ndeep nonlinear architectures warranting systematic study given its role in\nshaping how models solve tasks.\n","authors":["Katherine L. Hermann","Hossein Mobahi","Thomas Fel","Michael C. Mozer"],"pdf_url":"https://arxiv.org/pdf/2310.16228v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2407.08887v1","updated":"2024-07-11T22:46:18Z","published":"2024-07-11T22:46:18Z","title":"Automatic Pruning of Fine-tuning Datasets for Transformer-based Language\n  Models","summary":"  Transformer-based language models have shown state-of-the-art performance on\na variety of natural language understanding tasks. To achieve this performance,\nthese models are first pre-trained on general corpus and then fine-tuned on\ndownstream tasks. Previous work studied the effect of pruning the training set\nof the downstream tasks on the performance of the model on its evaluation set.\nIn this work, we propose an automatic dataset pruning method for the training\nset of fine-tuning tasks. Our method is based on the model's success rate in\ncorrectly classifying each training data point. Unlike previous work which\nrelies on user feedback to determine subset size, our method automatically\nextracts training subsets that are adapted for each pair of model and\nfine-tuning task. Our method provides multiple subsets for use in dataset\npruning that navigate the trade-off between subset size and evaluation\naccuracy. Our largest subset, which we also refer to as the winning ticket\nsubset, is on average $3 \\times$ smaller than the original training set of the\nfine-tuning task. Our experiments on 5 downstream tasks and 2 language models\nshow that, on average, fine-tuning on the winning ticket subsets results in a\n$0.1 \\%$ increase in the evaluation performance of the model.\n","authors":["Mohammadreza Tayaranian","Seyyed Hasan Mozafari","Brett H. Meyer","James J. Clark","Warren J. Gross"],"pdf_url":"https://arxiv.org/pdf/2407.08887v1.pdf","comment":"28 pages, 17 figures. Accepted at the Third Conference on Lifelong\n  Learning Agents (CoLLAs 2024)"},{"id":"http://arxiv.org/abs/2407.08886v1","updated":"2024-07-11T22:42:53Z","published":"2024-07-11T22:42:53Z","title":"Semi-Supervised Multi-Task Learning Based Framework for Power System\n  Security Assessment","summary":"  This paper develops a novel machine learning-based framework using\nSemi-Supervised Multi-Task Learning (SS-MTL) for power system dynamic security\nassessment that is accurate, reliable, and aware of topological changes. The\nlearning algorithm underlying the proposed framework integrates conditional\nmasked encoders and employs multi-task learning for classification-aware\nfeature representation, which improves the accuracy and scalability to larger\nsystems. Additionally, this framework incorporates a confidence measure for its\npredictions, enhancing its reliability and interpretability. A topological\nsimilarity index has also been incorporated to add topological awareness to the\nframework. Various experiments on the IEEE 68-bus system were conducted to\nvalidate the proposed method, employing two distinct database generation\ntechniques to generate the required data to train the machine learning\nalgorithm. The results demonstrate that our algorithm outperforms existing\nstate-of-the-art machine learning based techniques for security assessment in\nterms of accuracy and robustness. Finally, our work underscores the value of\nemploying auto-encoders for security assessment, highlighting improvements in\naccuracy, reliability, and robustness. All datasets and codes used have been\nmade publicly available to ensure reproducibility and transparency.\n","authors":["Muhy Eddin Za'ter","Amirhossein Sajadi","Bri-Mathias Hodge"],"pdf_url":"https://arxiv.org/pdf/2407.08886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01610v3","updated":"2024-07-11T21:35:37Z","published":"2023-09-04T13:49:48Z","title":"Fairness in Ranking under Disparate Uncertainty","summary":"  Ranking is a ubiquitous method for focusing the attention of human evaluators\non a manageable subset of options. Its use as part of human decision-making\nprocesses ranges from surfacing potentially relevant products on an e-commerce\nsite to prioritizing college applications for human review. While ranking can\nmake human evaluation more effective by focusing attention on the most\npromising options, we argue that it can introduce unfairness if the uncertainty\nof the underlying relevance model differs between groups of options.\nUnfortunately, such disparity in uncertainty appears widespread, often to the\ndetriment of minority groups for which relevance estimates can have higher\nuncertainty due to a lack of data or appropriate features. To address this\nfairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness\ncriterion for ranking and show that it corresponds to a group-wise fair lottery\namong the relevant options even in the presence of disparate uncertainty. EOR\noptimizes for an even cost burden on all groups, unlike the conventional\nProbability Ranking Principle, and is fundamentally different from existing\nnotions of fairness in rankings, such as demographic parity and proportional\nRooney rule constraints that are motivated by proportional representation\nrelative to group size. To make EOR ranking practical, we present an efficient\nalgorithm for computing it in time $O(n \\log(n))$ and prove its close\napproximation guarantee to the globally optimal solution. In a comprehensive\nempirical evaluation on synthetic data, a US Census dataset, and a real-world\naudit of Amazon search queries, we find that the algorithm reliably guarantees\nEOR fairness while providing effective rankings.\n","authors":["Richa Rastogi","Thorsten Joachims"],"pdf_url":"https://arxiv.org/pdf/2309.01610v3.pdf","comment":"A version of this paper was accepted as Spotlight (Oral) at UAI\n  workshop on Epistemic AI, 2023"},{"id":"http://arxiv.org/abs/2407.08868v1","updated":"2024-07-11T21:10:03Z","published":"2024-07-11T21:10:03Z","title":"Generalizable Physics-informed Learning for Stochastic Safety-critical\n  Systems","summary":"  Accurate estimate of long-term risk is critical for safe decision-making, but\nsampling from rare risk events and long-term trajectories can be prohibitively\ncostly. Risk gradient can be used in many first-order techniques for learning\nand control methods, but gradient estimate is difficult to obtain using Monte\nCarlo (MC) methods because the infinitesimal devisor may significantly amplify\nsampling noise. Motivated by this gap, we propose an efficient method to\nevaluate long-term risk probabilities and their gradients using short-term\nsamples without sufficient risk events. We first derive that four types of\nlong-term risk probability are solutions of certain partial differential\nequations (PDEs). Then, we propose a physics-informed learning technique that\nintegrates data and physics information (aforementioned PDEs). The physics\ninformation helps propagate information beyond available data and obtain\nprovable generalization beyond available data, which in turn enables long-term\nrisk to be estimated using short-term samples of safe events. Finally, we\ndemonstrate in simulation that the proposed technique has improved sample\nefficiency, generalizes well to unseen regions, and adapts to changing system\nparameters.\n","authors":["Zhuoyuan Wang","Albert Chern","Yorie Nakahira"],"pdf_url":"https://arxiv.org/pdf/2407.08868v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2305.06432"},{"id":"http://arxiv.org/abs/2402.02625v2","updated":"2024-07-11T20:43:59Z","published":"2024-02-04T22:12:29Z","title":"Enhancing Transformer RNNs with Multiple Temporal Perspectives","summary":"  We introduce the concept of multiple temporal perspectives, a novel approach\napplicable to Recurrent Neural Network (RNN) architectures for enhancing their\nunderstanding of sequential data. This method involves maintaining diverse\ntemporal views of previously encountered text, significantly enriching the\nlanguage models' capacity to interpret context. To show the efficacy of this\napproach, we incorporate it into the Receptance Weighted Key Value (RWKV)\narchitecture, addressing its inherent challenge of retaining all historical\ninformation within a single hidden state. Notably, this improvement is achieved\nwith a minimal increase in the number of parameters --even as little as\n$0.04\\%$ of the original number of parameters. Further, the additional\nparameters necessary for the multiple temporal perspectives are fine-tuned with\nminimal computational overhead, avoiding the need for a full pre-training. The\nresulting model maintains linear computational complexity during prompt\ninference, ensuring consistent efficiency across various sequence lengths. The\nempirical results and ablation studies included in our research validate the\neffectiveness of our approach, showcasing improved performance across multiple\nbenchmarks. The code, model weights and datasets are open-sourced at:\nhttps://github.com/RazvanDu/TemporalRNNs.\n","authors":["Razvan-Gabriel Dumitru","Darius Peteleaza","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2402.02625v2.pdf","comment":"13 pages, 8 figures, 4 tables, accepted at ICML 2024 - Next\n  Generation of Sequence Modeling Architectures workshop"},{"id":"http://arxiv.org/abs/2401.03609v2","updated":"2024-07-11T20:12:22Z","published":"2024-01-07T23:45:01Z","title":"Multi-Modal Federated Learning for Cancer Staging over Non-IID Datasets\n  with Unbalanced Modalities","summary":"  The use of machine learning (ML) for cancer staging through medical image\nanalysis has gained substantial interest across medical disciplines. When\naccompanied by the innovative federated learning (FL) framework, ML techniques\ncan further overcome privacy concerns related to patient data exposure. Given\nthe frequent presence of diverse data modalities within patient records,\nleveraging FL in a multi-modal learning framework holds considerable promise\nfor cancer staging.\n  However, existing works on multi-modal FL often presume that all\ndata-collecting institutions have access to all data modalities. This\noversimplified approach neglects institutions that have access to only a\nportion of data modalities within the system. In this work, we introduce a\nnovel FL architecture designed to accommodate not only the heterogeneity of\ndata samples, but also the inherent heterogeneity/non-uniformity of data\nmodalities across institutions. We shed light on the challenges associated with\nvarying convergence speeds observed across different data modalities within our\nFL system. Subsequently, we propose a solution to tackle these challenges by\ndevising a distributed gradient blending and proximity-aware client weighting\nstrategy tailored for multi-modal FL. To show the superiority of our method, we\nconduct experiments using The Cancer Genome Atlas program (TCGA) datalake\nconsidering different cancer types and three modalities of data: mRNA\nsequences, histopathological image data, and clinical information. Our results\nfurther unveil the impact and severity of class-based vs type-based\nheterogeneity across institutions on the model performance, which widens the\nperspective to the notion of data heterogeneity in multi-modal FL literature.\n","authors":["Kasra Borazjani","Naji Khosravan","Leslie Ying","Seyyedali Hosseinalipour"],"pdf_url":"https://arxiv.org/pdf/2401.03609v2.pdf","comment":"17 pages, 5 figures, 18 tables"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.07728v2","updated":"2024-07-11T03:06:21Z","published":"2024-07-10T15:00:08Z","title":"SaMoye: Zero-shot Singing Voice Conversion Based on Feature\n  Disentanglement and Synthesis","summary":"  Singing voice conversion (SVC) aims to convert a singer's voice in a given\nmusic piece to another singer while keeping the original content. We propose an\nend-to-end feature disentanglement-based model, which we named SaMoye, to\nenable zero-shot many-to-many singing voice conversion. SaMoye disentangles the\nfeatures of the singing voice into content features, timbre features, and pitch\nfeatures respectively. The content features are enhanced using a GPT-based\nmodel to perform cross-prediction with the phoneme of the lyrics. SaMoye can\ngenerate the music with converted voice by replacing the timbre features with\nthe target singer. We also establish an unparalleled large-scale dataset to\nguarantee zero-shot performance. The dataset consists of 1500k pure singing\nvocal clips containing at least 10,000 singers.\n","authors":["Zihao Wang","Le Ma","Yan Liu","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.07728v2.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.07325v2","updated":"2024-07-11T07:44:28Z","published":"2024-07-10T02:43:18Z","title":"HiLight: Technical Report on the Motern AI Video Language Model","summary":"  This technical report presents the implementation of a state-of-the-art video\nencoder for video-text modal alignment and a video conversation framework\ncalled HiLight, which features dual visual towers. The work is divided into two\nmain parts: 1.alignment of video and text modalities; 2.convenient and\nefficient way to interact with users. Our goal is to address the task of video\ncomprehension in the context of billiards. The report includes a discussion of\nthe concepts and the final solution developed during the task's implementation.\n","authors":["Zhiting Wang","Qiangong Zhou","Kangjie Yang","Zongyang Liu","Xin Mao"],"pdf_url":"https://arxiv.org/pdf/2407.07325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06524v2","updated":"2024-07-11T08:51:04Z","published":"2024-07-09T03:32:00Z","title":"Improving Speech Enhancement by Integrating Inter-Channel and Band\n  Features with Dual-branch Conformer","summary":"  Recent speech enhancement methods based on convolutional neural networks\n(CNNs) and transformer have been demonstrated to efficaciously capture\ntime-frequency (T-F) information on spectrogram. However, the correlation of\neach channels of speech features is failed to explore. Theoretically, each\nchannel map of speech features obtained by different convolution kernels\ncontains information with different scales demonstrating strong correlations.\nTo fill this gap, we propose a novel dual-branch architecture named\nchannel-aware dual-branch conformer (CADB-Conformer), which effectively\nexplores the long range time and frequency correlations among different\nchannels, respectively, to extract channel relation aware time-frequency\ninformation. Ablation studies conducted on DNS-Challenge 2020 dataset\ndemonstrate the importance of channel feature leveraging while showing the\nsignificance of channel relation aware T-F information for speech enhancement.\nExtensive experiments also show that the proposed model achieves superior\nperformance than recent methods with an attractive computational costs.\n","authors":["Jizhen Li","Xinmeng Xu","Weiping Tu","Yuhong Yang","Rong Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.06524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03188v2","updated":"2024-07-11T03:32:44Z","published":"2024-07-03T15:12:36Z","title":"MuDiT & MuSiT: Alignment with Colloquial Expression in\n  Description-to-Song Generation","summary":"  Amid the rising intersection of generative AI and human artistic processes,\nthis study probes the critical yet less-explored terrain of alignment in\nhuman-centric automatic song composition. We propose a novel task of Colloquial\nDescription-to-Song Generation, which focuses on aligning the generated content\nwith colloquial human expressions. This task is aimed at bridging the gap\nbetween colloquial language understanding and auditory expression within an AI\nmodel, with the ultimate goal of creating songs that accurately satisfy human\nauditory expectations and structurally align with musical norms. Current\ndatasets are limited due to their narrow descriptive scope, semantic gaps and\ninaccuracies. To overcome data scarcity in this domain, we present the Caichong\nMusic Dataset (CaiMD). CaiMD is manually annotated by both professional\nmusicians and amateurs, offering diverse perspectives and a comprehensive\nunderstanding of colloquial descriptions. Unlike existing datasets pre-set with\nexpert annotations or auto-generated ones with inherent biases, CaiMD caters\nmore sufficiently to our purpose of aligning AI-generated music with widespread\nuser-desired results. Moreover, we propose an innovative single-stage framework\ncalled MuDiT/MuSiT for enabling effective human-machine alignment in song\ncreation. This framework not only achieves cross-modal comprehension between\ncolloquial language and auditory music perceptions but also ensures generated\nsongs align with user-desired results. MuDiT/MuSiT employs one DiT/SiT model\nfor end-to-end generation of musical components like melody, harmony, rhythm,\nvocals, and instrumentation. The approach ensures harmonious sonic cohesiveness\namongst all generated musical components, facilitating better resonance with\nhuman auditory expectations.\n","authors":["Zihao Wang","Haoxuan Liu","Jiaxing Yu","Tao Zhang","Yan Liu","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.03188v2.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.08528v1","updated":"2024-07-11T14:16:41Z","published":"2024-07-11T14:16:41Z","title":"Enhancing octree-based context models for point cloud geometry\n  compression with attention-based child node number prediction","summary":"  In point cloud geometry compression, most octreebased context models use the\ncross-entropy between the onehot encoding of node occupancy and the probability\ndistribution predicted by the context model as the loss. This approach converts\nthe problem of predicting the number (a regression problem) and the position (a\nclassification problem) of occupied child nodes into a 255-dimensional\nclassification problem. As a result, it fails to accurately measure the\ndifference between the one-hot encoding and the predicted probability\ndistribution. We first analyze why the cross-entropy loss function fails to\naccurately measure the difference between the one-hot encoding and the\npredicted probability distribution. Then, we propose an attention-based child\nnode number prediction (ACNP) module to enhance the context models. The\nproposed module can predict the number of occupied child nodes and map it into\nan 8- dimensional vector to assist the context model in predicting the\nprobability distribution of the occupancy of the current node for efficient\nentropy coding. Experimental results demonstrate that the proposed module\nenhances the coding efficiency of octree-based context models.\n","authors":["Chang Sun","Hui Yuan","Xiaolong Mao","Xin Lu","Raouf Hamzaoui"],"pdf_url":"https://arxiv.org/pdf/2407.08528v1.pdf","comment":"2 figures and 2 tables"},{"id":"http://arxiv.org/abs/2407.08520v1","updated":"2024-07-11T14:08:37Z","published":"2024-07-11T14:08:37Z","title":"Enhancing context models for point cloud geometry compression with\n  context feature residuals and multi-loss","summary":"  In point cloud geometry compression, context models usually use the one-hot\nencoding of node occupancy as the label, and the cross-entropy between the\none-hot encoding and the probability distribution predicted by the context\nmodel as the loss function. However, this approach has two main weaknesses.\nFirst, the differences between contexts of different nodes are not significant,\nmaking it difficult for the context model to accurately predict the probability\ndistribution of node occupancy. Second, as the one-hot encoding is not the\nactual probability distribution of node occupancy, the cross-entropy loss\nfunction is inaccurate. To address these problems, we propose a general\nstructure that can enhance existing context models. We introduce the context\nfeature residuals into the context model to amplify the differences between\ncontexts. We also add a multi-layer perception branch, that uses the mean\nsquared error between its output and node occupancy as a loss function to\nprovide accurate gradients in backpropagation. We validate our method by\nshowing that it can improve the performance of an octree-based model\n(OctAttention) and a voxel-based model (VoxelDNN) on the object point cloud\ndatasets MPEG 8i and MVUB, as well as the LiDAR point cloud dataset\nSemanticKITTI.\n","authors":["Chang Sun","Hui Yuan","Shuai Li","Xin Lu","Raouf Hamzaoui"],"pdf_url":"https://arxiv.org/pdf/2407.08520v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.08130v1","updated":"2024-07-11T02:01:26Z","published":"2024-07-11T02:01:26Z","title":"Spiking Tucker Fusion Transformer for Audio-Visual Zero-Shot Learning","summary":"  The spiking neural networks (SNNs) that efficiently encode temporal sequences\nhave shown great potential in extracting audio-visual joint feature\nrepresentations. However, coupling SNNs (binary spike sequences) with\ntransformers (float-point sequences) to jointly explore the temporal-semantic\ninformation still facing challenges. In this paper, we introduce a novel\nSpiking Tucker Fusion Transformer (STFT) for audio-visual zero-shot learning\n(ZSL). The STFT leverage the temporal and semantic information from different\ntime steps to generate robust representations. The time-step factor (TSF) is\nintroduced to dynamically synthesis the subsequent inference information. To\nguide the formation of input membrane potentials and reduce the spike noise, we\npropose a global-local pooling (GLP) which combines the max and average pooling\noperations. Furthermore, the thresholds of the spiking neurons are dynamically\nadjusted based on semantic and temporal cues. Integrating the temporal and\nsemantic information extracted by SNNs and Transformers are difficult due to\nthe increased number of parameters in a straightforward bilinear model. To\naddress this, we introduce a temporal-semantic Tucker fusion module, which\nachieves multi-scale fusion of SNN and Transformer outputs while maintaining\nfull second-order interactions. Our experimental results demonstrate the\neffectiveness of the proposed approach in achieving state-of-the-art\nperformance in three benchmark datasets. The harmonic mean (HM) improvement of\nVGGSound, UCF101 and ActivityNet are around 15.4\\%, 3.9\\%, and 14.9\\%,\nrespectively.\n","authors":["Wenrui Li","Penghong Wang","Ruiqin Xiong","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2407.08130v1.pdf","comment":"Accepted by TIP"},{"id":"http://arxiv.org/abs/2407.08126v1","updated":"2024-07-11T01:57:08Z","published":"2024-07-11T01:57:08Z","title":"Label-anticipated Event Disentanglement for Audio-Visual Video Parsing","summary":"  Audio-Visual Video Parsing (AVVP) task aims to detect and temporally locate\nevents within audio and visual modalities. Multiple events can overlap in the\ntimeline, making identification challenging. While traditional methods usually\nfocus on improving the early audio-visual encoders to embed more effective\nfeatures, the decoding phase -- crucial for final event classification, often\nreceives less attention. We aim to advance the decoding phase and improve its\ninterpretability. Specifically, we introduce a new decoding paradigm,\n\\underline{l}abel s\\underline{e}m\\underline{a}ntic-based \\underline{p}rojection\n(LEAP), that employs labels texts of event categories, each bearing distinct\nand explicit semantics, for parsing potentially overlapping events.LEAP works\nby iteratively projecting encoded latent features of audio/visual segments onto\nsemantically independent label embeddings. This process, enriched by modeling\ncross-modal (audio/visual-label) interactions, gradually disentangles event\nsemantics within video segments to refine relevant label embeddings,\nguaranteeing a more discriminative and interpretable decoding process. To\nfacilitate the LEAP paradigm, we propose a semantic-aware optimization\nstrategy, which includes a novel audio-visual semantic similarity loss\nfunction. This function leverages the Intersection over Union of audio and\nvisual events (EIoU) as a novel metric to calibrate audio-visual similarities\nat the feature level, accommodating the varied event densities across\nmodalities. Extensive experiments demonstrate the superiority of our method,\nachieving new state-of-the-art performance for AVVP and also enhancing the\nrelevant audio-visual event localization task.\n","authors":["Jinxing Zhou","Dan Guo","Yuxin Mao","Yiran Zhong","Xiaojun Chang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08126v1.pdf","comment":"Accepted by ECCV2024"}]},"2024-07-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.12014v2","updated":"2024-07-12T17:39:19Z","published":"2024-03-18T17:51:16Z","title":"EnvGen: Generating and Adapting Environments via LLMs for Training\n  Embodied Agents","summary":"  Recent SOTA approaches for embodied learning via interaction directly employ\nlarge language models (LLMs) as agents to determine the next steps in an\nenvironment. Due to their world knowledge and reasoning capabilities, LLM\nagents achieve stronger performance than previous smaller agents based on\nreinforcement learning (RL); however, frequently calling LLMs is slow and\nexpensive. Instead of directly employing LLMs as agents, can we use LLMs'\nreasoning capabilities to adaptively create training environments to help\nsmaller RL agents learn useful skills that they are weak at? We propose EnvGen,\na novel framework to address this question. We first prompt an LLM to generate\ntraining environments by giving it the task description and simulator\nobjectives that the agents should learn and then asking it to generate a set of\nenvironment configurations (e.g., different terrains, items initially given to\nagents, etc.). Next, we train a small RL agent in a mixture of the original and\nLLM-generated environments. Then, we enable the LLM to continuously adapt the\ngenerated environments to progressively improve the skills that the agent is\nweak at, by providing feedback to the LLM in the form of the agent's\nperformance. We demonstrate the usefulness of EnvGen with comprehensive\nexperiments in Crafter and Heist environments. We find that a small RL agent\ntrained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and\nlearns long-horizon tasks significantly faster. We also show that using an LLM\nto adapt environments dynamically outperforms curriculum learning approaches\nand how the environments are adapted to help improve RL agents' weaker skills\nover time. Additionally, EnvGen is substantially more efficient as it only uses\na small number of LLM calls (e.g., 4 in total), whereas LLM agents require\nthousands of calls. Lastly, we present detailed ablation studies for EnvGen\ndesign choices.\n","authors":["Abhay Zala","Jaemin Cho","Han Lin","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2403.12014v2.pdf","comment":"COLM 2024; First two authors contributed equally; Project website:\n  https://envgen-llm.github.io/"},{"id":"http://arxiv.org/abs/2407.09453v1","updated":"2024-07-12T17:37:49Z","published":"2024-07-12T17:37:49Z","title":"Weight Block Sparsity: Training, Compilation, and AI Engine Accelerators","summary":"  Nowadays, increasingly larger Deep Neural Networks (DNNs) are being\ndeveloped, trained, and utilized. These networks require significant\ncomputational resources, putting a strain on both advanced and limited devices.\nOur solution is to implement {\\em weight block sparsity}, which is a structured\nsparsity that is friendly to hardware. By zeroing certain sections of the\nconvolution and fully connected layers parameters of pre-trained DNN models, we\ncan efficiently speed up the DNN's inference process. This results in a smaller\nmemory footprint, faster communication, and fewer operations.\n  Our work presents a vertical system that allows for the training of\nconvolution and matrix multiplication weights to exploit 8x8 block sparsity on\na single GPU within a reasonable amount of time. Compilers recognize this\nsparsity and use it for both data compaction and computation splitting into\nthreads. Blocks like these take full advantage of both spatial and temporal\nlocality, paving the way for fast vector operations and memory reuse. By using\nthis system on a Resnet50 model, we were able to reduce the weight by half with\nminimal accuracy loss, resulting in a two-times faster inference speed. We will\npresent performance estimates using accurate and complete code generation for\nAIE2 configuration sets (AMD Versal FPGAs) with Resnet50, Inception V3, and\nVGG16 to demonstrate the necessary synergy between hardware overlay designs and\nsoftware stacks for compiling and executing machine learning applications.\n","authors":["Paolo D'Alberto","Taehee Jeong","Akshai Jain","Shreyas Manjunath","Mrinal Sarmah","Samuel Hsu Yaswanth Raparti","Nitesh Pipralia"],"pdf_url":"https://arxiv.org/pdf/2407.09453v1.pdf","comment":"12 pages, 10 figures, 1 table"},{"id":"http://arxiv.org/abs/2311.09184v2","updated":"2024-07-12T17:35:18Z","published":"2023-11-15T18:25:26Z","title":"Benchmarking Generation and Evaluation Capabilities of Large Language\n  Models for Instruction Controllable Summarization","summary":"  While large language models (LLMs) can already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for desired summary\ncharacteristics. To this end, we curate an evaluation-only dataset for this\ntask setting and conduct human evaluations of five LLM-based systems to assess\ntheir instruction-following capabilities in controllable summarization. We then\nbenchmark LLM-based automatic evaluation for this task with 4 different\nevaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study\nreveals that instruction controllable text summarization remains a challenging\ntask for LLMs, since (1) all LLMs evaluated still make factual and other types\nof errors in their summaries; (2) no LLM-based evaluation methods can achieve a\nstrong alignment with human annotators when judging the quality of candidate\nsummaries; (3) different LLMs show large performance gaps in summary generation\nand evaluation capabilities. We make our collected benchmark InstruSum publicly\navailable to facilitate future research in this direction.\n","authors":["Yixin Liu","Alexander R. Fabbri","Jiawen Chen","Yilun Zhao","Simeng Han","Shafiq Joty","Pengfei Liu","Dragomir Radev","Chien-Sheng Wu","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2311.09184v2.pdf","comment":"NAACL 2024 Findings, GitHub Repo:\n  https://github.com/yale-nlp/InstruSum, LLM-evaluators Leaderboard:\n  https://huggingface.co/spaces/yale-nlp/InstruSumEval"},{"id":"http://arxiv.org/abs/2407.09450v1","updated":"2024-07-12T17:34:03Z","published":"2024-07-12T17:34:03Z","title":"Human-like Episodic Memory for Infinite Context LLMs","summary":"  Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs,\nenabling them to effectively handle practically infinite context lengths while\nmaintaining computational efficiency. EM-LLM organises sequences of tokens into\ncoherent episodic events using a combination of Bayesian surprise and\ngraph-theoretic boundary refinement in an on-line fashion. When needed, these\nevents are retrieved through a two-stage memory process, combining\nsimilarity-based and temporally contiguous retrieval for efficient and\nhuman-like access to relevant information. Experiments on the LongBench dataset\ndemonstrate EM-LLM's superior performance, outperforming the state-of-the-art\nInfLLM model with an overall relative improvement of 4.3% across various tasks,\nincluding a 33% improvement on the PassageRetrieval task. Furthermore, our\nanalysis reveals strong correlations between EM-LLM's event segmentation and\nhuman-perceived events, suggesting a bridge between this artificial system and\nits biological counterpart. This work not only advances LLM capabilities in\nprocessing extended contexts but also provides a computational framework for\nexploring human memory mechanisms, opening new avenues for interdisciplinary\nresearch in AI and cognitive science.\n","authors":["Zafeirios Fountas","Martin A Benfeghoul","Adnan Oomerjee","Fenia Christopoulou","Gerasimos Lampouras","Haitham Bou-Ammar","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09447v1","updated":"2024-07-12T17:33:34Z","published":"2024-07-12T17:33:34Z","title":"ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to\n  Identify Likely Toxic Prompts","summary":"  Typical schemes for automated red-teaming large language models (LLMs) focus\non discovering prompts that trigger a frozen language model (the defender) to\ngenerate toxic text. This often results in the prompting model (the adversary)\nproducing text that is unintelligible and unlikely to arise. Here, we propose a\nreinforcement learning formulation of the LLM red-teaming task which allows us\nto discover prompts that both (1) trigger toxic outputs from a frozen defender\nand (2) have low perplexity as scored by the defender. We argue these cases are\nmost pertinent in a red-teaming setting because of their likelihood to arise\nduring normal use of the defender model. We solve this formulation through a\nnovel online and weakly supervised variant of Identity Preference Optimization\n(IPO) on GPT-2 and GPT-2 XL defenders. We demonstrate that our policy is\ncapable of generating likely prompts that also trigger toxicity. Finally, we\nqualitatively analyze learned strategies, trade-offs of likelihood and\ntoxicity, and discuss implications. Source code is available for this project\nat: https://github.com/sisl/ASTPrompter/.\n","authors":["Amelia F. Hardy","Houjun Liu","Bernard Lange","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2407.09447v1.pdf","comment":"9 pages, 2 tables, 2 figures"},{"id":"http://arxiv.org/abs/2310.01691v2","updated":"2024-07-12T17:26:08Z","published":"2023-10-02T23:12:21Z","title":"Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across\n  Language Models","summary":"  Prompt tuning in natural language processing (NLP) has become an increasingly\npopular method for adapting large language models to specific tasks. However,\nthe transferability of these prompts, especially continuous prompts, between\ndifferent models remains a challenge. In this work, we propose a zero-shot\ncontinuous prompt transfer method, where source prompts are encoded into\nrelative space and the corresponding target prompts are searched for\ntransferring to target models. Experimental results confirm the effectiveness\nof our method, showing that 'task semantics' in continuous prompts can be\ngeneralized across various language models. Moreover, we find that combining\n'task semantics' from multiple source models can further enhance the\ngeneralizability of transfer.\n","authors":["Zijun Wu","Yongkang Wu","Lili Mou"],"pdf_url":"https://arxiv.org/pdf/2310.01691v2.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2407.01437v2","updated":"2024-07-12T17:20:34Z","published":"2024-07-01T16:32:16Z","title":"Needle in the Haystack for Memory Based Large Language Models","summary":"  Current large language models (LLMs) often perform poorly on simple fact\nretrieval tasks. Here we investigate if coupling a dynamically adaptable\nexternal memory to a LLM can alleviate this problem. For this purpose, we test\nLarimar, a recently proposed language model architecture which uses an external\nassociative memory, on long-context recall tasks including passkey and\nneedle-in-the-haystack tests. We demonstrate that the external memory of\nLarimar, which allows fast write and read of an episode of text samples, can be\nused at test time to handle contexts much longer than those seen during\ntraining. We further show that the latent readouts from the memory (to which\nlong contexts are written) control the decoder towards generating correct\noutputs, with the memory stored off of the GPU. Compared to existing\ntransformer-based LLM architectures for long-context recall tasks that use\nlarger parameter counts or modified attention mechanisms, a relatively smaller\nsize Larimar is able to maintain strong performance without any task-specific\ntraining or training on longer contexts.\n","authors":["Elliot Nelson","Georgios Kollias","Payel Das","Subhajit Chaudhury","Soham Dan"],"pdf_url":"https://arxiv.org/pdf/2407.01437v2.pdf","comment":"5 pages; slightly revised abstract"},{"id":"http://arxiv.org/abs/2407.04796v2","updated":"2024-07-12T17:13:47Z","published":"2024-07-05T18:12:19Z","title":"Toucan: Many-to-Many Translation for 150 African Language Pairs","summary":"  We address a notable gap in Natural Language Processing (NLP) by introducing\na collection of resources designed to improve Machine Translation (MT) for\nlow-resource languages, with a specific focus on African languages. First, we\nintroduce two language models (LMs), Cheetah-1.2B and Cheetah-3.7B, with 1.2\nbillion and 3.7 billion parameters respectively. Next, we finetune the\naforementioned models to create toucan, an Afrocentric machine translation\nmodel designed to support 156 African language pairs. To evaluate Toucan, we\ncarefully develop an extensive machine translation benchmark, dubbed\nAfroLingu-MT, tailored for evaluating machine translation. Toucan significantly\noutperforms other models, showcasing its remarkable performance on MT for\nAfrican languages. Finally, we train a new model, spBLEU-1K, to enhance\ntranslation evaluation metrics, covering 1K languages, including 614 African\nlanguages. This work aims to advance the field of NLP, fostering cross-cultural\nunderstanding and knowledge exchange, particularly in regions with limited\nlanguage resources such as Africa. The GitHub repository for the Toucan project\nis available at https://github.com/UBC-NLP/Toucan.\n","authors":["AbdelRahim Elmadany","Ife Adebara","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2407.04796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09429v1","updated":"2024-07-12T17:00:44Z","published":"2024-07-12T17:00:44Z","title":"Open (Clinical) LLMs are Sensitive to Instruction Phrasings","summary":"  Instruction-tuned Large Language Models (LLMs) can perform a wide range of\ntasks given natural language instructions to do so, but they are sensitive to\nhow such instructions are phrased. This issue is especially concerning in\nhealthcare, as clinicians are unlikely to be experienced prompt engineers and\nthe potential consequences of inaccurate outputs are heightened in this domain.\n  This raises a practical question: How robust are instruction-tuned LLMs to\nnatural variations in the instructions provided for clinical NLP tasks? We\ncollect prompts from medical doctors across a range of tasks and quantify the\nsensitivity of seven LLMs -- some general, others specialized -- to natural\n(i.e., non-adversarial) instruction phrasings. We find that performance varies\nsubstantially across all models, and that -- perhaps surprisingly --\ndomain-specific models explicitly trained on clinical data are especially\nbrittle, compared to their general domain counterparts. Further, arbitrary\nphrasing differences can affect fairness, e.g., valid but distinct instructions\nfor mortality prediction yield a range both in overall performance, and in\nterms of differences between demographic groups.\n","authors":["Alberto Mario Ceballos Arroyo","Monica Munnangi","Jiuding Sun","Karen Y. C. Zhang","Denis Jered McInerney","Byron C. Wallace","Silvio Amir"],"pdf_url":"https://arxiv.org/pdf/2407.09429v1.pdf","comment":"To appear at BioNLP, ACL 2024"},{"id":"http://arxiv.org/abs/2406.04313v4","updated":"2024-07-12T16:51:07Z","published":"2024-06-06T17:57:04Z","title":"Improving Alignment and Robustness with Circuit Breakers","summary":"  AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that interrupts the models as they respond with harmful outputs\nwith \"circuit breakers.\" Existing techniques aimed at improving alignment, such\nas refusal training, are often bypassed. Techniques such as adversarial\ntraining try to plug these holes by countering specific attacks. As an\nalternative to refusal training and adversarial training, circuit-breaking\ndirectly controls the representations that are responsible for harmful outputs\nin the first place. Our technique can be applied to both text-only and\nmultimodal language models to prevent the generation of harmful outputs without\nsacrificing utility -- even in the presence of powerful unseen attacks.\nNotably, while adversarial robustness in standalone image recognition remains\nan open challenge, circuit breakers allow the larger multimodal system to\nreliably withstand image \"hijacks\" that aim to produce harmful content.\nFinally, we extend our approach to AI agents, demonstrating considerable\nreductions in the rate of harmful actions when they are under attack. Our\napproach represents a significant step forward in the development of reliable\nsafeguards to harmful behavior and adversarial attacks.\n","authors":["Andy Zou","Long Phan","Justin Wang","Derek Duenas","Maxwell Lin","Maksym Andriushchenko","Rowan Wang","Zico Kolter","Matt Fredrikson","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2406.04313v4.pdf","comment":"Code and models are available at\n  https://github.com/GraySwanAI/circuit-breakers"},{"id":"http://arxiv.org/abs/2407.09417v1","updated":"2024-07-12T16:47:34Z","published":"2024-07-12T16:47:34Z","title":"Mitigating Entity-Level Hallucination in Large Language Models","summary":"  The emergence of Large Language Models (LLMs) has revolutionized how users\naccess information, shifting from traditional search engines to direct\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\ngenerate coherent yet factually inaccurate responses. This hallucination\nphenomenon has led to users' distrust in information retrieval systems based on\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\nAugmentation based on hallucination Detection (DRAD) as a novel method to\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\nretrieval augmentation by dynamically adapting the retrieval process based on\nreal-time hallucination detection. It features two main components: Real-time\nHallucination Detection (RHD) for identifying potential hallucinations without\nexternal models, and Self-correction based on External Knowledge (SEK) for\ncorrecting these errors using external knowledge. Experiment results show that\nDRAD demonstrates superior performance in both detecting and mitigating\nhallucinations in LLMs. All of our code and data are open-sourced at\nhttps://github.com/oneal2000/EntityHallucination.\n","authors":["Weihang Su","Yichen Tang","Qingyao Ai","Changyue Wang","Zhijing Wu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.09417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09413v1","updated":"2024-07-12T16:37:59Z","published":"2024-07-12T16:37:59Z","title":"SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers","summary":"  Seeking answers to questions within long scientific research articles is a\ncrucial area of study that aids readers in quickly addressing their inquiries.\nHowever, existing question-answering (QA) datasets based on scientific papers\nare limited in scale and focus solely on textual content. To address this\nlimitation, we introduce SPIQA (Scientific Paper Image Question Answering), the\nfirst large-scale QA dataset specifically designed to interpret complex figures\nand tables within the context of scientific research articles across various\ndomains of computer science. Leveraging the breadth of expertise and ability of\nmultimodal large language models (MLLMs) to understand figures, we employ\nautomatic and manual curation to create the dataset. We craft an\ninformation-seeking task involving multiple images that cover a wide variety of\nplots, charts, tables, schematic diagrams, and result visualizations. SPIQA\ncomprises 270K questions divided into training, validation, and three different\nevaluation splits. Through extensive experiments with 12 prominent foundational\nmodels, we evaluate the ability of current multimodal systems to comprehend the\nnuanced aspects of research articles. Additionally, we propose a\nChain-of-Thought (CoT) evaluation strategy with in-context retrieval that\nallows fine-grained, step-by-step assessment and improves model performance. We\nfurther explore the upper bounds of performance enhancement with additional\ntextual information, highlighting its promising potential for future research\nand the dataset's impact on revolutionizing how we interact with scientific\nliterature.\n","authors":["Shraman Pramanick","Rama Chellappa","Subhashini Venugopalan"],"pdf_url":"https://arxiv.org/pdf/2407.09413v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2311.09821v2","updated":"2024-07-12T16:37:14Z","published":"2023-11-16T11:49:29Z","title":"Towards Robust Temporal Reasoning of Large Language Models via a\n  Multi-Hop QA Dataset and Pseudo-Instruction Tuning","summary":"  Knowledge in the real world is being updated constantly. However, it is\ncostly to frequently update large language models (LLMs). Therefore, it is\ncrucial for LLMs to understand the concept of temporal knowledge. However,\nprior works on temporal question answering (TQA) did not emphasize multi-answer\nand multi-hop types of temporal reasoning. In this paper, we propose a complex\ntemporal question-answering dataset Complex-TR that focuses on multi-answer and\nmulti-hop temporal reasoning. Besides, we also propose a novel data\naugmentation strategy to improve the complex temporal reasoning capability and\nrobustness of LLMs. We conducted experiments on multiple temporal QA datasets.\nExperimental results show that our method is able to improve LLMs' performance\non temporal QA benchmarks by significant margins. Our code and data are\nreleased at: https://github.com/nusnlp/complex-tr.\n","authors":["Qingyu Tan","Hwee Tou Ng","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2311.09821v2.pdf","comment":"To appear in Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2407.09395v1","updated":"2024-07-12T16:18:05Z","published":"2024-07-12T16:18:05Z","title":"Deep Bag-of-Words Model: An Efficient and Interpretable Relevance\n  Architecture for Chinese E-Commerce","summary":"  Text relevance or text matching of query and product is an essential\ntechnique for the e-commerce search system to ensure that the displayed\nproducts can match the intent of the query. Many studies focus on improving the\nperformance of the relevance model in search system. Recently, pre-trained\nlanguage models like BERT have achieved promising performance on the text\nrelevance task. While these models perform well on the offline test dataset,\nthere are still obstacles to deploy the pre-trained language model to the\nonline system as their high latency. The two-tower model is extensively\nemployed in industrial scenarios, owing to its ability to harmonize performance\nwith computational efficiency. Regrettably, such models present an opaque\n``black box'' nature, which prevents developers from making special\noptimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an\nefficient and interpretable relevance architecture for Chinese e-commerce. Our\napproach proposes to encode the query and the product into the sparse BoW\nrepresentation, which is a set of word-weight pairs. The weight means the\nimportant or the relevant score between the corresponding word and the raw\ntext. The relevance score is measured by the accumulation of the matched word\nbetween the sparse BoW representation of the query and the product. Compared to\npopular dense distributed representation that usually suffers from the drawback\nof black-box, the most advantage of the proposed representation model is highly\nexplainable and interventionable, which is a superior advantage to the\ndeployment and operation of online search engines. Moreover, the online\nefficiency of the proposed model is even better than the most efficient inner\nproduct form of dense representation ...\n","authors":["Zhe Lin","Jiwei Tan","Dan Ou","Xi Chen","Shaowei Yao","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.09395v1.pdf","comment":"KDD'24 accepted paper"},{"id":"http://arxiv.org/abs/2407.09364v1","updated":"2024-07-12T15:44:56Z","published":"2024-07-12T15:44:56Z","title":"Is Contrasting All You Need? Contrastive Learning for the Detection and\n  Attribution of AI-generated Text","summary":"  The significant progress in the development of Large Language Models has\ncontributed to blurring the distinction between human and AI-generated text.\nThe increasing pervasiveness of AI-generated text and the difficulty in\ndetecting it poses new challenges for our society. In this paper, we tackle the\nproblem of detecting and attributing AI-generated text by proposing WhosAI, a\ntriplet-network contrastive learning framework designed to predict whether a\ngiven input text has been generated by humans or AI and to unveil the\nauthorship of the text. Unlike most existing approaches, our proposed framework\nis conceived to learn semantic similarity representations from multiple\ngenerators at once, thus equally handling both detection and attribution tasks.\nFurthermore, WhosAI is model-agnostic and scalable to the release of new AI\ntext-generation models by incorporating their generated instances into the\nembedding space learned by our framework. Experimental results on the\nTuringBench benchmark of 200K news articles show that our proposed framework\nachieves outstanding results in both the Turing Test and Authorship Attribution\ntasks, outperforming all the methods listed in the TuringBench benchmark\nleaderboards.\n","authors":["Lucio La Cava","Davide Costa","Andrea Tagarelli"],"pdf_url":"https://arxiv.org/pdf/2407.09364v1.pdf","comment":"Accepted for publication at the 27th European Conference on\n  Artificial Intelligence (ECAI-2024)"},{"id":"http://arxiv.org/abs/2402.17407v2","updated":"2024-07-12T15:42:45Z","published":"2024-02-27T10:57:07Z","title":"A Neural Rewriting System to Solve Algorithmic Problems","summary":"  Modern neural network architectures still struggle to learn algorithmic\nprocedures that require to systematically apply compositional rules to solve\nout-of-distribution problem instances. In this work, we focus on formula\nsimplification problems, a class of synthetic benchmarks used to study the\nsystematic generalization capabilities of neural architectures. We propose a\nmodular architecture designed to learn a general procedure for solving nested\nmathematical formulas by only relying on a minimal set of training examples.\nInspired by rewriting systems, a classic framework in symbolic artificial\nintelligence, we include in the architecture three specialized and interacting\nmodules: the Selector, trained to identify solvable sub-expressions; the\nSolver, mapping sub-expressions to their values; and the Combiner, replacing\nsub-expressions in the original formula with the solution provided by the\nSolver. We benchmark our system against the Neural Data Router, a recent model\nspecialized for systematic generalization, and a state-of-the-art large\nlanguage model (GPT-4) probed with advanced prompting strategies. We\ndemonstrate that our approach achieves a higher degree of out-of-distribution\ngeneralization compared to these alternative approaches on three different\ntypes of formula simplification problems, and we discuss its limitations by\nanalyzing its failures.\n","authors":["Flavio Petruzzellis","Alberto Testolin","Alessandro Sperduti"],"pdf_url":"https://arxiv.org/pdf/2402.17407v2.pdf","comment":"Updated version (v2) accepted at the 27th European Conference on\n  Artificial Intelligence (ECAI 24)"},{"id":"http://arxiv.org/abs/2407.09327v1","updated":"2024-07-12T15:04:09Z","published":"2024-07-12T15:04:09Z","title":"Sina at FigNews 2024: Multilingual Datasets Annotated with Bias and\n  Propaganda","summary":"  The proliferation of bias and propaganda on social media is an increasingly\nsignificant concern, leading to the development of techniques for automatic\ndetection. This article presents a multilingual corpus of 12, 000 Facebook\nposts fully annotated for bias and propaganda. The corpus was created as part\nof the FigNews 2024 Shared Task on News Media Narratives for framing the\nIsraeli War on Gaza. It covers various events during the War from October 7,\n2023 to January 31, 2024. The corpus comprises 12, 000 posts in five languages\n(Arabic, Hebrew, English, French, and Hindi), with 2, 400 posts for each\nlanguage. The annotation process involved 10 graduate students specializing in\nLaw. The Inter-Annotator Agreement (IAA) was used to evaluate the annotations\nof the corpus, with an average IAA of 80.8% for bias and 70.15% for propaganda\nannotations. Our team was ranked among the bestperforming teams in both Bias\nand Propaganda subtasks. The corpus is open-source and available at\nhttps://sina.birzeit.edu/fada\n","authors":["Lina Duaibes","Areej Jaber","Mustafa Jarrar","Ahmad Qadi","Mais Qandeel"],"pdf_url":"https://arxiv.org/pdf/2407.09327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15371v2","updated":"2024-07-12T14:52:49Z","published":"2024-03-22T17:50:43Z","title":"Can large language models explore in-context?","summary":"  We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.\n","authors":["Akshay Krishnamurthy","Keegan Harris","Dylan J. Foster","Cyril Zhang","Aleksandrs Slivkins"],"pdf_url":"https://arxiv.org/pdf/2403.15371v2.pdf","comment":"Minor updates, added references to related and concurrent work"},{"id":"http://arxiv.org/abs/2407.09311v1","updated":"2024-07-12T14:52:13Z","published":"2024-07-12T14:52:13Z","title":"Scalability of Bayesian Network Structure Elicitation with Large\n  Language Models: a Novel Methodology and Comparative Analysis","summary":"  In this work, we propose a novel method for Bayesian Networks (BNs) structure\nelicitation that is based on the initialization of several LLMs with different\nexperiences, independently querying them to create a structure of the BN, and\nfurther obtaining the final structure by majority voting. We compare the method\nwith one alternative method on various widely and not widely known BNs of\ndifferent sizes and study the scalability of both methods on them. We also\npropose an approach to check the contamination of BNs in LLM, which shows that\nsome widely known BNs are inapplicable for testing the LLM usage for BNs\nstructure elicitation. We also show that some BNs may be inapplicable for such\nexperiments because their node names are indistinguishable. The experiments on\nthe other BNs show that our method performs better than the existing method\nwith one of the three studied LLMs; however, the performance of both methods\nsignificantly decreases with the increase in BN size.\n","authors":["Nikolay Babakov","Ehud Reiter","Alberto Bugarin"],"pdf_url":"https://arxiv.org/pdf/2407.09311v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2405.03862v2","updated":"2024-07-12T14:50:25Z","published":"2024-05-06T21:20:35Z","title":"Conformity, Confabulation, and Impersonation: Persona Inconstancy in\n  Multi-Agent LLM Collaboration","summary":"  Multi-agent AI systems can be used for simulating collective decision-making\nin scientific and practical applications. They can also be used to introduce a\ndiverse group discussion step in chatbot pipelines, enhancing the cultural\nsensitivity of the chatbot's responses. These applications, however, are\npredicated on the ability of AI agents to reliably adopt assigned personas and\nmimic human interactions. To evaluate the ability of LLM agents to satisfy\nthese requirements, we examine AI agent ensembles engaged in cultural\ncollaboration and debate by analyzing their private responses and chat\ntranscripts. Our findings suggest that multi-agent discussions can encourage\ncollective decisions that reflect diverse perspectives, yet this benefit is\ntempered by the agents' susceptibility to conformity due to perceived peer\npressure and challenges in maintaining consistent personas and opinions.\nInstructions that encourage debate in support of one's opinions rather than\ncollaboration increase the rate of inconstancy. Without addressing the factors\nwe identify, the full potential of multi-agent frameworks for producing more\nculturally diverse AI outputs or more realistic simulations of group\ndecision-making will remain untapped.\n","authors":["Razan Baltaji","Babak Hemmatian","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2405.03862v2.pdf","comment":"16 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.01394v2","updated":"2024-07-12T14:44:33Z","published":"2024-07-01T15:46:45Z","title":"Gloss2Text: Sign Language Gloss translation using LLMs and Semantically\n  Aware Label Smoothing","summary":"  Sign language translation from video to spoken text presents unique\nchallenges owing to the distinct grammar, expression nuances, and high\nvariation of visual appearance across different speakers and contexts. The\nintermediate gloss annotations of videos aim to guide the translation process.\nIn our work, we focus on {\\em Gloss2Text} translation stage and propose several\nadvances by leveraging pre-trained large language models (LLMs), data\naugmentation, and novel label-smoothing loss function exploiting gloss\ntranslation ambiguities improving significantly the performance of\nstate-of-the-art approaches. Through extensive experiments and ablation studies\non the PHOENIX Weather 2014T dataset, our approach surpasses state-of-the-art\nperformance in {\\em Gloss2Text} translation, indicating its efficacy in\naddressing sign language translation and suggesting promising avenues for\nfuture research and development.\n","authors":["Pooya Fayyazsanavi","Antonios Anastasopoulos","Jana KoÅ¡eckÃ¡"],"pdf_url":"https://arxiv.org/pdf/2407.01394v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09298v1","updated":"2024-07-12T14:31:05Z","published":"2024-07-12T14:31:05Z","title":"Transformer Layers as Painters","summary":"  Despite their nearly universal adoption for large language models, the\ninternal workings of transformers are not well understood. We aim to better\nunderstand the impact of removing or reorganizing information throughout the\nlayers of a pretrained transformer. Such an understanding could both yield\nbetter usage of existing models as well as to make architectural improvements\nto produce new variants. We present a series of empirical studies on frozen\nmodels that show that the lower and final layers of pretrained transformers\ndiffer from middle layers, but that middle layers have a surprising amount of\nuniformity. We further show that some classes of problems have robustness to\nskipping layers, running the layers in an order different from how they were\ntrained, or running the layers in parallel. Our observations suggest that even\nfrozen pretrained models may gracefully trade accuracy for latency by skipping\nlayers or running layers in parallel.\n","authors":["Qi Sun","Marc Pickett","Aakash Kumar Nain","Llion Jones"],"pdf_url":"https://arxiv.org/pdf/2407.09298v1.pdf","comment":"15 pages total, including references and appendices"},{"id":"http://arxiv.org/abs/2301.13844v2","updated":"2024-07-12T14:24:46Z","published":"2023-01-31T18:40:46Z","title":"Do Multi-Document Summarization Models Synthesize?","summary":"  Multi-document summarization entails producing concise synopses of\ncollections of inputs. For some applications, the synopsis should accurately\nsynthesize inputs with respect to a key aspect, e.g., a synopsis of film\nreviews written about a particular movie should reflect the average critic\nconsensus. As a more consequential example, narrative summaries that accompany\nbiomedical systematic reviews of clinical trial results should accurately\nsummarize the potentially conflicting results from individual trials. In this\npaper we ask: To what extent do modern multi-document summarization models\nimplicitly perform this sort of synthesis? We run experiments over opinion and\nevidence synthesis datasets using a suite of summarization models, from\nfine-tuned transformers to GPT-4. We find that existing models partially\nperform synthesis, but imperfectly: even the best performing models are\nover-sensitive to changes in input ordering and under-sensitive to changes in\ninput compositions (e.g., ratio of positive to negative reviews). We propose a\nsimple, general, effective method for improving model synthesis capabilities by\ngenerating an explicitly diverse set of candidate outputs, and then selecting\nfrom these the string best aligned with the expected aggregate measure for the\ninputs, or abstaining when the model produces no good candidate.\n","authors":["Jay DeYoung","Stephanie C. Martinez","Iain J. Marshall","Byron C. Wallace"],"pdf_url":"https://arxiv.org/pdf/2301.13844v2.pdf","comment":"Accepted to TACL, to be presented at ACL 2024 in Bangkok, Thailand. 9\n  Figures, 11 Tables, 14 pages of main content, 20 pages total. This paper has\n  some _history_. Buy me a drink if you want to hear about it"},{"id":"http://arxiv.org/abs/2407.09283v1","updated":"2024-07-12T14:13:59Z","published":"2024-07-12T14:13:59Z","title":"DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection","summary":"  Semantic role labeling (SRL) enriches many downstream applications, e.g.,\nmachine translation, question answering, summarization, and stance/belief\ndetection. However, building multilingual SRL models is challenging due to the\nscarcity of semantically annotated corpora for multiple languages. Moreover,\nstate-of-the-art SRL projection (XSRL) based on large language models (LLMs)\nyields output that is riddled with spurious role labels. Remediation of such\nhallucinations is not straightforward due to the lack of explainability of\nLLMs. We show that hallucinated role labels are related to naturally occurring\ndivergence types that interfere with initial alignments. We implement\nDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging\nlinguistically-informed alignment remediation followed by greedy First-Come\nFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL\nprojection without additional transformer-based machinery, beating XSRL in both\nhuman and automatic comparisons, and advancing beyond headwords to accommodate\nphrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our\nground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%\n(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%\n(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our\napproach to other language pairs (e.g., English-Tagalog).\n","authors":["Sangpil Youm","Brodie Mather","Chathuri Jayaweera","Juliana Prada","Bonnie Dorr"],"pdf_url":"https://arxiv.org/pdf/2407.09283v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.09276v1","updated":"2024-07-12T14:09:40Z","published":"2024-07-12T14:09:40Z","title":"H2O-Danube3 Technical Report","summary":"  We present H2O-Danube3, a series of small language models consisting of\nH2O-Danube3-4B, trained on 6T tokens and H2O-Danube3-500M, trained on 4T\ntokens. Our models are pre-trained on high quality Web data consisting of\nprimarily English tokens in three stages with different data mixes before final\nsupervised tuning for chat version. The models exhibit highly competitive\nmetrics across a multitude of academic, chat, and fine-tuning benchmarks.\nThanks to its compact architecture, H2O-Danube3 can be efficiently run on a\nmodern smartphone, enabling local inference and rapid processing capabilities\neven on mobile devices. We make all models openly available under Apache 2.0\nlicense further democratizing LLMs to a wider audience economically.\n","authors":["Pascal Pfeiffer","Philipp Singer","Yauhen Babakhin","Gabor Fodor","Nischay Dhankhar","Sri Satish Ambati"],"pdf_url":"https://arxiv.org/pdf/2407.09276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05435v2","updated":"2024-07-12T13:46:47Z","published":"2024-02-08T06:20:01Z","title":"GPT-4 Generated Narratives of Life Events using a Structured Narrative\n  Prompt: A Validation Study","summary":"  Large Language Models (LLMs) play a pivotal role in generating vast arrays of\nnarratives, facilitating a systematic exploration of their effectiveness for\ncommunicating life events in narrative form. In this study, we employ a\nzero-shot structured narrative prompt to generate 24,000 narratives using\nOpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and\nevaluate their validity in conveying birth, death, hiring, and firing events.\nRemarkably, 87.43% of the narratives sufficiently convey the intention of the\nstructured prompt. To automate the identification of valid and invalid\nnarratives, we train and validate nine Machine Learning models on the\nclassified datasets. Leveraging these models, we extend our analysis to predict\nthe classifications of the remaining 21,120 narratives. All the ML models\nexcelled at classifying valid narratives as valid, but experienced challenges\nat simultaneously classifying invalid narratives as invalid. Our findings not\nonly advance the study of LLM capabilities, limitations, and validity but also\noffer practical insights for narrative generation and natural language\nprocessing applications.\n","authors":["Christopher J. Lynch","Erik Jensen","Madison H. Munro","Virginia Zamponi","Joseph Martinez","Kevin O'Brien","Brandon Feldhaus","Katherine Smith","Ann Marie Reinhold","Ross Gore"],"pdf_url":"https://arxiv.org/pdf/2402.05435v2.pdf","comment":"29 pages, 24 figures"},{"id":"http://arxiv.org/abs/2407.09252v1","updated":"2024-07-12T13:30:44Z","published":"2024-07-12T13:30:44Z","title":"Context Embeddings for Efficient Answer Generation in RAG","summary":"  Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge\nof LLMs by extending the input with external information. As a consequence, the\ncontextual inputs to the model become much longer which slows down decoding\ntime directly translating to the time a user has to wait for an answer. We\naddress this challenge by presenting COCOM, an effective context compression\nmethod, reducing long contexts to only a handful of Context Embeddings speeding\nup the generation time by a large margin. Our method allows for different\ncompression rates trading off decoding time for answer quality. Compared to\nearlier methods, COCOM allows for handling multiple contexts more effectively,\nsignificantly reducing decoding time for long inputs. Our method demonstrates a\nspeed-up of up to 5.69 $\\times$ while achieving higher performance compared to\nexisting efficient context compression methods.\n","authors":["David Rau","Shuai Wang","HervÃ© DÃ©jean","StÃ©phane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2407.09252v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.09241v1","updated":"2024-07-12T13:12:55Z","published":"2024-07-12T13:12:55Z","title":"The Sociolinguistic Foundations of Language Modeling","summary":"  In this paper, we introduce a sociolinguistic perspective on language\nmodeling. We claim that large language models are inherently models of\nvarieties of language, and we consider how this insight can inform the\ndevelopment and deployment of large language models. We begin by presenting a\ntechnical definition of the concept of a variety of language as developed in\nsociolinguistics. We then discuss how this perspective can help address five\nbasic challenges in language modeling: social bias, domain adaptation,\nalignment, language change, and scale. Ultimately, we argue that it is crucial\nto carefully define and compile training corpora that accurately represent the\nspecific varieties of language being modeled to maximize the performance and\nsocietal value of large language models.\n","authors":["Jack Grieve","Sara Bartl","Matteo Fuoli","Jason Grafmiller","Weihang Huang","Alejandro Jawerbaum","Akira Murakami","Marcus Perlman","Dana Roemling","Bodo Winter"],"pdf_url":"https://arxiv.org/pdf/2407.09241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09209v1","updated":"2024-07-12T12:16:14Z","published":"2024-07-12T12:16:14Z","title":"Pronunciation Assessment with Multi-modal Large Language Models","summary":"  Large language models (LLMs), renowned for their powerful conversational\nabilities, are widely recognized as exceptional tools in the field of\neducation, particularly in the context of automated intelligent instruction\nsystems for language learning. In this paper, we propose a scoring system based\non LLMs, motivated by their positive impact on text-related scoring tasks.\nSpecifically, the speech encoder first maps the learner's speech into\ncontextual features. The adapter layer then transforms these features to align\nwith the text embedding in latent space. The assessment task-specific prefix\nand prompt text are embedded and concatenated with the features generated by\nthe modality adapter layer, enabling the LLMs to predict accuracy and fluency\nscores. Our experiments demonstrate that the proposed scoring systems achieve\ncompetitive results compared to the baselines on the Speechocean762 datasets.\nMoreover, we also conducted an ablation study to better understand the\ncontributions of the prompt text and training strategy in the proposed scoring\nsystem.\n","authors":["Kaiqi Fu","Linkai Peng","Nan Yang","Shuran Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.09209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09197v1","updated":"2024-07-12T11:53:40Z","published":"2024-07-12T11:53:40Z","title":"A Chatbot for Asylum-Seeking Migrants in Europe","summary":"  We present ACME: A Chatbot for asylum-seeking Migrants in Europe. ACME relies\non computational argumentation and aims to help migrants identify the highest\nlevel of protection they can apply for. This would contribute to a more\nsustainable migration by reducing the load on territorial commissions, Courts,\nand humanitarian organizations supporting asylum applicants. We describe the\ncontext, system architectures, technologies, and the case study used to run the\ndemonstration.\n","authors":["Bettina Fazzinga","Elena Palmieri","Margherita Vestoso","Luca Bolognini","Andrea Galassi","Filippo Furfaro","Paolo Torroni"],"pdf_url":"https://arxiv.org/pdf/2407.09197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09187v1","updated":"2024-07-12T11:40:17Z","published":"2024-07-12T11:40:17Z","title":"Enhancing Depressive Post Detection in Bangla: A Comparative Study of\n  TF-IDF, BERT and FastText Embeddings","summary":"  Due to massive adoption of social media, detection of users' depression\nthrough social media analytics bears significant importance, particularly for\nunderrepresented languages, such as Bangla. This study introduces a\nwell-grounded approach to identify depressive social media posts in Bangla, by\nemploying advanced natural language processing techniques. The dataset used in\nthis work, annotated by domain experts, includes both depressive and\nnon-depressive posts, ensuring high-quality data for model training and\nevaluation. To address the prevalent issue of class imbalance, we utilised\nrandom oversampling for the minority class, thereby enhancing the model's\nability to accurately detect depressive posts. We explored various numerical\nrepresentation techniques, including Term Frequency-Inverse Document Frequency\n(TF-IDF), Bidirectional Encoder Representations from Transformers (BERT)\nembedding and FastText embedding, by integrating them with a deep\nlearning-based Convolutional Neural Network-Bidirectional Long Short-Term\nMemory (CNN-BiLSTM) model. The results obtained through extensive\nexperimentation, indicate that the BERT approach performed better the others,\nachieving a F1-score of 84%. This indicates that BERT, in combination with the\nCNN-BiLSTM architecture, effectively recognises the nuances of Bangla texts\nrelevant to depressive contents. Comparative analysis with the existing\nstate-of-the-art methods demonstrates that our approach with BERT embedding\nperforms better than others in terms of evaluation metrics and the reliability\nof dataset annotations. Our research significantly contribution to the\ndevelopment of reliable tools for detecting depressive posts in the Bangla\nlanguage. By highlighting the efficacy of different embedding techniques and\ndeep learning models, this study paves the way for improved mental health\nmonitoring through social media platforms.\n","authors":["Saad Ahmed Sazan","Mahdi H. Miraz","A B M Muntasir Rahman"],"pdf_url":"https://arxiv.org/pdf/2407.09187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09184v1","updated":"2024-07-12T11:33:41Z","published":"2024-07-12T11:33:41Z","title":"Does Incomplete Syntax Influence Korean Language Model? Focusing on Word\n  Order and Case Markers","summary":"  Syntactic elements, such as word order and case markers, are fundamental in\nnatural language processing. Recent studies show that syntactic information\nboosts language model performance and offers clues for people to understand\ntheir learning mechanisms. Unlike languages with a fixed word order such as\nEnglish, Korean allows for varied word sequences, despite its canonical\nstructure, due to case markers that indicate the functions of sentence\ncomponents. This study explores whether Korean language models can accurately\ncapture this flexibility. We note that incomplete word orders and omitted case\nmarkers frequently appear in ordinary Korean communication. To investigate this\nfurther, we introduce the Syntactically Incomplete Korean (SIKO) dataset.\nThrough SIKO, we assessed Korean language models' flexibility with incomplete\nsyntax and confirmed the dataset's training value. Results indicate these\nmodels reflect Korean's inherent flexibility, accurately handling incomplete\ninputs. Moreover, fine-tuning with SIKO enhances the ability to handle common\nincomplete Korean syntactic forms. The dataset's simple construction process,\ncoupled with significant performance enhancements, solidifies its standing as\nan effective data augmentation technique.\n","authors":["Jong Myoung Kim","Young-Jun Lee","Yong-jin Han","Sangkeun Jung","Ho-Jin Choi"],"pdf_url":"https://arxiv.org/pdf/2407.09184v1.pdf","comment":"COLM 2024; Code and dataset is available in\n  https://github.com/grayapple-git/SIKO"},{"id":"http://arxiv.org/abs/2407.09181v1","updated":"2024-07-12T11:30:10Z","published":"2024-07-12T11:30:10Z","title":"Exploring the Effectiveness of Methods for Persona Extraction","summary":"  The paper presents a study of methods for extracting information about\ndialogue participants and evaluating their performance in Russian. To train\nmodels for this task, the Multi-Session Chat dataset was translated into\nRussian using multiple translation models, resulting in improved data quality.\nA metric based on the F-score concept is presented to evaluate the\neffectiveness of the extraction models. The metric uses a trained classifier to\nidentify the dialogue participant to whom the persona belongs. Experiments were\nconducted on MBart, FRED-T5, Starling-7B, which is based on the Mistral, and\nEncoder2Encoder models. The results demonstrated that all models exhibited an\ninsufficient level of recall in the persona extraction task. The incorporation\nof the NCE Loss improved the model's precision at the expense of its recall.\nFurthermore, increasing the model's size led to enhanced extraction of\npersonas.\n","authors":["Konstantin Zaitsev"],"pdf_url":"https://arxiv.org/pdf/2407.09181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14522v2","updated":"2024-07-12T10:39:28Z","published":"2024-02-22T13:13:31Z","title":"Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap\n  for Prompt-Based Large Language Models and Beyond","summary":"  Task embedding, a meta-learning technique that captures task-specific\ninformation, has gained popularity, especially in areas such as multi-task\nlearning, model editing, and interpretability. However, it faces challenges\nwith the emergence of prompt-guided Large Language Models (LLMs) operating in a\ngradient-free manner. Existing task embedding methods rely on fine-tuned,\ntask-specific language models, which hinders the adaptability of task\nembeddings across diverse models, especially prompt-based LLMs. To hardness the\npotential of task embeddings in the era of LLMs, we propose a framework for\nunified task embeddings (FUTE), harmonizing task embeddings from various\nmodels, including smaller language models and LLMs with varied prompts, within\na single vector space. Such uniformity enables comparison and analysis of\nsimilarities amongst different models, broadening the scope and utility of\nexisting task embedding methods in multi-model scenarios, while maintaining\ntheir performance comparable to architecture-specific methods.\n","authors":["Xinyu Wang","Hainiu Xu","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2402.14522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09152v1","updated":"2024-07-12T10:34:46Z","published":"2024-07-12T10:34:46Z","title":"The Two Sides of the Coin: Hallucination Generation and Detection with\n  LLMs as Evaluators for LLMs","summary":"  Hallucination detection in Large Language Models (LLMs) is crucial for\nensuring their reliability. This work presents our participation in the CLEF\nELOQUENT HalluciGen shared task, where the goal is to develop evaluators for\nboth generating and detecting hallucinated content. We explored the\ncapabilities of four LLMs: Llama 3, Gemma, GPT-3.5 Turbo, and GPT-4, for this\npurpose. We also employed ensemble majority voting to incorporate all four\nmodels for the detection task. The results provide valuable insights into the\nstrengths and weaknesses of these LLMs in handling hallucination generation and\ndetection tasks.\n","authors":["Anh Thu Maria Bui","Saskia Felizitas Brech","Natalie HuÃfeldt","Tobias Jennert","Melanie Ullrich","Timo Breuer","Narjes Nikzad Khasmakhi","Philipp Schaer"],"pdf_url":"https://arxiv.org/pdf/2407.09152v1.pdf","comment":"Paper accepted at ELOQUENT@CLEF'24"},{"id":"http://arxiv.org/abs/2305.01626v2","updated":"2024-07-12T10:30:23Z","published":"2023-05-02T17:38:21Z","title":"Basic syntax from speech: Spontaneous concatenation in unsupervised deep\n  neural networks","summary":"  Computational models of syntax are predominantly text-based. Here we propose\nthat the most basic syntactic operations can be modeled directly from raw\nspeech in a fully unsupervised way. We focus on one of the most ubiquitous and\nelementary properties of syntax -- concatenation. We introduce spontaneous\nconcatenation: a phenomenon where convolutional neural networks (CNNs) trained\non acoustic recordings of individual words start generating outputs with two or\neven three words concatenated without ever accessing data with multiple words\nin the input. We replicate this finding in several independently trained models\nwith different hyperparameters and training data. Additionally, networks\ntrained on two words learn to embed words into novel unobserved word\ncombinations. To our knowledge, this is a previously unreported property of\nCNNs trained in the ciwGAN/fiwGAN setting on raw speech and has implications\nboth for our understanding of how these architectures learn as well as for\nmodeling syntax and its evolution from raw acoustic inputs.\n","authors":["GaÅ¡per BeguÅ¡","Thomas Lu","Zili Wang"],"pdf_url":"https://arxiv.org/pdf/2305.01626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10421v2","updated":"2024-07-12T10:17:49Z","published":"2024-06-14T21:52:21Z","title":"SciEx: Benchmarking Large Language Models on Scientific Exams with Human\n  Expert Grading and Automatic Grading","summary":"  With the rapid development of Large Language Models (LLMs), it is crucial to\nhave benchmarks which can evaluate the ability of LLMs on different domains.\nOne common use of LLMs is performing tasks on scientific topics, such as\nwriting algorithms, querying databases or giving mathematical proofs. Inspired\nby the way university students are evaluated on such tasks, in this paper, we\npropose SciEx - a benchmark consisting of university computer science exam\nquestions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1)\nmultilingual, containing both English and German exams, and (2) multi-modal,\ncontaining questions that involve images, and (3) contains various types of\nfreeform questions with different difficulty levels, due to the nature of\nuniversity exams. We evaluate the performance of various state-of-the-art LLMs\non our new benchmark. Since SciEx questions are freeform, it is not\nstraightforward to evaluate LLM performance. Therefore, we provide human expert\ngrading of the LLM outputs on SciEx. We show that the free-form exams in SciEx\nremain challenging for the current LLMs, where the best LLM only achieves\n59.4\\% exam grade on average. We also provide detailed comparisons between LLM\nperformance and student performance on SciEx. To enable future evaluation of\nnew LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx.\nOur experiments show that, although they do not perform perfectly on solving\nthe exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with\nexpert grading.\n","authors":["Tu Anh Dinh","Carlos Mullov","Leonard BÃ¤rmann","Zhaolin Li","Danni Liu","Simon ReiÃ","Jueun Lee","Nathan Lerzer","Fabian Ternava","Jianfeng Gao","Tobias RÃ¶ddiger","Alexander Waibel","Tamim Asfour","Michael Beigl","Rainer Stiefelhagen","Carsten Dachsbacher","Klemens BÃ¶hm","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2406.10421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09137v1","updated":"2024-07-12T10:16:03Z","published":"2024-07-12T10:16:03Z","title":"A Look Into News Avoidance Through AWRS: An Avoidance-Aware Recommender\n  System","summary":"  In recent years, journalists have expressed concerns about the increasing\ntrend of news article avoidance, especially within specific domains. This issue\nhas been exacerbated by the rise of recommender systems. Our research indicates\nthat recommender systems should consider avoidance as a fundamental factor. We\nargue that news articles can be characterized by three principal elements:\nexposure, relevance, and avoidance, all of which are closely interconnected. To\naddress these challenges, we introduce AWRS, an Avoidance-Aware Recommender\nSystem. This framework incorporates avoidance awareness when recommending news,\nbased on the premise that news article avoidance conveys significant\ninformation about user preferences. Evaluation results on three news datasets\nin different languages (English, Norwegian, and Japanese) demonstrate that our\nmethod outperforms existing approaches.\n","authors":["Igor L. R. Azevedo","Toyotaro Suzumura","Yuichiro Yasui"],"pdf_url":"https://arxiv.org/pdf/2407.09137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09136v1","updated":"2024-07-12T10:11:40Z","published":"2024-07-12T10:11:40Z","title":"Stepwise Verification and Remediation of Student Reasoning Errors with\n  Large Language Model Tutors","summary":"  Large language models (LLMs) present an opportunity to scale high-quality\npersonalized education to all. A promising approach towards this means is to\nbuild dialog tutoring models that scaffold students' problem-solving. However,\neven though existing LLMs perform well in solving reasoning questions, they\nstruggle to precisely detect student's errors and tailor their feedback to\nthese errors. Inspired by real-world teaching practice where teachers identify\nstudent errors and customize their response based on them, we focus on\nverifying student solutions and show how grounding to such verification\nimproves the overall quality of tutor response generation. We collect a dataset\nof 1K stepwise math reasoning chains with the first error step annotated by\nteachers. We show empirically that finding the mistake in a student solution is\nchallenging for current models. We propose and evaluate several verifiers for\ndetecting these errors. Using both automatic and human evaluation we show that\nthe student solution verifiers steer the generation model towards highly\ntargeted responses to student errors which are more often correct with less\nhallucinations compared to existing baselines.\n","authors":["Nico Daheim","Jakub Macina","Manu Kapur","Iryna Gurevych","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2407.09136v1.pdf","comment":"Preprint. Nico Daheim and Jakub Macina contributed equally. Code and\n  dataset can be found under: https://github.com/eth-lre/verify-then-generate"},{"id":"http://arxiv.org/abs/2404.15196v2","updated":"2024-07-12T10:06:15Z","published":"2024-04-23T16:34:34Z","title":"Setting up the Data Printer with Improved English to Ukrainian Machine\n  Translation","summary":"  To build large language models for Ukrainian we need to expand our corpora\nwith large amounts of new algorithmic tasks expressed in natural language.\nExamples of task performance expressed in English are abundant, so with a\nhigh-quality translation system our community will be enabled to curate\ndatasets faster. To aid this goal, we introduce a recipe to build a translation\nsystem using supervised finetuning of a large pretrained language model with a\nnoisy parallel dataset of 3M pairs of Ukrainian and English sentences followed\nby a second phase of training using 17K examples selected by k-fold perplexity\nfiltering on another dataset of higher quality. Our decoder-only model named\nDragoman beats performance of previous state of the art encoder-decoder models\non the FLORES devtest set.\n","authors":["Yurii Paniv","Dmytro Chaplynskyi","Nikita Trynus","Volodymyr Kyrylov"],"pdf_url":"https://arxiv.org/pdf/2404.15196v2.pdf","comment":"Published at Proceedings of the Third Ukrainian Natural Language\n  Processing Workshop (UNLP)@ LREC-COLING 2024 (pp. 41-50)"},{"id":"http://arxiv.org/abs/2407.06027v3","updated":"2024-07-12T10:04:50Z","published":"2024-07-08T15:25:33Z","title":"PAS: Data-Efficient Plug-and-Play Prompt Augmentation System","summary":"  In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering.\n","authors":["Miao Zheng","Hao Liang","Fan Yang","Haoze Sun","Tianpeng Li","Lingchu Xiong","Yan Zhang","Youzhen Wu","Kun Li","Yanjun Shen","Mingan Lin","Tao Zhang","Guosheng Dong","Yujing Qiao","Kun Fang","Weipeng Chen","Bin Cui","Wentao Zhang","Zenan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.06027v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09121v1","updated":"2024-07-12T09:36:33Z","published":"2024-07-12T09:36:33Z","title":"Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled\n  Refusal Training","summary":"  This study addresses a critical gap in safety tuning practices for Large\nLanguage Models (LLMs) by identifying and tackling a refusal position bias\nwithin safety tuning data, which compromises the models' ability to\nappropriately refuse generating unsafe content. We introduce a novel approach,\nDecoupled Refusal Training (DeRTa), designed to empower LLMs to refuse\ncompliance to harmful prompts at any response position, significantly enhancing\ntheir safety capabilities. DeRTa incorporates two novel components: (1) Maximum\nLikelihood Estimation (MLE) with Harmful Response Prefix, which trains models\nto recognize and avoid unsafe content by appending a segment of harmful\nresponse to the beginning of a safe response, and (2) Reinforced Transition\nOptimization (RTO), which equips models with the ability to transition from\npotential harm to safety refusal consistently throughout the harmful response\nsequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model\nfamilies across six attack scenarios, demonstrates that our method not only\nimproves model safety without compromising performance but also surpasses\nwell-known models such as GPT-4 in defending against attacks. Importantly, our\napproach successfully defends recent advanced attack methods (e.g., CodeAttack)\nthat have jailbroken GPT-4 and LLaMA3-70B-Instruct. Our code and data can be\nfound at https://github.com/RobustNLP/DeRTa.\n","authors":["Youliang Yuan","Wenxiang Jiao","Wenxuan Wang","Jen-tse Huang","Jiahao Xu","Tian Liang","Pinjia He","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2407.09121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09120v1","updated":"2024-07-12T09:35:25Z","published":"2024-07-12T09:35:25Z","title":"URRL-IMVC: Unified and Robust Representation Learning for Incomplete\n  Multi-View Clustering","summary":"  Incomplete multi-view clustering (IMVC) aims to cluster multi-view data that\nare only partially available. This poses two main challenges: effectively\nleveraging multi-view information and mitigating the impact of missing views.\nPrevailing solutions employ cross-view contrastive learning and missing view\nrecovery techniques. However, they either neglect valuable complementary\ninformation by focusing only on consensus between views or provide unreliable\nrecovered views due to the absence of supervision. To address these\nlimitations, we propose a novel Unified and Robust Representation Learning for\nIncomplete Multi-View Clustering (URRL-IMVC). URRL-IMVC directly learns a\nunified embedding that is robust to view missing conditions by integrating\ninformation from multiple views and neighboring samples. Firstly, to overcome\nthe limitations of cross-view contrastive learning, URRL-IMVC incorporates an\nattention-based auto-encoder framework to fuse multi-view information and\ngenerate unified embeddings. Secondly, URRL-IMVC directly enhances the\nrobustness of the unified embedding against view-missing conditions through KNN\nimputation and data augmentation techniques, eliminating the need for explicit\nmissing view recovery. Finally, incremental improvements are introduced to\nfurther enhance the overall performance, such as the Clustering Module and the\ncustomization of the Encoder. We extensively evaluate the proposed URRL-IMVC\nframework on various benchmark datasets, demonstrating its state-of-the-art\nperformance. Furthermore, comprehensive ablation studies are performed to\nvalidate the effectiveness of our design.\n","authors":["Ge Teng","Ting Mao","Chen Shen","Xiang Tian","Xuesong Liu","Yaowu Chen","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.09120v1.pdf","comment":"Accepted by ACM SIGKDD 2024"},{"id":"http://arxiv.org/abs/2407.04093v2","updated":"2024-07-12T09:15:10Z","published":"2024-07-04T17:59:41Z","title":"Stephanie: Step-by-Step Dialogues for Mimicking Human Interactions in\n  Social Conversations","summary":"  In the rapidly evolving field of natural language processing, dialogue\nsystems primarily employ a single-step dialogue paradigm. Although this\nparadigm is efficient, it lacks the depth and fluidity of human interactions\nand does not appear natural. We introduce a novel \\textbf{Step}-by-Step\nDialogue Paradigm (Stephanie), designed to mimic the ongoing dynamic nature of\nhuman conversations. By employing a dual learning strategy and a further-split\npost-editing method, we generated and utilized a high-quality step-by-step\ndialogue dataset to fine-tune existing large language models, enabling them to\nperform step-by-step dialogues. We thoroughly present Stephanie. Tailored\nautomatic and human evaluations are conducted to assess its effectiveness\ncompared to the traditional single-step dialogue paradigm. We will release\ncode, Stephanie datasets, and Stephanie LLMs to facilitate the future of\nchatbot eras.\n","authors":["Hao Yang","Hongyuan Lu","Xinhua Zeng","Yang Liu","Xiang Zhang","Haoran Yang","Yumeng Zhang","Shan Huang","Yiran Wei","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2407.04093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18977v2","updated":"2024-07-12T08:57:43Z","published":"2024-06-27T08:13:33Z","title":"RoboUniView: Visual-Language Model with Unified View Representation for\n  Robotic Manipulaiton","summary":"  Utilizing Vision-Language Models (VLMs) for robotic manipulation represents a\nnovel paradigm, aiming to enhance the model's ability to generalize to new\nobjects and instructions. However, due to variations in camera specifications\nand mounting positions, existing methods exhibit significant performance\ndisparities across different robotic platforms. To address this challenge, we\npropose RoboUniView in this paper, an innovative approach that decouples visual\nfeature extraction from action learning. We first learn a unified view\nrepresentation from multi-perspective views by pre-training on readily\naccessible data, and then derive actions from this unified view representation\nto control robotic manipulation. This unified view representation more\naccurately mirrors the physical world and is not constrained by the robotic\nplatform's camera parameters. Thanks to this methodology, we achieve\nstate-of-the-art performance on the demanding CALVIN benchmark, enhancing the\nsuccess rate in the $D \\to D$ setting from 93.0% to 96.2%, and in the $ABC \\to\nD$ setting from 92.2% to 94.2%. Moreover, our model exhibits outstanding\nadaptability and flexibility: it maintains high performance under unseen camera\nparameters, can utilize multiple datasets with varying camera parameters, and\nis capable of joint cross-task learning across datasets. Code is provided for\nre-implementation. https://github.com/liufanfanlff/RoboUniview\n","authors":["Fanfan Liu","Feng Yan","Liming Zheng","Chengjian Feng","Yiyang Huang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2406.18977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14393v2","updated":"2024-07-12T08:15:45Z","published":"2024-06-20T15:12:27Z","title":"Jailbreaking as a Reward Misspecification Problem","summary":"  The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. We introduce a metric ReGap to quantify the extent of reward\nmisspecification and demonstrate its effectiveness and robustness in detecting\nharmful backdoor prompts. Building upon these insights, we present ReMiss, a\nsystem for automated red teaming that generates adversarial prompts against\nvarious target aligned LLMs. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark while preserving the human readability of the\ngenerated prompts. Detailed analysis highlights the unique advantages brought\nby the proposed reward misspecification objective compared to previous methods.\n","authors":["Zhihui Xie","Jiahui Gao","Lei Li","Zhenguo Li","Qi Liu","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2406.14393v2.pdf","comment":"github url added"},{"id":"http://arxiv.org/abs/2407.09072v1","updated":"2024-07-12T07:52:32Z","published":"2024-07-12T07:52:32Z","title":"New Desiderata for Direct Preference Optimization","summary":"  Large language models in the past have typically relied on some form of\nreinforcement learning with human feedback (RLHF) to better align model\nresponses with human preferences. However, because of oft-observed\ninstabilities when implementing these RLHF pipelines, various\nreparameterization techniques have recently been introduced to sidestep the\nneed for separately learning an RL reward model. Instead, directly fine-tuning\nfor human preferences is achieved via the minimization of a single closed-form\ntraining objective, a process originally referred to as direct preference\noptimization (DPO) and followed by several notable descendants. Although\neffective in certain real-world settings, we introduce new evaluation criteria\nthat serve to highlight unresolved shortcomings in the ability of existing DPO\nmethods to interpolate between a pre-trained reference model and empirical\nmeasures of human preferences, as well as unavoidable trade-offs in how low-\nand high-quality responses are regularized and constraints are handled. Our\ninsights then motivate an alternative DPO-like loss that provably mitigates\nthese limitations. Empirical results serve to corroborate notable aspects of\nour analyses.\n","authors":["Xiangkun Hu","Tong He","David Wipf"],"pdf_url":"https://arxiv.org/pdf/2407.09072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04972v2","updated":"2024-07-12T06:48:09Z","published":"2024-01-10T07:33:32Z","title":"Whose wife is it anyway? Assessing bias against same-gender\n  relationships in machine translation","summary":"  Machine translation often suffers from biased data and algorithms that can\nlead to unacceptable errors in system output. While bias in gender norms has\nbeen investigated, less is known about whether MT systems encode bias about\nsocial relationships, e.g., \"the lawyer kissed her wife.\" We investigate the\ndegree of bias against same-gender relationships in MT systems, using generated\ntemplate sentences drawn from several noun-gender languages (e.g., Spanish) and\ncomprised of popular occupation nouns. We find that three popular MT services\nconsistently fail to accurately translate sentences concerning relationships\nbetween entities of the same gender. The error rate varies considerably based\non the context, and same-gender sentences referencing high\nfemale-representation occupations are translated with lower accuracy. We\nprovide this work as a case study in the evaluation of intrinsic bias in NLP\nsystems with respect to social relationships.\n","authors":["Ian Stewart","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2401.04972v2.pdf","comment":"5th Workshop on Gender Bias in Natural Language Processing 2024"},{"id":"http://arxiv.org/abs/2407.08273v2","updated":"2024-07-12T06:24:12Z","published":"2024-07-11T08:19:58Z","title":"RB-SQL: A Retrieval-based LLM Framework for Text-to-SQL","summary":"  Large language models (LLMs) with in-context learning have significantly\nimproved the performance of text-to-SQL task. Previous works generally focus on\nusing exclusive SQL generation prompt to improve the LLMs' reasoning ability.\nHowever, they are mostly hard to handle large databases with numerous tables\nand columns, and usually ignore the significance of pre-processing database and\nextracting valuable information for more efficient prompt engineering. Based on\nabove analysis, we propose RB-SQL, a novel retrieval-based LLM framework for\nin-context prompt engineering, which consists of three modules that retrieve\nconcise tables and columns as schema, and targeted examples for in-context\nlearning. Experiment results demonstrate that our model achieves better\nperformance than several competitive baselines on public datasets BIRD and\nSpider.\n","authors":["Zhenhe Wu","Zhongqiu Li","Jie Zhang","Mengxiang Li","Yu Zhao","Ruiyu Fang","Zhongjiang He","Xuelong Li","Zhoujun Li","Shuangyong Song"],"pdf_url":"https://arxiv.org/pdf/2407.08273v2.pdf","comment":"Further improvement and modification are needed."},{"id":"http://arxiv.org/abs/2407.09020v1","updated":"2024-07-12T06:22:45Z","published":"2024-07-12T06:22:45Z","title":"3M-Health: Multimodal Multi-Teacher Knowledge Distillation for Mental\n  Health Detection","summary":"  The significance of mental health classification is paramount in contemporary\nsociety, where digital platforms serve as crucial sources for monitoring\nindividuals' well-being. However, existing social media mental health datasets\nprimarily consist of text-only samples, potentially limiting the efficacy of\nmodels trained on such data. Recognising that humans utilise cross-modal\ninformation to comprehend complex situations or issues, we present a novel\napproach to address the limitations of current methodologies. In this work, we\nintroduce a Multimodal and Multi-Teacher Knowledge Distillation model for\nMental Health Classification, leveraging insights from cross-modal human\nunderstanding. Unlike conventional approaches that often rely on simple\nconcatenation to integrate diverse features, our model addresses the challenge\nof appropriately representing inputs of varying natures (e.g., texts and\nsounds). To mitigate the computational complexity associated with integrating\nall features into a single model, we employ a multimodal and multi-teacher\narchitecture. By distributing the learning process across multiple teachers,\neach specialising in a particular feature extraction aspect, we enhance the\noverall mental health classification performance. Through experimental\nvalidation, we demonstrate the efficacy of our model in achieving improved\nperformance. All relevant codes will be made available upon publication.\n","authors":["Rina Carines Cabral","Siwen Luo","Soyeon Caren Han","Josiah Poon"],"pdf_url":"https://arxiv.org/pdf/2407.09020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09014v1","updated":"2024-07-12T06:06:54Z","published":"2024-07-12T06:06:54Z","title":"CompAct: Compressing Retrieved Documents Actively for Question Answering","summary":"  Retrieval-augmented generation supports language models to strengthen their\nfactual groundings by providing external contexts. However, language models\noften face challenges when given extensive information, diminishing their\neffectiveness in solving questions. Context compression tackles this issue by\nfiltering out irrelevant information, but current methods still struggle in\nrealistic scenarios where crucial information cannot be captured with a\nsingle-step approach. To overcome this limitation, we introduce CompAct, a\nnovel framework that employs an active strategy to condense extensive documents\nwithout losing key information. Our experiments demonstrate that CompAct brings\nsignificant improvements in both performance and compression rate on multi-hop\nquestion-answering (QA) benchmarks. CompAct flexibly operates as a\ncost-efficient plug-in module with various off-the-shelf retrievers or readers,\nachieving exceptionally high compression rates (47x).\n","authors":["Chanwoong Yoon","Taewhoo Lee","Hyeon Hwang","Minbyul Jeong","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2407.09014v1.pdf","comment":"Code available at https://github.com/dmis-lab/CompAct"},{"id":"http://arxiv.org/abs/2407.09011v1","updated":"2024-07-12T06:01:51Z","published":"2024-07-12T06:01:51Z","title":"One Stone, Four Birds: A Comprehensive Solution for QA System Using\n  Supervised Contrastive Learning","summary":"  This paper presents a novel and comprehensive solution to enhance both the\nrobustness and efficiency of question answering (QA) systems through supervised\ncontrastive learning (SCL). Training a high-performance QA system has become\nstraightforward with pre-trained language models, requiring only a small amount\nof data and simple fine-tuning. However, despite recent advances, existing QA\nsystems still exhibit significant deficiencies in functionality and training\nefficiency. We address the functionality issue by defining four key tasks: user\ninput intent classification, out-of-domain input detection, new intent\ndiscovery, and continual learning. We then leverage a unified SCL-based\nrepresentation learning method to efficiently build an intra-class compact and\ninter-class scattered feature space, facilitating both known intent\nclassification and unknown intent detection and discovery. Consequently, with\nminimal additional tuning on downstream tasks, our approach significantly\nimproves model efficiency and achieves new state-of-the-art performance across\nall tasks.\n","authors":["Bo Wang","Tsunenori Mine"],"pdf_url":"https://arxiv.org/pdf/2407.09011v1.pdf","comment":"14 pages, under review"},{"id":"http://arxiv.org/abs/2407.09007v1","updated":"2024-07-12T05:55:22Z","published":"2024-07-12T05:55:22Z","title":"Benchmarking Language Model Creativity: A Case Study on Code Generation","summary":"  As LLMs become increasingly prevalent, it is interesting to consider how\n``creative'' these models can be. From cognitive science, creativity consists\nof at least two key characteristics: \\emph{convergent} thinking (purposefulness\nto achieve a given goal) and \\emph{divergent} thinking (adaptability to new\nenvironments or constraints) \\citep{runco2003critical}. In this work, we\nintroduce a framework for quantifying LLM creativity that incorporates the two\ncharacteristics. This is achieved by (1) Denial Prompting pushes LLMs to come\nup with more creative solutions to a given problem by incrementally imposing\nnew constraints on the previous solution, compelling LLMs to adopt new\nstrategies, and (2) defining and computing the NeoGauge metric which examines\nboth convergent and divergent thinking in the generated creative responses by\nLLMs. We apply the proposed framework on Codeforces problems, a natural data\nsource for collecting human coding solutions. We quantify NeoGauge for various\nproprietary and open-source models and find that even the most creative model,\nGPT-4, still falls short of demonstrating human-like creativity. We also\nexperiment with advanced reasoning strategies (MCTS, self-correction, etc.) and\nobserve no significant improvement in creativity. As a by-product of our\nanalysis, we release NeoCoder dataset for reproducing our results on future\nmodels.\n","authors":["Yining Lu","Dixuan Wang","Tianjian Li","Dongwei Jiang","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2407.09007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01047v3","updated":"2024-07-12T05:47:31Z","published":"2024-07-01T07:56:36Z","title":"Development of Cognitive Intelligence in Pre-trained Language Models","summary":"  Recent studies show evidence for emergent cognitive abilities in Large\nPre-trained Language Models (PLMs). The increasing cognitive alignment of these\nmodels has made them candidates for cognitive science theories. Prior research\ninto the emergent cognitive abilities of PLMs has largely been path-independent\nto model training, i.e., has focused on the final model weights and not the\nintermediate steps. However, building plausible models of human cognition using\nPLMs would benefit from considering the developmental alignment of their\nperformance during training to the trajectories of children's thinking. Guided\nby psychometric tests of human intelligence, we choose four sets of tasks to\ninvestigate the alignment of ten popular families of PLMs and evaluate their\navailable intermediate and final training steps. These tasks are Numerical\nability, Linguistic abilities, Conceptual understanding, and Fluid reasoning.\nWe find a striking regularity: regardless of model size, the developmental\ntrajectories of PLMs consistently exhibit a window of maximal alignment to\nhuman cognitive development. Before that window, training appears to endow\n\"blank slate\" models with the requisite structure to be poised to rapidly learn\nfrom experience. After that window, training appears to serve the engineering\ngoal of reducing loss but not the scientific goal of increasing alignment with\nhuman cognition.\n","authors":["Raj Sanjay Shah","Khushi Bhardwaj","Sashank Varma"],"pdf_url":"https://arxiv.org/pdf/2407.01047v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08995v1","updated":"2024-07-12T05:26:24Z","published":"2024-07-12T05:26:24Z","title":"Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs","summary":"  Recent advancements in LLMs have showcased their remarkable role-playing\ncapabilities, able to accurately simulate the dialogue styles and cognitive\nprocesses of various roles based on different instructions and contexts.\nStudies indicate that assigning LLMs the roles of experts, a strategy known as\nrole-play prompting, can enhance their performance in the corresponding\ndomains. However, the prompt needs to be manually designed for the given\nproblem, requiring certain expertise and iterative modifications. To this end,\nwe propose self-prompt tuning, making LLMs themselves generate role-play\nprompts through fine-tuning. Leveraging the LIMA dataset as our foundational\ncorpus, we employ GPT-4 to annotate role-play prompts for each data points,\nresulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like\nLlama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned\nLLMs can automatically generate expert role prompts for any given question. We\nextensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and\nopen-ended question test. Our empirical results illustrate that self-prompt\ntuned LLMs outperform standard instruction tuned baselines across most\ndatasets. This highlights the great potential of utilizing fine-tuning to\nenable LLMs to self-prompt, thereby automating complex prompting strategies. We\nrelease the dataset, models, and code at this\n\\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}.\n","authors":["Aobo Kong","Shiwan Zhao","Hao Chen","Qicheng Li","Yong Qin","Ruiqi Sun","Xin Zhou","Jiaming Zhou","Haoqin Sun"],"pdf_url":"https://arxiv.org/pdf/2407.08995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19232v5","updated":"2024-07-12T05:16:30Z","published":"2024-04-30T03:29:30Z","title":"GRAMMAR: Grounded and Modular Methodology for Assessment of\n  Closed-Domain Retrieval-Augmented Language Model","summary":"  Retrieval-augmented Generation (RAG) systems have been actively studied and\ndeployed across various industries to query on domain-specific knowledge base.\nHowever, evaluating these systems presents unique challenges due to the\nscarcity of domain-specific queries and corresponding ground truths, as well as\na lack of systematic approaches to diagnosing the cause of failure cases --\nwhether they stem from knowledge deficits or issues related to system\nrobustness. To address these challenges, we introduce GRAMMAR (GRounded And\nModular Methodology for Assessment of RAG), an evaluation framework comprising\ntwo key elements: 1) a data generation process that leverages relational\ndatabases and LLMs to efficiently produce scalable query-answer pairs for\nevaluation. This method facilitates the separation of query logic from\nlinguistic variations, enabling the testing of hypotheses related to non-robust\ntextual forms; and 2) an evaluation framework that differentiates knowledge\ngaps from robustness and enables the identification of defective modules. Our\nempirical results underscore the limitations of current reference-free\nevaluation approaches and the reliability of GRAMMAR to accurately identify\nmodel vulnerabilities. For implementation details, refer to our GitHub\nrepository: https://github.com/xinzhel/grammar.\n","authors":["Xinzhe Li","Ming Liu","Shang Gao"],"pdf_url":"https://arxiv.org/pdf/2404.19232v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19449v2","updated":"2024-07-12T05:10:32Z","published":"2024-02-29T18:47:52Z","title":"Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent\n  on Language Models","summary":"  Adam has been shown to outperform gradient descent on large language models\nby a larger margin than on other tasks, but it is unclear why. We show that a\nkey factor in this performance gap is the heavy-tailed class imbalance found in\nlanguage tasks. When trained with gradient descent, the loss of infrequent\nwords decreases more slowly than the loss of frequent ones. This leads to a\nslow decrease on the average loss as most samples come from infrequent words.\nOn the other hand, Adam and sign-based methods are less sensitive to this\nproblem. To establish that this behavior is caused by class imbalance, we show\nempirically that it can be reproduced across architectures and data types, on\nlanguage transformers, vision CNNs, and linear models. On a linear model with\ncross-entropy loss, we show that class imbalance leads to imbalanced,\ncorrelated gradients and Hessians that have been hypothesized to benefit Adam.\nWe also prove that, in continuous time, gradient descent converges slowly on\nlow-frequency classes while sign descent does not.\n","authors":["Frederik Kunstner","Robin Yadav","Alan Milligan","Mark Schmidt","Alberto Bietti"],"pdf_url":"https://arxiv.org/pdf/2402.19449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08989v1","updated":"2024-07-12T04:50:17Z","published":"2024-07-12T04:50:17Z","title":"Robustness of LLMs to Perturbations in Text","summary":"  Having a clean dataset has been the foundational assumption of most natural\nlanguage processing (NLP) systems. However, properly written text is rarely\nfound in real-world scenarios and hence, oftentimes invalidates the\naforementioned foundational assumption. Recently, Large language models (LLMs)\nhave shown impressive performance, but can they handle the inevitable noise in\nreal-world data? This work tackles this critical question by investigating\nLLMs' resilience against morphological variations in text. To that end, we\nartificially introduce varying levels of noise into a diverse set of datasets\nand systematically evaluate LLMs' robustness against the corrupt variations of\nthe original text. Our findings show that contrary to popular beliefs,\ngenerative LLMs are quiet robust to noisy perturbations in text. This is a\ndeparture from pre-trained models like BERT or RoBERTa whose performance has\nbeen shown to be sensitive to deteriorating noisy text. Additionally, we test\nLLMs' resilience on multiple real-world benchmarks that closely mimic commonly\nfound errors in the wild. With minimal prompting, LLMs achieve a new\nstate-of-the-art on the benchmark tasks of Grammar Error Correction (GEC) and\nLexical Semantic Change (LSC). To empower future research, we also release a\ndataset annotated by humans stating their preference for LLM vs.\nhuman-corrected outputs along with the code to reproduce our results.\n","authors":["Ayush Singh","Navpreet Singh","Shubham Vatsal"],"pdf_url":"https://arxiv.org/pdf/2407.08989v1.pdf","comment":"8 pages, 1 figure, 6 tables, updated with results also from GPT-4,\n  LLaMa-3"},{"id":"http://arxiv.org/abs/2402.13468v2","updated":"2024-07-12T04:44:39Z","published":"2024-02-21T01:54:58Z","title":"STENCIL: Submodular Mutual Information Based Weak Supervision for\n  Cold-Start Active Learning","summary":"  As supervised fine-tuning of pre-trained models within NLP applications\nincreases in popularity, larger corpora of annotated data are required,\nespecially with increasing parameter counts in large language models. Active\nlearning, which attempts to mine and annotate unlabeled instances to improve\nmodel performance maximally fast, is a common choice for reducing the\nannotation cost; however, most methods typically ignore class imbalance and\neither assume access to initial annotated data or require multiple rounds of\nactive learning selection before improving rare classes. We present STENCIL,\nwhich utilizes a set of text exemplars and the recently proposed submodular\nmutual information to select a set of weakly labeled rare-class instances that\nare then strongly labeled by an annotator. We show that STENCIL improves\noverall accuracy by $10\\%-18\\%$ and rare-class F-1 score by $17\\%-40\\%$ on\nmultiple text classification datasets over common active learning methods\nwithin the class-imbalanced cold-start setting.\n","authors":["Nathan Beck","Adithya Iyer","Rishabh Iyer"],"pdf_url":"https://arxiv.org/pdf/2402.13468v2.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2312.05235v3","updated":"2024-07-12T04:21:46Z","published":"2023-12-08T18:33:11Z","title":"Generative AI in Higher Education: Seeing ChatGPT Through Universities'\n  Policies, Resources, and Guidelines","summary":"  The advancements in Generative Artificial Intelligence (GenAI) provide\nopportunities to enrich educational experiences, but also raise concerns about\nacademic integrity. Many educators have expressed anxiety and hesitation in\nintegrating GenAI in their teaching practices, and are in needs of\nrecommendations and guidance from their institutions that can support them to\nincorporate GenAI in their classrooms effectively. In order to respond to\nhigher educators' needs, this study aims to explore how universities and\neducators respond and adapt to the development of GenAI in their academic\ncontexts by analyzing academic policies and guidelines established by\ntop-ranked U.S. universities regarding the use of GenAI, especially ChatGPT.\nData sources include academic policies, statements, guidelines, and relevant\nresources provided by the top 100 universities in the U.S. Results show that\nthe majority of these universities adopt an open but cautious approach towards\nGenAI. Primary concerns lie in ethical usage, accuracy, and data privacy. Most\nuniversities actively respond and provide diverse types of resources, such as\nsyllabus templates, workshops, shared articles, and one-on-one consultations\nfocusing on a range of topics: general technical introduction, ethical\nconcerns, pedagogical applications, preventive strategies, data privacy,\nlimitations, and detective tools. The findings provide four practical\npedagogical implications for educators in teaching practices: accept its\npresence, align its use with learning objectives, evolve curriculum to prevent\nmisuse, and adopt multifaceted evaluation strategies rather than relying on AI\ndetectors. Two recommendations are suggested for educators in policy making:\nestablish discipline-specific policies and guidelines, and manage sensitive\ninformation carefully.\n","authors":["Hui Wang","Anh Dang","Zihao Wu","Son Mac"],"pdf_url":"https://arxiv.org/pdf/2312.05235v3.pdf","comment":"13 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.08978v1","updated":"2024-07-12T04:18:22Z","published":"2024-07-12T04:18:22Z","title":"Towards Chapter-to-Chapter Context-Aware Literary Translation via Large\n  Language Models","summary":"  Discourse phenomena in existing document-level translation datasets are\nsparse, which has been a fundamental obstacle in the development of\ncontext-aware machine translation models. Moreover, most existing\ndocument-level corpora and context-aware machine translation methods rely on an\nunrealistic assumption on sentence-level alignments. To mitigate these issues,\nwe first curate a novel dataset of Chinese-English literature, which consists\nof 160 books with intricate discourse structures. Then, we propose a more\npragmatic and challenging setting for context-aware translation, termed\nchapter-to-chapter (Ch2Ch) translation, and investigate the performance of\ncommonly-used machine translation models under this setting. Furthermore, we\nintroduce a potential approach of finetuning large language models (LLMs)\nwithin the domain of Ch2Ch literary translation, yielding impressive\nimprovements over baselines. Through our comprehensive analysis, we unveil that\nliterary translation under the Ch2Ch setting is challenging in nature, with\nrespect to both model learning methods and translation decoding algorithms.\n","authors":["Linghao Jin","Li An","Xuezhe Ma"],"pdf_url":"https://arxiv.org/pdf/2407.08978v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.14230v2","updated":"2024-07-12T03:47:21Z","published":"2024-06-20T11:51:00Z","title":"Raising the Bar: Investigating the Values of Large Language Models via\n  Generative Evolving Testing","summary":"  Warning: this paper contains model outputs exhibiting unethical information.\nLarge Language Models (LLMs) have achieved significant breakthroughs, but their\ngenerated unethical content poses potential risks. Measuring value alignment of\nLLMs becomes crucial for their regulation and responsible deployment. Numerous\ndatasets have been constructed to assess social bias, toxicity, and ethics in\nLLMs, but they suffer from evaluation chronoeffect, that is, as models rapidly\nevolve, existing data becomes leaked or undemanding, overestimating\never-developing LLMs. To tackle this problem, we propose GETA, a novel\ngenerative evolving testing approach that dynamically probes the underlying\nmoral baselines of LLMs. Distinct from previous adaptive testing methods that\nrely on static datasets with limited difficulty, GETA incorporates an\niteratively-updated item generator which infers each LLM's moral boundaries and\ngenerates difficulty-tailored testing items, accurately reflecting the true\nalignment extent. This process theoretically learns a joint distribution of\nitem and model response, with item difficulty and value conformity as latent\nvariables, where the generator co-evolves with the LLM, addressing\nchronoeffect. We evaluate various popular LLMs with diverse capabilities and\ndemonstrate that GETA can create difficulty-matching testing items and more\naccurately assess LLMs' values, better consistent with their performance on\nunseen OOD and i.i.d. items, laying the groundwork for future evaluation\nparadigms.\n","authors":["Han Jiang","Xiaoyuan Yi","Zhihua Wei","Shu Wang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2406.14230v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2407.08967v1","updated":"2024-07-12T03:31:11Z","published":"2024-07-12T03:31:11Z","title":"Empowering Few-Shot Relation Extraction with The Integration of\n  Traditional RE Methods and Large Language Models","summary":"  Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE)\nthat utilizes limited training instances, appeals to more researchers in\nNatural Language Processing (NLP) due to its capability to extract textual\ninformation in extremely low-resource scenarios. The primary methodologies\nemployed for FSRE have been fine-tuning or prompt tuning techniques based on\nPre-trained Language Models (PLMs). Recently, the emergence of Large Language\nModels (LLMs) has prompted numerous researchers to explore FSRE through\nIn-Context Learning (ICL). However, there are substantial limitations\nassociated with methods based on either traditional RE models or LLMs.\nTraditional RE models are hampered by a lack of necessary prior knowledge,\nwhile LLMs fall short in their task-specific capabilities for RE. To address\nthese shortcomings, we propose a Dual-System Augmented Relation Extractor\n(DSARE), which synergistically combines traditional RE models with LLMs.\nSpecifically, DSARE innovatively injects the prior knowledge of LLMs into\ntraditional RE models, and conversely enhances LLMs' task-specific aptitude for\nRE through relation extraction augmentation. Moreover, an Integrated Prediction\nmodule is employed to jointly consider these two respective predictions and\nderive the final results. Extensive experiments demonstrate the efficacy of our\nproposed method.\n","authors":["Ye Liu","Kai Zhang","Aoran Gan","Linan Yue","Feng Hu","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.08967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08959v1","updated":"2024-07-12T03:21:57Z","published":"2024-07-12T03:21:57Z","title":"Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for\n  Few-shot Hierarchical Text Classification","summary":"  Recently, various pre-trained language models (PLMs) have been proposed to\nprove their impressive performances on a wide range of few-shot tasks. However,\nlimited by the unstructured prior knowledge in PLMs, it is difficult to\nmaintain consistent performance on complex structured scenarios, such as\nhierarchical text classification (HTC), especially when the downstream data is\nextremely scarce. The main challenge is how to transfer the unstructured\nsemantic space in PLMs to the downstream domain hierarchy. Unlike previous work\non HTC which directly performs multi-label classification or uses graph neural\nnetwork (GNN) to inject label hierarchy, in this work, we study the HTC problem\nunder a few-shot setting to adapt knowledge in PLMs from an unstructured manner\nto the downstream hierarchy. Technically, we design a simple yet effective\nmethod named Hierarchical Iterative Conditional Random Field (HierICRF) to\nsearch the most domain-challenging directions and exquisitely crafts\ndomain-hierarchy adaptation as a hierarchical iterative language modeling\nproblem, and then it encourages the model to make hierarchical consistency\nself-correction during the inference, thereby achieving knowledge transfer with\nhierarchical consistency preservation. We perform HierICRF on various\narchitectures, and extensive experiments on two popular HTC datasets\ndemonstrate that prompt with HierICRF significantly boosts the few-shot HTC\nperformance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29%\nto 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot\nsettings, while remaining SOTA hierarchical consistency performance.\n","authors":["Ke Ji","Peng Wang","Wenjun Ke","Guozheng Li","Jiajun Liu","Jingsheng Gao","Ziyu Shang"],"pdf_url":"https://arxiv.org/pdf/2407.08959v1.pdf","comment":"9 pages, 2 figures, Accepted by IJCAI2024"},{"id":"http://arxiv.org/abs/2407.08952v1","updated":"2024-07-12T03:15:01Z","published":"2024-07-12T03:15:01Z","title":"Detect, Investigate, Judge and Determine: A Novel LLM-based Framework\n  for Few-shot Fake News Detection","summary":"  Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Augmented Fake News Detection\n(DAFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DAFND first identifies the keywords of each news\narticle through a Detection Module. Subsequently, DAFND creatively designs an\nInvestigation Module to retrieve inside and outside valuable information\nconcerning to the current news, followed by another Judge Module to derive its\nrespective two prediction results. Finally, a Determination Module further\nintegrates these two predictions and derives the final result. Extensive\nexperiments on two publicly available datasets show the efficacy of our\nproposed method, particularly in low-resource settings.\n","authors":["Ye Liu","Jiajun Zhu","Kai Zhang","Haoyu Tang","Yanghai Zhang","Xukai Liu","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.08952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03645v2","updated":"2024-07-12T03:07:04Z","published":"2024-07-04T05:35:47Z","title":"Continual Learning Optimizations for Auto-regressive Decoder of\n  Multilingual ASR systems","summary":"  Continual Learning (CL) involves fine-tuning pre-trained models with new data\nwhile maintaining the performance on the pre-trained data. This is particularly\nrelevant for expanding multilingual ASR (MASR) capabilities. However, existing\nCL methods, mainly designed for computer vision and reinforcement learning\ntasks, often yield sub-optimal results when directly applied to MASR. We\nhypothesise that this is because CL of the auto-regressive decoder in the MASR\nmodel is difficult. To verify this, we propose four optimizations on the\ndecoder. They include decoder-layer gradient surgery, freezing unused token\nembeddings, suppressing output of newly added tokens, and learning rate\nre-scaling. Our experiments on adapting Whisper to 10 unseen languages from the\nCommon Voice dataset demonstrate that these optimizations reduce the Average\nWord Error Rate (AWER) of pretrained languages from 14.2% to 12.4% compared\nwith Experience Replay, without compromising the AWER of new languages.\n","authors":["Chin Yuen Kwok","Jia Qi Yip","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2407.03645v2.pdf","comment":"Accepted by INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.08940v1","updated":"2024-07-12T02:55:13Z","published":"2024-07-12T02:55:13Z","title":"Large Language Models as Biomedical Hypothesis Generators: A\n  Comprehensive Evaluation","summary":"  The rapid growth of biomedical knowledge has outpaced our ability to\nefficiently extract insights and generate novel hypotheses. Large language\nmodels (LLMs) have emerged as a promising tool to revolutionize knowledge\ninteraction and potentially accelerate biomedical discovery. In this paper, we\npresent a comprehensive evaluation of LLMs as biomedical hypothesis generators.\nWe construct a dataset of background-hypothesis pairs from biomedical\nliterature, carefully partitioned into training, seen, and unseen test sets\nbased on publication date to mitigate data contamination. Using this dataset,\nwe assess the hypothesis generation capabilities of top-tier instructed models\nin zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of\nuncertainty, a crucial aspect of scientific discovery, we incorporate tool use\nand multi-agent interactions in our evaluation framework. Furthermore, we\npropose four novel metrics grounded in extensive literature review to evaluate\nthe quality of generated hypotheses, considering both LLM-based and human\nassessments. Our experiments yield two key findings: 1) LLMs can generate novel\nand validated hypotheses, even when tested on literature unseen during\ntraining, and 2) Increasing uncertainty through multi-agent interactions and\ntool use can facilitate diverse candidate generation and improve zero-shot\nhypothesis generation performance. However, we also observe that the\nintegration of additional knowledge through few-shot learning and tool use may\nnot always lead to performance gains, highlighting the need for careful\nconsideration of the type and scope of external knowledge incorporated. These\nfindings underscore the potential of LLMs as powerful aids in biomedical\nhypothesis generation and provide valuable insights to guide further research\nin this area.\n","authors":["Biqing Qi","Kaiyan Zhang","Kai Tian","Haoxiang Li","Zhang-Ren Chen","Sihang Zeng","Ermo Hua","Hu Jinfang","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.08940v1.pdf","comment":"Accepted to COLM 2024. This is an extended version of the paper at\n  arXiv:2311.05965"},{"id":"http://arxiv.org/abs/2407.08937v1","updated":"2024-07-12T02:49:13Z","published":"2024-07-12T02:49:13Z","title":"Self-Evolving GPT: A Lifelong Autonomous Experiential Learner","summary":"  To improve the performance of large language models (LLMs), researchers have\nexplored providing LLMs with textual task-solving experience via prompts.\nHowever, they rely on manual efforts to acquire and apply such experience for\neach task, which is not feasible for the growing demand for LLMs and the\nvariety of user questions. To address this issue, we design a lifelong\nautonomous experiential learning framework based on LLMs to explore whether\nLLMs can imitate human ability for learning and utilizing experience. It\nautonomously learns and accumulates experience through experience transfer and\ninduction, categorizing the types of input questions to select which\naccumulated experience to employ for them. Experimental results on six widely\nused NLP datasets show that our framework performs reliably in each\nintermediate step and effectively improves the performance of GPT-3.5 and\nGPT-4. This validates the feasibility of using LLMs to mimic human experiential\nlearning and application capabilities. Additionally, we provide a detailed\nanalysis of the behavior of our framework at each step.\n","authors":["Jinglong Gao","Xiao Ding","Yiming Cui","Jianbai Zhao","Hepeng Wang","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2407.08937v1.pdf","comment":"Accepted by ACL 2024 MAIN"},{"id":"http://arxiv.org/abs/2403.02558v2","updated":"2024-07-12T02:46:33Z","published":"2024-03-05T00:27:43Z","title":"The Minimum Information about CLinical Artificial Intelligence Checklist\n  for Generative Modeling Research (MI-CLAIM-GEN)","summary":"  Recent advances in generative models, including large language models (LLMs),\nvision language models (VLMs), and diffusion models, have accelerated the field\nof natural language and image processing in medicine and marked a significant\nparadigm shift in how biomedical models can be developed and deployed. While\nthese models are highly adaptable to new tasks, scaling and evaluating their\nusage presents new challenges not addressed in previous frameworks. In\nparticular, the ability of these models to produce useful outputs with little\nto no specialized training data (\"zero-\" or \"few-shot\" approaches), as well as\nthe open-ended nature of their outputs, necessitate the development of new\nguidelines for robust reporting of clinical generative model research. In\nresponse to gaps in standards and best practices for the development of\nclinical AI tools identified by US Executive Order 141103 and several emerging\nnational networks for clinical AI evaluation, we begin to formalize some of\nthese guidelines by building on the original MI-CLAIM checklist. The new\nchecklist, MI-CLAIM-GEN (Table 1), aims to address differences in training,\nevaluation, interpretability, and reproducibility of new generative models\ncompared to non-generative (\"predictive\") AI models. This MI-CLAIM-GEN\nchecklist also seeks to clarify cohort selection reporting with unstructured\nclinical data and adds additional items on alignment with ethical standards for\nclinical AI research.\n","authors":["Brenda Y. Miao","Irene Y. Chen","Christopher YK Williams","JaysÃ³n Davidson","Augusto Garcia-Agundez","Shenghuan Sun","Travis Zack","Suchi Saria","Rima Arnaout","Giorgio Quer","Hossein J. Sadaei","Ali Torkamani","Brett Beaulieu-Jones","Bin Yu","Milena Gianfrancesco","Atul J. Butte","Beau Norgeot","Madhumita Sushil"],"pdf_url":"https://arxiv.org/pdf/2403.02558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08103v2","updated":"2024-07-12T01:02:43Z","published":"2024-07-11T00:25:01Z","title":"Automata-based constraints for language model decoding","summary":"  LMs are often expected to generate strings in some formal language; for\nexample, structured data, API calls, or code snippets. Although LMs can be\ntuned to improve their adherence to formal syntax, this does not guarantee\nconformance, especially with smaller LMs suitable for large-scale deployment.\nIn addition, tuning requires significant resources, making it impractical for\nuncommon or task-specific formats. To prevent downstream parsing errors we\nwould ideally constrain the LM to only produce valid output, but this is\nseverely complicated by tokenization, which is typically both ambiguous and\nmisaligned with the formal grammar. We solve these issues through the\napplication of automata theory, deriving an efficient closed-form solution for\nthe regular languages, a broad class of formal languages with many practical\napplications, including API calls or schema-guided JSON and YAML. We also\ndiscuss pragmatic extensions for coping with the issue of high branching\nfactor. Finally, we extend our techniques to deterministic context-free\nlanguages, which similarly admit an efficient closed-form solution. In spite of\nits flexibility and representative power, our approach only requires access to\nper-token decoding logits and lowers into simple calculations that are\nindependent of LM size, making it both efficient and easy to apply to almost\nany LM architecture.\n","authors":["Terry Koo","Frederick Liu","Luheng He"],"pdf_url":"https://arxiv.org/pdf/2407.08103v2.pdf","comment":"Accepted to CoLM 2024"},{"id":"http://arxiv.org/abs/2402.02619v8","updated":"2024-07-12T00:34:01Z","published":"2024-02-04T21:33:18Z","title":"Increasing Trust in Language Models through the Reuse of Verified\n  Circuits","summary":"  Language Models (LMs) are increasingly used for a wide range of prediction\ntasks, but their training can often neglect rare edge cases, reducing their\nreliability. Here, we define a stringent standard of trustworthiness whereby\nthe task algorithm and circuit implementation must be verified, accounting for\nedge cases, with no known failure modes. We show that a model can be trained to\nmeet this standard if built using mathematically and logically specified\nframeworks. In this paper, we fully verify an auto-regressive transformer model\nfor n-digit integer addition. To exhibit the reusability of verified modules,\nwe insert the trained integer addition model into a larger untrained model and\ntrain the combined model to perform both addition and subtraction. We find\nextensive reuse of the addition circuits for both tasks, easing verification of\nthe more complex subtractor model. We discuss how inserting verified task\nmodules into LMs can leverage model reuse to improve verifiability and\ntrustworthiness of language models built using them. The reuse of verified\ncircuits reduces the effort to verify more complex composite models which we\nbelieve to be a significant step towards safety of language models.\n","authors":["Philip Quirke","Clement Neo","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2402.02619v8.pdf","comment":"8 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.08898v1","updated":"2024-07-12T00:07:43Z","published":"2024-07-12T00:07:43Z","title":"IDAT: A Multi-Modal Dataset and Toolkit for Building and Evaluating\n  Interactive Task-Solving Agents","summary":"  Seamless interaction between AI agents and humans using natural language\nremains a key goal in AI research. This paper addresses the challenges of\ndeveloping interactive agents capable of understanding and executing grounded\nnatural language instructions through the IGLU competition at NeurIPS. Despite\nadvancements, challenges such as a scarcity of appropriate datasets and the\nneed for effective evaluation platforms persist. We introduce a scalable data\ncollection tool for gathering interactive grounded language instructions within\na Minecraft-like environment, resulting in a Multi-Modal dataset with around\n9,000 utterances and over 1,000 clarification questions. Additionally, we\npresent a Human-in-the-Loop interactive evaluation platform for qualitative\nanalysis and comparison of agent performance through multi-turn communication\nwith human annotators. We offer to the community these assets referred to as\nIDAT (IGLU Dataset And Toolkit) which aim to advance the development of\nintelligent, interactive AI agents and provide essential resources for further\nresearch.\n","authors":["Shrestha Mohanty","Negar Arabzadeh","Andrea Tupini","Yuxuan Sun","Alexey Skrynnik","Artem Zholus","Marc-Alexandre CÃ´tÃ©","Julia Kiseleva"],"pdf_url":"https://arxiv.org/pdf/2407.08898v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.09475v1","updated":"2024-07-12T17:57:00Z","published":"2024-07-12T17:57:00Z","title":"Adaptive Prediction Ensemble: Improving Out-of-Distribution\n  Generalization of Motion Forecasting","summary":"  Deep learning-based trajectory prediction models for autonomous driving often\nstruggle with generalization to out-of-distribution (OOD) scenarios, sometimes\nperforming worse than simple rule-based models. To address this limitation, we\npropose a novel framework, Adaptive Prediction Ensemble (APE), which integrates\ndeep learning and rule-based prediction experts. A learned routing function,\ntrained concurrently with the deep learning model, dynamically selects the most\nreliable prediction based on the input scenario. Our experiments on large-scale\ndatasets, including Waymo Open Motion Dataset (WOMD) and Argoverse, demonstrate\nimprovement in zero-shot generalization across datasets. We show that our\nmethod outperforms individual prediction models and other variants,\nparticularly in long-horizon prediction and scenarios with a high proportion of\nOOD data. This work highlights the potential of hybrid approaches for robust\nand generalizable motion prediction in autonomous driving.\n","authors":["Jinning Li","Jiachen Li","Sangjae Bae","David Isele"],"pdf_url":"https://arxiv.org/pdf/2407.09475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09473v1","updated":"2024-07-12T17:55:08Z","published":"2024-07-12T17:55:08Z","title":"StyleSplat: 3D Object Style Transfer with Gaussian Splatting","summary":"  Recent advancements in radiance fields have opened new avenues for creating\nhigh-quality 3D assets and scenes. Style transfer can enhance these 3D assets\nwith diverse artistic styles, transforming creative expression. However,\nexisting techniques are often slow or unable to localize style transfer to\nspecific objects. We introduce StyleSplat, a lightweight method for stylizing\n3D objects in scenes represented by 3D Gaussians from reference style images.\nOur approach first learns a photorealistic representation of the scene using 3D\nGaussian splatting while jointly segmenting individual 3D objects. We then use\na nearest-neighbor feature matching loss to finetune the Gaussians of the\nselected objects, aligning their spherical harmonic coefficients with the style\nimage to ensure consistency and visual appeal. StyleSplat allows for quick,\ncustomizable style transfer and localized stylization of multiple objects\nwithin a scene, each with a different style. We demonstrate its effectiveness\nacross various 3D scenes and styles, showcasing enhanced control and\ncustomization in 3D creation.\n","authors":["Sahil Jain","Avik Kuthiala","Prabhdeep Singh Sethi","Prakanshul Saxena"],"pdf_url":"https://arxiv.org/pdf/2407.09473v1.pdf","comment":"for code and results, see http://bernard0047.github.io/stylesplat"},{"id":"http://arxiv.org/abs/2404.05052v2","updated":"2024-07-12T17:52:03Z","published":"2024-04-07T19:23:28Z","title":"Facial Affective Behavior Analysis with Instruction Tuning","summary":"  Facial affective behavior analysis (FABA) is crucial for understanding human\nmental states from images. However, traditional approaches primarily deploy\nmodels to discriminate among discrete emotion categories, and lack the fine\ngranularity and reasoning capability for complex facial behaviors. The advent\nof Multi-modal Large Language Models (MLLMs) has been proven successful in\ngeneral visual understanding tasks. However, directly harnessing MLLMs for FABA\nis challenging due to the scarcity of datasets and benchmarks, neglecting\nfacial prior knowledge, and low training efficiency. To address these\nchallenges, we introduce (i) an instruction-following dataset for two FABA\ntasks, e.g., emotion and action unit recognition, (ii) a benchmark FABA-Bench\nwith a new metric considering both recognition and generation ability, and\n(iii) a new MLLM \"EmoLA\" as a strong baseline to the community. Our initiative\non the dataset and benchmarks reveal the nature and rationale of facial\naffective behaviors, i.e., fine-grained facial movement, interpretability, and\nreasoning. Moreover, to build an effective and efficient FABA MLLM, we\nintroduce a facial prior expert module with face structure knowledge and a\nlow-rank adaptation module into pre-trained MLLM. We conduct extensive\nexperiments on FABA-Bench and four commonly-used FABA datasets. The results\ndemonstrate that the proposed facial prior expert can boost the performance and\nEmoLA achieves the best results on our FABA-Bench. On commonly-used FABA\ndatasets, EmoLA is competitive rivaling task-specific state-of-the-art models.\n","authors":["Yifan Li","Anh Dao","Wentao Bao","Zhen Tan","Tianlong Chen","Huan Liu","Yu Kong"],"pdf_url":"https://arxiv.org/pdf/2404.05052v2.pdf","comment":"V2.0, project page: https://johnx69.github.io/FABA/"},{"id":"http://arxiv.org/abs/2403.19649v2","updated":"2024-07-12T17:48:07Z","published":"2024-03-28T17:57:27Z","title":"GraspXL: Generating Grasping Motions for Diverse Objects at Scale","summary":"  Human hands possess the dexterity to interact with diverse objects such as\ngrasping specific parts of the objects and/or approaching them from desired\ndirections. More importantly, humans can grasp objects of any shape without\nobject-specific skills. Recent works synthesize grasping motions following\nsingle objectives such as a desired approach heading direction or a grasping\narea. Moreover, they usually rely on expensive 3D hand-object data during\ntraining and inference, which limits their capability to synthesize grasping\nmotions for unseen objects at scale. In this paper, we unify the generation of\nhand-object grasping motions across multiple motion objectives, diverse object\nshapes and dexterous hand morphologies in a policy learning framework GraspXL.\nThe objectives are composed of the graspable area, heading direction during\napproach, wrist rotation, and hand position. Without requiring any 3D\nhand-object interaction data, our policy trained with 58 objects can robustly\nsynthesize diverse grasping motions for more than 500k unseen objects with a\nsuccess rate of 82.2%. At the same time, the policy adheres to objectives,\nwhich enables the generation of diverse grasps per object. Moreover, we show\nthat our framework can be deployed to different dexterous hands and work with\nreconstructed or generated objects. We quantitatively and qualitatively\nevaluate our method to show the efficacy of our approach. Our model, code, and\nthe large-scale generated motions are available at\nhttps://eth-ait.github.io/graspxl/.\n","authors":["Hui Zhang","Sammy Christen","Zicong Fan","Otmar Hilliges","Jie Song"],"pdf_url":"https://arxiv.org/pdf/2403.19649v2.pdf","comment":"Camera ready for ECCV2024. Project Page:\n  https://eth-ait.github.io/graspxl/"},{"id":"http://arxiv.org/abs/2407.09437v1","updated":"2024-07-12T17:14:33Z","published":"2024-07-12T17:14:33Z","title":"Let Me DeCode You: Decoder Conditioning with Tabular Data","summary":"  Training deep neural networks for 3D segmentation tasks can be challenging,\noften requiring efficient and effective strategies to improve model\nperformance. In this study, we introduce a novel approach, DeCode, that\nutilizes label-derived features for model conditioning to support the decoder\nin the reconstruction process dynamically, aiming to enhance the efficiency of\nthe training process. DeCode focuses on improving 3D segmentation performance\nthrough the incorporation of conditioning embedding with learned numerical\nrepresentation of 3D-label shape features. Specifically, we develop an\napproach, where conditioning is applied during the training phase to guide the\nnetwork toward robust segmentation. When labels are not available during\ninference, our model infers the necessary conditioning embedding directly from\nthe input data, thanks to a feed-forward network learned during the training\nphase. This approach is tested using synthetic data and cone-beam computed\ntomography (CBCT) images of teeth. For CBCT, three datasets are used: one\npublicly available and two in-house. Our results show that DeCode significantly\noutperforms traditional, unconditioned models in terms of generalization to\nunseen data, achieving higher accuracy at a reduced computational cost. This\nwork represents the first of its kind to explore conditioning strategies in 3D\ndata segmentation, offering a novel and more efficient method for leveraging\nannotated data. Our code, pre-trained models are publicly available at\nhttps://github.com/SanoScience/DeCode .\n","authors":["Tomasz SzczepaÅski","Michal K. Grzeszczyk","Szymon PÅotka","Arleta Adamowicz","Piotr Fudalej","PrzemysÅaw Korzeniowski","Tomasz TrzciÅski","Arkadiusz Sitek"],"pdf_url":"https://arxiv.org/pdf/2407.09437v1.pdf","comment":"Accepted for the 27th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2024"},{"id":"http://arxiv.org/abs/2307.15250v3","updated":"2024-07-12T17:10:55Z","published":"2023-07-28T01:20:12Z","title":"D2S: Representing sparse descriptors and 3D coordinates for camera\n  relocalization","summary":"  State-of-the-art visual localization methods mostly rely on complex\nprocedures to match local descriptors and 3D point clouds. However, these\nprocedures can incur significant costs in terms of inference, storage, and\nupdates over time. In this study, we propose a direct learning-based approach\nthat utilizes a simple network named D2S to represent complex local descriptors\nand their scene coordinates. Our method is characterized by its simplicity and\ncost-effectiveness. It solely leverages a single RGB image for localization\nduring the testing phase and only requires a lightweight model to encode a\ncomplex sparse scene. The proposed D2S employs a combination of a simple loss\nfunction and graph attention to selectively focus on robust descriptors while\ndisregarding areas such as clouds, trees, and several dynamic objects. This\nselective attention enables D2S to effectively perform a binary-semantic\nclassification for sparse descriptors. Additionally, we propose a simple\noutdoor dataset to evaluate the capabilities of visual localization methods in\nscene-specific generalization and self-updating from unlabeled observations.\nOur approach outperforms the state-of-the-art CNN-based methods in scene\ncoordinate regression in indoor and outdoor environments. It demonstrates the\nability to generalize beyond training data, including scenarios involving\ntransitions from day to night and adapting to domain shifts, even in the\nabsence of the labeled data sources. The source code, trained models, dataset,\nand demo videos are available at the following link:\nhttps://thpjp.github.io/d2s.\n","authors":["Bach-Thuan Bui","Huy-Hoang Bui","Dinh-Tuan Tran","Joo-Ho Lee"],"pdf_url":"https://arxiv.org/pdf/2307.15250v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09431v1","updated":"2024-07-12T17:03:14Z","published":"2024-07-12T17:03:14Z","title":"Rethinking temporal self-similarity for repetitive action counting","summary":"  Counting repetitive actions in long untrimmed videos is a challenging task\nthat has many applications such as rehabilitation. State-of-the-art methods\npredict action counts by first generating a temporal self-similarity matrix\n(TSM) from the sampled frames and then feeding the matrix to a predictor\nnetwork. The self-similarity matrix, however, is not an optimal input to a\nnetwork since it discards too much information from the frame-wise embeddings.\nWe thus rethink how a TSM can be utilized for counting repetitive actions and\npropose a framework that learns embeddings and predicts action start\nprobabilities at full temporal resolution. The number of repeated actions is\nthen inferred from the action start probabilities. In contrast to current\napproaches that have the TSM as an intermediate representation, we propose a\nnovel loss based on a generated reference TSM, which enforces that the\nself-similarity of the learned frame-wise embeddings is consistent with the\nself-similarity of repeated actions. The proposed framework achieves\nstate-of-the-art results on three datasets, i.e., RepCount, UCFRep, and\nCountix.\n","authors":["Yanan Luo","Jinhui Yi","Yazan Abu Farha","Moritz Wolter","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2407.09431v1.pdf","comment":"Accepted to ICIP 2024"},{"id":"http://arxiv.org/abs/2406.04313v4","updated":"2024-07-12T16:51:07Z","published":"2024-06-06T17:57:04Z","title":"Improving Alignment and Robustness with Circuit Breakers","summary":"  AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that interrupts the models as they respond with harmful outputs\nwith \"circuit breakers.\" Existing techniques aimed at improving alignment, such\nas refusal training, are often bypassed. Techniques such as adversarial\ntraining try to plug these holes by countering specific attacks. As an\nalternative to refusal training and adversarial training, circuit-breaking\ndirectly controls the representations that are responsible for harmful outputs\nin the first place. Our technique can be applied to both text-only and\nmultimodal language models to prevent the generation of harmful outputs without\nsacrificing utility -- even in the presence of powerful unseen attacks.\nNotably, while adversarial robustness in standalone image recognition remains\nan open challenge, circuit breakers allow the larger multimodal system to\nreliably withstand image \"hijacks\" that aim to produce harmful content.\nFinally, we extend our approach to AI agents, demonstrating considerable\nreductions in the rate of harmful actions when they are under attack. Our\napproach represents a significant step forward in the development of reliable\nsafeguards to harmful behavior and adversarial attacks.\n","authors":["Andy Zou","Long Phan","Justin Wang","Derek Duenas","Maxwell Lin","Maksym Andriushchenko","Rowan Wang","Zico Kolter","Matt Fredrikson","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2406.04313v4.pdf","comment":"Code and models are available at\n  https://github.com/GraySwanAI/circuit-breakers"},{"id":"http://arxiv.org/abs/2404.08351v2","updated":"2024-07-12T16:45:46Z","published":"2024-04-12T09:31:55Z","title":"OmniSat: Self-Supervised Modality Fusion for Earth Observation","summary":"  The field of Earth Observations (EO) offers a wealth of data from diverse\nsensors, presenting a great opportunity for advancing self-supervised\nmultimodal learning. However, current multimodal EO datasets and models focus\non a single data type, either mono-date images or time series, which limits\ntheir expressivity. We introduce OmniSat, a novel architecture that exploits\nthe spatial alignment between multiple EO modalities to learn expressive\nmultimodal representations without labels. To demonstrate the advantages of\ncombining modalities of different natures, we augment two existing datasets\nwith new modalities. As demonstrated on three downstream tasks: forestry, land\ncover classification, and crop mapping. OmniSat can learn rich representations\nin an unsupervised manner, leading to improved performance in the semi- and\nfully-supervised settings, even when only one modality is available for\ninference. The code and dataset are available at\nhttps://github.com/gastruc/OmniSat.\n","authors":["Guillaume Astruc","Nicolas Gonthier","Clement Mallet","Loic Landrieu"],"pdf_url":"https://arxiv.org/pdf/2404.08351v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09622v2","updated":"2024-07-12T16:39:31Z","published":"2024-03-14T17:55:33Z","title":"Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering","summary":"  Visual text rendering poses a fundamental challenge for contemporary\ntext-to-image generation models, with the core problem lying in text encoder\ndeficiencies. To achieve accurate text rendering, we identify two crucial\nrequirements for text encoders: character awareness and alignment with glyphs.\nOur solution involves crafting a series of customized text encoder, Glyph-ByT5,\nby fine-tuning the character-aware ByT5 encoder using a meticulously curated\npaired glyph-text dataset. We present an effective method for integrating\nGlyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for\ndesign image generation. This significantly enhances text rendering accuracy,\nimproving it from less than $20\\%$ to nearly $90\\%$ on our design image\nbenchmark. Noteworthy is Glyph-SDXL's newfound ability for text paragraph\nrendering, achieving high spelling accuracy for tens to hundreds of characters\nwith automated multi-line layouts. Finally, through fine-tuning Glyph-SDXL with\na small set of high-quality, photorealistic images featuring visual text, we\nshowcase a substantial improvement in scene text rendering capabilities in\nopen-domain real images. These compelling outcomes aim to encourage further\nexploration in designing customized text encoders for diverse and challenging\ntasks.\n","authors":["Zeyu Liu","Weicong Liang","Zhanhao Liang","Chong Luo","Ji Li","Gao Huang","Yuhui Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.09622v2.pdf","comment":"ECCV 2024, 19 pages, 19 figures"},{"id":"http://arxiv.org/abs/2407.09413v1","updated":"2024-07-12T16:37:59Z","published":"2024-07-12T16:37:59Z","title":"SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers","summary":"  Seeking answers to questions within long scientific research articles is a\ncrucial area of study that aids readers in quickly addressing their inquiries.\nHowever, existing question-answering (QA) datasets based on scientific papers\nare limited in scale and focus solely on textual content. To address this\nlimitation, we introduce SPIQA (Scientific Paper Image Question Answering), the\nfirst large-scale QA dataset specifically designed to interpret complex figures\nand tables within the context of scientific research articles across various\ndomains of computer science. Leveraging the breadth of expertise and ability of\nmultimodal large language models (MLLMs) to understand figures, we employ\nautomatic and manual curation to create the dataset. We craft an\ninformation-seeking task involving multiple images that cover a wide variety of\nplots, charts, tables, schematic diagrams, and result visualizations. SPIQA\ncomprises 270K questions divided into training, validation, and three different\nevaluation splits. Through extensive experiments with 12 prominent foundational\nmodels, we evaluate the ability of current multimodal systems to comprehend the\nnuanced aspects of research articles. Additionally, we propose a\nChain-of-Thought (CoT) evaluation strategy with in-context retrieval that\nallows fine-grained, step-by-step assessment and improves model performance. We\nfurther explore the upper bounds of performance enhancement with additional\ntextual information, highlighting its promising potential for future research\nand the dataset's impact on revolutionizing how we interact with scientific\nliterature.\n","authors":["Shraman Pramanick","Rama Chellappa","Subhashini Venugopalan"],"pdf_url":"https://arxiv.org/pdf/2407.09413v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2404.00292v4","updated":"2024-07-12T16:28:46Z","published":"2024-03-30T08:51:23Z","title":"LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge\n  Retrieval-Augmented Diffusion","summary":"  Camouflaged vision perception is an important vision task with numerous\npractical applications. Due to the expensive collection and labeling costs,\nthis community struggles with a major bottleneck that the species category of\nits datasets is limited to a small number of object species. However, the\nexisting camouflaged generation methods require specifying the background\nmanually, thus failing to extend the camouflaged sample diversity in a low-cost\nmanner. In this paper, we propose a Latent Background Knowledge\nRetrieval-Augmented Diffusion (LAKE-RED) for camouflaged image generation. To\nour knowledge, our contributions mainly include: (1) For the first time, we\npropose a camouflaged generation paradigm that does not need to receive any\nbackground inputs. (2) Our LAKE-RED is the first knowledge retrieval-augmented\nmethod with interpretability for camouflaged generation, in which we propose an\nidea that knowledge retrieval and reasoning enhancement are separated\nexplicitly, to alleviate the task-specific challenges. Moreover, our method is\nnot restricted to specific foreground targets or backgrounds, offering a\npotential for extending camouflaged vision perception to more diverse domains.\n(3) Experimental results demonstrate that our method outperforms the existing\napproaches, generating more realistic camouflage images.\n","authors":["Pancheng Zhao","Peng Xu","Pengda Qin","Deng-Ping Fan","Zhicheng Zhang","Guoli Jia","Bowen Zhou","Jufeng Yang"],"pdf_url":"https://arxiv.org/pdf/2404.00292v4.pdf","comment":"Accepted by CVPR 2024, Fig.2 and Equation 4 revised"},{"id":"http://arxiv.org/abs/2403.09302v2","updated":"2024-07-12T16:27:06Z","published":"2024-03-14T11:49:43Z","title":"StainFuser: Controlling Diffusion for Faster Neural Style Transfer in\n  Multi-Gigapixel Histology Images","summary":"  Stain normalization algorithms aim to transform the color and intensity\ncharacteristics of a source multi-gigapixel histology image to match those of a\ntarget image, mitigating inconsistencies in the appearance of stains used to\nhighlight cellular components in the images. We propose a new approach,\nStainFuser, which treats this problem as a style transfer task using a novel\nConditional Latent Diffusion architecture, eliminating the need for handcrafted\ncolor components. With this method, we curate SPI-2M the largest stain\nnormalization dataset to date of over 2 million histology images with neural\nstyle transfer for high-quality transformations. Trained on this data,\nStainFuser outperforms current state-of-the-art deep learning and handcrafted\nmethods in terms of the quality of normalized images and in terms of downstream\nmodel performance on the CoNIC dataset.\n","authors":["Robert Jewsbury","Ruoyu Wang","Abhir Bhalerao","Nasir Rajpoot","Quoc Dang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.09302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10208v2","updated":"2024-07-12T16:26:40Z","published":"2024-06-14T17:44:09Z","title":"Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual\n  Visual Text Rendering","summary":"  Recently, Glyph-ByT5 has achieved highly accurate visual text rendering\nperformance in graphic design images. However, it still focuses solely on\nEnglish and performs relatively poorly in terms of visual appeal. In this work,\nwe address these two fundamental limitations by presenting Glyph-ByT5-v2 and\nGlyph-SDXL-v2, which not only support accurate visual text rendering for 10\ndifferent languages but also achieve much better aesthetic quality. To achieve\nthis, we make the following contributions: (i) creating a high-quality\nmultilingual glyph-text and graphic design dataset consisting of more than 1\nmillion glyph-text pairs and 10 million graphic design image-text pairs\ncovering nine other languages, (ii) building a multilingual visual paragraph\nbenchmark consisting of 1,000 prompts, with 100 for each language, to assess\nmultilingual visual spelling accuracy, and (iii) leveraging the latest\nstep-aware preference learning approach to enhance the visual aesthetic\nquality. With the combination of these techniques, we deliver a powerful\ncustomized multilingual text encoder, Glyph-ByT5-v2, and a strong aesthetic\ngraphic generation model, Glyph-SDXL-v2, that can support accurate spelling in\n10 different languages. We perceive our work as a significant advancement,\nconsidering that the latest DALL-E3 and Ideogram 1.0 still struggle with the\nmultilingual visual text rendering task.\n","authors":["Zeyu Liu","Weicong Liang","Yiming Zhao","Bohan Chen","Lin Liang","Lijuan Wang","Ji Li","Yuhui Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.10208v2.pdf","comment":"Project page: https://glyph-byt5-v2.github.io/"},{"id":"http://arxiv.org/abs/2407.09392v1","updated":"2024-07-12T16:16:24Z","published":"2024-07-12T16:16:24Z","title":"Open-Canopy: A Country-Scale Benchmark for Canopy Height Estimation at\n  Very High Resolution","summary":"  Estimating canopy height and canopy height change at meter resolution from\nsatellite imagery has numerous applications, such as monitoring forest health,\nlogging activities, wood resources, and carbon stocks. However, many existing\nforest datasets are based on commercial or closed data sources, restricting the\nreproducibility and evaluation of new approaches. To address this gap, we\nintroduce Open-Canopy, the first open-access and country-scale benchmark for\nvery high resolution (1.5 m) canopy height estimation. Covering more than\n87,000 km$^2$ across France, Open-Canopy combines SPOT satellite imagery with\nhigh resolution aerial LiDAR data. We also propose Open-Canopy-$\\Delta$, the\nfirst benchmark for canopy height change detection between two images taken at\ndifferent years, a particularly challenging task even for recent models. To\nestablish a robust foundation for these benchmarks, we evaluate a comprehensive\nlist of state-of-the-art computer vision models for canopy height estimation.\nThe dataset and associated codes can be accessed at\nhttps://github.com/fajwel/Open-Canopy.\n","authors":["Fajwel Fogel","Yohann Perron","Nikola Besic","Laurent Saint-AndrÃ©","AgnÃ¨s Pellissier-Tanon","Martin Schwartz","Thomas Boudras","Ibrahim Fayad","Alexandre d'Aspremont","Loic Landrieu","Phillipe Ciais"],"pdf_url":"https://arxiv.org/pdf/2407.09392v1.pdf","comment":"22 pages, 8 figures, Submitted to NeurIPS 2024 Datasets and\n  Benchmarks Track"},{"id":"http://arxiv.org/abs/2405.11067v2","updated":"2024-07-12T16:14:33Z","published":"2024-05-17T19:49:02Z","title":"Bayesian Learning-driven Prototypical Contrastive Loss for\n  Class-Incremental Learning","summary":"  The primary objective of methods in continual learning is to learn tasks in a\nsequential manner over time from a stream of data, while mitigating the\ndetrimental phenomenon of catastrophic forgetting. In this paper, we focus on\nlearning an optimal representation between previous class prototypes and newly\nencountered ones. We propose a prototypical network with a Bayesian\nlearning-driven contrastive loss (BLCL) tailored specifically for\nclass-incremental learning scenarios. Therefore, we introduce a contrastive\nloss that incorporates new classes into the latent representation by reducing\nthe intra-class distance and increasing the inter-class distance. Our approach\ndynamically adapts the balance between the cross-entropy and contrastive loss\nfunctions with a Bayesian learning technique. Empirical evaluations conducted\non both the CIFAR-10 and CIFAR-100 dataset for image classification and images\nof a GNSS-based dataset for interference classification validate the efficacy\nof our method, showcasing its superiority over existing state-of-the-art\napproaches.\n","authors":["Nisha L. Raichur","Lucas Heublein","Tobias Feigl","Alexander RÃ¼gamer","Christopher Mutschler","Felix Ott"],"pdf_url":"https://arxiv.org/pdf/2405.11067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09386v1","updated":"2024-07-12T16:06:51Z","published":"2024-07-12T16:06:51Z","title":"Radiance Fields from Photons","summary":"  Neural radiance fields, or NeRFs, have become the de facto approach for\nhigh-quality view synthesis from a collection of images captured from multiple\nviewpoints. However, many issues remain when capturing images in-the-wild under\nchallenging conditions, such as low light, high dynamic range, or rapid motion\nleading to smeared reconstructions with noticeable artifacts. In this work, we\nintroduce quanta radiance fields, a novel class of neural radiance fields that\nare trained at the granularity of individual photons using single-photon\ncameras (SPCs). We develop theory and practical computational techniques for\nbuilding radiance fields and estimating dense camera poses from unconventional,\nstochastic, and high-speed binary frame sequences captured by SPCs. We\ndemonstrate, both via simulations and a SPC hardware prototype, high-fidelity\nreconstructions under high-speed motion, in low light, and for extreme dynamic\nrange settings.\n","authors":["Sacha Jungerman","Mohit Gupta"],"pdf_url":"https://arxiv.org/pdf/2407.09386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09379v1","updated":"2024-07-12T15:57:52Z","published":"2024-07-12T15:57:52Z","title":"FANet: Feature Amplification Network for Semantic Segmentation in\n  Cluttered Background","summary":"  Existing deep learning approaches leave out the semantic cues that are\ncrucial in semantic segmentation present in complex scenarios including\ncluttered backgrounds and translucent objects, etc. To handle these challenges,\nwe propose a feature amplification network (FANet) as a backbone network that\nincorporates semantic information using a novel feature enhancement module at\nmulti-stages. To achieve this, we propose an adaptive feature enhancement (AFE)\nblock that benefits from both a spatial context module (SCM) and a feature\nrefinement module (FRM) in a parallel fashion. SCM aims to exploit larger\nkernel leverages for the increased receptive field to handle scale variations\nin the scene. Whereas our novel FRM is responsible for generating semantic cues\nthat can capture both low-frequency and high-frequency regions for better\nsegmentation tasks. We perform experiments over challenging real-world\nZeroWaste-f dataset which contains background-cluttered and translucent\nobjects. Our experimental results demonstrate the state-of-the-art performance\ncompared to existing methods.\n","authors":["Muhammad Ali","Mamoona Javaid","Mubashir Noman","Mustansar Fiaz","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2407.09379v1.pdf","comment":"Accepted at ICIP 2024"},{"id":"http://arxiv.org/abs/2407.09372v1","updated":"2024-07-12T15:53:15Z","published":"2024-07-12T15:53:15Z","title":"ConRebSeg: A Segmentation Dataset for Reinforced Concrete Construction","summary":"  The construction industry has been traditionally slow in adopting digital\ntechnologies. However, these are becoming increasingly necessary due to a\nplentitude of challenges, such as a shortage of skilled labor and decreasing\nproductivity levels compared to other industries. Autonomous robotic systems\ncan alleviate this problem, but the software development process for these\nsystems is heavily driven by data, a resource usually challenging to find in\nthe construction domain due to the lack of public availability. In our work, we\ntherefore provide a dataset of 14,805 RGB images with segmentation labels for\nreinforced concrete construction and make it publicly available. We conduct a\ndetailed analysis of our dataset and discuss how to deal with labeling\ninconsistencies. Furthermore, we establish baselines for the YOLOv8L-seg,\nDeepLabV3, and U-Net segmentation models and investigate the influence of data\navailability and label inconsistencies on the performance of these models. Our\nstudy showed that the models are precise in their predictions but would benefit\nfrom more data to increase the number of recalled instances. Label\ninconsistencies had a negligible effect on model performance, and we,\ntherefore, advocate for a crowd-sourced dataset to boost the development of\nautonomous robotic systems in the construction industry.\n","authors":["Patrick Schmidt","Lazaros Nalpantidis"],"pdf_url":"https://arxiv.org/pdf/2407.09372v1.pdf","comment":"The Dataset DOI and GitHub repository linked in the Data Availability\n  Statement are under review and will be made public as soon as possible"},{"id":"http://arxiv.org/abs/2407.09367v1","updated":"2024-07-12T15:48:40Z","published":"2024-07-12T15:48:40Z","title":"Reshaping the Online Data Buffering and Organizing Mechanism for\n  Continual Test-Time Adaptation","summary":"  Continual Test-Time Adaptation (CTTA) involves adapting a pre-trained source\nmodel to continually changing unsupervised target domains. In this paper, we\nsystematically analyze the challenges of this task: online environment,\nunsupervised nature, and the risks of error accumulation and catastrophic\nforgetting under continual domain shifts. To address these challenges, we\nreshape the online data buffering and organizing mechanism for CTTA. We propose\nan {uncertainty-aware buffering approach} to identify {and aggregate}\nsignificant samples with high certainty from the unsupervised, single-pass data\nstream. {Based on this}, we propose a graph-based class relation preservation\nconstraint to overcome catastrophic forgetting. Furthermore, a pseudo-target\nreplay objective is used to mitigate error accumulation. Extensive experiments\ndemonstrate the superiority of our method in both segmentation and\nclassification CTTA tasks. Code is available at\n\\href{https://github.com/z1358/OBAO}{this https URL}.\n","authors":["Zhilin Zhu","Xiaopeng Hong","Zhiheng Ma","Weijun Zhuang","Yaohui Ma","Dai Yong","Yaowei Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09367v1.pdf","comment":"This is the preprint version of our paper and supplemental material\n  to appear in ECCV 2024"},{"id":"http://arxiv.org/abs/2303.07814v2","updated":"2024-07-12T15:48:09Z","published":"2023-03-14T11:44:58Z","title":"MS-TCRNet: Multi-Stage Temporal Convolutional Recurrent Networks for\n  Action Segmentation Using Sensor-Augmented Kinematics","summary":"  Action segmentation is a challenging task in high-level process analysis,\ntypically performed on video or kinematic data obtained from various sensors.\nThis work presents two contributions related to action segmentation on\nkinematic data. Firstly, we introduce two versions of Multi-Stage Temporal\nConvolutional Recurrent Networks (MS-TCRNet), specifically designed for\nkinematic data. The architectures consist of a prediction generator with\nintra-stage regularization and Bidirectional LSTM or GRU-based refinement\nstages. Secondly, we propose two new data augmentation techniques, World Frame\nRotation and Hand Inversion, which utilize the strong geometric structure of\nkinematic data to improve algorithm performance and robustness. We evaluate our\nmodels on three datasets of surgical suturing tasks: the Variable Tissue\nSimulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS)\nDataset, both of which are open surgery simulation datasets collected by us, as\nwell as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a\nwell-known benchmark in robotic surgery. Our methods achieved state-of-the-art\nperformance.\n","authors":["Adam Goldbraikh","Omer Shubi","Or Rubin","Carla M Pugh","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2303.07814v2.pdf","comment":"41 pages, 7 figures. Submitted to Pattern Recognition"},{"id":"http://arxiv.org/abs/2402.17766v3","updated":"2024-07-12T15:36:15Z","published":"2024-02-27T18:57:12Z","title":"ShapeLLM: Universal 3D Object Understanding for Embodied Interaction","summary":"  This paper presents ShapeLLM, the first 3D Multimodal Large Language Model\n(LLM) designed for embodied interaction, exploring a universal 3D object\nunderstanding with 3D point clouds and languages. ShapeLLM is built upon an\nimproved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view\nimage distillation for enhanced geometry understanding. By utilizing ReCon++ as\nthe 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed\ninstruction-following data and tested on our newly human-curated benchmark, 3D\nMM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3D\ngeometry understanding and language-unified 3D interaction tasks, such as\nembodied visual grounding. Project page: https://qizekun.github.io/shapellm/\n","authors":["Zekun Qi","Runpei Dong","Shaochen Zhang","Haoran Geng","Chunrui Han","Zheng Ge","Li Yi","Kaisheng Ma"],"pdf_url":"https://arxiv.org/pdf/2402.17766v3.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.09359v1","updated":"2024-07-12T15:33:37Z","published":"2024-07-12T15:33:37Z","title":"A Unified Anomaly Synthesis Strategy with Gradient Ascent for Industrial\n  Anomaly Detection and Localization","summary":"  Anomaly synthesis strategies can effectively enhance unsupervised anomaly\ndetection. However, existing strategies have limitations in the coverage and\ncontrollability of anomaly synthesis, particularly for weak defects that are\nvery similar to normal regions. In this paper, we propose Global and Local\nAnomaly co-Synthesis Strategy (GLASS), a novel unified framework designed to\nsynthesize a broader coverage of anomalies under the manifold and hypersphere\ndistribution constraints of Global Anomaly Synthesis (GAS) at the feature level\nand Local Anomaly Synthesis (LAS) at the image level. Our method synthesizes\nnear-in-distribution anomalies in a controllable way using Gaussian noise\nguided by gradient ascent and truncated projection. GLASS achieves\nstate-of-the-art results on the MVTec AD (detection AUROC of 99.9\\%), VisA, and\nMPDD datasets and excels in weak defect detection. The effectiveness and\nefficiency have been further validated in industrial applications for woven\nfabric defect detection. The code and dataset are available at:\n\\url{https://github.com/cqylunlun/GLASS}.\n","authors":["Qiyu Chen","Huiyuan Luo","Chengkan Lv","Zhengtao Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.09359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09352v1","updated":"2024-07-12T15:25:54Z","published":"2024-07-12T15:25:54Z","title":"Imaging Interiors: An Implicit Solution to Electromagnetic Inverse\n  Scattering Problems","summary":"  Electromagnetic Inverse Scattering Problems (EISP) have gained wide\napplications in computational imaging. By solving EISP, the internal relative\npermittivity of the scatterer can be non-invasively determined based on the\nscattered electromagnetic fields. Despite previous efforts to address EISP,\nachieving better solutions to this problem has remained elusive, due to the\nchallenges posed by inversion and discretization. This paper tackles those\nchallenges in EISP via an implicit approach. By representing the scatterer's\nrelative permittivity as a continuous implicit representation, our method is\nable to address the low-resolution problems arising from discretization.\nFurther, optimizing this implicit representation within a forward framework\nallows us to conveniently circumvent the challenges posed by inverse\nestimation. Our approach outperforms existing methods on standard benchmark\ndatasets. Project page: https://luo-ziyuan.github.io/Imaging-Interiors\n","authors":["Ziyuan Luo","Boxin Shi","Haoliang Li","Renjie Wan"],"pdf_url":"https://arxiv.org/pdf/2407.09352v1.pdf","comment":"33 pages, accepted by ECCV 2024 non-camera-ready version"},{"id":"http://arxiv.org/abs/2407.09344v1","updated":"2024-07-12T15:18:14Z","published":"2024-07-12T15:18:14Z","title":"Pre-training Point Cloud Compact Model with Partial-aware Reconstruction","summary":"  The pre-trained point cloud model based on Masked Point Modeling (MPM) has\nexhibited substantial improvements across various tasks. However, two drawbacks\nhinder their practical application. Firstly, the positional embedding of masked\npatches in the decoder results in the leakage of their central coordinates,\nleading to limited 3D representations. Secondly, the excessive model size of\nexisting MPM methods results in higher demands for devices. To address these,\nwe propose to pre-train Point cloud Compact Model with Partial-aware\n\\textbf{R}econstruction, named Point-CPR. Specifically, in the decoder, we\ncouple the vanilla masked tokens with their positional embeddings as randomly\nmasked queries and introduce a partial-aware prediction module before each\ndecoder layer to predict them from the unmasked partial. It prevents the\ndecoder from creating a shortcut between the central coordinates of masked\npatches and their reconstructed coordinates, enhancing the robustness of\nmodels. We also devise a compact encoder composed of local aggregation and\nMLPs, reducing the parameters and computational requirements compared to\nexisting Transformer-based encoders. Extensive experiments demonstrate that our\nmodel exhibits strong performance across various tasks, especially surpassing\nthe leading MPM-based model PointGPT-B with only 2% of its parameters.\n","authors":["Yaohua Zha","Yanzi Wang","Tao Dai","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2407.09344v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.17149"},{"id":"http://arxiv.org/abs/2407.06124v2","updated":"2024-07-12T15:15:03Z","published":"2024-07-08T17:00:28Z","title":"Structured Generations: Using Hierarchical Clusters to guide Diffusion\n  Models","summary":"  This paper introduces Diffuse-TreeVAE, a deep generative model that\nintegrates hierarchical clustering into the framework of Denoising Diffusion\nProbabilistic Models (DDPMs). The proposed approach generates new images by\nsampling from a root embedding of a learned latent tree VAE-based structure, it\nthen propagates through hierarchical paths, and utilizes a second-stage DDPM to\nrefine and generate distinct, high-quality images for each data cluster. The\nresult is a model that not only improves image clarity but also ensures that\nthe generated samples are representative of their respective clusters,\naddressing the limitations of previous VAE-based methods and advancing the\nstate of clustering-based generative modeling.\n","authors":["Jorge da Silva Goncalves","Laura Manduchi","Moritz Vandenhirtz","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2407.06124v2.pdf","comment":"8 pages, 7 figures, Structured Probabilistic Inference & Generative\n  Modeling workshop of ICML 2024"},{"id":"http://arxiv.org/abs/2311.00565v2","updated":"2024-07-12T15:05:24Z","published":"2023-11-01T15:07:03Z","title":"Detecting Visual Cues in the Intensive Care Unit and Association with\n  Patient Clinical Status","summary":"  Intensive Care Units (ICU) provide close supervision and continuous care to\npatients with life-threatening conditions. However, continuous patient\nassessment in the ICU is still limited due to time constraints and the workload\non healthcare providers. Existing patient assessments in the ICU such as pain\nor mobility assessment are mostly sporadic and administered manually, thus\nintroducing the potential for human errors. Developing Artificial intelligence\n(AI) tools that can augment human assessments in the ICU can be beneficial for\nproviding more objective and granular monitoring capabilities. For example,\ncapturing the variations in a patient's facial cues related to pain or\nagitation can help in adjusting pain-related medications or detecting\nagitation-inducing conditions such as delirium. Additionally, subtle changes in\nvisual cues during or prior to adverse clinical events could potentially aid in\ncontinuous patient monitoring when combined with high-resolution physiological\nsignals and Electronic Health Record (EHR) data. In this paper, we examined the\nassociation between visual cues and patient condition including acuity status,\nacute brain dysfunction, and pain. We leveraged our AU-ICU dataset with 107,064\nframes collected in the ICU annotated with facial action units (AUs) labels by\ntrained annotators. We developed a new \"masked loss computation\" technique that\naddresses the data imbalance problem by maximizing data resource utilization.\nWe trained the model using our AU-ICU dataset in conjunction with three\nexternal datasets to detect 18 AUs. The SWIN Transformer model achieved 0.57\nmean F1-score and 0.89 mean accuracy on the test set. Additionally, we\nperformed AU inference on 634,054 frames to evaluate the association between\nfacial AUs and clinically important patient conditions such as acuity status,\nacute brain dysfunction, and pain.\n","authors":["Subhash Nerella","Ziyuan Guan","Andrea Davidson","Yuanfang Ren","Tezcan Baslanti","Brooke Armfield","Patrick Tighe","Azra Bihorac","Parisa Rashidi"],"pdf_url":"https://arxiv.org/pdf/2311.00565v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19286v2","updated":"2024-07-12T14:55:37Z","published":"2024-04-30T06:33:07Z","title":"Soft Prompt Generation for Domain Generalization","summary":"  Large pre-trained vision language models (VLMs) have shown impressive\nzero-shot ability on downstream tasks with manually designed prompt. To further\nadapt VLMs to downstream tasks, soft prompt is proposed to replace manually\ndesigned prompt, which undergoes fine-tuning based on specific domain data.\nPrior prompt learning methods primarily learn a fixed prompt or residuled\nprompt from training samples. However, the learned prompts lack diversity and\nignore information about unseen domains. In this paper, we reframe the prompt\nlearning framework from a generative perspective and propose a simple yet\nefficient method for the Domain Generalization (DG) task, namely Soft Prompt\nGeneration (SPG). Specifically, SPG consists of a two-stage training phase and\nan inference phase. During the training phase, we introduce soft prompt label\nfor each domain, aiming to incorporate the generative model domain knowledge.\nDuring the inference phase, the generator of the generative model is employed\nto obtain instance-specific soft prompts for the unseen target domain.\nExtensive experiments on five domain generalization benchmarks of three DG\ntasks demonstrate that SPG achieves state-of-the-art performance. The code is\navailable at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.\n","authors":["Shuanghao Bai","Yuedi Zhang","Wanqi Zhou","Zhirong Luan","Badong Chen"],"pdf_url":"https://arxiv.org/pdf/2404.19286v2.pdf","comment":"25 pages, 4 figures, accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.01394v2","updated":"2024-07-12T14:44:33Z","published":"2024-07-01T15:46:45Z","title":"Gloss2Text: Sign Language Gloss translation using LLMs and Semantically\n  Aware Label Smoothing","summary":"  Sign language translation from video to spoken text presents unique\nchallenges owing to the distinct grammar, expression nuances, and high\nvariation of visual appearance across different speakers and contexts. The\nintermediate gloss annotations of videos aim to guide the translation process.\nIn our work, we focus on {\\em Gloss2Text} translation stage and propose several\nadvances by leveraging pre-trained large language models (LLMs), data\naugmentation, and novel label-smoothing loss function exploiting gloss\ntranslation ambiguities improving significantly the performance of\nstate-of-the-art approaches. Through extensive experiments and ablation studies\non the PHOENIX Weather 2014T dataset, our approach surpasses state-of-the-art\nperformance in {\\em Gloss2Text} translation, indicating its efficacy in\naddressing sign language translation and suggesting promising avenues for\nfuture research and development.\n","authors":["Pooya Fayyazsanavi","Antonios Anastasopoulos","Jana KoÅ¡eckÃ¡"],"pdf_url":"https://arxiv.org/pdf/2407.01394v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06322v2","updated":"2024-07-12T14:43:01Z","published":"2024-03-10T21:43:47Z","title":"Leveraging Computer Vision in the Intensive Care Unit (ICU) for\n  Examining Visitation and Mobility","summary":"  Despite the importance of closely monitoring patients in the Intensive Care\nUnit (ICU), many aspects are still assessed in a limited manner due to the time\nconstraints imposed on healthcare providers. For example, although excessive\nvisitations during rest hours can potentially exacerbate the risk of circadian\nrhythm disruption and delirium, it is not captured in the ICU. Likewise, while\nmobility can be an important indicator of recovery or deterioration in ICU\npatients, it is only captured sporadically or not captured at all. In the past\nfew years, the computer vision field has found application in many domains by\nreducing the human burden. Using computer vision systems in the ICU can also\npotentially enable non-existing assessments or enhance the frequency and\naccuracy of existing assessments while reducing the staff workload. In this\nstudy, we leverage a state-of-the-art noninvasive computer vision system based\non depth imaging to characterize ICU visitations and patients' mobility. We\nthen examine the relationship between visitation and several patient outcomes,\nsuch as pain, acuity, and delirium. We found an association between\ndeteriorating patient acuity and the incidence of delirium with increased\nvisitations. In contrast, self-reported pain, reported using the Defense and\nVeteran Pain Rating Scale (DVPRS), was correlated with decreased visitations.\nOur findings highlight the feasibility and potential of using noninvasive\nautonomous systems to monitor ICU patients.\n","authors":["Scott Siegel","Jiaqing Zhang","Sabyasachi Bandyopadhyay","Subhash Nerella","Brandon Silva","Tezcan Baslanti","Azra Bihorac","Parisa Rashidi"],"pdf_url":"https://arxiv.org/pdf/2403.06322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06464v2","updated":"2024-07-12T14:38:29Z","published":"2024-07-09T00:04:54Z","title":"SideSeeing: A multimodal dataset and collection of tools for sidewalk\n  assessment","summary":"  This paper introduces SideSeeing, a novel initiative that provides tools and\ndatasets for assessing the built environment. We present a framework for\nstreet-level data acquisition, loading, and analysis. Using the framework, we\ncollected a novel dataset that integrates synchronized video footaged captured\nfrom chest-mounted mobile devices with sensor data (accelerometer, gyroscope,\nmagnetometer, and GPS). Each data sample represents a path traversed by a user\nfilming sidewalks near hospitals in Brazil and the USA. The dataset encompasses\nthree hours of content covering 12 kilometers around nine hospitals, and\nincludes 325,000 video frames with corresponding sensor data. Additionally, we\npresent a novel 68-element taxonomy specifically created for sidewalk scene\nidentification. SideSeeing is a step towards a suite of tools that urban\nexperts can use to perform in-depth sidewalk accessibility evaluations.\nSideSeeing data and tools are publicly available at\nhttps://sites.usp.br/sideseeing/.\n","authors":["R. J. P. Damaceno","L. Ferreira","F. Miranda","M. Hosseini","R. M. Cesar Jr"],"pdf_url":"https://arxiv.org/pdf/2407.06464v2.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.09303v1","updated":"2024-07-12T14:37:49Z","published":"2024-07-12T14:37:49Z","title":"ProDepth: Boosting Self-Supervised Multi-Frame Monocular Depth with\n  Probabilistic Fusion","summary":"  Self-supervised multi-frame monocular depth estimation relies on the\ngeometric consistency between successive frames under the assumption of a\nstatic scene. However, the presence of moving objects in dynamic scenes\nintroduces inevitable inconsistencies, causing misaligned multi-frame feature\nmatching and misleading self-supervision during training. In this paper, we\npropose a novel framework called ProDepth, which effectively addresses the\nmismatch problem caused by dynamic objects using a probabilistic approach. We\ninitially deduce the uncertainty associated with static scene assumption by\nadopting an auxiliary decoder. This decoder analyzes inconsistencies embedded\nin the cost volume, inferring the probability of areas being dynamic. We then\ndirectly rectify the erroneous cost volume for dynamic areas through a\nProbabilistic Cost Volume Modulation (PCVM) module. Specifically, we derive\nprobability distributions of depth candidates from both single-frame and\nmulti-frame cues, modulating the cost volume by adaptively fusing those\ndistributions based on the inferred uncertainty. Additionally, we present a\nself-supervision loss reweighting strategy that not only masks out incorrect\nsupervision with high uncertainty but also mitigates the risks in remaining\npossible dynamic areas in accordance with the probability. Our proposed method\nexcels over state-of-the-art approaches in all metrics on both Cityscapes and\nKITTI datasets, and demonstrates superior generalization ability on the Waymo\nOpen dataset.\n","authors":["Sungmin Woo","Wonjoon Lee","Woo Jin Kim","Dogyoon Lee","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2407.09303v1.pdf","comment":"Accepted by ECCV 2024. Project Page:\n  https://sungmin-woo.github.io/prodepth/"},{"id":"http://arxiv.org/abs/2407.09299v1","updated":"2024-07-12T14:32:30Z","published":"2024-07-12T14:32:30Z","title":"PID: Physics-Informed Diffusion Model for Infrared Image Generation","summary":"  Infrared imaging technology has gained significant attention for its reliable\nsensing ability in low visibility conditions, prompting many studies to convert\nthe abundant RGB images to infrared images. However, most existing image\ntranslation methods treat infrared images as a stylistic variation, neglecting\nthe underlying physical laws, which limits their practical application. To\naddress these issues, we propose a Physics-Informed Diffusion (PID) model for\ntranslating RGB images to infrared images that adhere to physical laws. Our\nmethod leverages the iterative optimization of the diffusion model and\nincorporates strong physical constraints based on prior knowledge of infrared\nlaws during training. This approach enhances the similarity between translated\ninfrared images and the real infrared domain without increasing extra training\nparameters. Experimental results demonstrate that PID significantly outperforms\nexisting state-of-the-art methods. Our code is available at\nhttps://github.com/fangyuanmao/PID.\n","authors":["Fangyuan Mao","Jilin Mei","Shun Lu","Fuyang Liu","Liang Chen","Fangzhou Zhao","Yu Hu"],"pdf_url":"https://arxiv.org/pdf/2407.09299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09294v1","updated":"2024-07-12T14:29:00Z","published":"2024-07-12T14:29:00Z","title":"SS-SfP:Neural Inverse Rendering for Self Supervised Shape from (Mixed)\n  Polarization","summary":"  We present a novel inverse rendering-based framework to estimate the 3D shape\n(per-pixel surface normals and depth) of objects and scenes from single-view\npolarization images, the problem popularly known as Shape from Polarization\n(SfP). The existing physics-based and learning-based methods for SfP perform\nunder certain restrictions, i.e., (a) purely diffuse or purely specular\nreflections, which are seldom in the real surfaces, (b) availability of the\nground truth surface normals for direct supervision that are hard to acquire\nand are limited by the scanner's resolution, and (c) known refractive index. To\novercome these restrictions, we start by learning to separate the\npartially-polarized diffuse and specular reflection components, which we call\nreflectance cues, based on a modified polarization reflection model and then\nestimate shape under mixed polarization through an inverse-rendering based\nself-supervised deep learning framework called SS-SfP, guided by the\npolarization data and estimated reflectance cues. Furthermore, we also obtain\nthe refractive index as a non-linear least squares solution. Through extensive\nquantitative and qualitative evaluation, we establish the efficacy of the\nproposed framework over simple single-object scenes from DeepSfP dataset and\ncomplex in-the-wild scenes from SPW dataset in an entirely self-supervised\nsetting. To the best of our knowledge, this is the first learning-based\napproach to address SfP under mixed polarization in a completely\nself-supervised framework.\n","authors":["Ashish Tiwari","Shanmuganathan Raman"],"pdf_url":"https://arxiv.org/pdf/2407.09294v1.pdf","comment":"Published in Pacific Graphics 2023"},{"id":"http://arxiv.org/abs/2407.09288v1","updated":"2024-07-12T14:20:12Z","published":"2024-07-12T14:20:12Z","title":"WSESeg: Introducing a Dataset for the Segmentation of Winter Sports\n  Equipment with a Baseline for Interactive Segmentation","summary":"  In this paper we introduce a new dataset containing instance segmentation\nmasks for ten different categories of winter sports equipment, called WSESeg\n(Winter Sports Equipment Segmentation). Furthermore, we carry out interactive\nsegmentation experiments on said dataset to explore possibilities for efficient\nfurther labeling. The SAM and HQ-SAM models are conceptualized as foundation\nmodels for performing user guided segmentation. In order to measure their\nclaimed generalization capability we evaluate them on WSESeg. Since interactive\nsegmentation offers the benefit of creating easily exploitable ground truth\ndata during test-time, we are going to test various online adaptation methods\nfor the purpose of exploring potentials for improvements without having to\nfine-tune the models explicitly. Our experiments show that our adaptation\nmethods drastically reduce the Failure Rate (FR) and Number of Clicks (NoC)\nmetrics, which generally leads faster to better interactive segmentation\nresults.\n","authors":["Robin SchÃ¶n","Daniel Kienzle","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2407.09288v1.pdf","comment":"7 pages, 1 figure, 3 tables, Accepted at CBMI 2024"},{"id":"http://arxiv.org/abs/2407.09285v1","updated":"2024-07-12T14:15:48Z","published":"2024-07-12T14:15:48Z","title":"MetaFood CVPR 2024 Challenge on Physically Informed 3D Food\n  Reconstruction: Methods and Results","summary":"  The increasing interest in computer vision applications for nutrition and\ndietary monitoring has led to the development of advanced 3D reconstruction\ntechniques for food items. However, the scarcity of high-quality data and\nlimited collaboration between industry and academia have constrained progress\nin this field. Building on recent advancements in 3D reconstruction, we host\nthe MetaFood Workshop and its challenge for Physically Informed 3D Food\nReconstruction. This challenge focuses on reconstructing volume-accurate 3D\nmodels of food items from 2D images, using a visible checkerboard as a size\nreference. Participants were tasked with reconstructing 3D models for 20\nselected food items of varying difficulty levels: easy, medium, and hard. The\neasy level provides 200 images, the medium level provides 30 images, and the\nhard level provides only 1 image for reconstruction. In total, 16 teams\nsubmitted results in the final testing phase. The solutions developed in this\nchallenge achieved promising results in 3D food reconstruction, with\nsignificant potential for improving portion estimation for dietary assessment\nand nutritional monitoring. More details about this workshop challenge and\naccess to the dataset can be found at\nhttps://sites.google.com/view/cvpr-metafood-2024.\n","authors":["Jiangpeng He","Yuhao Chen","Gautham Vinod","Talha Ibn Mahmud","Fengqing Zhu","Edward Delp","Alexander Wong","Pengcheng Xi","Ahmad AlMughrabi","Umair Haroon","Ricardo Marques","Petia Radeva","Jiadong Tang","Dianyi Yang","Yu Gao","Zhaoxiang Liang","Yawei Jueluo","Chengyu Shi","Pengyu Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09285v1.pdf","comment":"Technical report for MetaFood CVPR 2024 Challenge on Physically\n  Informed 3D Food Reconstruction. arXiv admin note: substantial text overlap\n  with arXiv:2407.01717"},{"id":"http://arxiv.org/abs/2403.18717v2","updated":"2024-07-12T14:13:41Z","published":"2024-03-27T16:06:37Z","title":"Semi-Supervised Learning for Deep Causal Generative Models","summary":"  Developing models that are capable of answering questions of the form \"How\nwould x change if y had been z?'\" is fundamental to advancing medical image\nanalysis. Training causal generative models that address such counterfactual\nquestions, though, currently requires that all relevant variables have been\nobserved and that the corresponding labels are available in the training data.\nHowever, clinical data may not have complete records for all patients and state\nof the art causal generative models are unable to take full advantage of this.\nWe thus develop, for the first time, a semi-supervised deep causal generative\nmodel that exploits the causal relationships between variables to maximise the\nuse of all available data. We explore this in the setting where each sample is\neither fully labelled or fully unlabelled, as well as the more clinically\nrealistic case of having different labels missing for each sample. We leverage\ntechniques from causal inference to infer missing values and subsequently\ngenerate realistic counterfactuals, even for samples with incomplete labels.\n","authors":["Yasin Ibrahim","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2403.18717v2.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.09271v1","updated":"2024-07-12T13:57:49Z","published":"2024-07-12T13:57:49Z","title":"iNeMo: Incremental Neural Mesh Models for Robust Class-Incremental\n  Learning","summary":"  Different from human nature, it is still common practice today for vision\ntasks to train deep learning models only initially and on fixed datasets. A\nvariety of approaches have recently addressed handling continual data streams.\nHowever, extending these methods to manage out-of-distribution (OOD) scenarios\nhas not effectively been investigated. On the other hand, it has recently been\nshown that non-continual neural mesh models exhibit strong performance in\ngeneralizing to such OOD scenarios. To leverage this decisive property in a\ncontinual learning setting, we propose incremental neural mesh models that can\nbe extended with new meshes over time. In addition, we present a latent space\ninitialization strategy that enables us to allocate feature space for future\nunseen classes in advance and a positional regularization term that forces the\nfeatures of the different classes to consistently stay in respective latent\nspace regions. We demonstrate the effectiveness of our method through extensive\nexperiments on the Pascal3D and ObjectNet3D datasets and show that our approach\noutperforms the baselines for classification by $2-6\\%$ in the in-domain and by\n$6-50\\%$ in the OOD setting. Our work also presents the first incremental\nlearning approach for pose estimation. Our code and model can be found at\nhttps://github.com/Fischer-Tom/iNeMo.\n","authors":["Tom Fischer","Yaoyao Liu","Artur Jesslen","Noor Ahmed","Prakhar Kaushik","Angtian Wang","Alan Yuille","Adam Kortylewski","Eddy Ilg"],"pdf_url":"https://arxiv.org/pdf/2407.09271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00853v2","updated":"2024-07-12T13:55:40Z","published":"2023-12-01T14:40:07Z","title":"Motion-Guided Latent Diffusion for Temporally Consistent Real-world\n  Video Super-resolution","summary":"  Real-world low-resolution (LR) videos have diverse and complex degradations,\nimposing great challenges on video super-resolution (VSR) algorithms to\nreproduce their high-resolution (HR) counterparts with high quality. Recently,\nthe diffusion models have shown compelling performance in generating realistic\ndetails for image restoration tasks. However, the diffusion process has\nrandomness, making it hard to control the contents of restored images. This\nissue becomes more serious when applying diffusion models to VSR tasks because\ntemporal consistency is crucial to the perceptual quality of videos. In this\npaper, we propose an effective real-world VSR algorithm by leveraging the\nstrength of pre-trained latent diffusion models. To ensure the content\nconsistency among adjacent frames, we exploit the temporal dynamics in LR\nvideos to guide the diffusion process by optimizing the latent sampling path\nwith a motion-guided loss, ensuring that the generated HR video maintains a\ncoherent and continuous visual flow. To further mitigate the discontinuity of\ngenerated details, we insert temporal module to the decoder and fine-tune it\nwith an innovative sequence-oriented loss. The proposed motion-guided latent\ndiffusion (MGLD) based VSR algorithm achieves significantly better perceptual\nquality than state-of-the-arts on real-world VSR benchmark datasets, validating\nthe effectiveness of the proposed model design and training strategies.\n","authors":["Xi Yang","Chenhang He","Jianqi Ma","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.00853v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09268v1","updated":"2024-07-12T13:52:05Z","published":"2024-07-12T13:52:05Z","title":"Region Attention Transformer for Medical Image Restoration","summary":"  Transformer-based methods have demonstrated impressive results in medical\nimage restoration, attributed to the multi-head self-attention (MSA) mechanism\nin the spatial dimension. However, the majority of existing Transformers\nconduct attention within fixed and coarsely partitioned regions (\\text{e.g.}\nthe entire image or fixed patches), resulting in interference from irrelevant\nregions and fragmentation of continuous image content. To overcome these\nchallenges, we introduce a novel Region Attention Transformer (RAT) that\nutilizes a region-based multi-head self-attention mechanism (R-MSA). The R-MSA\ndynamically partitions the input image into non-overlapping semantic regions\nusing the robust Segment Anything Model (SAM) and then performs self-attention\nwithin these regions. This region partitioning is more flexible and\ninterpretable, ensuring that only pixels from similar semantic regions\ncomplement each other, thereby eliminating interference from irrelevant\nregions. Moreover, we introduce a focal region loss to guide our model to\nadaptively focus on recovering high-difficulty regions. Extensive experiments\ndemonstrate the effectiveness of RAT in various medical image restoration\ntasks, including PET image synthesis, CT image denoising, and pathological\nimage super-resolution. Code is available at\n\\href{https://github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git}{https://github.com/RAT}.\n","authors":["Zhiwen Yang","Haowei Chen","Ziniu Qian","Yang Zhou","Hui Zhang","Dan Zhao","Bingzheng Wei","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2407.09268v1.pdf","comment":"This paper has been accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.01105v2","updated":"2024-07-12T13:45:53Z","published":"2024-03-02T06:29:44Z","title":"Depth Information Assisted Collaborative Mutual Promotion Network for\n  Single Image Dehazing","summary":"  Recovering a clear image from a single hazy image is an open inverse problem.\nAlthough significant research progress has been made, most existing methods\nignore the effect that downstream tasks play in promoting upstream dehazing.\nFrom the perspective of the haze generation mechanism, there is a potential\nrelationship between the depth information of the scene and the hazy image.\nBased on this, we propose a dual-task collaborative mutual promotion framework\nto achieve the dehazing of a single image. This framework integrates depth\nestimation and dehazing by a dual-task interaction mechanism and achieves\nmutual enhancement of their performance. To realize the joint optimization of\nthe two tasks, an alternative implementation mechanism with the difference\nperception is developed. On the one hand, the difference perception between the\ndepth maps of the dehazing result and the ideal image is proposed to promote\nthe dehazing network to pay attention to the non-ideal areas of the dehazing.\nOn the other hand, by improving the depth estimation performance in the\ndifficult-to-recover areas of the hazy image, the dehazing network can\nexplicitly use the depth information of the hazy image to assist the clear\nimage recovery. To promote the depth estimation, we propose to use the\ndifference between the dehazed image and the ground truth to guide the depth\nestimation network to focus on the dehazed unideal areas. It allows dehazing\nand depth estimation to leverage their strengths in a mutually reinforcing\nmanner. Experimental results show that the proposed method can achieve better\nperformance than that of the state-of-the-art approaches.\n","authors":["Yafei Zhang","Shen Zhou","Huafeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.01105v2.pdf","comment":"ACCEPT BY CVPR2024"},{"id":"http://arxiv.org/abs/2407.09248v1","updated":"2024-07-12T13:21:25Z","published":"2024-07-12T13:21:25Z","title":"Semantic UV mapping to improve texture inpainting for indoor scenes","summary":"  This work aims to improve texture inpainting after clutter removal in scanned\nindoor meshes. This is achieved with a new UV mapping pre-processing step which\nleverages semantic information of indoor scenes to more accurately match the UV\nislands with the 3D representation of distinct structural elements like walls\nand floors. Semantic UV Mapping enriches classic UV unwrapping algorithms by\nnot only relying on geometric features but also visual features originating\nfrom the present texture. The segmentation improves the UV mapping and\nsimultaneously simplifies the 3D geometric reconstruction of the scene after\nthe removal of loose objects. Each segmented element can be reconstructed\nseparately using the boundary conditions of the adjacent elements. Because this\nis performed as a pre-processing step, other specialized methods for geometric\nand texture reconstruction can be used in the future to improve the results\neven further.\n","authors":["Jelle Vermandere","Maarten Bassier","Maarten Vergauwen"],"pdf_url":"https://arxiv.org/pdf/2407.09248v1.pdf","comment":"EG UK Computer Graphics & Visual Computin"},{"id":"http://arxiv.org/abs/2403.02408v3","updated":"2024-07-12T13:05:46Z","published":"2024-03-04T19:06:13Z","title":"A Spatio-temporal Aligned SUNet Model for Low-light Video Enhancement","summary":"  Distortions caused by low-light conditions are not only visually unpleasant\nbut also degrade the performance of computer vision tasks. The restoration and\nenhancement have proven to be highly beneficial. However, there are only a\nlimited number of enhancement methods explicitly designed for videos acquired\nin low-light conditions. We propose a Spatio-Temporal Aligned SUNet (STA-SUNet)\nmodel using a Swin Transformer as a backbone to capture low light video\nfeatures and exploit their spatio-temporal correlations. The STA-SUNet model is\ntrained on a novel, fully registered dataset (BVI), which comprises dynamic\nscenes captured under varying light conditions. It is further analysed\ncomparatively against various other models over three test datasets. The model\ndemonstrates superior adaptivity across all datasets, obtaining the highest\nPSNR and SSIM values. It is particularly effective in extreme low-light\nconditions, yielding fairly good visualisation results.\n","authors":["Ruirui Lin","Nantheera Anantrasirichai","Alexandra Malyugina","David Bull"],"pdf_url":"https://arxiv.org/pdf/2403.02408v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09236v1","updated":"2024-07-12T13:05:27Z","published":"2024-07-12T13:05:27Z","title":"Modelling the Human Intuition to Complete the Missing Information in\n  Images for Convolutional Neural Networks","summary":"  In this study, we attempt to model intuition and incorporate this formalism\nto improve the performance of the Convolutional Neural Networks. Despite\ndecades of research, ambiguities persist on principles of intuition.\nExperimental psychology reveals many types of intuition, which depend on state\nof the human mind. We focus on visual intuition, useful for completing missing\ninformation during visual cognitive tasks. First, we set up a scenario to\ngradually decrease the amount of visual information in the images of a dataset\nto examine its impact on CNN accuracy. Then, we represent a model for visual\nintuition using Gestalt theory. The theory claims that humans derive a set of\ntemplates according to their subconscious experiences. When the brain decides\nthat there is missing information in a scene, such as occlusion, it\ninstantaneously completes the information by replacing the missing parts with\nthe most similar ones. Based upon Gestalt theory, we model the visual\nintuition, in two layers. Details of these layers are provided throughout the\npaper. We use the MNIST data set to test the suggested intuition model for\ncompleting the missing information. Experiments show that the augmented CNN\narchitecture provides higher performances compared to the classic models when\nusing incomplete images.\n","authors":["Robin KoÃ§","FatoÅ T. Yarman Vural"],"pdf_url":"https://arxiv.org/pdf/2407.09236v1.pdf","comment":"7 pages, 5 figures, 1 table, submitted to 2024 IEEE International\n  Conference on Image Processing (ICIP), did not get accepted"},{"id":"http://arxiv.org/abs/2303.12533v2","updated":"2024-07-12T13:04:31Z","published":"2023-03-22T13:06:39Z","title":"Pixel-wise Agricultural Image Time Series Classification: Comparisons\n  and a Deformable Prototype-based Approach","summary":"  Improvements in Earth observation by satellites allow for imagery of ever\nhigher temporal and spatial resolution. Leveraging this data for agricultural\nmonitoring is key for addressing environmental and economic challenges. Current\nmethods for crop segmentation using temporal data either rely on annotated data\nor are heavily engineered to compensate the lack of supervision. In this paper,\nwe present and compare datasets and methods for both supervised and\nunsupervised pixel-wise segmentation of satellite image time series (SITS). We\nalso introduce an approach to add invariance to spectral deformations and\ntemporal shifts to classical prototype-based methods such as K-means and\nNearest Centroid Classifier (NCC). We study different levels of supervision and\nshow this simple and highly interpretable method achieves the best performance\nin the low data regime and significantly improves the state of the art for\nunsupervised classification of agricultural time series on four recent SITS\ndatasets.\n","authors":["Elliot Vincent","Jean Ponce","Mathieu Aubry"],"pdf_url":"https://arxiv.org/pdf/2303.12533v2.pdf","comment":"Revised version. Added references and baselines. Corrected typos.\n  Added discussion section and Appendix A, B and C"},{"id":"http://arxiv.org/abs/2404.10267v2","updated":"2024-07-12T13:03:00Z","published":"2024-04-16T03:45:45Z","title":"OneActor: Consistent Character Generation via Cluster-Conditioned\n  Guidance","summary":"  Text-to-image diffusion models benefit artists with high-quality image\ngeneration. Yet their stochastic nature hinders artists from creating\nconsistent images of the same subject. Existing methods try to tackle this\nchallenge and generate consistent content in various ways. However, they either\ndepend on external restricted data or require expensive tuning of the diffusion\nmodel. For this issue, we propose a novel one-shot tuning paradigm, termed as\nOneActor. It efficiently performs consistent subject generation solely driven\nby prompts via a learned semantic guidance to bypass the laborious backbone\ntuning. We lead the way to formalize the objective of consistent subject\ngeneration from a clustering perspective, and thus design a cluster-conditioned\nmodel. To mitigate the overfitting challenge shared by one-shot tuning\npipelines, we augment the tuning with auxiliary samples and devise two\ninference strategies: semantic interpolation and cluster guidance. These\ntechniques are later verified to significantly enhance the generation quality.\nComprehensive experiments show that our method outperforms a variety of\nbaselines with satisfactory subject consistency, superior prompt conformity as\nwell as high image quality. Our method is capable of multi-subject generation\nand compatible with popular diffusion extensions. Besides, we achieve a 4 times\nfaster tuning speed than tuning-based baselines and, if desired, avoid\nincreasing inference time. Furthermore, to our best knowledge, we are the first\nto prove that the semantic space of the diffusion model has the same\ninterpolation property as the latent space does. This property can serve as\nanother promising tool for fine generation control.\n","authors":["Jiahao Wang","Caixia Yan","Haonan Lin","Weizhan Zhang","Mengmeng Wang","Tieliang Gong","Guang Dai","Hao Sun"],"pdf_url":"https://arxiv.org/pdf/2404.10267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09377v2","updated":"2024-07-12T12:54:42Z","published":"2024-03-14T13:27:42Z","title":"Introducing Routing Functions to Vision-Language Parameter-Efficient\n  Fine-Tuning with Low-Rank Bottlenecks","summary":"  Mainstream parameter-efficient fine-tuning (PEFT) methods, such as LoRA or\nAdapter, project a model's hidden states to a lower dimension, allowing\npre-trained models to adapt to new data through this low-rank bottleneck.\nHowever, PEFT tasks involving multiple modalities, like vision-language (VL)\ntasks, require not only adaptation to new data but also learning the\nrelationship between different modalities. Targeting at VL PEFT tasks, we\npropose a family of operations, called routing functions, to enhance VL\nalignment in the low-rank bottlenecks. These feature routing functions adopt\nlinear operations and do not introduce new trainable parameters. In-depth\nanalyses are conducted to study their behavior. In various VL PEFT settings,\nthe routing functions significantly improve performance of the original PEFT\nmethods, achieving over 20\\% improvement on VQAv2\n($\\text{RoBERTa}_{\\text{large}}$+ViT-L/16) and 30\\% on COCO Captioning\n(GPT2-medium+ViT-L/16). Also when fine-tuning a pre-trained multimodal model\nsuch as CLIP-BART, we observe smaller but consistent improvements across a\nrange of VL PEFT tasks. Our code is available at\nhttps://github.com/tingyu215/Routing_VLPEFT.\n","authors":["Tingyu Qu","Tinne Tuytelaars","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2403.09377v2.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2401.00025v3","updated":"2024-07-12T12:51:00Z","published":"2023-12-28T23:34:43Z","title":"Any-point Trajectory Modeling for Policy Learning","summary":"  Learning from demonstration is a powerful method for teaching robots new\nskills, and having more demonstration data often improves policy learning.\nHowever, the high cost of collecting demonstration data is a significant\nbottleneck. Videos, as a rich data source, contain knowledge of behaviors,\nphysics, and semantics, but extracting control-specific information from them\nis challenging due to the lack of action labels. In this work, we introduce a\nnovel framework, Any-point Trajectory Modeling (ATM), that utilizes video\ndemonstrations by pre-training a trajectory model to predict future\ntrajectories of arbitrary points within a video frame. Once trained, these\ntrajectories provide detailed control guidance, enabling the learning of robust\nvisuomotor policies with minimal action-labeled data. Across over 130\nlanguage-conditioned tasks we evaluated in both simulation and the real world,\nATM outperforms strong video pre-training baselines by 80% on average.\nFurthermore, we show effective transfer learning of manipulation skills from\nhuman videos and videos from a different robot morphology. Visualizations and\ncode are available at: \\url{https://xingyu-lin.github.io/atm}.\n","authors":["Chuan Wen","Xingyu Lin","John So","Kai Chen","Qi Dou","Yang Gao","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2401.00025v3.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2407.09230v1","updated":"2024-07-12T12:49:11Z","published":"2024-07-12T12:49:11Z","title":"Surgical Text-to-Image Generation","summary":"  Acquiring surgical data for research and development is significantly\nhindered by high annotation costs and practical and ethical constraints.\nUtilizing synthetically generated images could offer a valuable alternative. In\nthis work, we conduct an in-depth analysis on adapting text-to-image generative\nmodels for the surgical domain, leveraging the CholecT50 dataset, which\nprovides surgical images annotated with surgical action triplets (instrument,\nverb, target). We investigate various language models and find T5 to offer more\ndistinct features for differentiating surgical actions based on triplet-based\ntextual inputs. Our analysis demonstrates strong alignment between long and\ntriplet-based captions, supporting the use of triplet-based labels. We address\nthe challenges in training text-to-image models on triplet-based captions\nwithout additional input signals by uncovering that triplet text embeddings are\ninstrument-centric in the latent space and then, by designing an\ninstrument-based class balancing technique to counteract the imbalance and\nskewness in the surgical data, improving training convergence. Extending\nImagen, a diffusion-based generative model, we develop Surgical Imagen to\ngenerate photorealistic and activity-aligned surgical images from triplet-based\ntextual prompts. We evaluate our model using diverse metrics, including human\nexpert surveys and automated methods like FID and CLIP scores. We assess the\nmodel performance on key aspects: quality, alignment, reasoning, knowledge, and\nrobustness, demonstrating the effectiveness of our approach in providing a\nrealistic alternative to real data collection.\n","authors":["Chinedu Innocent Nwoye","Rupak Bose","Kareem Elgohary","Lorenzo Arboit","Giorgio Carlino","JoÃ«l L. Lavanchy","Pietro Mascagni","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2407.09230v1.pdf","comment":"11 pages, 11 figures, 3 tables, project page at\n  https://camma-public.github.io/surgicalimagen/"},{"id":"http://arxiv.org/abs/2402.09303v3","updated":"2024-07-12T12:47:19Z","published":"2024-02-14T16:47:20Z","title":"Comparing supervised learning dynamics: Deep neural networks match human\n  data efficiency but show a generalisation lag","summary":"  Recent research has seen many behavioral comparisons between humans and deep\nneural networks (DNNs) in the domain of image classification. Often, comparison\nstudies focus on the end-result of the learning process by measuring and\ncomparing the similarities in the representations of object categories once\nthey have been formed. However, the process of how these representations emerge\n-- that is, the behavioral changes and intermediate stages observed during the\nacquisition -- is less often directly and empirically compared. Here we report\na detailed investigation of the learning dynamics in human observers and\nvarious classic and state-of-the-art DNNs. We develop a constrained supervised\nlearning environment to align learning-relevant conditions such as starting\npoint, input modality, available input data and the feedback provided. Across\nthe whole learning process we evaluate and compare how well learned\nrepresentations can be generalized to previously unseen test data. Comparisons\nacross the entire learning process indicate that DNNs demonstrate a level of\ndata efficiency comparable to human learners, challenging some prevailing\nassumptions in the field. However, our results also reveal representational\ndifferences: while DNNs' learning is characterized by a pronounced\ngeneralisation lag, humans appear to immediately acquire generalizable\nrepresentations without a preliminary phase of learning training set-specific\ninformation that is only later transferred to novel data.\n","authors":["Lukas S. Huber","Fred W. Mast","Felix A. Wichmann"],"pdf_url":"https://arxiv.org/pdf/2402.09303v3.pdf","comment":"Final version accepted @ ICLR 2024 Workshop on Representational\n  Alignment (Re-Align)"},{"id":"http://arxiv.org/abs/2407.09216v1","updated":"2024-07-12T12:28:08Z","published":"2024-07-12T12:28:08Z","title":"A Fair Ranking and New Model for Panoptic Scene Graph Generation","summary":"  In panoptic scene graph generation (PSGG), models retrieve interactions\nbetween objects in an image which are grounded by panoptic segmentation masks.\nPrevious evaluations on panoptic scene graphs have been subject to an erroneous\nevaluation protocol where multiple masks for the same object can lead to\nmultiple relation distributions per mask-mask pair. This can be exploited to\nincrease the final score. We correct this flaw and provide a fair ranking over\na wide range of existing PSGG models. The observed scores for existing methods\nincrease by up to 7.4 mR@50 for all two-stage methods, while dropping by up to\n19.3 mR@50 for all one-stage methods, highlighting the importance of a correct\nevaluation. Contrary to recent publications, we show that existing two-stage\nmethods are competitive to one-stage methods. Building on this, we introduce\nthe Decoupled SceneFormer (DSFormer), a novel two-stage model that outperforms\nall existing scene graph models by a large margin of +11 mR@50 and +10 mNgR@50\non the corrected evaluation, thus setting a new SOTA. As a core design\nprinciple, DSFormer encodes subject and object masks directly into feature\nspace.\n","authors":["Julian Lorenz","Alexander Pest","Daniel Kienzle","Katja Ludwig","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2407.09216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09215v1","updated":"2024-07-12T12:25:42Z","published":"2024-07-12T12:25:42Z","title":"HUP-3D: A 3D multi-view synthetic dataset for assisted-egocentric\n  hand-ultrasound pose estimation","summary":"  We present HUP-3D, a 3D multi-view multi-modal synthetic dataset for\nhand-ultrasound (US) probe pose estimation in the context of obstetric\nultrasound. Egocentric markerless 3D joint pose estimation has potential\napplications in mixed reality based medical education. The ability to\nunderstand hand and probe movements programmatically opens the door to tailored\nguidance and mentoring applications. Our dataset consists of over 31k sets of\nRGB, depth and segmentation mask frames, including pose related ground truth\ndata, with a strong emphasis on image diversity and complexity. Adopting a\ncamera viewpoint-based sphere concept allows us to capture a variety of views\nand generate multiple hand grasp poses using a pre-trained network.\nAdditionally, our approach includes a software-based image rendering concept,\nenhancing diversity with various hand and arm textures, lighting conditions,\nand background images. Furthermore, we validated our proposed dataset with\nstate-of-the-art learning models and we obtained the lowest hand-object\nkeypoint errors. The dataset and other details are provided with the\nsupplementary material. The source code of our grasp generation and rendering\npipeline will be made publicly available.\n","authors":["Manuel Birlo","Razvan Caramalau","Philip J. \"Eddie\" Edwards","Brian Dromey","Matthew J. Clarkson","Danail Stoyanov"],"pdf_url":"https://arxiv.org/pdf/2407.09215v1.pdf","comment":"https://conferences.miccai.org/2024/en/"},{"id":"http://arxiv.org/abs/2407.04061v3","updated":"2024-07-12T12:13:48Z","published":"2024-07-04T17:06:16Z","title":"Detect Closer Surfaces that can be Seen: New Modeling and Evaluation in\n  Cross-domain 3D Object Detection","summary":"  The performance of domain adaptation technologies has not yet reached an\nideal level in the current 3D object detection field for autonomous driving,\nwhich is mainly due to significant differences in the size of vehicles, as well\nas the environments they operate in when applied across domains. These factors\ntogether hinder the effective transfer and application of knowledge learned\nfrom specific datasets. Since the existing evaluation metrics are initially\ndesigned for evaluation on a single domain by calculating the 2D or 3D overlap\nbetween the prediction and ground-truth bounding boxes, they often suffer from\nthe overfitting problem caused by the size differences among datasets. This\nraises a fundamental question related to the evaluation of the 3D object\ndetection models' cross-domain performance: Do we really need models to\nmaintain excellent performance in their original 3D bounding boxes after being\napplied across domains? From a practical application perspective, one of our\nmain focuses is actually on preventing collisions between vehicles and other\nobstacles, especially in cross-domain scenarios where correctly predicting the\nsize of vehicles is much more difficult. In other words, as long as a model can\naccurately identify the closest surfaces to the ego vehicle, it is sufficient\nto effectively avoid obstacles. In this paper, we propose two metrics to\nmeasure 3D object detection models' ability of detecting the closer surfaces to\nthe sensor on the ego vehicle, which can be used to evaluate their cross-domain\nperformance more comprehensively and reasonably. Furthermore, we propose a\nrefinement head, named EdgeHead, to guide models to focus more on the learnable\ncloser surfaces, which can greatly improve the cross-domain performance of\nexisting models not only under our new metrics, but even also under the\noriginal BEV/3D metrics.\n","authors":["Ruixiao Zhang","Yihong Wu","Juheon Lee","Adam Prugel-Bennett","Xiaohao Cai"],"pdf_url":"https://arxiv.org/pdf/2407.04061v3.pdf","comment":"Accepted by the 27th European Conference on Artificial Intelligence\n  (ECAI 2024)"},{"id":"http://arxiv.org/abs/2407.04504v2","updated":"2024-07-12T12:06:25Z","published":"2024-07-05T13:44:15Z","title":"Segment Any 4D Gaussians","summary":"  Modeling, understanding, and reconstructing the real world are crucial in\nXR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable\nsuccess in modeling and understanding 3D scenes. Similarly, various 4D\nrepresentations have demonstrated the ability to capture the dynamics of the 4D\nworld. However, there is a dearth of research focusing on segmentation within\n4D representations. In this paper, we propose Segment Any 4D Gaussians (SA4D),\none of the first frameworks to segment anything in the 4D digital world based\non 4D Gaussians. In SA4D, an efficient temporal identity feature field is\nintroduced to handle Gaussian drifting, with the potential to learn precise\nidentity features from noisy and sparse input. Additionally, a 4D segmentation\nrefinement process is proposed to remove artifacts. Our SA4D achieves precise,\nhigh-quality segmentation within seconds in 4D Gaussians and shows the ability\nto remove, recolor, compose, and render high-quality anything masks. More demos\nare available at: https://jsxzs.github.io/sa4d/.\n","authors":["Shengxiang Ji","Guanjun Wu","Jiemin Fang","Jiazhong Cen","Taoran Yi","Wenyu Liu","Qi Tian","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.04504v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2407.09192v1","updated":"2024-07-12T11:50:39Z","published":"2024-07-12T11:50:39Z","title":"Salt & Pepper Heatmaps: Diffusion-informed Landmark Detection Strategy","summary":"  Anatomical Landmark Detection is the process of identifying key areas of an\nimage for clinical measurements. Each landmark is a single ground truth point\nlabelled by a clinician. A machine learning model predicts the locus of a\nlandmark as a probability region represented by a heatmap. Diffusion models\nhave increased in popularity for generative modelling due to their high quality\nsampling and mode coverage, leading to their adoption in medical image\nprocessing for semantic segmentation. Diffusion modelling can be further\nadapted to learn a distribution over landmarks. The stochastic nature of\ndiffusion models captures fluctuations in the landmark prediction, which we\nleverage by blurring into meaningful probability regions. In this paper, we\nreformulate automatic Anatomical Landmark Detection as a precise generative\nmodelling task, producing a few-hot pixel heatmap. Our method achieves\nstate-of-the-art MRE and comparable SDR performance with existing work.\n","authors":["Julian Wyatt","Irina Voiculescu"],"pdf_url":"https://arxiv.org/pdf/2407.09192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09191v1","updated":"2024-07-12T11:48:33Z","published":"2024-07-12T11:48:33Z","title":"From Easy to Hard: Learning Curricular Shape-aware Features for Robust\n  Panoptic Scene Graph Generation","summary":"  Panoptic Scene Graph Generation (PSG) aims to generate a comprehensive\ngraph-structure representation based on panoptic segmentation masks. Despite\nremarkable progress in PSG, almost all existing methods neglect the importance\nof shape-aware features, which inherently focus on the contours and boundaries\nof objects. To bridge this gap, we propose a model-agnostic Curricular\nshApe-aware FEature (CAFE) learning strategy for PSG. Specifically, we\nincorporate shape-aware features (i.e., mask features and boundary features)\ninto PSG, moving beyond reliance solely on bbox features. Furthermore, drawing\ninspiration from human cognition, we propose to integrate shape-aware features\nin an easy-to-hard manner. To achieve this, we categorize the predicates into\nthree groups based on cognition learning difficulty and correspondingly divide\nthe training process into three stages. Each stage utilizes a specialized\nrelation classifier to distinguish specific groups of predicates. As the\nlearning difficulty of predicates increases, these classifiers are equipped\nwith features of ascending complexity. We also incorporate knowledge\ndistillation to retain knowledge acquired in earlier stages. Due to its\nmodel-agnostic nature, CAFE can be seamlessly incorporated into any PSG model.\nExtensive experiments and ablations on two PSG tasks under both robust and\nzero-shot PSG have attested to the superiority and robustness of our proposed\nCAFE, which outperforms existing state-of-the-art methods by a large margin.\n","authors":["Hanrong Shi","Lin Li","Jun Xiao","Yueting Zhuang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2407.09191v1.pdf","comment":"Accepted by IJCV"},{"id":"http://arxiv.org/abs/2309.03774v3","updated":"2024-07-12T11:46:08Z","published":"2023-09-07T15:25:47Z","title":"Deep Learning Safety Concerns in Automated Driving Perception","summary":"  Recent advances in the field of deep learning and impressive performance of\ndeep neural networks (DNNs) for perception have resulted in an increased demand\nfor their use in automated driving (AD) systems. The safety of such systems is\nof utmost importance and thus requires to consider the unique properties of\nDNNs.\n  In order to achieve safety of AD systems with DNN-based perception components\nin a systematic and comprehensive approach, so-called safety concerns have been\nintroduced as a suitable structuring element. On the one hand, the concept of\nsafety concerns is -- by design -- well aligned to existing standards relevant\nfor safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has\nalready inspired several academic publications and upcoming standards on AI\nsafety such as ISO PAS 8800.\n  While the concept of safety concerns has been previously introduced, this\npaper extends and refines it, leveraging feedback from various domain and\nsafety experts in the field. In particular, this paper introduces an additional\ncategorization for a better understanding as well as enabling cross-functional\nteams to jointly address the concerns.\n","authors":["Stephanie Abrecht","Alexander Hirsch","Shervin Raafatnia","Matthias Woehrle"],"pdf_url":"https://arxiv.org/pdf/2309.03774v3.pdf","comment":"Added note regarding accepted version at IEEE Transactions on\n  Intelligent Vehicles with DOI"},{"id":"http://arxiv.org/abs/2210.03919v5","updated":"2024-07-12T11:44:29Z","published":"2022-10-08T05:12:25Z","title":"CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features\n  for a Disentangled, Interpretable, and Controllable Text-Guided Face\n  Manipulation","summary":"  Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges\nimages and text by embedding them into a joint latent space. This opens the\ndoor to ample literature that aims to manipulate an input image by providing a\ntextual explanation. However, due to the discrepancy between image and text\nembeddings in the joint space, using text embeddings as the optimization target\noften introduces undesired artifacts in the resulting images. Disentanglement,\ninterpretability, and controllability are also hard to guarantee for\nmanipulation. To alleviate these problems, we propose to define corpus\nsubspaces spanned by relevant prompts to capture specific image\ncharacteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as\nan optimization target to improve the performance of text-guided image\nmanipulation. Our method is a simple and general paradigm that can be easily\ncomputed and adapted, and smoothly incorporated into any CLIP-based image\nmanipulation algorithm. To demonstrate the effectiveness of our method, we\nconduct several theoretical and empirical studies. As a case study, we utilize\nthe method for text-guided semantic face editing. We quantitatively and\nqualitatively demonstrate that PAE facilitates a more disentangled,\ninterpretable, and controllable image manipulation with state-of-the-art\nquality and accuracy. Project page: https://chenliang-zhou.github.io/CLIP-PAE/.\n","authors":["Chenliang Zhou","Fangcheng Zhong","Cengiz Oztireli"],"pdf_url":"https://arxiv.org/pdf/2210.03919v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04820v2","updated":"2024-07-12T11:41:33Z","published":"2023-09-09T15:18:46Z","title":"ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class\n  Class-agnostic Counting","summary":"  Class-agnostic counting methods enumerate objects of an arbitrary class,\nproviding tremendous utility in many fields. Prior works have limited\nusefulness as they require either a set of examples of the type to be counted\nor that the query image contains only a single type of object. A significant\nfactor in these shortcomings is the lack of a dataset to properly address\ncounting in settings with more than one kind of object present. To address\nthese issues, we propose the first Multi-class, Class-Agnostic Counting dataset\n(MCAC) and A Blind Counter (ABC123), a method that can count multiple types of\nobjects simultaneously without using examples of type during training or\ninference. ABC123 introduces a new paradigm where instead of requiring\nexemplars to guide the enumeration, examples are found after the counting stage\nto help a user understand the generated outputs. We show that ABC123\noutperforms contemporary methods on MCAC without needing human in-the-loop\nannotations. We also show that this performance transfers to FSC-147, the\nstandard class-agnostic counting dataset. MCAC is available at\nMCAC.active.vision and ABC123 is available at ABC123.active.vision.\n","authors":["Michael A. Hobley","Victor A. Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2309.04820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09189v1","updated":"2024-07-12T11:41:18Z","published":"2024-07-12T11:41:18Z","title":"Segmenting Medical Images with Limited Data","summary":"  While computer vision has proven valuable for medical image segmentation, its\napplication faces challenges such as limited dataset sizes and the complexity\nof effectively leveraging unlabeled images. To address these challenges, we\npresent a novel semi-supervised, consistency-based approach termed the\ndata-efficient medical segmenter (DEMS). The DEMS features an encoder-decoder\narchitecture and incorporates the developed online automatic augmenter (OAA)\nand residual robustness enhancement (RRE) blocks. The OAA augments input data\nwith various image transformations, thereby diversifying the dataset to improve\nthe generalization ability. The RRE enriches feature diversity and introduces\nperturbations to create varied inputs for different decoders, thereby providing\nenhanced variability. Moreover, we introduce a sensitive loss to further\nenhance consistency across different decoders and stabilize the training\nprocess. Extensive experimental results on both our own and three public\ndatasets affirm the effectiveness of DEMS. Under extreme data shortage\nscenarios, our DEMS achieves 16.85\\% and 10.37\\% improvement in dice score\ncompared with the U-Net and top-performed state-of-the-art method,\nrespectively. Given its superior data efficiency, DEMS could present\nsignificant advancements in medical segmentation under small data regimes. The\nproject homepage can be accessed at https://github.com/NUS-Tim/DEMS.\n","authors":["Zhaoshan Liua","Qiujie Lv","Chau Hung Lee","Lei Shen"],"pdf_url":"https://arxiv.org/pdf/2407.09189v1.pdf","comment":"Neural Networks Accepted"},{"id":"http://arxiv.org/abs/2406.10744v3","updated":"2024-07-12T11:31:25Z","published":"2024-06-15T21:44:17Z","title":"Technique Report of CVPR 2024 PBDL Challenges","summary":"  The intersection of physics-based vision and deep learning presents an\nexciting frontier for advancing computer vision technologies. By leveraging the\nprinciples of physics to inform and enhance deep learning models, we can\ndevelop more robust and accurate vision systems. Physics-based vision aims to\ninvert the processes to recover scene properties such as shape, reflectance,\nlight distribution, and medium properties from images. In recent years, deep\nlearning has shown promising improvements for various vision tasks, and when\ncombined with physics-based vision, these approaches can enhance the robustness\nand accuracy of vision systems. This technical report summarizes the outcomes\nof the Physics-Based Vision Meets Deep Learning (PBDL) 2024 challenge, held in\nCVPR 2024 workshop. The challenge consisted of eight tracks, focusing on\nLow-Light Enhancement and Detection as well as High Dynamic Range (HDR)\nImaging. This report details the objectives, methodologies, and results of each\ntrack, highlighting the top-performing solutions and their innovative\napproaches.\n","authors":["Ying Fu","Yu Li","Shaodi You","Boxin Shi","Linwei Chen","Yunhao Zou","Zichun Wang","Yichen Li","Yuze Han","Yingkai Zhang","Jianan Wang","Qinglin Liu","Wei Yu","Xiaoqian Lv","Jianing Li","Shengping Zhang","Xiangyang Ji","Yuanpei Chen","Yuhan Zhang","Weihang Peng","Liwen Zhang","Zhe Xu","Dingyong Gou","Cong Li","Senyan Xu","Yunkang Zhang","Siyuan Jiang","Xiaoqiang Lu","Licheng Jiao","Fang Liu","Xu Liu","Lingling Li","Wenping Ma","Shuyuan Yang","Haiyang Xie","Jian Zhao","Shihua Huang","Peng Cheng","Xi Shen","Zheng Wang","Shuai An","Caizhi Zhu","Xuelong Li","Tao Zhang","Liang Li","Yu Liu","Chenggang Yan","Gengchen Zhang","Linyan Jiang","Bingyi Song","Zhuoyu An","Haibo Lei","Qing Luo","Jie Song","Yuan Liu","Qihang Li","Haoyuan Zhang","Lingfeng Wang","Wei Chen","Aling Luo","Cheng Li","Jun Cao","Shu Chen","Zifei Dou","Xinyu Liu","Jing Zhang","Kexin Zhang","Yuting Yang","Xuejian Gou","Qinliang Wang","Yang Liu","Shizhan Zhao","Yanzhao Zhang","Libo Yan","Yuwei Guo","Guoxin Li","Qiong Gao","Chenyue Che","Long Sun","Xiang Chen","Hao Li","Jinshan Pan","Chuanlong Xie","Hongming Chen","Mingrui Li","Tianchen Deng","Jingwei Huang","Yufeng Li","Fei Wan","Bingxin Xu","Jian Cheng","Hongzhe Liu","Cheng Xu","Yuxiang Zou","Weiguo Pan","Songyin Dai","Sen Jia","Junpei Zhang","Puhua Chen","Qihang Li"],"pdf_url":"https://arxiv.org/pdf/2406.10744v3.pdf","comment":"CVPR 2024 PBDL Challenges:\n  https://pbdl-ws.github.io/pbdl2024/challenge/index.html"},{"id":"http://arxiv.org/abs/2311.12090v2","updated":"2024-07-12T11:29:11Z","published":"2023-11-20T18:43:31Z","title":"FrePolad: Frequency-Rectified Point Latent Diffusion for Point Cloud\n  Generation","summary":"  We propose FrePolad: frequency-rectified point latent diffusion, a point\ncloud generation pipeline integrating a variational autoencoder (VAE) with a\ndenoising diffusion probabilistic model (DDPM) for the latent distribution.\nFrePolad simultaneously achieves high quality, diversity, and flexibility in\npoint cloud cardinality for generation tasks while maintaining high\ncomputational efficiency. The improvement in generation quality and diversity\nis achieved through (1) a novel frequency rectification via spherical harmonics\ndesigned to retain high-frequency content while learning the point cloud\ndistribution; and (2) a latent DDPM to learn the regularized yet complex latent\ndistribution. In addition, FrePolad supports variable point cloud cardinality\nby formulating the sampling of points as conditional distributions over a\nlatent shape distribution. Finally, the low-dimensional latent space encoded by\nthe VAE contributes to FrePolad's fast and scalable sampling. Our quantitative\nand qualitative results demonstrate FrePolad's state-of-the-art performance in\nterms of quality, diversity, and computational efficiency. Project page:\nhttps://chenliang-zhou.github.io/FrePolad/.\n","authors":["Chenliang Zhou","Fangcheng Zhong","Param Hanji","Zhilin Guo","Kyle Fogarty","Alejandro Sztrajman","Hongyun Gao","Cengiz Oztireli"],"pdf_url":"https://arxiv.org/pdf/2311.12090v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16037v5","updated":"2024-07-12T11:28:05Z","published":"2023-05-25T13:16:39Z","title":"GenerateCT: Text-Conditional Generation of 3D Chest CT Volumes","summary":"  GenerateCT, the first approach to generating 3D medical imaging conditioned\non free-form medical text prompts, incorporates a text encoder and three key\ncomponents: a novel causal vision transformer for encoding 3D CT volumes, a\ntext-image transformer for aligning CT and text tokens, and a text-conditional\nsuper-resolution diffusion model. Without directly comparable methods in 3D\nmedical imaging, we benchmarked GenerateCT against cutting-edge methods,\ndemonstrating its superiority across all key metrics. Importantly, we evaluated\nGenerateCT's clinical applications in a multi-abnormality classification task.\nFirst, we established a baseline by training a multi-abnormality classifier on\nour real dataset. To further assess the model's generalization to external data\nand performance with unseen prompts in a zero-shot scenario, we employed an\nexternal set to train the classifier, setting an additional benchmark. We\nconducted two experiments in which we doubled the training datasets by\nsynthesizing an equal number of volumes for each set using GenerateCT. The\nfirst experiment demonstrated an 11% improvement in the AP score when training\nthe classifier jointly on real and generated volumes. The second experiment\nshowed a 7% improvement when training on both real and generated volumes based\non unseen prompts. Moreover, GenerateCT enables the scaling of synthetic\ntraining datasets to arbitrary sizes. As an example, we generated 100,000 3D\nCTs, fivefold the number in our real set, and trained the classifier\nexclusively on these synthetic CTs. Impressively, this classifier surpassed the\nperformance of the one trained on all available real data by a margin of 8%.\nLast, domain experts evaluated the generated volumes, confirming a high degree\nof alignment with the text prompt. Access our code, model weights, training\ndata, and generated data at https://github.com/ibrahimethemhamamci/GenerateCT\n","authors":["Ibrahim Ethem Hamamci","Sezgin Er","Anjany Sekuboyina","Enis Simsar","Alperen Tezcan","Ayse Gulnihan Simsek","Sevval Nil Esirgun","Furkan Almas","Irem Dogan","Muhammed Furkan Dasdelen","Chinmay Prabhakar","Hadrien Reynaud","Sarthak Pati","Christian Bluethgen","Mehmet Kemal Ozdemir","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2305.16037v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17325v2","updated":"2024-07-12T11:23:31Z","published":"2023-11-29T02:44:54Z","title":"Alternate Diverse Teaching for Semi-supervised Medical Image\n  Segmentation","summary":"  Semi-supervised medical image segmentation studies have shown promise in\ntraining models with limited labeled data. However, current dominant\nteacher-student based approaches can suffer from the confirmation bias. To\naddress this challenge, we propose AD-MT, an alternate diverse teaching\napproach in a teacher-student framework. It involves a single student model and\ntwo non-trainable teacher models that are momentum-updated periodically and\nrandomly in an alternate fashion. To mitigate the confirmation bias from the\ndiverse supervision, the core of AD-MT lies in two proposed modules: the Random\nPeriodic Alternate (RPA) Updating Module and the Conflict-Combating Module\n(CCM). The RPA schedules the alternating diverse updating process with\ncomplementary data batches, distinct data augmentation, and random switching\nperiods to encourage diverse reasoning from different teaching perspectives.\nThe CCM employs an entropy-based ensembling strategy to encourage the model to\nlearn from both the consistent and conflicting predictions between the\nteachers. Experimental results demonstrate the effectiveness and superiority of\nour AD-MT on the 2D and 3D medical segmentation benchmarks across various\nsemi-supervised settings.\n","authors":["Zhen Zhao","Zicheng Wang","Longyue Wang","Dian Yu","Yixuan Yuan","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.17325v2.pdf","comment":"code:https://github.com/ZhenZHAO/AD-MT, accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2406.07516v2","updated":"2024-07-12T11:23:26Z","published":"2024-06-11T17:47:27Z","title":"Instant 3D Human Avatar Generation using Image Diffusion Models","summary":"  We present AvatarPopUp, a method for fast, high quality 3D human avatar\ngeneration from different input modalities, such as images and text prompts and\nwith control over the generated pose and shape. The common theme is the use of\ndiffusion-based image generation networks that are specialized for each\nparticular task, followed by a 3D lifting network. We purposefully decouple the\ngeneration from the 3D modeling which allow us to leverage powerful image\nsynthesis priors, trained on billions of text-image pairs. We fine-tune latent\ndiffusion networks with additional image conditioning for image generation and\nback-view prediction, and to support qualitatively different multiple 3D\nhypotheses. Our partial fine-tuning approach allows to adapt the networks for\neach task without inducing catastrophic forgetting. In our experiments, we\ndemonstrate that our method produces accurate, high-quality 3D avatars with\ndiverse appearance that respect the multimodal text, image, and body control\nsignals. Our approach can produce a 3D model in as few as 2 seconds, a four\norders of magnitude speedup wrt the vast majority of existing methods, most of\nwhich solve only a subset of our tasks, and with fewer controls. AvatarPopUp\nenables applications that require the controlled 3D generation of human avatars\nat scale. The project website can be found at\nhttps://www.nikoskolot.com/avatarpopup/.\n","authors":["Nikos Kolotouros","Thiemo Alldieck","Enric Corona","Eduard Gabriel Bazavan","Cristian Sminchisescu"],"pdf_url":"https://arxiv.org/pdf/2406.07516v2.pdf","comment":"Camera-ready version"},{"id":"http://arxiv.org/abs/2407.09174v1","updated":"2024-07-12T11:16:44Z","published":"2024-07-12T11:16:44Z","title":"DART: An Automated End-to-End Object Detection Pipeline with Data\n  Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label\n  Review, and Model Training","summary":"  Swift and accurate detection of specified objects is crucial for many\nindustrial applications, such as safety monitoring on construction sites.\nHowever, traditional approaches rely heavily on arduous manual annotation and\ndata collection, which struggle to adapt to ever-changing environments and\nnovel target objects. To address these limitations, this paper presents DART,\nan automated end-to-end pipeline designed to streamline the entire workflow of\nan object detection application from data collection to model deployment. DART\neliminates the need for human labeling and extensive data collection while\nexcelling in diverse scenarios. It employs a subject-driven image generation\nmodule (DreamBooth with SDXL) for data diversification, followed by an\nannotation stage where open-vocabulary object detection (Grounding DINO)\ngenerates bounding box annotations for both generated and original images.\nThese pseudo-labels are then reviewed by a large multimodal model (GPT-4o) to\nguarantee credibility before serving as ground truth to train real-time object\ndetectors (YOLO). We apply DART to a self-collected dataset of construction\nmachines named Liebherr Product, which contains over 15K high-quality images\nacross 23 categories. The current implementation of DART significantly\nincreases average precision (AP) from 0.064 to 0.832. Furthermore, we adopt a\nmodular design for DART to ensure easy exchangeability and extensibility. This\nallows for a smooth transition to more advanced algorithms in the future,\nseamless integration of new object categories without manual labeling, and\nadaptability to customized environments without extra data collection. The code\nand dataset are released at https://github.com/chen-xin-94/DART.\n","authors":["Chen Xin","Andreas Hartel","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2407.09174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09172v1","updated":"2024-07-12T11:11:19Z","published":"2024-07-12T11:11:19Z","title":"Machine Apophenia: The Kaleidoscopic Generation of Architectural Images","summary":"  This study investigates the application of generative artificial intelligence\nin architectural design. We present a novel methodology that combines multiple\nneural networks to create an unsupervised and unmoderated stream of unique\narchitectural images. Our approach is grounded in the conceptual framework\ncalled machine apophenia. We hypothesize that neural networks, trained on\ndiverse human-generated data, internalize aesthetic preferences and tend to\nproduce coherent designs even from random inputs. The methodology involves an\niterative process of image generation, description, and refinement, resulting\nin captioned architectural postcards automatically shared on several social\nmedia platforms. Evaluation and ablation studies show the improvement both in\ntechnical and aesthetic metrics of resulting images on each step.\n","authors":["Alexey Tikhonov","Dmitry Sinyavin"],"pdf_url":"https://arxiv.org/pdf/2407.09172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09159v1","updated":"2024-07-12T10:45:25Z","published":"2024-07-12T10:45:25Z","title":"Weakly-supervised Autism Severity Assessment in Long Videos","summary":"  Autism Spectrum Disorder (ASD) is a diverse collection of neurobiological\nconditions marked by challenges in social communication and reciprocal\ninteractions, as well as repetitive and stereotypical behaviors. Atypical\nbehavior patterns in a long, untrimmed video can serve as biomarkers for\nchildren with ASD. In this paper, we propose a video-based weakly-supervised\nmethod that takes spatio-temporal features of long videos to learn typical and\natypical behaviors for autism detection. On top of that, we propose a shallow\nTCN-MLP network, which is designed to further categorize the severity score. We\nevaluate our method on actual evaluation videos of children with autism\ncollected and annotated (for severity score) by clinical professionals.\nExperimental results demonstrate the effectiveness of behavioral biomarkers\nthat could help clinicians in autism spectrum analysis.\n","authors":["Abid Ali","Mahmoud Ali","Jean-Marc Odobez","Camilla Barbini","SÃ©verine Dubuisson","Francois Bremond","Susanne ThÃ¼mmler"],"pdf_url":"https://arxiv.org/pdf/2407.09159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13556v2","updated":"2024-07-12T10:42:30Z","published":"2024-03-20T12:51:30Z","title":"Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban\n  Environments","summary":"  In this work, we tackle the limitations of current LiDAR-based 3D object\ndetection systems, which are hindered by a restricted class vocabulary and the\nhigh costs associated with annotating new object classes. Our exploration of\nopen-vocabulary (OV) learning in urban environments aims to capture novel\ninstances using pre-trained vision-language models (VLMs) with multi-sensor\ndata. We design and benchmark a set of four potential solutions as baselines,\ncategorizing them into either top-down or bottom-up approaches based on their\ninput data strategies. While effective, these methods exhibit certain\nlimitations, such as missing novel objects in 3D box estimation or applying\nrigorous priors, leading to biases towards objects near the camera or of\nrectangular geometries. To overcome these limitations, we introduce a universal\n\\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the\nrecall of novel objects and propagating this detection capability to more\ndistant areas thereby progressively capturing more. In particular, we utilize a\ngreedy box seeker to search against 3D novel boxes of varying orientations and\ndepth in each generated frustum and ensure the reliability of newly identified\nboxes by cross alignment and density ranker. Additionally, the inherent bias\ntowards camera-proximal objects is alleviated by the proposed remote simulator,\nwhich randomly diversifies pseudo-labeled novel instances in the self-training\nprocess, combined with the fusion of base samples in the memory bank. Extensive\nexperiments demonstrate a 53% improvement in novel recall across diverse OV\nsettings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold\nincrease in Average Precision (AP) for novel object classes. The source code is\nmade available at https://github.com/djamahl99/findnpropagate.\n","authors":["Djamahl Etchegaray","Zi Huang","Tatsuya Harada","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13556v2.pdf","comment":"To appear in ECCV 2024. Source code:\n  https://github.com/djamahl99/findnpropagate"},{"id":"http://arxiv.org/abs/2407.09150v1","updated":"2024-07-12T10:32:53Z","published":"2024-07-12T10:32:53Z","title":"Evaluating the Adversarial Robustness of Semantic Segmentation: Trying\n  Harder Pays Off","summary":"  Machine learning models are vulnerable to tiny adversarial input\nperturbations optimized to cause a very large output error. To measure this\nvulnerability, we need reliable methods that can find such adversarial\nperturbations. For image classification models, evaluation methodologies have\nemerged that have stood the test of time. However, we argue that in the area of\nsemantic segmentation, a good approximation of the sensitivity to adversarial\nperturbations requires significantly more effort than what is currently\nconsidered satisfactory. To support this claim, we re-evaluate a number of\nwell-known robust segmentation models in an extensive empirical study. We\npropose new attacks and combine them with the strongest attacks available in\nthe literature. We also analyze the sensitivity of the models in fine detail.\nThe results indicate that most of the state-of-the-art models have a\ndramatically larger sensitivity to adversarial perturbations than previously\nreported. We also demonstrate a size-bias: small objects are often more easily\nattacked, even if the large objects are robust, a phenomenon not revealed by\ncurrent evaluation metrics. Our results also demonstrate that a diverse set of\nstrong attacks is necessary, because different models are often vulnerable to\ndifferent attacks.\n","authors":["Levente Halmosi","BÃ¡lint Mohos","MÃ¡rk Jelasity"],"pdf_url":"https://arxiv.org/pdf/2407.09150v1.pdf","comment":"Accepted for ECCV 2024. For the implementation, see\n  https://github.com/szegedai/Robust-Segmentation-Evaluation"},{"id":"http://arxiv.org/abs/2405.17520v3","updated":"2024-07-12T10:25:59Z","published":"2024-05-27T11:41:48Z","title":"Advancing Medical Image Segmentation with Mini-Net: A Lightweight\n  Solution Tailored for Efficient Segmentation of Medical Images","summary":"  Accurate segmentation of anatomical structures and abnormalities in medical\nimages is crucial for computer-aided diagnosis and analysis. While deep\nlearning techniques excel at this task, their computational demands pose\nchallenges. Additionally, some cutting-edge segmentation methods, though\neffective for general object segmentation, may not be optimised for medical\nimages. To address these issues, we propose Mini-Net, a lightweight\nsegmentation network specifically designed for medical images. With fewer than\n38,000 parameters, Mini-Net efficiently captures both high- and low-frequency\nfeatures, enabling real-time applications in various medical imaging scenarios.\nWe evaluate Mini-Net on various datasets, including DRIVE, STARE, ISIC-2016,\nISIC-2018, and MoNuSeg, demonstrating its robustness and good performance\ncompared to state-of-the-art methods.\n","authors":["Syed Javed","Tariq M. Khan","Abdul Qayyum","Arcot Sowmya","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2405.17520v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15022v4","updated":"2024-07-12T10:16:55Z","published":"2024-01-26T17:29:01Z","title":"Applications of artificial intelligence in the analysis of\n  histopathology images of gliomas: a review","summary":"  In recent years, the diagnosis of gliomas has become increasingly complex.\nAnalysis of glioma histopathology images using artificial intelligence (AI)\noffers new opportunities to support diagnosis and outcome prediction. To give\nan overview of the current state of research, this review examines 83 publicly\navailable research studies that have proposed AI-based methods for whole-slide\nhistopathology images of human gliomas, covering the diagnostic tasks of\nsubtyping (23/83), grading (27/83), molecular marker prediction (20/83), and\nsurvival prediction (29/83). All studies were reviewed with regard to\nmethodological aspects as well as clinical applicability. It was found that the\nfocus of current research is the assessment of hematoxylin and eosin-stained\ntissue sections of adult-type diffuse gliomas. The majority of studies (52/83)\nare based on the publicly available glioblastoma and low-grade glioma datasets\nfrom The Cancer Genome Atlas (TCGA) and only a few studies employed other\ndatasets in isolation (16/83) or in addition to the TCGA datasets (15/83).\nCurrent approaches mostly rely on convolutional neural networks (63/83) for\nanalyzing tissue at 20x magnification (35/83). A new field of research is the\nintegration of clinical data, omics data, or magnetic resonance imaging\n(29/83). So far, AI-based methods have achieved promising results, but are not\nyet used in real clinical settings. Future work should focus on the independent\nvalidation of methods on larger, multi-site datasets with high-quality and\nup-to-date clinical and molecular pathology annotations to demonstrate routine\napplicability.\n","authors":["Jan-Philipp Redlich","Friedrich Feuerhake","Joachim Weis","Nadine S. Schaadt","Sarah Teuber-Hanselmann","Christoph Buck","Sabine Luttmann","Andrea Eberle","Stefan Nikolin","Arno Appenzeller","Andreas Portmann","AndrÃ© Homeyer"],"pdf_url":"https://arxiv.org/pdf/2401.15022v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08341v2","updated":"2024-07-12T10:15:45Z","published":"2024-07-11T09:44:40Z","title":"Adaptive Deep Iris Feature Extractor at Arbitrary Resolutions","summary":"  This paper proposes a deep feature extractor for iris recognition at\narbitrary resolutions. Resolution degradation reduces the recognition\nperformance of deep learning models trained by high-resolution images. Using\nvarious-resolution images for training can improve the model's robustness while\nsacrificing recognition performance for high-resolution images. To achieve\nhigher recognition performance at various resolutions, we propose a method of\nresolution-adaptive feature extraction with automatically switching networks.\nOur framework includes resolution expert modules specialized for different\nresolution degradations, including down-sampling and out-of-focus blurring. The\nframework automatically switches them depending on the degradation condition of\nan input image. Lower-resolution experts are trained by knowledge-distillation\nfrom the high-resolution expert in such a manner that both experts can extract\ncommon identity features. We applied our framework to three conventional neural\nnetwork models. The experimental results show that our method enhances the\nrecognition performance at low-resolution in the conventional methods and also\nmaintains their performance at high-resolution.\n","authors":["Yuho Shoji","Yuka Ogino","Takahiro Toizumi","Atsushi Ito"],"pdf_url":"https://arxiv.org/pdf/2407.08341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09120v1","updated":"2024-07-12T09:35:25Z","published":"2024-07-12T09:35:25Z","title":"URRL-IMVC: Unified and Robust Representation Learning for Incomplete\n  Multi-View Clustering","summary":"  Incomplete multi-view clustering (IMVC) aims to cluster multi-view data that\nare only partially available. This poses two main challenges: effectively\nleveraging multi-view information and mitigating the impact of missing views.\nPrevailing solutions employ cross-view contrastive learning and missing view\nrecovery techniques. However, they either neglect valuable complementary\ninformation by focusing only on consensus between views or provide unreliable\nrecovered views due to the absence of supervision. To address these\nlimitations, we propose a novel Unified and Robust Representation Learning for\nIncomplete Multi-View Clustering (URRL-IMVC). URRL-IMVC directly learns a\nunified embedding that is robust to view missing conditions by integrating\ninformation from multiple views and neighboring samples. Firstly, to overcome\nthe limitations of cross-view contrastive learning, URRL-IMVC incorporates an\nattention-based auto-encoder framework to fuse multi-view information and\ngenerate unified embeddings. Secondly, URRL-IMVC directly enhances the\nrobustness of the unified embedding against view-missing conditions through KNN\nimputation and data augmentation techniques, eliminating the need for explicit\nmissing view recovery. Finally, incremental improvements are introduced to\nfurther enhance the overall performance, such as the Clustering Module and the\ncustomization of the Encoder. We extensively evaluate the proposed URRL-IMVC\nframework on various benchmark datasets, demonstrating its state-of-the-art\nperformance. Furthermore, comprehensive ablation studies are performed to\nvalidate the effectiveness of our design.\n","authors":["Ge Teng","Ting Mao","Chen Shen","Xiang Tian","Xuesong Liu","Yaowu Chen","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.09120v1.pdf","comment":"Accepted by ACM SIGKDD 2024"},{"id":"http://arxiv.org/abs/2407.09115v1","updated":"2024-07-12T09:30:09Z","published":"2024-07-12T09:30:09Z","title":"Layer-Wise Relevance Propagation with Conservation Property for ResNet","summary":"  The transparent formulation of explanation methods is essential for\nelucidating the predictions of neural networks, which are typically black-box\nmodels. Layer-wise Relevance Propagation (LRP) is a well-established method\nthat transparently traces the flow of a model's prediction backward through its\narchitecture by backpropagating relevance scores. However, the conventional LRP\ndoes not fully consider the existence of skip connections, and thus its\napplication to the widely used ResNet architecture has not been thoroughly\nexplored. In this study, we extend LRP to ResNet models by introducing\nRelevance Splitting at points where the output from a skip connection converges\nwith that from a residual block. Our formulation guarantees the conservation\nproperty throughout the process, thereby preserving the integrity of the\ngenerated explanations. To evaluate the effectiveness of our approach, we\nconduct experiments on ImageNet and the Caltech-UCSD Birds-200-2011 dataset.\nOur method achieves superior performance to that of baseline methods on\nstandard evaluation metrics such as the Insertion-Deletion score while\nmaintaining its conservation property. We will release our code for further\nresearch at https://5ei74r0.github.io/lrp-for-resnet.page/\n","authors":["Seitaro Otsuki","Tsumugi Iida","FÃ©lix Doublet","Tsubasa Hirakawa","Takayoshi Yamashita","Hironobu Fujiyoshi","Komei Sugiura"],"pdf_url":"https://arxiv.org/pdf/2407.09115v1.pdf","comment":"Accepted for presentation at ECCV2024"},{"id":"http://arxiv.org/abs/2404.15256v3","updated":"2024-07-12T09:12:19Z","published":"2024-04-23T17:42:45Z","title":"TOP-Nav: Legged Navigation Integrating Terrain, Obstacle and\n  Proprioception Estimation","summary":"  Legged navigation is typically examined within open-world, off-road, and\nchallenging environments. In these scenarios, estimating external disturbances\nrequires a complex synthesis of multi-modal information. This underlines a\nmajor limitation in existing works that primarily focus on avoiding obstacles.\nIn this work, we propose TOP-Nav, a novel legged navigation framework that\nintegrates a comprehensive path planner with Terrain awareness, Obstacle\navoidance and close-loop Proprioception. TOP-Nav underscores the synergies\nbetween vision and proprioception in both path and motion planning. Within the\npath planner, we present and integrate a terrain estimator that enables the\nrobot to select waypoints on terrains with higher traversability while\neffectively avoiding obstacles. In the motion planning level, we not only\nimplement a locomotion controller to track the navigation commands, but also\nconstruct a proprioception advisor to provide motion evaluations for the path\nplanner. Based on the close-loop motion feedback, we make online corrections\nfor the vision-based terrain and obstacle estimations. Consequently, TOP-Nav\nachieves open-world navigation that the robot can handle terrains or\ndisturbances beyond the distribution of prior knowledge and overcomes\nconstraints imposed by visual conditions. Building upon extensive experiments\nconducted in both simulation and real-world environments, TOP-Nav demonstrates\nsuperior performance in open-world navigation compared to existing methods.\n","authors":["Junli Ren","Yikai Liu","Yingru Dai","Junfeng Long","Guijin Wang"],"pdf_url":"https://arxiv.org/pdf/2404.15256v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05309v2","updated":"2024-07-12T09:10:30Z","published":"2024-04-08T08:57:32Z","title":"CLIPping the Limits: Finding the Sweet Spot for Relevant Images in\n  Automated Driving Systems Perception Testing","summary":"  Perception systems, especially cameras, are the eyes of automated driving\nsystems. Ensuring that they function reliably and robustly is therefore an\nimportant building block in the automation of vehicles. There are various\napproaches to test the perception of automated driving systems. Ultimately,\nhowever, it always comes down to the investigation of the behavior of\nperception systems under specific input data. Camera images are a crucial part\nof the input data. Image data sets are therefore collected for the testing of\nautomated driving systems, but it is non-trivial to find specific images in\nthese data sets. Thanks to recent developments in neural networks, there are\nnow methods for sorting the images in a data set according to their similarity\nto a prompt in natural language. In order to further automate the provision of\nsearch results, we make a contribution by automating the threshold definition\nin these sorted results and returning only the images relevant to the prompt as\na result. Our focus is on preventing false positives and false negatives\nequally. It is also important that our method is robust and in the case that\nour assumptions are not fulfilled, we provide a fallback solution.\n","authors":["Philipp Rigoll","Laurenz Adolph","Lennart Ries","Eric Sax"],"pdf_url":"https://arxiv.org/pdf/2404.05309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04569v2","updated":"2024-07-12T09:09:39Z","published":"2024-06-07T01:11:31Z","title":"Camera-Pose Robust Crater Detection from Chang'e 5","summary":"  As space missions aim to explore increasingly hazardous terrain, accurate and\ntimely position estimates are required to ensure safe navigation. Vision-based\nnavigation achieves this goal through correlating impact craters visible\nthrough onboard imagery with a known database to estimate a craft's pose.\nHowever, existing literature has not sufficiently evaluated crater-detection\nalgorithm (CDA) performance from imagery containing off-nadir view angles. In\nthis work, we evaluate the performance of Mask R-CNN for crater detection,\ncomparing models pretrained on simulated data containing off-nadir view angles\nand to pretraining on real-lunar images. We demonstrate pretraining on\nreal-lunar images is superior despite the lack of images containing off-nadir\nview angles, achieving detection performance of 63.1 F1-score and\nellipse-regression performance of 0.701 intersection over union. This work\nprovides the first quantitative analysis of performance of CDAs on images\ncontaining off-nadir view angles. Towards the development of increasingly\nrobust CDAs, we additionally provide the first annotated CDA dataset with\noff-nadir view angles from the Chang'e 5 Landing Camera.\n","authors":["Matthew Rodda","Sofia McLeod","Ky Cuong Pham","Tat-Jun Chin"],"pdf_url":"https://arxiv.org/pdf/2406.04569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05547v2","updated":"2024-07-12T09:03:04Z","published":"2024-07-08T01:40:32Z","title":"LaSe-E2V: Towards Language-guided Semantic-Aware Event-to-Video\n  Reconstruction","summary":"  Event cameras harness advantages such as low latency, high temporal\nresolution, and high dynamic range (HDR), compared to standard cameras. Due to\nthe distinct imaging paradigm shift, a dominant line of research focuses on\nevent-to-video (E2V) reconstruction to bridge event-based and standard computer\nvision. However, this task remains challenging due to its inherently ill-posed\nnature: event cameras only detect the edge and motion information locally.\nConsequently, the reconstructed videos are often plagued by artifacts and\nregional blur, primarily caused by the ambiguous semantics of event data. In\nthis paper, we find language naturally conveys abundant semantic information,\nrendering it stunningly superior in ensuring semantic consistency for E2V\nreconstruction. Accordingly, we propose a novel framework, called LaSe-E2V,\nthat can achieve semantic-aware high-quality E2V reconstruction from a\nlanguage-guided perspective, buttressed by the text-conditional diffusion\nmodels. However, due to diffusion models' inherent diversity and randomness, it\nis hardly possible to directly apply them to achieve spatial and temporal\nconsistency for E2V reconstruction. Thus, we first propose an Event-guided\nSpatiotemporal Attention (ESA) module to condition the event data to the\ndenoising pipeline effectively. We then introduce an event-aware mask loss to\nensure temporal coherence and a noise initialization strategy to enhance\nspatial consistency. Given the absence of event-text-video paired data, we\naggregate existing E2V datasets and generate textual descriptions using the\ntagging models for training and evaluation. Extensive experiments on three\ndatasets covering diverse challenging scenarios (e.g., fast motion, low light)\ndemonstrate the superiority of our method.\n","authors":["Kanghao Chen","Hangyu Li","JiaZhou Zhou","Zeyu Wang","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05547v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2402.04324 by other authors"},{"id":"http://arxiv.org/abs/2404.00469v3","updated":"2024-07-12T09:00:03Z","published":"2024-03-30T20:25:16Z","title":"SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs","summary":"  We introduce a novel problem, i.e., the localization of an input image within\na multi-modal reference map represented by a database of 3D scene graphs. These\ngraphs comprise multiple modalities, including object-level point clouds,\nimages, attributes, and relationships between objects, offering a lightweight\nand efficient alternative to conventional methods that rely on extensive image\ndatabases. Given the available modalities, the proposed method SceneGraphLoc\nlearns a fixed-sized embedding for each node (i.e., representing an object\ninstance) in the scene graph, enabling effective matching with the objects\nvisible in the input query image. This strategy significantly outperforms other\ncross-modal methods, even without incorporating images into the map embeddings.\nWhen images are leveraged, SceneGraphLoc achieves performance close to that of\nstate-of-the-art techniques depending on large image databases, while requiring\nthree orders-of-magnitude less storage and operating orders-of-magnitude\nfaster. The code will be made public.\n","authors":["Yang Miao","Francis Engelmann","Olga Vysotska","Federico Tombari","Marc Pollefeys","DÃ¡niel BÃ©la BarÃ¡th"],"pdf_url":"https://arxiv.org/pdf/2404.00469v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18977v2","updated":"2024-07-12T08:57:43Z","published":"2024-06-27T08:13:33Z","title":"RoboUniView: Visual-Language Model with Unified View Representation for\n  Robotic Manipulaiton","summary":"  Utilizing Vision-Language Models (VLMs) for robotic manipulation represents a\nnovel paradigm, aiming to enhance the model's ability to generalize to new\nobjects and instructions. However, due to variations in camera specifications\nand mounting positions, existing methods exhibit significant performance\ndisparities across different robotic platforms. To address this challenge, we\npropose RoboUniView in this paper, an innovative approach that decouples visual\nfeature extraction from action learning. We first learn a unified view\nrepresentation from multi-perspective views by pre-training on readily\naccessible data, and then derive actions from this unified view representation\nto control robotic manipulation. This unified view representation more\naccurately mirrors the physical world and is not constrained by the robotic\nplatform's camera parameters. Thanks to this methodology, we achieve\nstate-of-the-art performance on the demanding CALVIN benchmark, enhancing the\nsuccess rate in the $D \\to D$ setting from 93.0% to 96.2%, and in the $ABC \\to\nD$ setting from 92.2% to 94.2%. Moreover, our model exhibits outstanding\nadaptability and flexibility: it maintains high performance under unseen camera\nparameters, can utilize multiple datasets with varying camera parameters, and\nis capable of joint cross-task learning across datasets. Code is provided for\nre-implementation. https://github.com/liufanfanlff/RoboUniview\n","authors":["Fanfan Liu","Feng Yan","Liming Zheng","Chengjian Feng","Yiyang Huang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2406.18977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08418v3","updated":"2024-07-12T08:54:51Z","published":"2024-06-12T17:01:04Z","title":"OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images\n  Interleaved with Text","summary":"  Image-text interleaved data, consisting of multiple images and texts arranged\nin a natural document format, aligns with the presentation paradigm of internet\ndata and closely resembles human reading habits. Recent studies have shown that\nsuch data aids multimodal in-context learning and maintains the capabilities of\nlarge language models during multimodal fine-tuning. However, the limited scale\nand diversity of current image-text interleaved data restrict the development\nof multimodal large language models. In this paper, we introduce OmniCorpus, a\n10 billion-scale image-text interleaved dataset. Using an efficient data\nengine, we filter and extract large-scale high-quality documents, which contain\n8.6 billion images and 1,696 billion text tokens. Compared to counterparts\n(e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while\nmaintaining good data quality; 2) features more diverse sources, including both\nEnglish and non-English websites as well as video-centric websites; 3) is more\nflexible, easily degradable from an image-text interleaved format to pure text\ncorpus and image-text pairs. Through comprehensive analysis and experiments, we\nvalidate the quality, usability, and effectiveness of the proposed dataset. We\nhope this could provide a solid data foundation for future multimodal model\nresearch. Code and data are released at\nhttps://github.com/OpenGVLab/OmniCorpus.\n","authors":["Qingyun Li","Zhe Chen","Weiyun Wang","Wenhai Wang","Shenglong Ye","Zhenjiang Jin","Guanzhou Chen","Yinan He","Zhangwei Gao","Erfei Cui","Jiashuo Yu","Hao Tian","Jiasheng Zhou","Chao Xu","Bin Wang","Xingjian Wei","Wei Li","Wenjian Zhang","Bo Zhang","Pinlong Cai","Licheng Wen","Xiangchao Yan","Zhenxiang Li","Pei Chu","Yi Wang","Min Dou","Changyao Tian","Xizhou Zhu","Lewei Lu","Yushi Chen","Junjun He","Zhongying Tu","Tong Lu","Yali Wang","Limin Wang","Dahua Lin","Yu Qiao","Botian Shi","Conghui He","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2406.08418v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03566v3","updated":"2024-07-12T08:54:33Z","published":"2023-02-07T16:26:45Z","title":"Look Around and Learn: Self-Training Object Detection by Exploration","summary":"  When an object detector is deployed in a novel setting it often experiences a\ndrop in performance. This paper studies how an embodied agent can automatically\nfine-tune a pre-existing object detector while exploring and acquiring images\nin a new environment without relying on human intervention, i.e., a fully\nself-supervised approach. In our setting, an agent initially learns to explore\nthe environment using a pre-trained off-the-shelf detector to locate objects\nand associate pseudo-labels. By assuming that pseudo-labels for the same object\nmust be consistent across different views, we learn the exploration policy Look\nAround to mine hard samples, and we devise a novel mechanism called\nDisagreement Reconciliation for producing refined pseudo-labels from the\nconsensus among observations. We implement a unified benchmark of the current\nstate-of-the-art and compare our approach with pre-existing exploration\npolicies and perception mechanisms. Our method is shown to outperform existing\napproaches, improving the object detector by 6.2% in a simulated scenario, a\n3.59% advancement over other state-of-the-art methods, and by 9.97% in the real\nrobotic test without relying on ground-truth. Code for the proposed approach\nand baselines are available at\nhttps://iit-pavis.github.io/Look_Around_And_Learn/.\n","authors":["Gianluca Scarpellini","Stefano Rosa","Pietro Morerio","Lorenzo Natale","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2302.03566v3.pdf","comment":"Paper accepted at ECCV2024"},{"id":"http://arxiv.org/abs/2405.04299v2","updated":"2024-07-12T08:43:20Z","published":"2024-05-07T13:15:07Z","title":"ViewFormer: Exploring Spatiotemporal Modeling for Multi-View 3D\n  Occupancy Perception via View-Guided Transformers","summary":"  3D occupancy, an advanced perception technology for driving scenarios,\nrepresents the entire scene without distinguishing between foreground and\nbackground by quantifying the physical space into a grid map. The widely\nadopted projection-first deformable attention, efficient in transforming image\nfeatures into 3D representations, encounters challenges in aggregating\nmulti-view features due to sensor deployment constraints. To address this\nissue, we propose our learning-first view attention mechanism for effective\nmulti-view feature aggregation. Moreover, we showcase the scalability of our\nview attention across diverse multi-view 3D tasks, including map construction\nand 3D object detection. Leveraging the proposed view attention as well as an\nadditional multi-frame streaming temporal attention, we introduce ViewFormer, a\nvision-centric transformer-based framework for spatiotemporal feature\naggregation. To further explore occupancy-level flow representation, we present\nFlowOcc3D, a benchmark built on top of existing high-quality datasets.\nQualitative and quantitative analyses on this benchmark reveal the potential to\nrepresent fine-grained dynamic scenes. Extensive experiments show that our\napproach significantly outperforms prior state-of-the-art methods. The codes\nare available at \\url{https://github.com/ViewFormerOcc/ViewFormer-Occ}.\n","authors":["Jinke Li","Xiao He","Chonghua Zhou","Xiaoqiang Cheng","Yang Wen","Dan Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.04299v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09094v1","updated":"2024-07-12T08:43:11Z","published":"2024-07-12T08:43:11Z","title":"Beyond Image Prior: Embedding Noise Prior into Conditional Denoising\n  Transformer","summary":"  Existing learning-based denoising methods typically train models to\ngeneralize the image prior from large-scale datasets, suffering from the\nvariability in noise distributions encountered in real-world scenarios. In this\nwork, we propose a new perspective on the denoising challenge by highlighting\nthe distinct separation between noise and image priors. This insight forms the\nbasis for our development of conditional optimization framework, designed to\novercome the constraints of traditional denoising framework. To this end, we\nintroduce a Locally Noise Prior Estimation (LoNPE) algorithm, which accurately\nestimates the noise prior directly from a single raw noisy image. This\nestimation acts as an explicit prior representation of the camera sensor's\nimaging environment, distinct from the image prior of scenes. Additionally, we\ndesign an auxiliary learnable LoNPE network tailored for practical application\nto sRGB noisy images. Leveraging the estimated noise prior, we present a novel\nConditional Denoising Transformer (Condformer), by incorporating the noise\nprior into a conditional self-attention mechanism. This integration allows the\nCondformer to segment the optimization process into multiple explicit\nsubspaces, significantly enhancing the model's generalization and flexibility.\nExtensive experimental evaluations on both synthetic and real-world datasets,\ndemonstrate that the proposed method achieves superior performance over current\nstate-of-the-art methods. The source code is available at\nhttps://github.com/YuanfeiHuang/Condformer.\n","authors":["Yuanfei Huang","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2407.09094v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2402.06165v2","updated":"2024-07-12T08:41:21Z","published":"2024-02-09T03:48:20Z","title":"Learning Contrastive Feature Representations for Facial Action Unit\n  Detection","summary":"  Facial action unit (AU) detection has long encountered the challenge of\ndetecting subtle feature differences when AUs activate. Existing methods often\nrely on encoding pixel-level information of AUs, which not only encodes\nadditional redundant information but also leads to increased model complexity\nand limited generalizability. Additionally, the accuracy of AU detection is\nnegatively impacted by the class imbalance issue of each AU type, and the\npresence of noisy and false AU labels. In this paper, we introduce a novel\ncontrastive learning framework aimed for AU detection that incorporates both\nself-supervised and supervised signals, thereby enhancing the learning of\ndiscriminative features for accurate AU detection. To tackle the class\nimbalance issue, we employ a negative sample re-weighting strategy that adjusts\nthe step size of updating parameters for minority and majority class samples.\nMoreover, to address the challenges posed by noisy and false AU labels, we\nemploy a sampling technique that encompasses three distinct types of positive\nsample pairs. This enables us to inject self-supervised signals into the\nsupervised signal, effectively mitigating the adverse effects of noisy labels.\nOur experimental assessments, conducted on four widely-utilized benchmark\ndatasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance\nof our approach compared to state-of-the-art methods of AU detection. Our code\nis available at \\url{https://github.com/Ziqiao-Shang/AUNCE}.\n","authors":["Ziqiao Shang","Bin Liu","Fengmao Lv","Fei Teng","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2402.06165v2.pdf","comment":"13 pages, 17 figures, submitted to IEEE Transactions on Circuits and\n  Systems for Video Technology (TCSVT)"},{"id":"http://arxiv.org/abs/2407.07673v2","updated":"2024-07-12T08:36:55Z","published":"2024-07-10T14:00:19Z","title":"Towards Adaptive Pseudo-label Learning for Semi-Supervised Temporal\n  Action Localization","summary":"  Alleviating noisy pseudo labels remains a key challenge in Semi-Supervised\nTemporal Action Localization (SS-TAL). Existing methods often filter pseudo\nlabels based on strict conditions, but they typically assess classification and\nlocalization quality separately, leading to suboptimal pseudo-label ranking and\nselection. In particular, there might be inaccurate pseudo labels within\nselected positives, alongside reliable counterparts erroneously assigned to\nnegatives. To tackle these problems, we propose a novel Adaptive Pseudo-label\nLearning (APL) framework to facilitate better pseudo-label selection.\nSpecifically, to improve the ranking quality, Adaptive Label Quality Assessment\n(ALQA) is proposed to jointly learn classification confidence and localization\nreliability, followed by dynamically selecting pseudo labels based on the joint\nscore. Additionally, we propose an Instance-level Consistency Discriminator\n(ICD) for eliminating ambiguous positives and mining potential positives\nsimultaneously based on inter-instance intrinsic consistency, thereby leading\nto a more precise selection. We further introduce a general unsupervised\nAction-aware Contrastive Pre-training (ACP) to enhance the discrimination both\nwithin actions and between actions and backgrounds, which benefits SS-TAL.\nExtensive experiments on THUMOS14 and ActivityNet v1.3 demonstrate that our\nmethod achieves state-of-the-art performance under various semi-supervised\nsettings.\n","authors":["Feixiang Zhou","Bryan Williams","Hossein Rahmani"],"pdf_url":"https://arxiv.org/pdf/2407.07673v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.09088v1","updated":"2024-07-12T08:29:25Z","published":"2024-07-12T08:29:25Z","title":"FD-SOS: Vision-Language Open-Set Detectors for Bone Fenestration and\n  Dehiscence Detection from Intraoral Images","summary":"  Accurate detection of bone fenestration and dehiscence (FD) is crucial for\neffective treatment planning in dentistry. While cone-beam computed tomography\n(CBCT) is the gold standard for evaluating FD, it comes with limitations such\nas radiation exposure, limited accessibility, and higher cost compared to\nintraoral images. In intraoral images, dentists face challenges in the\ndifferential diagnosis of FD. This paper presents a novel and clinically\nsignificant application of FD detection solely from intraoral images. To\nachieve this, we propose FD-SOS, a novel open-set object detector for FD\ndetection from intraoral images. FD-SOS has two novel components: conditional\ncontrastive denoising (CCDN) and teeth-specific matching assignment (TMA).\nThese modules enable FD-SOS to effectively leverage external dental semantics.\nExperimental results showed that our method outperformed existing detection\nmethods and surpassed dental professionals by 35% recall under the same level\nof precision. Code is available at: https://github.com/xmed-lab/FD-SOS.\n","authors":["Marawan Elbatel","Keyuan Liu","Yanqi Yang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2407.09088v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.09087v1","updated":"2024-07-12T08:25:31Z","published":"2024-07-12T08:25:31Z","title":"On the Role of Discrete Tokenization in Visual Representation Learning","summary":"  In the realm of self-supervised learning (SSL), masked image modeling (MIM)\nhas gained popularity alongside contrastive learning methods. MIM involves\nreconstructing masked regions of input images using their unmasked portions. A\nnotable subset of MIM methodologies employs discrete tokens as the\nreconstruction target, but the theoretical underpinnings of this choice remain\nunderexplored. In this paper, we explore the role of these discrete tokens,\naiming to unravel their benefits and limitations. Building upon the connection\nbetween MIM and contrastive learning, we provide a comprehensive theoretical\nunderstanding on how discrete tokenization affects the model's generalization\ncapabilities. Furthermore, we propose a novel metric named TCAS, which is\nspecifically designed to assess the effectiveness of discrete tokens within the\nMIM framework. Inspired by this metric, we contribute an innovative tokenizer\ndesign and propose a corresponding MIM method named ClusterMIM. It demonstrates\nsuperior performance on a variety of benchmark datasets and ViT backbones. Code\nis available at https://github.com/PKU-ML/ClusterMIM.\n","authors":["Tianqi Du","Yifei Wang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09087v1.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2407.09073v1","updated":"2024-07-12T07:53:54Z","published":"2024-07-12T07:53:54Z","title":"Open Vocabulary Multi-Label Video Classification","summary":"  Pre-trained vision-language models (VLMs) have enabled significant progress\nin open vocabulary computer vision tasks such as image classification, object\ndetection and image segmentation. Some recent works have focused on extending\nVLMs to open vocabulary single label action classification in videos. However,\nprevious methods fall short in holistic video understanding which requires the\nability to simultaneously recognize multiple actions and entities e.g., objects\nin the video in an open vocabulary setting. We formulate this problem as open\nvocabulary multilabel video classification and propose a method to adapt a\npre-trained VLM such as CLIP to solve this task. We leverage large language\nmodels (LLMs) to provide semantic guidance to the VLM about class labels to\nimprove its open vocabulary performance with two key contributions. First, we\npropose an end-to-end trainable architecture that learns to prompt an LLM to\ngenerate soft attributes for the CLIP text-encoder to enable it to recognize\nnovel classes. Second, we integrate a temporal modeling module into CLIP's\nvision encoder to effectively model the spatio-temporal dynamics of video\nconcepts as well as propose a novel regularized finetuning technique to ensure\nstrong open vocabulary classification performance in the video domain. Our\nextensive experimentation showcases the efficacy of our approach on multiple\nbenchmark datasets.\n","authors":["Rohit Gupta","Mamshad Nayeem Rizve","Jayakrishnan Unnikrishnan","Ashish Tawari","Son Tran","Mubarak Shah","Benjamin Yao","Trishul Chilimbi"],"pdf_url":"https://arxiv.org/pdf/2407.09073v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2308.15216v5","updated":"2024-07-12T07:49:05Z","published":"2023-08-29T11:12:53Z","title":"On-the-Fly Guidance Training for Medical Image Registration","summary":"  This study introduces a novel On-the-Fly Guidance (OFG) training framework\nfor enhancing existing learning-based image registration models, addressing the\nlimitations of weakly-supervised and unsupervised methods. Weakly-supervised\nmethods struggle due to the scarcity of labeled data, and unsupervised methods\ndirectly depend on image similarity metrics for accuracy. Our method proposes a\nsupervised fashion for training registration models, without the need for any\nlabeled data. OFG generates pseudo-ground truth during training by refining\ndeformation predictions with a differentiable optimizer, enabling direct\nsupervised learning. OFG optimizes deformation predictions efficiently,\nimproving the performance of registration models without sacrificing inference\nspeed. Our method is tested across several benchmark datasets and leading\nmodels, it significantly enhanced performance, providing a plug-and-play\nsolution for training learning-based registration models. Code available at:\nhttps://github.com/cilix-ai/on-the-fly-guidance\n","authors":["Yuelin Xin","Yicheng Chen","Shengxiang Ji","Kun Han","Xiaohui Xie"],"pdf_url":"https://arxiv.org/pdf/2308.15216v5.pdf","comment":"MICCAI 2024, 13 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.18241v2","updated":"2024-07-12T07:30:00Z","published":"2024-03-27T04:09:34Z","title":"NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,\n  Reconstruction, and Generation","summary":"  3D shape generation aims to produce innovative 3D content adhering to\nspecific conditions and constraints. Existing methods often decompose 3D shapes\ninto a sequence of localized components, treating each element in isolation\nwithout considering spatial consistency. As a result, these approaches exhibit\nlimited versatility in 3D data representation and shape generation, hindering\ntheir ability to generate highly diverse 3D shapes that comply with the\nspecified constraints. In this paper, we introduce a novel spatial-aware 3D\nshape generation framework that leverages 2D plane representations for enhanced\n3D shape modeling. To ensure spatial coherence and reduce memory usage, we\nincorporate a hybrid shape representation technique that directly learns a\ncontinuous signed distance field representation of the 3D shape using\northogonal 2D planes. Additionally, we meticulously enforce spatial\ncorrespondences across distinct planes using a transformer-based autoencoder\nstructure, promoting the preservation of spatial relationships in the generated\n3D shapes. This yields an algorithm that consistently outperforms\nstate-of-the-art 3D shape generation methods on various tasks, including\nunconditional shape generation, multi-modal shape completion, single-view\nreconstruction, and text-to-shape synthesis. Our project page is available at\nhttps://weizheliu.github.io/NeuSDFusion/ .\n","authors":["Ruikai Cui","Weizhe Liu","Weixuan Sun","Senbo Wang","Taizhang Shang","Yang Li","Xibin Song","Han Yan","Zhennan Wu","Shenzhou Chen","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18241v2.pdf","comment":"ECCV 2024, project page: https://weizheliu.github.io/NeuSDFusion/"},{"id":"http://arxiv.org/abs/2406.14794v3","updated":"2024-07-12T07:28:55Z","published":"2024-06-20T23:51:32Z","title":"ImageFlowNet: Forecasting Multiscale Trajectories of Disease Progression\n  with Irregularly-Sampled Longitudinal Medical Images","summary":"  The forecasting of disease progression from images is a holy grail for\nclinical decision making. However, this task is complicated by the inherent\nhigh dimensionality, temporal sparsity and sampling irregularity in\nlongitudinal image acquisitions. Existing methods often rely on extracting\nhand-crafted features and performing time-series analysis in this vector space,\nleading to a loss of rich spatial information within the images. To overcome\nthese challenges, we introduce ImageFlowNet, a novel framework that learns\nlatent-space flow fields that evolve multiscale representations in joint\nembedding spaces using neural ODEs and SDEs to model disease progression in the\nimage domain. Notably, ImageFlowNet learns multiscale joint representation\nspaces by combining cohorts of patients together so that information can be\ntransferred between the patient samples. The dynamics then provide plausible\ntrajectories of progression, with the SDE providing alternative trajectories\nfrom the same starting point. We provide theoretical insights that support our\nformulation of ODEs, and motivate our regularizations involving high-level\nvisual features, latent space organization, and trajectory smoothness. We then\ndemonstrate ImageFlowNet's effectiveness through empirical evaluations on three\nlongitudinal medical image datasets depicting progression in retinal geographic\natrophy, multiple sclerosis, and glioblastoma.\n","authors":["Chen Liu","Ke Xu","Liangbo L. Shen","Guillaume Huguet","Zilong Wang","Alexander Tong","Danilo Bzdok","Jay Stewart","Jay C. Wang","Lucian V. Del Priore","Smita Krishnaswamy"],"pdf_url":"https://arxiv.org/pdf/2406.14794v3.pdf","comment":"Fixed some typos. Merged multibib"},{"id":"http://arxiv.org/abs/2407.09059v1","updated":"2024-07-12T07:28:01Z","published":"2024-07-12T07:28:01Z","title":"Domain-adaptive Video Deblurring via Test-time Blurring","summary":"  Dynamic scene video deblurring aims to remove undesirable blurry artifacts\ncaptured during the exposure process. Although previous video deblurring\nmethods have achieved impressive results, they suffer from significant\nperformance drops due to the domain gap between training and testing videos,\nespecially for those captured in real-world scenarios. To address this issue,\nwe propose a domain adaptation scheme based on a blurring model to achieve\ntest-time fine-tuning for deblurring models in unseen domains. Since blurred\nand sharp pairs are unavailable for fine-tuning during inference, our scheme\ncan generate domain-adaptive training pairs to calibrate a deblurring model for\nthe target domain. First, a Relative Sharpness Detection Module is proposed to\nidentify relatively sharp regions from the blurry input images and regard them\nas pseudo-sharp images. Next, we utilize a blurring model to produce blurred\nimages based on the pseudo-sharp images extracted during testing. To synthesize\nblurred images in compliance with the target data distribution, we propose a\nDomain-adaptive Blur Condition Generation Module to create domain-specific blur\nconditions for the blurring model. Finally, the generated pseudo-sharp and\nblurred pairs are used to fine-tune a deblurring model for better performance.\nExtensive experimental results demonstrate that our approach can significantly\nimprove state-of-the-art video deblurring methods, providing performance gains\nof up to 7.54dB on various real-world video deblurring datasets. The source\ncode is available at https://github.com/Jin-Ting-He/DADeblur.\n","authors":["Jin-Ting He","Fu-Jen Tsai","Jia-Hao Wu","Yan-Tsung Peng","Chung-Chi Tsai","Chia-Wen Lin","Yen-Yu Lin"],"pdf_url":"https://arxiv.org/pdf/2407.09059v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.09057v1","updated":"2024-07-12T07:27:07Z","published":"2024-07-12T07:27:07Z","title":"PersonificationNet: Making customized subject act like a person","summary":"  Recently customized generation has significant potential, which uses as few\nas 3-5 user-provided images to train a model to synthesize new images of a\nspecified subject. Though subsequent applications enhance the flexibility and\ndiversity of customized generation, fine-grained control over the given subject\nacting like the person's pose is still lack of study. In this paper, we propose\na PersonificationNet, which can control the specified subject such as a cartoon\ncharacter or plush toy to act the same pose as a given referenced person's\nimage. It contains a customized branch, a pose condition branch and a structure\nalignment module. Specifically, first, the customized branch mimics specified\nsubject appearance. Second, the pose condition branch transfers the body\nstructure information from the human to variant instances. Last, the structure\nalignment module bridges the structure gap between human and specified subject\nin the inference stage. Experimental results show our proposed\nPersonificationNet outperforms the state-of-the-art methods.\n","authors":["Tianchu Guo","Pengyu Li","Biao Wang","Xiansheng Hua"],"pdf_url":"https://arxiv.org/pdf/2407.09057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03939v2","updated":"2024-07-12T07:21:33Z","published":"2024-07-04T13:52:37Z","title":"SfM on-the-fly: Get better 3D from What You Capture","summary":"  In the last twenty years, Structure from Motion (SfM) has been a constant\nresearch hotspot in the fields of photogrammetry, computer vision, robotics\netc., whereas real-time performance is just a recent topic of growing interest.\nThis work builds upon the original on-the-fly SfM (Zhan et al., 2024) and\npresents an updated version with three new advancements to get better 3D from\nwhat you capture: (i) real-time image matching is further boosted by employing\nthe Hierarchical Navigable Small World (HNSW) graphs, thus more true positive\noverlapping image candidates are faster identified; (ii) a self-adaptive\nweighting strategy is proposed for robust hierarchical local bundle adjustment\nto improve the SfM results; (iii) multiple agents are included for supporting\ncollaborative SfM and seamlessly merge multiple 3D reconstructions into a\ncomplete 3D scene when commonly registered images appear. Various comprehensive\nexperiments demonstrate that the proposed SfM method (named on-the-fly SfMv2)\ncan generate more complete and robust 3D reconstructions in a high\ntime-efficient way. Code is available at\nhttp://yifeiyu225.github.io/on-the-flySfMv2.github.io/.\n","authors":["Zhan Zongqian","Yu Yifei","Xia Rui","Gan Wentian","Xie Hong","Perda Giulio","Morelli Luca","Remondino Fabio","Wang Xin"],"pdf_url":"https://arxiv.org/pdf/2407.03939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09051v1","updated":"2024-07-12T07:18:18Z","published":"2024-07-12T07:18:18Z","title":"DroneMOT: Drone-based Multi-Object Tracking Considering Detection\n  Difficulties and Simultaneous Moving of Drones and Objects","summary":"  Multi-object tracking (MOT) on static platforms, such as by surveillance\ncameras, has achieved significant progress, with various paradigms providing\nattractive performances. However, the effectiveness of traditional MOT methods\nis significantly reduced when it comes to dynamic platforms like drones. This\ndecrease is attributed to the distinctive challenges in the MOT-on-drone\nscenario: (1) objects are generally small in the image plane, blurred, and\nfrequently occluded, making them challenging to detect and recognize; (2)\ndrones move and see objects from different angles, causing the unreliability of\nthe predicted positions and feature embeddings of the objects. This paper\nproposes DroneMOT, which firstly proposes a Dual-domain Integrated Attention\n(DIA) module that considers the fast movements of drones to enhance the\ndrone-based object detection and feature embedding for small-sized, blurred,\nand occluded objects. Then, an innovative Motion-Driven Association (MDA)\nscheme is introduced, considering the concurrent movements of both the drone\nand the objects. Within MDA, an Adaptive Feature Synchronization (AFS)\ntechnique is presented to update the object features seen from different\nangles. Additionally, a Dual Motion-based Prediction (DMP) method is employed\nto forecast the object positions. Finally, both the refined feature embeddings\nand the predicted positions are integrated to enhance the object association.\nComprehensive evaluations on VisDrone2019-MOT and UAVDT datasets show that\nDroneMOT provides substantial performance improvements over the\nstate-of-the-art in the domain of MOT on drones.\n","authors":["Peng Wang","Yongcai Wang","Deying Li"],"pdf_url":"https://arxiv.org/pdf/2407.09051v1.pdf","comment":"8 pages, 6 figures, ICRA 2024"},{"id":"http://arxiv.org/abs/2407.09050v1","updated":"2024-07-12T07:18:05Z","published":"2024-07-12T07:18:05Z","title":"Refusing Safe Prompts for Multi-modal Large Language Models","summary":"  Multimodal large language models (MLLMs) have become the cornerstone of\ntoday's generative AI ecosystem, sparking intense competition among tech giants\nand startups. In particular, an MLLM generates a text response given a prompt\nconsisting of an image and a question. While state-of-the-art MLLMs use safety\nfilters and alignment techniques to refuse unsafe prompts, in this work, we\nintroduce MLLM-Refusal, the first method that induces refusals for safe\nprompts. In particular, our MLLM-Refusal optimizes a nearly-imperceptible\nrefusal perturbation and adds it to an image, causing target MLLMs to likely\nrefuse a safe prompt containing the perturbed image and a safe question.\nSpecifically, we formulate MLLM-Refusal as a constrained optimization problem\nand propose an algorithm to solve it. Our method offers competitive advantages\nfor MLLM model providers by potentially disrupting user experiences of\ncompeting MLLMs, since competing MLLM's users will receive unexpected refusals\nwhen they unwittingly use these perturbed images in their prompts. We evaluate\nMLLM-Refusal on four MLLMs across four datasets, demonstrating its\neffectiveness in causing competing MLLMs to refuse safe prompts while not\naffecting non-competing MLLMs. Furthermore, we explore three potential\ncountermeasures -- adding Gaussian noise, DiffPure, and adversarial training.\nOur results show that they are insufficient: though they can mitigate\nMLLM-Refusal's effectiveness, they also sacrifice the accuracy and/or\nefficiency of the competing MLLM. The code is available at\nhttps://github.com/Sadcardation/MLLM-Refusal.\n","authors":["Zedian Shao","Hongbin Liu","Yuepeng Hu","Neil Zhenqiang Gong"],"pdf_url":"https://arxiv.org/pdf/2407.09050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09047v1","updated":"2024-07-12T07:15:26Z","published":"2024-07-12T07:15:26Z","title":"Cs2K: Class-specific and Class-shared Knowledge Guidance for Incremental\n  Semantic Segmentation","summary":"  Incremental semantic segmentation endeavors to segment newly encountered\nclasses while maintaining knowledge of old classes. However, existing methods\neither 1) lack guidance from class-specific knowledge (i.e., old class\nprototypes), leading to a bias towards new classes, or 2) constrain\nclass-shared knowledge (i.e., old model weights) excessively without\ndiscrimination, resulting in a preference for old classes. In this paper, to\ntrade off model performance, we propose the Class-specific and Class-shared\nKnowledge (Cs2K) guidance for incremental semantic segmentation. Specifically,\nfrom the class-specific knowledge aspect, we design a prototype-guided pseudo\nlabeling that exploits feature proximity from prototypes to correct pseudo\nlabels, thereby overcoming catastrophic forgetting. Meanwhile, we develop a\nprototype-guided class adaptation that aligns class distribution across\ndatasets via learning old augmented prototypes. Moreover, from the class-shared\nknowledge aspect, we propose a weight-guided selective consolidation to\nstrengthen old memory while maintaining new memory by integrating old and new\nmodel weights based on weight importance relative to old classes. Experiments\non public datasets demonstrate that our proposed Cs2K significantly improves\nsegmentation performance and is plug-and-play.\n","authors":["Wei Cong","Yang Cong","Yuyang Liu","Gan Sun"],"pdf_url":"https://arxiv.org/pdf/2407.09047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08443v2","updated":"2024-07-12T07:12:05Z","published":"2024-07-11T12:33:56Z","title":"Infinite Motion: Extended Motion Generation via Long Text Instructions","summary":"  In the realm of motion generation, the creation of long-duration,\nhigh-quality motion sequences remains a significant challenge. This paper\npresents our groundbreaking work on \"Infinite Motion\", a novel approach that\nleverages long text to extended motion generation, effectively bridging the gap\nbetween short and long-duration motion synthesis. Our core insight is the\nstrategic extension and reassembly of existing high-quality text-motion\ndatasets, which has led to the creation of a novel benchmark dataset to\nfacilitate the training of models for extended motion sequences. A key\ninnovation of our model is its ability to accept arbitrary lengths of text as\ninput, enabling the generation of motion sequences tailored to specific\nnarratives or scenarios. Furthermore, we incorporate the timestamp design for\ntext which allows precise editing of local segments within the generated\nsequences, offering unparalleled control and flexibility in motion synthesis.\nWe further demonstrate the versatility and practical utility of \"Infinite\nMotion\" through three specific applications: natural language interactive\nediting, motion sequence editing within long sequences and splicing of\nindependent motion sequences. Each application highlights the adaptability of\nour approach and broadens the spectrum of possibilities for research and\ndevelopment in motion generation. Through extensive experiments, we demonstrate\nthe superior performance of our model in generating long sequence motions\ncompared to existing methods.Project page:\nhttps://shuochengzhai.github.io/Infinite-motion.github.io/\n","authors":["Mengtian Li","Chengshuo Zhai","Shengxiang Yao","Zhifeng Xie","Keyu Chen","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.08443v2.pdf","comment":"12 pages,13 figures"},{"id":"http://arxiv.org/abs/2403.14530v3","updated":"2024-07-12T07:01:32Z","published":"2024-03-21T16:28:58Z","title":"HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression","summary":"  3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel\nview synthesis, boasting rapid rendering speed with high fidelity. However, the\nsubstantial Gaussians and their associated attributes necessitate effective\ncompression techniques. Nevertheless, the sparse and unorganized nature of the\npoint cloud of Gaussians (or anchors in our paper) presents challenges for\ncompression. To address this, we make use of the relations between the\nunorganized anchors and the structured hash grid, leveraging their mutual\ninformation for context modeling, and propose a Hash-grid Assisted Context\n(HAC) framework for highly compact 3DGS representation. Our approach introduces\na binary hash grid to establish continuous spatial consistencies, allowing us\nto unveil the inherent spatial relations of anchors through a carefully\ndesigned context model. To facilitate entropy coding, we utilize Gaussian\ndistributions to accurately estimate the probability of each quantized\nattribute, where an adaptive quantization module is proposed to enable\nhigh-precision quantization of these attributes for improved fidelity\nrestoration. Additionally, we incorporate an adaptive masking strategy to\neliminate invalid Gaussians and anchors. Importantly, our work is the pioneer\nto explore context-based compression for 3DGS representation, resulting in a\nremarkable size reduction of over $75\\times$ compared to vanilla 3DGS, while\nsimultaneously improving fidelity, and achieving over $11\\times$ size reduction\nover SOTA 3DGS compression approach Scaffold-GS. Our code is available here:\nhttps://github.com/YihangChen-ee/HAC\n","authors":["Yihang Chen","Qianyi Wu","Weiyao Lin","Mehrtash Harandi","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2403.14530v3.pdf","comment":"Project Page: https://yihangchen-ee.github.io/project_hac/ Code:\n  https://github.com/YihangChen-ee/HAC"},{"id":"http://arxiv.org/abs/2407.09035v1","updated":"2024-07-12T06:54:31Z","published":"2024-07-12T06:54:31Z","title":"GPC: Generative and General Pathology Image Classifier","summary":"  Deep learning has been increasingly incorporated into various computational\npathology applications to improve its efficiency, accuracy, and robustness.\nAlthough successful, most previous approaches for image classification have\ncrucial drawbacks. There exist numerous tasks in pathology, but one needs to\nbuild a model per task, i.e., a task-specific model, thereby increasing the\nnumber of models, training resources, and cost. Moreover, transferring\narbitrary task-specific model to another task is still a challenging problem.\nHerein, we propose a task-agnostic generative and general pathology image\nclassifier, so called GPC, that aims at learning from diverse kinds of\npathology images and conducting numerous classification tasks in a unified\nmodel. GPC, equipped with a convolutional neural network and a\nTransformer-based language model, maps pathology images into a high-dimensional\nfeature space and generates pertinent class labels as texts via the\nimage-to-text classification mechanism. We evaluate GPC on six datasets for\nfour different pathology image classification tasks. Experimental results show\nthat GPC holds considerable potential for developing an effective and efficient\nuniversal model for pathology image analysis.\n","authors":["Anh Tien Nguyen","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.09035v1.pdf","comment":"MICCAI-MedAGI 2023 (Best Paper Honorable Mention)"},{"id":"http://arxiv.org/abs/2311.17833v3","updated":"2024-07-12T06:53:50Z","published":"2023-11-29T17:35:29Z","title":"DiG-IN: Diffusion Guidance for Investigating Networks -- Uncovering\n  Classifier Differences Neuron Visualisations and Visual Counterfactual\n  Explanations","summary":"  While deep learning has led to huge progress in complex image classification\ntasks like ImageNet, unexpected failure modes, e.g. via spurious features, call\ninto question how reliably these classifiers work in the wild. Furthermore, for\nsafety-critical tasks the black-box nature of their decisions is problematic,\nand explanations or at least methods which make decisions plausible are needed\nurgently. In this paper, we address these problems by generating images that\noptimize a classifier-derived objective using a framework for guided image\ngeneration. We analyze the decisions of image classifiers by visual\ncounterfactual explanations (VCEs), detection of systematic mistakes by\nanalyzing images where classifiers maximally disagree, and visualization of\nneurons and spurious features. In this way, we validate existing observations,\ne.g. the shape bias of adversarially robust models, as well as novel failure\nmodes, e.g. systematic errors of zero-shot CLIP classifiers. Moreover, our VCEs\noutperform previous work while being more versatile.\n","authors":["Maximilian Augustin","Yannic Neuhaus","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2311.17833v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2407.09033v1","updated":"2024-07-12T06:49:16Z","published":"2024-07-12T06:49:16Z","title":"Textual Query-Driven Mask Transformer for Domain Generalized\n  Segmentation","summary":"  In this paper, we introduce a method to tackle Domain Generalized Semantic\nSegmentation (DGSS) by utilizing domain-invariant semantic knowledge from text\nembeddings of vision-language models. We employ the text embeddings as object\nqueries within a transformer-based segmentation framework (textual object\nqueries). These queries are regarded as a domain-invariant basis for pixel\ngrouping in DGSS. To leverage the power of textual object queries, we introduce\na novel framework named the textual query-driven mask transformer (tqdm). Our\ntqdm aims to (1) generate textual object queries that maximally encode\ndomain-invariant semantics and (2) enhance the semantic clarity of dense visual\nfeatures. Additionally, we suggest three regularization losses to improve the\nefficacy of tqdm by aligning between visual and textual features. By utilizing\nour method, the model can comprehend inherent semantic information for classes\nof interest, enabling it to generalize to extreme domains (e.g., sketch style).\nOur tqdm achieves 68.9 mIoU on GTA5$\\rightarrow$Cityscapes, outperforming the\nprior state-of-the-art method by 2.5 mIoU. The project page is available at\nhttps://byeonghyunpak.github.io/tqdm.\n","authors":["Byeonghyun Pak","Byeongju Woo","Sunghwan Kim","Dae-hwan Kim","Hoseong Kim"],"pdf_url":"https://arxiv.org/pdf/2407.09033v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.09030v1","updated":"2024-07-12T06:45:37Z","published":"2024-07-12T06:45:37Z","title":"CAMP: Continuous and Adaptive Learning Model in Pathology","summary":"  There exist numerous diagnostic tasks in pathology. Conventional\ncomputational pathology formulates and tackles them as independent and\nindividual image classification problems, thereby resulting in computational\ninefficiency and high costs. To address the challenges, we propose a generic,\nunified, and universal framework, called a continuous and adaptive learning\nmodel in pathology (CAMP), for pathology image classification. CAMP is a\ngenerative, efficient, and adaptive classification model that can continuously\nadapt to any classification task by leveraging pathology-specific prior\nknowledge and learning taskspecific knowledge with minimal computational cost\nand without forgetting the knowledge from the existing tasks. We evaluated CAMP\non 22 datasets, including 1,171,526 patches and 11,811 pathology slides, across\n17 classification tasks. CAMP achieves state-of-theart classification\nperformance on a wide range of datasets and tasks at both patch- and\nslide-levels and reduces up to 94% of computation time and 85% of storage\nmemory in comparison to the conventional classification models. Our results\ndemonstrate that CAMP can offer a fundamental transformation in pathology image\nclassification, paving the way for the fully digitized and computerized\npathology practice.\n","authors":["Anh Tien Nguyen","Keunho Byeon","Kyungeun Kim","Boram Song","Seoung Wan Chae","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.09030v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.09029v1","updated":"2024-07-12T06:44:42Z","published":"2024-07-12T06:44:42Z","title":"Enhancing Emotion Recognition in Incomplete Data: A Novel Cross-Modal\n  Alignment, Reconstruction, and Refinement Framework","summary":"  Multimodal emotion recognition systems rely heavily on the full availability\nof modalities, suffering significant performance declines when modal data is\nincomplete. To tackle this issue, we present the Cross-Modal Alignment,\nReconstruction, and Refinement (CM-ARR) framework, an innovative approach that\nsequentially engages in cross-modal alignment, reconstruction, and refinement\nphases to handle missing modalities and enhance emotion recognition. This\nframework utilizes unsupervised distribution-based contrastive learning to\nalign heterogeneous modal distributions, reducing discrepancies and modeling\nsemantic uncertainty effectively. The reconstruction phase applies normalizing\nflow models to transform these aligned distributions and recover missing\nmodalities. The refinement phase employs supervised point-based contrastive\nlearning to disrupt semantic correlations and accentuate emotional traits,\nthereby enriching the affective content of the reconstructed representations.\nExtensive experiments on the IEMOCAP and MSP-IMPROV datasets confirm the\nsuperior performance of CM-ARR under conditions of both missing and complete\nmodalities. Notably, averaged across six scenarios of missing modalities,\nCM-ARR achieves absolute improvements of 2.11% in WAR and 2.12% in UAR on the\nIEMOCAP dataset, and 1.71% and 1.96% in WAR and UAR, respectively, on the\nMSP-IMPROV dataset.\n","authors":["Haoqin Sun","Shiwan Zhao","Shaokai Li","Xiangyu Kong","Xuechen Wang","Aobo Kong","Jiaming Zhou","Yong Chen","Wenjia Zeng","Yong Qin"],"pdf_url":"https://arxiv.org/pdf/2407.09029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09026v1","updated":"2024-07-12T06:34:24Z","published":"2024-07-12T06:34:24Z","title":"HPC: Hierarchical Progressive Coding Framework for Volumetric Video","summary":"  Volumetric video based on Neural Radiance Field (NeRF) holds vast potential\nfor various 3D applications, but its substantial data volume poses significant\nchallenges for compression and transmission. Current NeRF compression lacks the\nflexibility to adjust video quality and bitrate within a single model for\nvarious network and device capacities. To address these issues, we propose HPC,\na novel hierarchical progressive volumetric video coding framework achieving\nvariable bitrate using a single model. Specifically, HPC introduces a\nhierarchical representation with a multi-resolution residual radiance field to\nreduce temporal redundancy in long-duration sequences while simultaneously\ngenerating various levels of detail. Then, we propose an end-to-end progressive\nlearning approach with a multi-rate-distortion loss function to jointly\noptimize both hierarchical representation and compression. Our HPC trained only\nonce can realize multiple compression levels, while the current methods need to\ntrain multiple fixed-bitrate models for different rate-distortion (RD)\ntradeoffs. Extensive experiments demonstrate that HPC achieves flexible quality\nlevels with variable bitrate by a single model and exhibits competitive RD\nperformance, even outperforming fixed-bitrate models across various datasets.\n","authors":["Zihan Zheng","Houqiang Zhong","Qiang Hu","Xiaoyun Zhang","Li Song","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09026v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.02700v2","updated":"2024-07-12T06:25:13Z","published":"2023-12-05T12:03:00Z","title":"Revisit Human-Scene Interaction via Space Occupancy","summary":"  Human-scene Interaction (HSI) generation is a challenging task and crucial\nfor various downstream tasks. However, one of the major obstacles is its\nlimited data scale. High-quality data with simultaneously captured human and 3D\nenvironments is hard to acquire, resulting in limited data diversity and\ncomplexity. In this work, we argue that interaction with a scene is essentially\ninteracting with the space occupancy of the scene from an abstract physical\nperspective, leading us to a unified novel view of Human-Occupancy Interaction.\nBy treating pure motion sequences as records of humans interacting with\ninvisible scene occupancy, we can aggregate motion-only data into a large-scale\npaired human-occupancy interaction database: Motion Occupancy Base (MOB). Thus,\nthe need for costly paired motion-scene datasets with high-quality scene scans\ncan be substantially alleviated. With this new unified view of Human-Occupancy\ninteraction, a single motion controller is proposed to reach the target state\ngiven the surrounding occupancy. Once trained on MOB with complex occupancy\nlayout, which is stringent to human movements, the controller could handle\ncramped scenes and generalize well to general scenes with limited complexity\nlike regular living rooms. With no GT 3D scenes for training, our method can\ngenerate realistic and stable HSI motions in diverse scenarios, including both\nstatic and dynamic scenes. The project is available at\nhttps://foruck.github.io/occu-page/.\n","authors":["Xinpeng Liu","Haowen Hou","Yanchao Yang","Yong-Lu Li","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2312.02700v2.pdf","comment":"To appear in ECCV 2024. The first two authors contributed equally.\n  Yong-Lu Li is the corresponding author. Project page:\n  https://foruck.github.io/occu-page/"},{"id":"http://arxiv.org/abs/2308.14334v4","updated":"2024-07-12T06:19:52Z","published":"2023-08-28T06:25:40Z","title":"MetaWeather: Few-Shot Weather-Degraded Image Restoration","summary":"  Real-world weather conditions are intricate and often occur concurrently.\nHowever, most existing restoration approaches are limited in their\napplicability to specific weather conditions in training data and struggle to\ngeneralize to unseen weather types, including real-world weather conditions. To\naddress this issue, we introduce MetaWeather, a universal approach that can\nhandle diverse and novel weather conditions with a single unified model.\nExtending a powerful meta-learning framework, MetaWeather formulates the task\nof weather-degraded image restoration as a few-shot adaptation problem that\npredicts the degradation pattern of a query image, and learns to adapt to\nunseen weather conditions through a novel spatial-channel matching algorithm.\nExperimental results on the BID Task II.A, SPA-Data, and RealSnow datasets\ndemonstrate that the proposed method can adapt to unseen weather conditions,\nsignificantly outperforming the state-of-the-art multi-weather image\nrestoration methods.\n","authors":["Youngrae Kim","Younggeol Cho","Thanh-Tung Nguyen","Seunghoon Hong","Dongman Lee"],"pdf_url":"https://arxiv.org/pdf/2308.14334v4.pdf","comment":"Accepted to ECCV 2024. Code is available at\n  https://github.com/RangeWING/MetaWeather"},{"id":"http://arxiv.org/abs/2406.16109v3","updated":"2024-07-12T06:18:13Z","published":"2024-06-23T13:53:35Z","title":"X-ray2CTPA: Generating 3D CTPA scans from 2D X-ray conditioning","summary":"  Chest X-rays or chest radiography (CXR), commonly used for medical\ndiagnostics, typically enables limited imaging compared to computed tomography\n(CT) scans, which offer more detailed and accurate three-dimensional data,\nparticularly contrast-enhanced scans like CT Pulmonary Angiography (CTPA).\nHowever, CT scans entail higher costs, greater radiation exposure, and are less\naccessible than CXRs. In this work we explore cross-modal translation from a 2D\nlow contrast-resolution X-ray input to a 3D high contrast and\nspatial-resolution CTPA scan. Driven by recent advances in generative AI, we\nintroduce a novel diffusion-based approach to this task. We evaluate the models\nperformance using both quantitative metrics and qualitative feedback from\nradiologists, ensuring diagnostic relevance of the generated images.\nFurthermore, we employ the synthesized 3D images in a classification framework\nand show improved AUC in a PE categorization task, using the initial CXR input.\nThe proposed method is generalizable and capable of performing additional\ncross-modality translations in medical imaging. It may pave the way for more\naccessible and cost-effective advanced diagnostic tools. The code for this\nproject is available: https://github.com/NoaCahan/X-ray2CTPA .\n","authors":["Noa Cahan","Eyal Klang","Galit Aviram","Yiftach Barash","Eli Konen","Raja Giryes","Hayit Greenspan"],"pdf_url":"https://arxiv.org/pdf/2406.16109v3.pdf","comment":"preprint, project code: https://github.com/NoaCahan/X-ray2CTPA"},{"id":"http://arxiv.org/abs/2406.12254v2","updated":"2024-07-12T06:03:31Z","published":"2024-06-18T04:06:02Z","title":"Enhancing Single-Slice Segmentation with 3D-to-2D Unpaired Scan\n  Distillation","summary":"  2D single-slice abdominal computed tomography (CT) enables the assessment of\nbody habitus and organ health with low radiation exposure. However,\nsingle-slice data necessitates the use of 2D networks for segmentation, but\nthese networks often struggle to capture contextual information effectively.\nConsequently, even when trained on identical datasets, 3D networks typically\nachieve superior segmentation results. In this work, we propose a novel\n3D-to-2D distillation framework, leveraging pre-trained 3D models to enhance 2D\nsingle-slice segmentation. Specifically, we extract the prediction distribution\ncentroid from the 3D representations, to guide the 2D student by learning\nintra- and inter-class correlation. Unlike traditional knowledge distillation\nmethods that require the same data input, our approach employs unpaired 3D CT\nscans with any contrast to guide the 2D student model. Experiments conducted on\n707 subjects from the single-slice Baltimore Longitudinal Study of Aging (BLSA)\ndataset demonstrate that state-of-the-art 2D multi-organ segmentation methods\ncan benefit from the 3D teacher model, achieving enhanced performance in\nsingle-slice multi-organ segmentation. Notably, our approach demonstrates\nconsiderable efficacy in low-data regimes, outperforming the model trained with\nall available training subjects even when utilizing only 200 training subjects.\nThus, this work underscores the potential to alleviate manual annotation\nburdens.\n","authors":["Xin Yu","Qi Yang","Han Liu","Ho Hin Lee","Yucheng Tang","Lucas W. Remedios","Michael E. Kim","Rendong Zhang","Shunxing Bao","Yuankai Huo","Ann Zenobia Moore","Luigi Ferrucci","Bennett A. Landman"],"pdf_url":"https://arxiv.org/pdf/2406.12254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09012v1","updated":"2024-07-12T06:02:13Z","published":"2024-07-12T06:02:13Z","title":"TCAN: Animating Human Images with Temporally Consistent Pose Guidance\n  using Diffusion Models","summary":"  Pose-driven human-image animation diffusion models have shown remarkable\ncapabilities in realistic human video synthesis. Despite the promising results\nachieved by previous approaches, challenges persist in achieving temporally\nconsistent animation and ensuring robustness with off-the-shelf pose detectors.\nIn this paper, we present TCAN, a pose-driven human image animation method that\nis robust to erroneous poses and consistent over time. In contrast to previous\nmethods, we utilize the pre-trained ControlNet without fine-tuning to leverage\nits extensive pre-acquired knowledge from numerous pose-image-caption pairs. To\nkeep the ControlNet frozen, we adapt LoRA to the UNet layers, enabling the\nnetwork to align the latent space between the pose and appearance features.\nAdditionally, by introducing an additional temporal layer to the ControlNet, we\nenhance robustness against outliers of the pose detector. Through the analysis\nof attention maps over the temporal axis, we also designed a novel temperature\nmap leveraging pose information, allowing for a more static background.\nExtensive experiments demonstrate that the proposed method can achieve\npromising results in video synthesis tasks encompassing various poses, like\nchibi. Project Page: https://eccv2024tcan.github.io/\n","authors":["Jeongho Kim","Min-Jung Kim","Junsoo Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2407.09012v1.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2407.09005v1","updated":"2024-07-12T05:48:53Z","published":"2024-07-12T05:48:53Z","title":"Introducing VaDA: Novel Image Segmentation Model for Maritime Object\n  Segmentation Using New Dataset","summary":"  The maritime shipping industry is undergoing rapid evolution driven by\nadvancements in computer vision artificial intelligence (AI). Consequently,\nresearch on AI-based object recognition models for maritime transportation is\nsteadily growing, leveraging advancements in sensor technology and computing\nperformance. However, object recognition in maritime environments faces\nchallenges such as light reflection, interference, intense lighting, and\nvarious weather conditions. To address these challenges, high-performance deep\nlearning algorithms tailored to maritime imagery and high-quality datasets\nspecialized for maritime scenes are essential. Existing AI recognition models\nand datasets have limited suitability for composing autonomous navigation\nsystems. Therefore, in this paper, we propose a Vertical and Detail Attention\n(VaDA) model for maritime object segmentation and a new model evaluation\nmethod, the Integrated Figure of Calculation Performance (IFCP), to verify its\nsuitability for the system in real-time. Additionally, we introduce a benchmark\nmaritime dataset, OASIs (Ocean AI Segmentation Initiatives) to standardize\nmodel performance evaluation across diverse maritime environments. OASIs\ndataset and details are available at our website:\nhttps://www.navlue.com/dataset\n","authors":["Yongjin Kim","Jinbum Park","Sanha Kang","Hanguen Kim"],"pdf_url":"https://arxiv.org/pdf/2407.09005v1.pdf","comment":"11 pages, 9 figures, whitepaper"},{"id":"http://arxiv.org/abs/2407.08994v1","updated":"2024-07-12T05:19:19Z","published":"2024-07-12T05:19:19Z","title":"Global Attention-Guided Dual-Domain Point Cloud Feature Learning for\n  Classification and Segmentation","summary":"  Previous studies have demonstrated the effectiveness of point-based neural\nmodels on the point cloud analysis task. However, there remains a crucial issue\non producing the efficient input embedding for raw point coordinates. Moreover,\nanother issue lies in the limited efficiency of neighboring aggregations, which\nis a critical component in the network stem. In this paper, we propose a Global\nAttention-guided Dual-domain Feature Learning network (GAD) to address the\nabove-mentioned issues. We first devise the Contextual Position-enhanced\nTransformer (CPT) module, which is armed with an improved global attention\nmechanism, to produce a global-aware input embedding that serves as the\nguidance to subsequent aggregations. Then, the Dual-domain K-nearest neighbor\nFeature Fusion (DKFF) is cascaded to conduct effective feature aggregation\nthrough novel dual-domain feature learning which appreciates both local\ngeometric relations and long-distance semantic connections. Extensive\nexperiments on multiple point cloud analysis tasks (e.g., classification, part\nsegmentation, and scene semantic segmentation) demonstrate the superior\nperformance of the proposed method and the efficacy of the devised modules.\n","authors":["Zihao Li","Pan Gao","Kang You","Chuan Yan","Manoranjan Paul"],"pdf_url":"https://arxiv.org/pdf/2407.08994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08993v1","updated":"2024-07-12T05:18:26Z","published":"2024-07-12T05:18:26Z","title":"Task-driven single-image super-resolution reconstruction of document\n  scans","summary":"  Super-resolution reconstruction is aimed at generating images of high spatial\nresolution from low-resolution observations. State-of-the-art super-resolution\ntechniques underpinned with deep learning allow for obtaining results of\noutstanding visual quality, but it is seldom verified whether they constitute a\nvaluable source for specific computer vision applications. In this paper, we\ninvestigate the possibility of employing super-resolution as a preprocessing\nstep to improve optical character recognition from document scans. To achieve\nthat, we propose to train deep networks for single-image super-resolution in a\ntask-driven way to make them better adapted for the purpose of text detection.\nAs problems limited to a specific task are heavily ill-posed, we introduce a\nmulti-task loss function that embraces components related with text detection\ncoupled with those guided by image similarity. The obtained results reported in\nthis paper are encouraging and they constitute an important step towards\nreal-world super-resolution of document images.\n","authors":["Maciej Zyrek","Michal Kawulok"],"pdf_url":"https://arxiv.org/pdf/2407.08993v1.pdf","comment":"Accepted for FedCSIS 2024"},{"id":"http://arxiv.org/abs/2407.08153v2","updated":"2024-07-12T04:58:00Z","published":"2024-07-11T03:12:28Z","title":"Lifelong Histopathology Whole Slide Image Retrieval via Distance\n  Consistency Rehearsal","summary":"  Content-based histopathological image retrieval (CBHIR) has gained attention\nin recent years, offering the capability to return histopathology images that\nare content-wise similar to the query one from an established database.\nHowever, in clinical practice, the continuously expanding size of WSI databases\nlimits the practical application of the current CBHIR methods. In this paper,\nwe propose a Lifelong Whole Slide Retrieval (LWSR) framework to address the\nchallenges of catastrophic forgetting by progressive model updating on\ncontinuously growing retrieval database. Our framework aims to achieve the\nbalance between stability and plasticity during continuous learning. To\npreserve system plasticity, we utilize local memory bank with reservoir\nsampling method to save instances, which can comprehensively encompass the\nfeature spaces of both old and new tasks. Furthermore, A distance consistency\nrehearsal (DCR) module is designed to ensure the retrieval queue's consistency\nfor previous tasks, which is regarded as stability within a lifelong CBHIR\nsystem. We evaluated the proposed method on four public WSI datasets from TCGA\nprojects. The experimental results have demonstrated the proposed method is\neffective and is superior to the state-of-the-art methods.\n","authors":["Xinyu Zhu","Zhiguo Jiang","Kun Wu","Jun Shi","Yushan Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.08153v2.pdf","comment":"Accepted for MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.06581v3","updated":"2024-07-12T04:55:18Z","published":"2024-07-09T06:20:17Z","title":"Vision language models are blind","summary":"  Large language models with vision capabilities (VLMs), e.g., GPT-4o and\nGemini 1.5 Pro are powering countless image-text applications and scoring high\non many vision-understanding benchmarks. We propose BlindTest, a suite of 7\nvisual tasks absurdly easy to humans such as identifying (a) whether two\ncircles overlap; (b) whether two lines intersect; (c) which letter is being\ncircled in a word; and (d) counting the number of circles in a Olympic-like\nlogo. Surprisingly, four state-of-the-art VLMs are, on average, only 56.20%\naccurate on our benchmark, with \\newsonnet being the best (73.77% accuracy). On\nBlindTest, VLMs struggle with tasks that requires precise spatial information\nand counting (from 0 to 10), sometimes providing an impression of a person with\nmyopia seeing fine details as blurry and making educated guesses. Code is\navailable at: https://vlmsareblind.github.io/\n","authors":["Pooyan Rahmanzadehgervi","Logan Bolton","Mohammad Reza Taesiri","Anh Totti Nguyen"],"pdf_url":"https://arxiv.org/pdf/2407.06581v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02068v3","updated":"2024-07-12T04:55:07Z","published":"2024-07-02T08:58:19Z","title":"LPViT: Low-Power Semi-structured Pruning for Vision Transformers","summary":"  Vision transformers have emerged as a promising alternative to convolutional\nneural networks for various image analysis tasks, offering comparable or\nsuperior performance. However, one significant drawback of ViTs is their\nresource-intensive nature, leading to increased memory footprint, computation\ncomplexity, and power consumption. To democratize this high-performance\ntechnology and make it more environmentally friendly, it is essential to\ncompress ViT models, reducing their resource requirements while maintaining\nhigh performance. In this paper, we introduce a new block-structured pruning to\naddress the resource-intensive issue for ViTs, offering a balanced trade-off\nbetween accuracy and hardware acceleration. Unlike unstructured pruning or\nchannel-wise structured pruning, block pruning leverages the block-wise\nstructure of linear layers, resulting in more efficient matrix multiplications.\nTo optimize this pruning scheme, our paper proposes a novel hardware-aware\nlearning objective that simultaneously maximizes speedup and minimizes power\nconsumption during inference, tailored to the block sparsity structure. This\nobjective eliminates the need for empirical look-up tables and focuses solely\non reducing parametrized layer connections. Moreover, our paper provides a\nlightweight algorithm to achieve post-training pruning for ViTs, utilizing\nsecond-order Taylor approximation and empirical optimization to solve the\nproposed hardware-aware objective. Extensive experiments on ImageNet are\nconducted across various ViT architectures, including DeiT-B and DeiT-S,\ndemonstrating competitive performance with other pruning methods and achieving\na remarkable balance between accuracy preservation and power savings.\nEspecially, we achieve up to 3.93x and 1.79x speedups on dedicated hardware and\nGPUs respectively for DeiT-B, and also observe an inference power reduction by\n1.4x on real-world GPUs.\n","authors":["Kaixin Xu","Zhe Wang","Chunyun Chen","Xue Geng","Jie Lin","Xulei Yang","Min Wu","Xiaoli Li","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2407.02068v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12986v2","updated":"2024-07-12T04:43:48Z","published":"2024-03-04T06:43:16Z","title":"BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced\n  Feature-Level Contrastive Learning","summary":"  Semi-supervised Learning (SSL) reduces the need for extensive annotations in\ndeep learning, but the more realistic challenge of imbalanced data distribution\nin SSL remains largely unexplored. In Class Imbalanced Semi-supervised Learning\n(CISSL), the bias introduced by unreliable pseudo-labels can be exacerbated by\nimbalanced data distributions. Most existing methods address this issue at\ninstance-level through reweighting or resampling, but the performance is\nheavily limited by their reliance on biased backbone representation. Some other\nmethods do perform feature-level adjustments like feature blending but might\nintroduce unfavorable noise. In this paper, we discuss the bonus of a more\nbalanced feature distribution for the CISSL problem, and further propose a\nBalanced Feature-Level Contrastive Learning method (BaCon). Our method directly\nregularizes the distribution of instances' representations in a well-designed\ncontrastive manner. Specifically, class-wise feature centers are computed as\nthe positive anchors, while negative anchors are selected by a straightforward\nyet effective mechanism. A distribution-related temperature adjustment is\nleveraged to control the class-wise contrastive degrees dynamically. Our method\ndemonstrates its effectiveness through comprehensive experiments on the\nCIFAR10-LT, CIFAR100-LT, STL10-LT, and SVHN-LT datasets across various\nsettings. For example, BaCon surpasses instance-level method FixMatch-based ABC\non CIFAR10-LT with a 1.21% accuracy improvement, and outperforms\nstate-of-the-art feature-level method CoSSL on CIFAR100-LT with a 0.63%\naccuracy improvement. When encountering more extreme imbalance degree, BaCon\nalso shows better robustness than other methods.\n","authors":["Qianhan Feng","Lujing Xie","Shijie Fang","Tong Lin"],"pdf_url":"https://arxiv.org/pdf/2403.12986v2.pdf","comment":"Accpeted paper of AAAI2024"},{"id":"http://arxiv.org/abs/2404.18245v2","updated":"2024-07-12T04:38:19Z","published":"2024-04-28T16:55:44Z","title":"FAD-SAR: A Novel Fishing Activity Detection System via Synthetic\n  Aperture Radar Images Based on Deep Learning Method","summary":"  Illegal, unreported, and unregulated (IUU) fishing activities seriously\naffect various aspects of human life. However, traditional methods for\ndetecting and monitoring IUU fishing activities at sea have limitations.\nAlthough synthetic aperture radar (SAR) can complement existing vessel\ndetection systems, extracting useful information from SAR images using\ntraditional methods remains a challenge, especially in IUU fishing. This paper\nproposes a deep learning based fishing activity detection system, which is\nimplemented on the xView3 dataset using six classical object detection models:\nSSD, RetinaNet, FSAF, FCOS, Faster R-CNN, and Cascade R-CNN. In addition, this\nwork employs different enhancement techniques to improve the performance of the\nFaster R-CNN model. The experimental results demonstrate that training the\nFaster R-CNN model using the Online Hard Example Mining (OHEM) strategy\nincreases the Avg-F1 value from 0.212 to 0.216.\n","authors":["Yanbing Bai","Siao Li","Rui-Yang Ju","Zihao Yang","Jinze Yu","Jen-Shiun Chiang"],"pdf_url":"https://arxiv.org/pdf/2404.18245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15040v3","updated":"2024-07-12T04:10:23Z","published":"2023-11-25T14:38:54Z","title":"InstaStyle: Inversion Noise of a Stylized Image is Secretly a Style\n  Adviser","summary":"  Stylized text-to-image generation focuses on creating images from textual\ndescriptions while adhering to a style specified by a few reference images.\nHowever, subtle style variations within different reference images can hinder\nthe model from accurately learning the target style. In this paper, we propose\nInstaStyle, a novel approach that excels in generating high-fidelity stylized\nimages with only a single reference image. Our approach is based on the finding\nthat the inversion noise from a stylized reference image inherently carries the\nstyle signal, as evidenced by their non-zero signal-to-noise ratio. We employ\nDDIM inversion to extract this noise from the reference image and leverage a\ndiffusion model to generate new stylized images from the \"style\" noise.\nAdditionally, the inherent ambiguity and bias of textual prompts impede the\nprecise conveying of style. To address this, we introduce a learnable style\ntoken via prompt refinement, which enhances the accuracy of the style\ndescription for the reference image. Qualitative and quantitative experimental\nresults demonstrate that InstaStyle achieves superior performance compared to\ncurrent benchmarks. Furthermore, our approach also showcases its capability in\nthe creative task of style combination with mixed inversion noise.\n","authors":["Xing Cui","Zekun Li","Pei Pei Li","Huaibo Huang","Xuannan Liu","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2311.15040v3.pdf","comment":"Accepted by European Conference on Computer Vision (ECCV 2024).\n  Project page: https://cuixing100876.github.io/instastyle.github.io/"},{"id":"http://arxiv.org/abs/2404.15734v2","updated":"2024-07-12T04:01:46Z","published":"2024-04-24T08:46:25Z","title":"ODMixer: Fine-grained Spatial-temporal MLP for Metro Origin-Destination\n  Prediction","summary":"  Metro Origin-Destination (OD) prediction is a crucial yet challenging\nspatial-temporal prediction task in urban computing, which aims to accurately\nforecast cross-station ridership for optimizing metro scheduling and enhancing\noverall transport efficiency. Analyzing fine-grained and comprehensive\nrelations among stations effectively is imperative for metro OD prediction.\nHowever, existing metro OD models either mix information from multiple OD pairs\nfrom the station's perspective or exclusively focus on a subset of OD pairs.\nThese approaches may overlook fine-grained relations among OD pairs, leading to\ndifficulties in predicting potential anomalous conditions. To address these\nchallenges, we analyze traffic variations from the perspective of all OD pairs\nand propose a fine-grained spatial-temporal MLP architecture for metro OD\nprediction, namely ODMixer. Specifically, our ODMixer has double-branch\nstructure and involves the Channel Mixer, the Multi-view Mixer, and the\nBidirectional Trend Learner. The Channel Mixer aims to capture short-term\ntemporal relations among OD pairs, the Multi-view Mixer concentrates on\ncapturing relations from both origin and destination perspectives. To model\nlong-term temporal relations, we introduce the Bidirectional Trend Learner.\nExtensive experiments on two large-scale metro OD prediction datasets HZMOD and\nSHMO demonstrate the advantages of our ODMixer. Our code is available at\nhttps://github.com/KLatitude/ODMixer.\n","authors":["Yang Liu","Binglin Chen","Yongsen Zheng","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2404.15734v2.pdf","comment":"Our code is available at https://github.com/KLatitude/ODMixer"},{"id":"http://arxiv.org/abs/2407.07403v2","updated":"2024-07-12T03:58:05Z","published":"2024-07-10T06:57:58Z","title":"A Survey of Attacks on Large Vision-Language Models: Resources,\n  Advances, and Future Trends","summary":"  With the significant development of large models in recent years, Large\nVision-Language Models (LVLMs) have demonstrated remarkable capabilities across\na wide range of multimodal understanding and reasoning tasks. Compared to\ntraditional Large Language Models (LLMs), LVLMs present great potential and\nchallenges due to its closer proximity to the multi-resource real-world\napplications and the complexity of multi-modal processing. However, the\nvulnerability of LVLMs is relatively underexplored, posing potential security\nrisks in daily usage. In this paper, we provide a comprehensive review of the\nvarious forms of existing LVLM attacks. Specifically, we first introduce the\nbackground of attacks targeting LVLMs, including the attack preliminary, attack\nchallenges, and attack resources. Then, we systematically review the\ndevelopment of LVLM attack methods, such as adversarial attacks that manipulate\nmodel outputs, jailbreak attacks that exploit model vulnerabilities for\nunauthorized actions, prompt injection attacks that engineer the prompt type\nand pattern, and data poisoning that affects model training. Finally, we\ndiscuss promising research directions in the future. We believe that our survey\nprovides insights into the current landscape of LVLM vulnerabilities, inspiring\nmore researchers to explore and mitigate potential safety issues in LVLM\ndevelopments. The latest papers on LVLM attacks are continuously collected in\nhttps://github.com/liudaizong/Awesome-LVLM-Attack.\n","authors":["Daizong Liu","Mingyu Yang","Xiaoye Qu","Pan Zhou","Yu Cheng","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2407.07403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08972v1","updated":"2024-07-12T03:55:20Z","published":"2024-07-12T03:55:20Z","title":"Revealing the Dark Secrets of Extremely Large Kernel ConvNets on\n  Robustness","summary":"  Robustness is a vital aspect to consider when deploying deep learning models\ninto the wild. Numerous studies have been dedicated to the study of the\nrobustness of vision transformers (ViTs), which have dominated as the\nmainstream backbone choice for vision tasks since the dawn of 2020s. Recently,\nsome large kernel convnets make a comeback with impressive performance and\nefficiency. However, it still remains unclear whether large kernel networks are\nrobust and the attribution of their robustness. In this paper, we first conduct\na comprehensive evaluation of large kernel convnets' robustness and their\ndifferences from typical small kernel counterparts and ViTs on six diverse\nrobustness benchmark datasets. Then to analyze the underlying factors behind\ntheir strong robustness, we design experiments from both quantitative and\nqualitative perspectives to reveal large kernel convnets' intriguing properties\nthat are completely different from typical convnets. Our experiments\ndemonstrate for the first time that pure CNNs can achieve exceptional\nrobustness comparable or even superior to that of ViTs. Our analysis on\nocclusion invariance, kernel attention patterns and frequency characteristics\nprovide novel insights into the source of robustness.\n","authors":["Honghao Chen","Yurong Zhang","Xiaokun Feng","Xiangxiang Chu","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2407.08972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08971v1","updated":"2024-07-12T03:53:55Z","published":"2024-07-12T03:53:55Z","title":"Full-Stage Pseudo Label Quality Enhancement for Weakly-supervised\n  Temporal Action Localization","summary":"  Weakly-supervised Temporal Action Localization (WSTAL) aims to localize\nactions in untrimmed videos using only video-level supervision. Latest WSTAL\nmethods introduce pseudo label learning framework to bridge the gap between\nclassification-based training and inferencing targets at localization, and\nachieve cutting-edge results. In these frameworks, a classification-based model\nis used to generate pseudo labels for a regression-based student model to learn\nfrom. However, the quality of pseudo labels in the framework, which is a key\nfactor to the final result, is not carefully studied. In this paper, we propose\na set of simple yet efficient pseudo label quality enhancement mechanisms to\nbuild our FuSTAL framework. FuSTAL enhances pseudo label quality at three\nstages: cross-video contrastive learning at proposal Generation-Stage,\nprior-based filtering at proposal Selection-Stage and EMA-based distillation at\nTraining-Stage. These designs enhance pseudo label quality at different stages\nin the framework, and help produce more informative, less false and smoother\naction proposals. With the help of these comprehensive designs at all stages,\nFuSTAL achieves an average mAP of 50.8% on THUMOS'14, outperforming the\nprevious best method by 1.2%, and becomes the first method to reach the\nmilestone of 50%.\n","authors":["Qianhan Feng","Wenshuo Li","Tong Lin","Xinghao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.08971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02813v2","updated":"2024-07-12T03:39:05Z","published":"2024-07-03T05:17:26Z","title":"Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm\n  and Compiler Co-Design","summary":"  Deep neural networks (DNNs) are frequently employed in a variety of computer\nvision applications. Nowadays, an emerging trend in the current video\ndistribution system is to take advantage of DNN's overfitting properties to\nperform video resolution upscaling. By splitting videos into chunks and\napplying a super-resolution (SR) model to overfit each chunk, this scheme of SR\nmodels plus video chunks is able to replace traditional video transmission to\nenhance video quality and transmission efficiency. However, many models and\nchunks are needed to guarantee high performance, which leads to tremendous\noverhead on model switching and memory footprints at the user end. To resolve\nsuch problems, we propose a Dynamic Deep neural network assisted by a\nContent-Aware data processing pipeline to reduce the model number down to one\n(Dy-DCA), which helps promote performance while conserving computational\nresources. Additionally, to achieve real acceleration on the user end, we\ndesigned a framework that optimizes dynamic features (e.g., dynamic shapes,\nsizes, and control flow) in Dy-DCA to enable a series of compilation\noptimizations, including fused code generation, static execution planning, etc.\nBy employing such techniques, our method achieves better PSNR and real-time\nperformance (33 FPS) on an off-the-shelf mobile phone. Meanwhile, assisted by\nour compilation optimization, we achieve a 1.7$\\times$ speedup while saving up\nto 1.61$\\times$ memory consumption. Code available in\nhttps://github.com/coulsonlee/Dy-DCA-ECCV2024.\n","authors":["Gen Li","Zhihao Shu","Jie Ji","Minghai Qin","Fatemeh Afghah","Wei Niu","Xiaolong Ma"],"pdf_url":"https://arxiv.org/pdf/2407.02813v2.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2405.02066v4","updated":"2024-07-12T03:35:19Z","published":"2024-05-03T12:56:34Z","title":"WateRF: Robust Watermarks in Radiance Fields for Protection of\n  Copyrights","summary":"  The advances in the Neural Radiance Fields (NeRF) research offer extensive\napplications in diverse domains, but protecting their copyrights has not yet\nbeen researched in depth. Recently, NeRF watermarking has been considered one\nof the pivotal solutions for safely deploying NeRF-based 3D representations.\nHowever, existing methods are designed to apply only to implicit or explicit\nNeRF representations. In this work, we introduce an innovative watermarking\nmethod that can be employed in both representations of NeRF. This is achieved\nby fine-tuning NeRF to embed binary messages in the rendering process. In\ndetail, we propose utilizing the discrete wavelet transform in the NeRF space\nfor watermarking. Furthermore, we adopt a deferred back-propagation technique\nand introduce a combination with the patch-wise loss to improve rendering\nquality and bit accuracy with minimum trade-offs. We evaluate our method in\nthree different aspects: capacity, invisibility, and robustness of the embedded\nwatermarks in the 2D-rendered images. Our method achieves state-of-the-art\nperformance with faster training speed over the compared state-of-the-art\nmethods.\n","authors":["Youngdong Jang","Dong In Lee","MinHyuk Jang","Jong Wook Kim","Feng Yang","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2405.02066v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12800v4","updated":"2024-07-12T03:32:19Z","published":"2024-03-19T15:01:18Z","title":"Learning Neural Volumetric Pose Features for Camera Localization","summary":"  We introduce a novel neural volumetric pose feature, termed PoseMap, designed\nto enhance camera localization by encapsulating the information between images\nand the associated camera poses. Our framework leverages an Absolute Pose\nRegression (APR) architecture, together with an augmented NeRF module. This\nintegration not only facilitates the generation of novel views to enrich the\ntraining dataset but also enables the learning of effective pose features.\nAdditionally, we extend our architecture for self-supervised online alignment,\nallowing our method to be used and fine-tuned for unlabelled images within a\nunified framework. Experiments demonstrate that our method achieves 14.28% and\n20.51% performance gain on average in indoor and outdoor benchmark scenes,\noutperforming existing APR methods with state-of-the-art accuracy.\n","authors":["Jingyu Lin","Jiaqi Gu","Bojian Wu","Lubin Fan","Renjie Chen","Ligang Liu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2403.12800v4.pdf","comment":"Accepted at ECCV 2024. Project page:\n  https://gujiaqivadin.github.io/posemap/"},{"id":"http://arxiv.org/abs/2407.08968v1","updated":"2024-07-12T03:32:13Z","published":"2024-07-12T03:32:13Z","title":"SlideGCD: Slide-based Graph Collaborative Training with Knowledge\n  Distillation for Whole Slide Image Classification","summary":"  Existing WSI analysis methods lie on the consensus that histopathological\ncharacteristics of tumors are significant guidance for cancer diagnostics.\nParticularly, as the evolution of cancers is a continuous process, the\ncorrelations and differences across various stages, anatomical locations and\npatients should be taken into account. However, recent research mainly focuses\non the inner-contextual information in a single WSI, ignoring the correlations\nbetween slides. To verify whether introducing the slide inter-correlations can\nbring improvements to WSI representation learning, we propose a generic WSI\nanalysis pipeline SlideGCD that considers the existing multi-instance learning\n(MIL) methods as the backbone and forge the WSI classification task as a node\nclassification problem. More specifically, SlideGCD declares a node buffer that\nstores previous slide embeddings for subsequent extensive slide-based graph\nconstruction and conducts graph learning to explore the inter-correlations\nimplied in the slide-based graph. Moreover, we frame the MIL classifier and\ngraph learning into two parallel workflows and deploy the knowledge\ndistillation to transfer the differentiable information to the graph neural\nnetwork. The consistent performance boosting, brought by SlideGCD, of four\nprevious state-of-the-art MIL methods is observed on two TCGA benchmark\ndatasets. The code is available at https://github.com/HFUT-miaLab/SlideGCD.\n","authors":["Tong Shu","Jun Shi","Dongdong Sun","Zhiguo Jiang","Yushan Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.08968v1.pdf","comment":"Submitted to MICCAI-2024"},{"id":"http://arxiv.org/abs/2407.08966v1","updated":"2024-07-12T03:30:53Z","published":"2024-07-12T03:30:53Z","title":"LAPT: Label-driven Automated Prompt Tuning for OOD Detection with\n  Vision-Language Models","summary":"  Out-of-distribution (OOD) detection is crucial for model reliability, as it\nidentifies samples from unknown classes and reduces errors due to unexpected\ninputs. Vision-Language Models (VLMs) such as CLIP are emerging as powerful\ntools for OOD detection by integrating multi-modal information. However, the\npractical application of such systems is challenged by manual prompt\nengineering, which demands domain expertise and is sensitive to linguistic\nnuances. In this paper, we introduce Label-driven Automated Prompt Tuning\n(LAPT), a novel approach to OOD detection that reduces the need for manual\nprompt engineering. We develop distribution-aware prompts with in-distribution\n(ID) class names and negative labels mined automatically. Training samples\nlinked to these class labels are collected autonomously via image synthesis and\nretrieval methods, allowing for prompt learning without manual effort. We\nutilize a simple cross-entropy loss for prompt optimization, with cross-modal\nand cross-distribution mixing strategies to reduce image noise and explore the\nintermediate space between distributions, respectively. The LAPT framework\noperates autonomously, requiring only ID class names as input and eliminating\nthe need for manual intervention. With extensive experiments, LAPT consistently\noutperforms manually crafted prompts, setting a new standard for OOD detection.\nMoreover, LAPT not only enhances the distinction between ID and OOD samples,\nbut also improves the ID classification accuracy and strengthens the\ngeneralization robustness to covariate shifts, resulting in outstanding\nperformance in challenging full-spectrum OOD detection tasks. Codes are\navailable at \\url{https://github.com/YBZh/LAPT}.\n","authors":["Yabin Zhang","Wenjie Zhu","Chenhang He","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.08966v1.pdf","comment":"ECCV2024; Codes and Supp. are available at:\n  https://github.com/YBZh/LAPT"},{"id":"http://arxiv.org/abs/2312.06653v2","updated":"2024-07-12T03:30:42Z","published":"2023-12-11T18:59:12Z","title":"Adaptive Human Trajectory Prediction via Latent Corridors","summary":"  Human trajectory prediction is typically posed as a zero-shot generalization\nproblem: a predictor is learnt on a dataset of human motion in training scenes,\nand then deployed on unseen test scenes. While this paradigm has yielded\ntremendous progress, it fundamentally assumes that trends in human behavior\nwithin the deployment scene are constant over time. As such, current prediction\nmodels are unable to adapt to scene-specific transient human behaviors, such as\ncrowds temporarily gathering to see buskers, pedestrians hurrying through the\nrain and avoiding puddles, or a protest breaking out. We formalize the problem\nof scene-specific adaptive trajectory prediction and propose a new adaptation\napproach inspired by prompt tuning called latent corridors. By augmenting the\ninput of any pre-trained human trajectory predictor with learnable image\nprompts, the predictor can improve in the deployment scene by inferring trends\nfrom extremely small amounts of new data (e.g., 2 humans observed for 30\nseconds). With less than 0.1% additional model parameters, we see up to 23.9%\nADE improvement in MOTSynth simulated data and 16.4% ADE in MOT and Wildtrack\nreal pedestrian data. Qualitatively, we observe that latent corridors imbue\npredictors with an awareness of scene geometry and scene-specific human\nbehaviors that non-adaptive predictors struggle to capture. The project website\ncan be found at https://neerja.me/atp_latent_corridors/.\n","authors":["Neerja Thakkar","Karttikeya Mangalam","Andrea Bajcsy","Jitendra Malik"],"pdf_url":"https://arxiv.org/pdf/2312.06653v2.pdf","comment":"Accepted to ECCV 2024. Project website can be found at\n  https://neerja.me/atp_latent_corridors/"},{"id":"http://arxiv.org/abs/2407.08965v1","updated":"2024-07-12T03:28:46Z","published":"2024-07-12T03:28:46Z","title":"Lite-SAM Is Actually What You Need for Segment Everything","summary":"  This paper introduces Lite-SAM, an efficient end-to-end solution for the\nSegEvery task designed to reduce computational costs and redundancy. Lite-SAM\nis composed of four main components: a streamlined CNN-Transformer hybrid\nencoder (LiteViT), an automated prompt proposal network (AutoPPN), a\ntraditional prompt encoder, and a mask decoder. All these components are\nintegrated within the SAM framework. Our LiteViT, a high-performance\nlightweight backbone network, has only 1.16M parameters, which is a 23%\nreduction compared to the lightest existing backbone network Shufflenet. We\nalso introduce AutoPPN, an innovative end-to-end method for prompt boxes and\npoints generation. This is an improvement over traditional grid search sampling\nmethods, and its unique design allows for easy integration into any SAM series\nalgorithm, extending its usability. we have thoroughly benchmarked Lite-SAM\nacross a plethora of both public and private datasets. The evaluation\nencompassed a broad spectrum of universal metrics, including the number of\nparameters, SegEvery execution time, and accuracy. The findings reveal that\nLite-SAM, operating with a lean 4.2M parameters, significantly outpaces its\ncounterparts, demonstrating performance improvements of 43x, 31x, 20x, 21x, and\n1.6x over SAM, MobileSAM, Edge-SAM, EfficientViT-SAM, and MobileSAM-v2\nrespectively, all the while maintaining competitive accuracy. This underscores\nLite-SAM's prowess in achieving an optimal equilibrium between performance and\nprecision, thereby setting a new state-of-the-art(SOTA) benchmark in the\ndomain.\n","authors":["Jianhai Fu","Yuanjie Yu","Ningchuan Li","Yi Zhang","Qichao Chen","Jianping Xiong","Jun Yin","Zhiyu Xiang"],"pdf_url":"https://arxiv.org/pdf/2407.08965v1.pdf","comment":"ECCV 2024 Accepted"},{"id":"http://arxiv.org/abs/2407.08961v1","updated":"2024-07-12T03:24:17Z","published":"2024-07-12T03:24:17Z","title":"Tissue-Contrastive Semi-Masked Autoencoders for Segmentation Pretraining\n  on Chest CT","summary":"  Existing Masked Image Modeling (MIM) depends on a spatial patch-based\nmasking-reconstruction strategy to perceive objects'features from unlabeled\nimages, which may face two limitations when applied to chest CT: 1) inefficient\nfeature learning due to complex anatomical details presented in CT images, and\n2) suboptimal knowledge transfer owing to input disparity between upstream and\ndownstream models. To address these issues, we propose a new MIM method named\nTissue-Contrastive Semi-Masked Autoencoder (TCS-MAE) for modeling chest CT\nimages. Our method has two novel designs: 1) a tissue-based\nmasking-reconstruction strategy to capture more fine-grained anatomical\nfeatures, and 2) a dual-AE architecture with contrastive learning between the\nmasked and original image views to bridge the gap of the upstream and\ndownstream models. To validate our method, we systematically investigate\nrepresentative contrastive, generative, and hybrid self-supervised learning\nmethods on top of tasks involving segmenting pneumonia, mediastinal tumors, and\nvarious organs. The results demonstrate that, compared to existing methods, our\nTCS-MAE more effectively learns tissue-aware representations, thereby\nsignificantly enhancing segmentation performance across all tasks.\n","authors":["Jie Zheng","Ru Wen","Haiqin Hu","Lina Wei","Kui Su","Wei Chen","Chen Liu","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08457v2","updated":"2024-07-12T03:19:07Z","published":"2024-07-11T12:54:21Z","title":"Neural Poisson Solver: A Universal and Continuous Framework for Natural\n  Signal Blending","summary":"  Implicit Neural Representation (INR) has become a popular method for\nrepresenting visual signals (e.g., 2D images and 3D scenes), demonstrating\npromising results in various downstream applications. Given its potential as a\nmedium for visual signals, exploring the development of a neural blending\nmethod that utilizes INRs is a natural progression. Neural blending involves\nmerging two INRs to create a new INR that encapsulates information from both\noriginal representations. A direct approach involves applying traditional image\nediting methods to the INR rendering process. However, this method often\nresults in blending distortions, artifacts, and color shifts, primarily due to\nthe discretization of the underlying pixel grid and the introduction of\nboundary conditions for solving variational problems. To tackle this issue, we\nintroduce the Neural Poisson Solver, a plug-and-play and universally applicable\nframework across different signal dimensions for blending visual signals\nrepresented by INRs. Our Neural Poisson Solver offers a variational\nproblem-solving approach based on the continuous Poisson equation,\ndemonstrating exceptional performance across various domains. Specifically, we\npropose a gradient-guided neural solver to represent the solution process of\nthe variational problem, refining the target signal to achieve natural blending\nresults. We also develop a Poisson equation-based loss and optimization scheme\nto train our solver, ensuring it effectively blends the input INR scenes while\npreserving their inherent structure and semantic content. The lack of\ndependence on additional prior knowledge makes our method easily adaptable to\nvarious task categories, highlighting its versatility. Comprehensive\nexperimental results validate the robustness of our approach across multiple\ndimensions and blending tasks.\n","authors":["Delong Wu","Hao Zhu","Qi Zhang","You Li","Zhan Ma","Xun Cao"],"pdf_url":"https://arxiv.org/pdf/2407.08457v2.pdf","comment":"accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.08950v1","updated":"2024-07-12T03:10:08Z","published":"2024-07-12T03:10:08Z","title":"Exploring Richer and More Accurate Information via Frequency Selection\n  for Image Restoration","summary":"  Image restoration aims to recover high-quality images from their corrupted\ncounterparts. Many existing methods primarily focus on the spatial domain,\nneglecting the understanding of frequency variations and ignoring the impact of\nimplicit noise in skip connections. In this paper, we introduce a multi-scale\nfrequency selection network (MSFSNet) that seamlessly integrates spatial and\nfrequency domain knowledge, selectively recovering richer and more accurate\ninformation. Specifically, we initially capture spatial features and input them\ninto dynamic filter selection modules (DFS) at different scales to integrate\nfrequency knowledge. DFS utilizes learnable filters to generate high and\nlow-frequency information and employs a frequency cross-attention mechanism\n(FCAM) to determine the most information to recover. To learn a multi-scale and\naccurate set of hybrid features, we develop a skip feature fusion block (SFF)\nthat leverages contextual features to discriminatively determine which\ninformation should be propagated in skip-connections. It is worth noting that\nour DFS and SFF are generic plug-in modules that can be directly employed in\nexisting networks without any adjustments, leading to performance improvements.\nExtensive experiments across various image restoration tasks demonstrate that\nour MSFSNet achieves performance that is either superior or comparable to\nstate-of-the-art algorithms.\n","authors":["Hu Gao","Depeng Dang"],"pdf_url":"https://arxiv.org/pdf/2407.08950v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2403.20106"},{"id":"http://arxiv.org/abs/2407.08949v1","updated":"2024-07-12T03:09:07Z","published":"2024-07-12T03:09:07Z","title":"One-Shot Pose-Driving Face Animation Platform","summary":"  The objective of face animation is to generate dynamic and expressive talking\nhead videos from a single reference face, utilizing driving conditions derived\nfrom either video or audio inputs. Current approaches often require fine-tuning\nfor specific identities and frequently fail to produce expressive videos due to\nthe limited effectiveness of Wav2Pose modules. To facilitate the generation of\none-shot and more consecutive talking head videos, we refine an existing\nImage2Video model by integrating a Face Locator and Motion Frame mechanism. We\nsubsequently optimize the model using extensive human face video datasets,\nsignificantly enhancing its ability to produce high-quality and expressive\ntalking head videos. Additionally, we develop a demo platform using the Gradio\nframework, which streamlines the process, enabling users to quickly create\ncustomized talking head videos.\n","authors":["He Feng","Donglin Di","Yongjia Ma","Wei Chen","Tonghua Su"],"pdf_url":"https://arxiv.org/pdf/2407.08949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08948v1","updated":"2024-07-12T03:08:15Z","published":"2024-07-12T03:08:15Z","title":"Symmetry Awareness Encoded Deep Learning Framework for Brain Imaging\n  Analysis","summary":"  The heterogeneity of neurological conditions, ranging from structural\nanomalies to functional impairments, presents a significant challenge in\nmedical imaging analysis tasks. Moreover, the limited availability of\nwell-annotated datasets constrains the development of robust analysis models.\nAgainst this backdrop, this study introduces a novel approach leveraging the\ninherent anatomical symmetrical features of the human brain to enhance the\nsubsequent detection and segmentation analysis for brain diseases. A novel\nSymmetry-Aware Cross-Attention (SACA) module is proposed to encode symmetrical\nfeatures of left and right hemispheres, and a proxy task to detect symmetrical\nfeatures as the Symmetry-Aware Head (SAH) is proposed, which guides the\npretraining of the whole network on a vast 3D brain imaging dataset comprising\nboth healthy and diseased brain images across various MRI and CT. Through\nmeticulous experimentation on downstream tasks, including both classification\nand segmentation for brain diseases, our model demonstrates superior\nperformance over state-of-the-art methodologies, particularly highlighting the\nsignificance of symmetry-aware learning. Our findings advocate for the\neffectiveness of incorporating symmetry awareness into pretraining and set a\nnew benchmark for medical imaging analysis, promising significant strides\ntoward accurate and efficient diagnostic processes. Code is available at\nhttps://github.com/bitMyron/sa-swin.\n","authors":["Yang Ma","Dongang Wang","Peilin Liu","Lynette Masters","Michael Barnett","Weidong Cai","Chenyu Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08948v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.08947v1","updated":"2024-07-12T03:07:28Z","published":"2024-07-12T03:07:28Z","title":"Constructing Concept-based Models to Mitigate Spurious Correlations with\n  Minimal Human Effort","summary":"  Enhancing model interpretability can address spurious correlations by\nrevealing how models draw their predictions. Concept Bottleneck Models (CBMs)\ncan provide a principled way of disclosing and guiding model behaviors through\nhuman-understandable concepts, albeit at a high cost of human efforts in data\nannotation. In this paper, we leverage a synergy of multiple foundation models\nto construct CBMs with nearly no human effort. We discover undesirable biases\nin CBMs built on pre-trained models and propose a novel framework designed to\nexploit pre-trained models while being immune to these biases, thereby reducing\nvulnerability to spurious correlations. Specifically, our method offers a\nseamless pipeline that adopts foundation models for assessing potential\nspurious correlations in datasets, annotating concepts for images, and refining\nthe annotations for improved robustness. We evaluate the proposed method on\nmultiple datasets, and the results demonstrate its effectiveness in reducing\nmodel reliance on spurious correlations while preserving its interpretability.\n","authors":["Jeeyung Kim","Ze Wang","Qiang Qiu"],"pdf_url":"https://arxiv.org/pdf/2407.08947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08944v1","updated":"2024-07-12T03:00:25Z","published":"2024-07-12T03:00:25Z","title":"Bora: Biomedical Generalist Video Generation Model","summary":"  Generative models hold promise for revolutionizing medical education,\nrobot-assisted surgery, and data augmentation for medical AI development.\nDiffusion models can now generate realistic images from text prompts, while\nrecent advancements have demonstrated their ability to create diverse,\nhigh-quality videos. However, these models often struggle with generating\naccurate representations of medical procedures and detailed anatomical\nstructures. This paper introduces Bora, the first spatio-temporal diffusion\nprobabilistic model designed for text-guided biomedical video generation. Bora\nleverages Transformer architecture and is pre-trained on general-purpose video\ngeneration tasks. It is fine-tuned through model alignment and instruction\ntuning using a newly established medical video corpus, which includes paired\ntext-video data from various biomedical fields. To the best of our knowledge,\nthis is the first attempt to establish such a comprehensive annotated\nbiomedical video dataset. Bora is capable of generating high-quality video data\nacross four distinct biomedical domains, adhering to medical expert standards\nand demonstrating consistency and diversity. This generalist video generative\nmodel holds significant potential for enhancing medical consultation and\ndecision-making, particularly in resource-limited settings. Additionally, Bora\ncould pave the way for immersive medical training and procedure planning.\nExtensive experiments on distinct medical modalities such as endoscopy,\nultrasound, MRI, and cell tracking validate the effectiveness of our model in\nunderstanding biomedical instructions and its superior performance across\nsubjects compared to state-of-the-art generation models.\n","authors":["Weixiang Sun","Xiaocao You","Ruizhe Zheng","Zhengqing Yuan","Xiang Li","Lifang He","Quanzheng Li","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2407.08944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00334v2","updated":"2024-07-12T02:55:40Z","published":"2023-04-01T15:10:02Z","title":"TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking\n  Styles","summary":"  Audio-driven talking head generation has drawn growing attention. To produce\ntalking head videos with desired facial expressions, previous methods rely on\nextra reference videos to provide expression information, which may be\ndifficult to find and hence limits their usage. In this work, we propose\nTalkCLIP, a framework that can generate talking heads where the expressions are\nspecified by natural language, hence allowing for specifying expressions more\nconveniently. To model the mapping from text to expressions, we first construct\na text-video paired talking head dataset where each video has diverse text\ndescriptions that depict both coarse-grained emotions and fine-grained facial\nmovements. Leveraging the proposed dataset, we introduce a CLIP-based style\nencoder that projects natural language-based descriptions to the\nrepresentations of expressions. TalkCLIP can even infer expressions for\ndescriptions unseen during training. TalkCLIP can also use text to modulate\nexpression intensity and edit expressions. Extensive experiments demonstrate\nthat TalkCLIP achieves the advanced capability of generating photo-realistic\ntalking heads with vivid facial expressions guided by text descriptions.\n","authors":["Yifeng Ma","Suzhen Wang","Yu Ding","Lincheng Li","Bowen Ma","Tangjie Lv","Changjie Fan","Zhipeng Hu","Zhidong Deng","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2304.00334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08418v2","updated":"2024-07-12T02:55:16Z","published":"2024-07-11T11:51:36Z","title":"PredBench: Benchmarking Spatio-Temporal Prediction across Diverse\n  Disciplines","summary":"  In this paper, we introduce PredBench, a benchmark tailored for the holistic\nevaluation of spatio-temporal prediction networks. Despite significant progress\nin this field, there remains a lack of a standardized framework for a detailed\nand comparative analysis of various prediction network architectures. PredBench\naddresses this gap by conducting large-scale experiments, upholding\nstandardized and appropriate experimental settings, and implementing\nmulti-dimensional evaluations. This benchmark integrates 12 widely adopted\nmethods with 15 diverse datasets across multiple application domains, offering\nextensive evaluation of contemporary spatio-temporal prediction networks.\nThrough meticulous calibration of prediction settings across various\napplications, PredBench ensures evaluations relevant to their intended use and\nenables fair comparisons. Moreover, its multi-dimensional evaluation framework\nbroadens the analysis with a comprehensive set of metrics, providing deep\ninsights into the capabilities of models. The findings from our research offer\nstrategic directions for future developments in the field. Our codebase is\navailable at https://github.com/OpenEarthLab/PredBench.\n","authors":["ZiDong Wang","Zeyu Lu","Di Huang","Tong He","Xihui Liu","Wanli Ouyang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2407.08418v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08939v1","updated":"2024-07-12T02:54:43Z","published":"2024-07-12T02:54:43Z","title":"LightenDiffusion: Unsupervised Low-Light Image Enhancement with\n  Latent-Retinex Diffusion Models","summary":"  In this paper, we propose a diffusion-based unsupervised framework that\nincorporates physically explainable Retinex theory with diffusion models for\nlow-light image enhancement, named LightenDiffusion. Specifically, we present a\ncontent-transfer decomposition network that performs Retinex decomposition\nwithin the latent space instead of image space as in previous approaches,\nenabling the encoded features of unpaired low-light and normal-light images to\nbe decomposed into content-rich reflectance maps and content-free illumination\nmaps. Subsequently, the reflectance map of the low-light image and the\nillumination map of the normal-light image are taken as input to the diffusion\nmodel for unsupervised restoration with the guidance of the low-light feature,\nwhere a self-constrained consistency loss is further proposed to eliminate the\ninterference of normal-light content on the restored results to improve overall\nvisual quality. Extensive experiments on publicly available real-world\nbenchmarks show that the proposed LightenDiffusion outperforms state-of-the-art\nunsupervised competitors and is comparable to supervised methods while being\nmore generalizable to various scenes. Our code is available at\nhttps://github.com/JianghaiSCU/LightenDiffusion.\n","authors":["Hai Jiang","Ao Luo","Xiaohong Liu","Songchen Han","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08939v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07427v2","updated":"2024-07-12T02:49:45Z","published":"2024-07-10T07:30:51Z","title":"Unified Embedding Alignment for Open-Vocabulary Video Instance\n  Segmentation","summary":"  Open-Vocabulary Video Instance Segmentation (VIS) is attracting increasing\nattention due to its ability to segment and track arbitrary objects. However,\nthe recent Open-Vocabulary VIS attempts obtained unsatisfactory results,\nespecially in terms of generalization ability of novel categories. We discover\nthat the domain gap between the VLM features (e.g., CLIP) and the instance\nqueries and the underutilization of temporal consistency are two central\ncauses. To mitigate these issues, we design and train a novel Open-Vocabulary\nVIS baseline called OVFormer. OVFormer utilizes a lightweight module for\nunified embedding alignment between query embeddings and CLIP image embeddings\nto remedy the domain gap. Unlike previous image-based training methods, we\nconduct video-based model training and deploy a semi-online inference scheme to\nfully mine the temporal consistency in the video. Without bells and whistles,\nOVFormer achieves 21.9 mAP with a ResNet-50 backbone on LV-VIS, exceeding the\nprevious state-of-the-art performance by 7.7. Extensive experiments on some\nClose-Vocabulary VIS datasets also demonstrate the strong zero-shot\ngeneralization ability of OVFormer (+ 7.6 mAP on YouTube-VIS 2019, + 3.9 mAP on\nOVIS). Code is available at https://github.com/fanghaook/OVFormer.\n","authors":["Hao Fang","Peng Wu","Yawei Li","Xinxin Zhang","Xiankai Lu"],"pdf_url":"https://arxiv.org/pdf/2407.07427v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2403.02558v2","updated":"2024-07-12T02:46:33Z","published":"2024-03-05T00:27:43Z","title":"The Minimum Information about CLinical Artificial Intelligence Checklist\n  for Generative Modeling Research (MI-CLAIM-GEN)","summary":"  Recent advances in generative models, including large language models (LLMs),\nvision language models (VLMs), and diffusion models, have accelerated the field\nof natural language and image processing in medicine and marked a significant\nparadigm shift in how biomedical models can be developed and deployed. While\nthese models are highly adaptable to new tasks, scaling and evaluating their\nusage presents new challenges not addressed in previous frameworks. In\nparticular, the ability of these models to produce useful outputs with little\nto no specialized training data (\"zero-\" or \"few-shot\" approaches), as well as\nthe open-ended nature of their outputs, necessitate the development of new\nguidelines for robust reporting of clinical generative model research. In\nresponse to gaps in standards and best practices for the development of\nclinical AI tools identified by US Executive Order 141103 and several emerging\nnational networks for clinical AI evaluation, we begin to formalize some of\nthese guidelines by building on the original MI-CLAIM checklist. The new\nchecklist, MI-CLAIM-GEN (Table 1), aims to address differences in training,\nevaluation, interpretability, and reproducibility of new generative models\ncompared to non-generative (\"predictive\") AI models. This MI-CLAIM-GEN\nchecklist also seeks to clarify cohort selection reporting with unstructured\nclinical data and adds additional items on alignment with ethical standards for\nclinical AI research.\n","authors":["Brenda Y. Miao","Irene Y. Chen","Christopher YK Williams","JaysÃ³n Davidson","Augusto Garcia-Agundez","Shenghuan Sun","Travis Zack","Suchi Saria","Rima Arnaout","Giorgio Quer","Hossein J. Sadaei","Ali Torkamani","Brett Beaulieu-Jones","Bin Yu","Milena Gianfrancesco","Atul J. Butte","Beau Norgeot","Madhumita Sushil"],"pdf_url":"https://arxiv.org/pdf/2403.02558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08136v2","updated":"2024-07-12T02:41:51Z","published":"2024-07-11T02:26:51Z","title":"EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable\n  Landmark Conditions","summary":"  The area of portrait image animation, propelled by audio input, has witnessed\nnotable progress in the generation of lifelike and dynamic portraits.\nConventional methods are limited to utilizing either audios or facial key\npoints to drive images into videos, while they can yield satisfactory results,\ncertain issues exist. For instance, methods driven solely by audios can be\nunstable at times due to the relatively weaker audio signal, while methods\ndriven exclusively by facial key points, although more stable in driving, can\nresult in unnatural outcomes due to the excessive control of key point\ninformation. In addressing the previously mentioned challenges, in this paper,\nwe introduce a novel approach which we named EchoMimic. EchoMimic is\nconcurrently trained using both audios and facial landmarks. Through the\nimplementation of a novel training strategy, EchoMimic is capable of generating\nportrait videos not only by audios and facial landmarks individually, but also\nby a combination of both audios and selected facial landmarks. EchoMimic has\nbeen comprehensively compared with alternative algorithms across various public\ndatasets and our collected dataset, showcasing superior performance in both\nquantitative and qualitative evaluations. Additional visualization and access\nto the source code can be located on the EchoMimic project page.\n","authors":["Zhiyuan Chen","Jiajiong Cao","Zhiquan Chen","Yuming Li","Chenguang Ma"],"pdf_url":"https://arxiv.org/pdf/2407.08136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08931v1","updated":"2024-07-12T02:34:11Z","published":"2024-07-12T02:34:11Z","title":"Global-Local Collaborative Inference with LLM for Lidar-Based\n  Open-Vocabulary Detection","summary":"  Open-Vocabulary Detection (OVD) is the task of detecting all interesting\nobjects in a given scene without predefined object classes. Extensive work has\nbeen done to deal with the OVD for 2D RGB images, but the exploration of 3D OVD\nis still limited. Intuitively, lidar point clouds provide 3D information, both\nobject level and scene level, to generate trustful detection results. However,\nprevious lidar-based OVD methods only focus on the usage of object-level\nfeatures, ignoring the essence of scene-level information. In this paper, we\npropose a Global-Local Collaborative Scheme (GLIS) for the lidar-based OVD\ntask, which contains a local branch to generate object-level detection result\nand a global branch to obtain scene-level global feature. With the global-local\ninformation, a Large Language Model (LLM) is applied for chain-of-thought\ninference, and the detection result can be refined accordingly. We further\npropose Reflected Pseudo Labels Generation (RPLG) to generate high-quality\npseudo labels for supervision and Background-Aware Object Localization (BAOL)\nto select precise object proposals. Extensive experiments on ScanNetV2 and SUN\nRGB-D demonstrate the superiority of our methods. Code is released at\nhttps://github.com/GradiusTwinbee/GLIS.\n","authors":["Xingyu Peng","Yan Bai","Chen Gao","Lirong Yang","Fei Xia","Beipeng Mu","Xiaofei Wang","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08931v1.pdf","comment":"accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.06617v2","updated":"2024-07-12T02:03:17Z","published":"2024-07-09T07:47:16Z","title":"Mobius: A High Efficient Spatial-Temporal Parallel Training Paradigm for\n  Text-to-Video Generation Task","summary":"  Inspired by the success of the text-to-image (T2I) generation task, many\nresearchers are devoting themselves to the text-to-video (T2V) generation task.\nMost of the T2V frameworks usually inherit from the T2I model and add\nextra-temporal layers of training to generate dynamic videos, which can be\nviewed as a fine-tuning task. However, the traditional 3D-Unet is a serial mode\nand the temporal layers follow the spatial layers, which will result in high\nGPU memory and training time consumption according to its serial feature flow.\nWe believe that this serial mode will bring more training costs with the large\ndiffusion model and massive datasets, which are not environmentally friendly\nand not suitable for the development of the T2V. Therefore, we propose a highly\nefficient spatial-temporal parallel training paradigm for T2V tasks, named\nMobius. In our 3D-Unet, the temporal layers and spatial layers are parallel,\nwhich optimizes the feature flow and backpropagation. The Mobius will save 24%\nGPU memory and 12% training time, which can greatly improve the T2V fine-tuning\ntask and provide a novel insight for the AIGC community. We will release our\ncodes in the future.\n","authors":["Yiran Yang","Jinchao Zhang","Ying Deng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.06617v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2310.06744v3","updated":"2024-07-12T01:55:26Z","published":"2023-10-10T16:14:20Z","title":"HiFi-123: Towards High-fidelity One Image to 3D Content Generation","summary":"  Recent advances in diffusion models have enabled 3D generation from a single\nimage. However, current methods often produce suboptimal results for novel\nviews, with blurred textures and deviations from the reference image, limiting\ntheir practical applications. In this paper, we introduce HiFi-123, a method\ndesigned for high-fidelity and multi-view consistent 3D generation. Our\ncontributions are twofold: First, we propose a Reference-Guided Novel View\nEnhancement (RGNV) technique that significantly improves the fidelity of\ndiffusion-based zero-shot novel view synthesis methods. Second, capitalizing on\nthe RGNV, we present a novel Reference-Guided State Distillation (RGSD) loss.\nWhen incorporated into the optimization-based image-to-3D pipeline, our method\nsignificantly improves 3D generation quality, achieving state-of-the-art\nperformance. Comprehensive evaluations demonstrate the effectiveness of our\napproach over existing methods, both qualitatively and quantitatively. Video\nresults are available on the project page.\n","authors":["Wangbo Yu","Li Yuan","Yan-Pei Cao","Xiangjun Gao","Xiaoyu Li","Wenbo Hu","Long Quan","Ying Shan","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2310.06744v3.pdf","comment":"Accepted by ECCV 2024. Project Page:\n  https://drexubery.github.io/HiFi-123/"},{"id":"http://arxiv.org/abs/2407.06886v2","updated":"2024-07-12T01:48:00Z","published":"2024-07-09T14:14:47Z","title":"Aligning Cyber Space with Physical World: A Comprehensive Survey on\n  Embodied AI","summary":"  Embodied Artificial Intelligence (Embodied AI) is crucial for achieving\nArtificial General Intelligence (AGI) and serves as a foundation for various\napplications that bridge cyberspace and the physical world. Recently, the\nemergence of Multi-modal Large Models (MLMs) and World Models (WMs) have\nattracted significant attention due to their remarkable perception,\ninteraction, and reasoning capabilities, making them a promising architecture\nfor the brain of embodied agents. However, there is no comprehensive survey for\nEmbodied AI in the era of MLMs. In this survey, we give a comprehensive\nexploration of the latest advancements in Embodied AI. Our analysis firstly\nnavigates through the forefront of representative works of embodied robots and\nsimulators, to fully understand the research focuses and their limitations.\nThen, we analyze four main research targets: 1) embodied perception, 2)\nembodied interaction, 3) embodied agent, and 4) sim-to-real adaptation,\ncovering the state-of-the-art methods, essential paradigms, and comprehensive\ndatasets. Additionally, we explore the complexities of MLMs in virtual and real\nembodied agents, highlighting their significance in facilitating interactions\nin dynamic digital and physical environments. Finally, we summarize the\nchallenges and limitations of embodied AI and discuss their potential future\ndirections. We hope this survey will serve as a foundational reference for the\nresearch community and inspire continued innovation. The associated project can\nbe found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.\n","authors":["Yang Liu","Weixing Chen","Yongjie Bai","Jingzhou Luo","Xinshuai Song","Kaixuan Jiang","Zhida Li","Ganlong Zhao","Junyi Lin","Guanbin Li","Wen Gao","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2407.06886v2.pdf","comment":"The first comprehensive review of Embodied AI in the era of MLMs, 37\n  pages. We also provide the paper list for Embodied AI:\n  https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List"},{"id":"http://arxiv.org/abs/2404.00230v2","updated":"2024-07-12T01:41:44Z","published":"2024-03-30T03:19:50Z","title":"Latent Watermark: Inject and Detect Watermarks in Latent Diffusion Space","summary":"  Watermarking is a tool for actively identifying and attributing the images\ngenerated by latent diffusion models. Existing methods face the dilemma of\nimage quality and watermark robustness. Watermarks with superior image quality\nusually have inferior robustness against attacks such as blurring and JPEG\ncompression, while watermarks with superior robustness usually significantly\ndamage image quality. This dilemma stems from the traditional paradigm where\nwatermarks are injected and detected in pixel space, relying on pixel\nperturbation for watermark detection and resilience against attacks. In this\npaper, we highlight that an effective solution to the problem is to both inject\nand detect watermarks in the latent diffusion space, and propose Latent\nWatermark with a progressive training strategy. It weakens the direct\nconnection between quality and robustness and thus alleviates their\ncontradiction. We conduct evaluations on two datasets and against 10 watermark\nattacks. 6 metrics measure the image quality and watermark robustness. Results\nshow that compared to the recently proposed methods such as StegaStamp,\nStableSignature, RoSteALS, and TreeRing, LW not only surpasses them in terms of\nrobustness but also offers superior image quality. Our code will be available\nat https://github.com/RichardSunnyMeng/LatentWatermark.\n","authors":["Zheling Meng","Bo Peng","Jing Dong"],"pdf_url":"https://arxiv.org/pdf/2404.00230v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.09417v1","updated":"2024-07-12T16:47:34Z","published":"2024-07-12T16:47:34Z","title":"Mitigating Entity-Level Hallucination in Large Language Models","summary":"  The emergence of Large Language Models (LLMs) has revolutionized how users\naccess information, shifting from traditional search engines to direct\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\ngenerate coherent yet factually inaccurate responses. This hallucination\nphenomenon has led to users' distrust in information retrieval systems based on\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\nAugmentation based on hallucination Detection (DRAD) as a novel method to\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\nretrieval augmentation by dynamically adapting the retrieval process based on\nreal-time hallucination detection. It features two main components: Real-time\nHallucination Detection (RHD) for identifying potential hallucinations without\nexternal models, and Self-correction based on External Knowledge (SEK) for\ncorrecting these errors using external knowledge. Experiment results show that\nDRAD demonstrates superior performance in both detecting and mitigating\nhallucinations in LLMs. All of our code and data are open-sourced at\nhttps://github.com/oneal2000/EntityHallucination.\n","authors":["Weihang Su","Yichen Tang","Qingyao Ai","Changyue Wang","Zhijing Wu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.09417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09395v1","updated":"2024-07-12T16:18:05Z","published":"2024-07-12T16:18:05Z","title":"Deep Bag-of-Words Model: An Efficient and Interpretable Relevance\n  Architecture for Chinese E-Commerce","summary":"  Text relevance or text matching of query and product is an essential\ntechnique for the e-commerce search system to ensure that the displayed\nproducts can match the intent of the query. Many studies focus on improving the\nperformance of the relevance model in search system. Recently, pre-trained\nlanguage models like BERT have achieved promising performance on the text\nrelevance task. While these models perform well on the offline test dataset,\nthere are still obstacles to deploy the pre-trained language model to the\nonline system as their high latency. The two-tower model is extensively\nemployed in industrial scenarios, owing to its ability to harmonize performance\nwith computational efficiency. Regrettably, such models present an opaque\n``black box'' nature, which prevents developers from making special\noptimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an\nefficient and interpretable relevance architecture for Chinese e-commerce. Our\napproach proposes to encode the query and the product into the sparse BoW\nrepresentation, which is a set of word-weight pairs. The weight means the\nimportant or the relevant score between the corresponding word and the raw\ntext. The relevance score is measured by the accumulation of the matched word\nbetween the sparse BoW representation of the query and the product. Compared to\npopular dense distributed representation that usually suffers from the drawback\nof black-box, the most advantage of the proposed representation model is highly\nexplainable and interventionable, which is a superior advantage to the\ndeployment and operation of online search engines. Moreover, the online\nefficiency of the proposed model is even better than the most efficient inner\nproduct form of dense representation ...\n","authors":["Zhe Lin","Jiwei Tan","Dan Ou","Xi Chen","Shaowei Yao","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.09395v1.pdf","comment":"KDD'24 accepted paper"},{"id":"http://arxiv.org/abs/2407.09394v1","updated":"2024-07-12T16:18:00Z","published":"2024-07-12T16:18:00Z","title":"PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with\n  User-Centric Agents","summary":"  Large Language Models (LLMs) struggle with generating reliable outputs due to\noutdated knowledge and hallucinations. Retrieval-Augmented Generation (RAG)\nmodels address this by enhancing LLMs with external knowledge, but often fail\nto personalize the retrieval process. This paper introduces PersonaRAG, a novel\nframework incorporating user-centric agents to adapt retrieval and generation\nbased on real-time user data and interactions. Evaluated across various\nquestion answering datasets, PersonaRAG demonstrates superiority over baseline\nmodels, providing tailored answers to user needs. The results suggest promising\ndirections for user-adapted information retrieval systems.\n","authors":["Saber Zerhoudi","Michael Granitzer"],"pdf_url":"https://arxiv.org/pdf/2407.09394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09252v1","updated":"2024-07-12T13:30:44Z","published":"2024-07-12T13:30:44Z","title":"Context Embeddings for Efficient Answer Generation in RAG","summary":"  Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge\nof LLMs by extending the input with external information. As a consequence, the\ncontextual inputs to the model become much longer which slows down decoding\ntime directly translating to the time a user has to wait for an answer. We\naddress this challenge by presenting COCOM, an effective context compression\nmethod, reducing long contexts to only a handful of Context Embeddings speeding\nup the generation time by a large margin. Our method allows for different\ncompression rates trading off decoding time for answer quality. Compared to\nearlier methods, COCOM allows for handling multiple contexts more effectively,\nsignificantly reducing decoding time for long inputs. Our method demonstrates a\nspeed-up of up to 5.69 $\\times$ while achieving higher performance compared to\nexisting efficient context compression methods.\n","authors":["David Rau","Shuai Wang","HervÃ© DÃ©jean","StÃ©phane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2407.09252v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.09157v1","updated":"2024-07-12T10:44:51Z","published":"2024-07-12T10:44:51Z","title":"Movie Recommendation with Poster Attention via Multi-modal Transformer\n  Feature Fusion","summary":"  Pre-trained models learn general representations from large datsets which can\nbe fine-turned for specific tasks to significantly reduce training time.\nPre-trained models like generative pretrained transformers (GPT), bidirectional\nencoder representations from transformers (BERT), vision transfomers (ViT) have\nbecome a cornerstone of current research in machine learning. This study\nproposes a multi-modal movie recommendation system by extract features of the\nwell designed posters for each movie and the narrative text description of the\nmovie. This system uses the BERT model to extract the information of text\nmodality, the ViT model applied to extract the information of poster/image\nmodality, and the Transformer architecture for feature fusion of all modalities\nto predict users' preference. The integration of pre-trained foundational\nmodels with some smaller data sets in downstream applications capture\nmulti-modal content features in a more comprehensive manner, thereby providing\nmore accurate recommendations. The efficiency of the proof-of-concept model is\nverified by the standard benchmark problem the MovieLens 100K and 1M datasets.\nThe prediction accuracy of user ratings is enhanced in comparison to the\nbaseline algorithm, thereby demonstrating the potential of this cross-modal\nalgorithm to be applied for movie or video recommendation.\n","authors":["Linhan Xia","Yicheng Yang","Ziou Chen","Zheng Yang","Shengxin Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.09157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09147v1","updated":"2024-07-12T10:30:45Z","published":"2024-07-12T10:30:45Z","title":"AI-Powered Immersive Assistance for Interactive Task Execution in\n  Industrial Environments","summary":"  Many industrial sectors rely on well-trained employees that are able to\noperate complex machinery. In this work, we demonstrate an AI-powered immersive\nassistance system that supports users in performing complex tasks in industrial\nenvironments. Specifically, our system leverages a VR environment that\nresembles a juice mixer setup. This digital twin of a physical setup simulates\ncomplex industrial machinery used to mix preparations or liquids (e.g., similar\nto the pharmaceutical industry) and includes various containers, sensors,\npumps, and flow controllers. This setup demonstrates our system's capabilities\nin a controlled environment while acting as a proof-of-concept for broader\nindustrial applications. The core components of our multimodal AI assistant are\na large language model and a speech-to-text model that process a video and\naudio recording of an expert performing the task in a VR environment. The video\nand speech input extracted from the expert's video enables it to provide\nstep-by-step guidance to support users in executing complex tasks. This\ndemonstration showcases the potential of our AI-powered assistant to reduce\ncognitive load, increase productivity, and enhance safety in industrial\nenvironments.\n","authors":["Tomislav Duricic","Peter MÃ¼llner","Nicole Weidinger","Neven ElSayed","Dominik Kowald","Eduardo Veas"],"pdf_url":"https://arxiv.org/pdf/2407.09147v1.pdf","comment":"3 pages, 2 figures, Demo Paper accepted at the 50th European\n  Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2407.09138v1","updated":"2024-07-12T10:16:21Z","published":"2024-07-12T10:16:21Z","title":"Distinct citation distributions complicate research evaluations. A\n  single indicator that universally reveals research efficiency cannot be\n  formulated","summary":"  Purpose: Analyze the diversity of citation distributions to publications in\ndifferent research topics to investigate the accuracy of size-independent,\nrank-based indicators. Top percentile-based indicators are the most common\nindicators of this type, and the evaluations of Japan are the most evident\nmisjudgments. Design/methodology/approach: The distributions of citations to\npublications from countries and in journals in several research topics were\nanalyzed along with the corresponding global publications using histograms with\nlogarithmic binning, double rank plots, and normal probability plots of\nlog-transformed numbers of citations. Findings: Size-independent, top\npercentile-based indicators are accurate when the global ranks of local\npublications fit a power law, but deviations in the least cited papers are\nfrequent in countries and occur in all journals with high impact factors. In\nthese cases, a single indicator is misleading. Comparisons of proportions of\nuncited papers are the best way to predict these deviations. Research\nlimitations: The study is fundamentally analytical; its results describe\nmathematical facts that are self-evident. Practical implications: Respectable\ninstitutions, such as the OECD, European Commission, US National Science Board,\nand others, produce research country rankings and individual evaluations using\nsize-independent percentile indicators that are misleading in many countries.\nThese misleading evaluations should be discontinued because they cause\nconfusion among research policymakers and lead to incorrect research policies.\nOriginality/value: Studies linking the lower tail of citation distribution,\nincluding uncited papers, to percentile research indicators have not been\nperformed previously. The present results demonstrate that studies of this type\nare necessary to find reliable procedures for research assessments.\n","authors":["Alonso RodrÃ­guez-Navarro"],"pdf_url":"https://arxiv.org/pdf/2407.09138v1.pdf","comment":"30 pages, 6 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.09137v1","updated":"2024-07-12T10:16:03Z","published":"2024-07-12T10:16:03Z","title":"A Look Into News Avoidance Through AWRS: An Avoidance-Aware Recommender\n  System","summary":"  In recent years, journalists have expressed concerns about the increasing\ntrend of news article avoidance, especially within specific domains. This issue\nhas been exacerbated by the rise of recommender systems. Our research indicates\nthat recommender systems should consider avoidance as a fundamental factor. We\nargue that news articles can be characterized by three principal elements:\nexposure, relevance, and avoidance, all of which are closely interconnected. To\naddress these challenges, we introduce AWRS, an Avoidance-Aware Recommender\nSystem. This framework incorporates avoidance awareness when recommending news,\nbased on the premise that news article avoidance conveys significant\ninformation about user preferences. Evaluation results on three news datasets\nin different languages (English, Norwegian, and Japanese) demonstrate that our\nmethod outperforms existing approaches.\n","authors":["Igor L. R. Azevedo","Toyotaro Suzumura","Yuichiro Yasui"],"pdf_url":"https://arxiv.org/pdf/2407.09137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09064v1","updated":"2024-07-12T07:34:10Z","published":"2024-07-12T07:34:10Z","title":"Multi-Modal Dataset Creation for Federated~Learning with DICOM\n  Structured Reports","summary":"  Purpose: Federated training is often hindered by heterogeneous datasets due\nto divergent data storage options, inconsistent naming schemes, varied\nannotation procedures, and disparities in label quality. This is particularly\nevident in the emerging multi-modal learning paradigms, where dataset\nharmonization including a uniform data representation and filtering options are\nof paramount importance.\n  Methods: DICOM structured reports enable the standardized linkage of\narbitrary information beyond the imaging domain and can be used within Python\ndeep learning pipelines with highdicom. Building on this, we developed an open\nplatform for data integration and interactive filtering capabilities that\nsimplifies the process of assembling multi-modal datasets.\n  Results: In this study, we extend our prior work by showing its applicability\nto more and divergent data types, as well as streamlining datasets for\nfederated training within an established consortium of eight university\nhospitals in Germany. We prove its concurrent filtering ability by creating\nharmonized multi-modal datasets across all locations for predicting the outcome\nafter minimally invasive heart valve replacement. The data includes DICOM data\n(i.e. computed tomography images, electrocardiography scans) as well as\nannotations (i.e. calcification segmentations, pointsets and pacemaker\ndependency), and metadata (i.e. prosthesis and diagnoses).\n  Conclusion: Structured reports bridge the traditional gap between imaging\nsystems and information systems. Utilizing the inherent DICOM reference system\narbitrary data types can be queried concurrently to create meaningful cohorts\nfor clinical studies. The graphical interface as well as example structured\nreport templates will be made publicly available.\n","authors":["Malte TÃ¶lle","Lukas Burger","Halvar Kelm","Florian AndrÃ©","Peter Bannas","Gerhard Diller","Norbert Frey","Philipp Garthe","Stefan GroÃ","Anja Hennemuth","Lars Kaderali","Nina KrÃ¼ger","Andreas Leha","Simon Martin","Alexander Meyer","Eike Nagel","Stefan Orwat","Clemens Scherer","Moritz Seiffert","Jan Moritz Seliger","Stefan Simm","Tim Friede","Tim Seidler","Sandy Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2407.09064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09045v1","updated":"2024-07-12T07:10:47Z","published":"2024-07-12T07:10:47Z","title":"Time-Frequency Analysis of Variable-Length WiFi CSI Signals for Person\n  Re-Identification","summary":"  Person re-identification (ReID), as a crucial technology in the field of\nsecurity, plays an important role in security detection and people counting.\nCurrent security and monitoring systems largely rely on visual information,\nwhich may infringe on personal privacy and be susceptible to interference from\npedestrian appearances and clothing in certain scenarios. Meanwhile, the\nwidespread use of routers offers new possibilities for ReID. This letter\nintroduces a method using WiFi Channel State Information (CSI), leveraging the\nmultipath propagation characteristics of WiFi signals as a basis for\ndistinguishing different pedestrian features. We propose a two-stream network\nstructure capable of processing variable-length data, which analyzes the\namplitude in the time domain and the phase in the frequency domain of WiFi\nsignals, fuses time-frequency information through continuous lateral\nconnections, and employs advanced objective functions for representation and\nmetric learning. Tested on a dataset collected in the real world, our method\nachieves 93.68% mAP and 98.13% Rank-1.\n","authors":["Chen Mao","Chong Tan","Jingqi Hu","Min Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.09045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09017v1","updated":"2024-07-12T06:10:01Z","published":"2024-07-12T06:10:01Z","title":"AI-Driven Guided Response for Security Operation Centers with Microsoft\n  Copilot for Security","summary":"  Security operation centers contend with a constant stream of security\nincidents, ranging from straightforward to highly complex. To address this, we\ndeveloped Copilot Guided Response (CGR), an industry-scale ML architecture that\nguides security analysts across three key tasks -- (1) investigation, providing\nessential historical context by identifying similar incidents; (2) triaging to\nascertain the nature of the incident -- whether it is a true positive, false\npositive, or benign positive; and (3) remediation, recommending tailored\ncontainment actions. CGR is integrated into the Microsoft Defender XDR product\nand deployed worldwide, generating millions of recommendations across thousands\nof customers. Our extensive evaluation, incorporating internal evaluation,\ncollaboration with security experts, and customer feedback, demonstrates that\nCGR delivers high-quality recommendations across all three tasks. We provide a\ncomprehensive overview of the CGR architecture, setting a precedent as the\nfirst cybersecurity company to openly discuss these capabilities in such depth.\nAdditionally, we GUIDE, the largest public collection of real-world security\nincidents, spanning 13M evidences across 1M annotated incidents. By enabling\nresearchers and practitioners to conduct research on real-world data, GUIDE\nadvances the state of cybersecurity and supports the development of\nnext-generation machine learning systems.\n","authors":["Scott Freitas","Jovan Kalajdjieski","Amir Gharib","Rob McCann"],"pdf_url":"https://arxiv.org/pdf/2407.09017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08334v2","updated":"2024-07-12T03:36:01Z","published":"2024-07-11T09:35:08Z","title":"ADMM Based Semi-Structured Pattern Pruning Framework For Transformer","summary":"  NLP(natural language processsing) has achieved great success through the\ntransformer model.However, the model has hundreds of millions or billions\nparameters,which is huge burden for its deployment on personal computer or\nsmall scale of server.To deal with it, we either make the model's weight matrix\nrelatively sparser, or compress attention layer. Pattern pruning ,one of the\nmost important pruning methods, permits selecting fixed number of parameters in\neach divided pattern block and prunes it. However, the effect of pattern\npruning is strictly limited by the sparsity within a region of weights in each\nlayer. In this paper,we first introduced Alternating Direction Method of\nMultipliers(ADMM) based pattern pruning framework to reshape the distribution\nof activation map. Specifically, we propose to formulate the pattern pruning on\ntransformer as a constrained optimization and use ADMM to optimize the problem.\nIn this way, the initial dense feature maps is transformed to rather regionally\nsparsified ones.Therefore, we can then achieve higher compression ratio with\nbetter performance based on pattern pruning method. Additionally, this paper\nprovides a theoretical derivations of the ADMM with local sparsity. Finally, we\nalso extend the proposed ADMM based framework on quantization to demonstrate\nits generalization and use SR-STE to avoid gradient vanishing problem. We\nconduct extensive experiments on classification tasks over GLUE datasets.\nSignificantly, we achieve 50% percent compression ratio while maintaining\noverall score 80.1 on GLUE datasets.\n","authors":["TianChen Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08334v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.08942v1","updated":"2024-07-12T02:58:07Z","published":"2024-07-12T02:58:07Z","title":"A Neural Matrix Decomposition Recommender System Model based on the\n  Multimodal Large Language Model","summary":"  Recommendation systems have become an important solution to information\nsearch problems. This article proposes a neural matrix factorization\nrecommendation system model based on the multimodal large language model called\nBoNMF. This model combines BoBERTa's powerful capabilities in natural language\nprocessing, ViT in computer in vision, and neural matrix decomposition\ntechnology. By capturing the potential characteristics of users and items, and\nafter interacting with a low-dimensional matrix composed of user and item IDs,\nthe neural network outputs the results. recommend. Cold start and ablation\nexperimental results show that the BoNMF model exhibits excellent performance\non large public data sets and significantly improves the accuracy of\nrecommendations.\n","authors":["Ao Xiang","Bingjie Huang","Xinyu Guo","Haowei Yang","Tianyao Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.08942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08926v1","updated":"2024-07-12T02:19:02Z","published":"2024-07-12T02:19:02Z","title":"Toward Automatic Group Membership Annotation for Group Fairness\n  Evaluation","summary":"  With the increasing research attention on fairness in information retrieval\nsystems, more and more fairness-aware algorithms have been proposed to ensure\nfairness for a sustainable and healthy retrieval ecosystem. However, as the\nmost adopted measurement of fairness-aware algorithms, group fairness\nevaluation metrics, require group membership information that needs massive\nhuman annotations and is barely available for general information retrieval\ndatasets. This data sparsity significantly impedes the development of\nfairness-aware information retrieval studies. Hence, a practical, scalable,\nlow-cost group membership annotation method is needed to assist or replace\nhuman annotations. This study explored how to leverage language models to\nautomatically annotate group membership for group fairness evaluations,\nfocusing on annotation accuracy and its impact. Our experimental results show\nthat BERT-based models outperformed state-of-the-art large language models,\nincluding GPT and Mistral, achieving promising annotation accuracy with minimal\nsupervision in recent fair-ranking datasets. Our impact-oriented evaluations\nreveal that minimal annotation error will not degrade the effectiveness and\nrobustness of group fairness evaluation. The proposed annotation method reduces\ntremendous human efforts and expands the frontier of fairness-aware studies to\nmore datasets.\n","authors":["Fumian Chen","Dayu Yang","Hui Fang"],"pdf_url":"https://arxiv.org/pdf/2407.08926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08916v1","updated":"2024-07-12T01:26:33Z","published":"2024-07-12T01:26:33Z","title":"Transforming Movie Recommendations with Advanced Machine Learning: A\n  Study of NMF, SVD,and K-Means Clustering","summary":"  This study develops a robust movie recommendation system using various\nmachine learning techniques, including Non- Negative Matrix Factorization\n(NMF), Truncated Singular Value Decomposition (SVD), and K-Means clustering.\nThe primary objective is to enhance user experience by providing personalized\nmovie recommendations. The research encompasses data preprocessing, model\ntraining, and evaluation, highlighting the efficacy of the employed methods.\nResults indicate that the proposed system achieves high accuracy and relevance\nin recommendations, making significant contributions to the field of\nrecommendations systems.\n","authors":["Yubing Yan","Camille Moreau","Zhuoyue Wang","Wenhan Fan","Chengqian Fu"],"pdf_url":"https://arxiv.org/pdf/2407.08916v1.pdf","comment":"Accepted by 2024 4th International Symposium on Computer Technology\n  and Information Science, IEEE"},{"id":"http://arxiv.org/abs/2407.08908v1","updated":"2024-07-12T00:59:32Z","published":"2024-07-12T00:59:32Z","title":"Are They the Same Picture? Adapting Concept Bottleneck Models for\n  Human-AI Collaboration in Image Retrieval","summary":"  Image retrieval plays a pivotal role in applications from wildlife\nconservation to healthcare, for finding individual animals or relevant images\nto aid diagnosis. Although deep learning techniques for image retrieval have\nadvanced significantly, their imperfect real-world performance often\nnecessitates including human expertise. Human-in-the-loop approaches typically\nrely on humans completing the task independently and then combining their\nopinions with an AI model in various ways, as these models offer very little\ninterpretability or \\textit{correctability}. To allow humans to intervene in\nthe AI model instead, thereby saving human time and effort, we adapt the\nConcept Bottleneck Model (CBM) and propose \\texttt{CHAIR}. \\texttt{CHAIR} (a)\nenables humans to correct intermediate concepts, which helps \\textit{improve}\nembeddings generated, and (b) allows for flexible levels of intervention that\naccommodate varying levels of human expertise for better retrieval. To show the\nefficacy of \\texttt{CHAIR}, we demonstrate that our method performs better than\nsimilar models on image retrieval metrics without any external intervention.\nFurthermore, we also showcase how human intervention helps further improve\nretrieval performance, thereby achieving human-AI complementarity.\n","authors":["Vaibhav Balloli","Sara Beery","Elizabeth Bondi-Kelly"],"pdf_url":"https://arxiv.org/pdf/2407.08908v1.pdf","comment":"Accepted at Human-Centred AI Track at IJCAI 2024"},{"id":"http://arxiv.org/abs/2407.09691v1","updated":"2024-07-12T21:20:57Z","published":"2024-07-12T21:20:57Z","title":"EVOLVE: Predicting User Evolution and Network Dynamics in Social Media\n  Using Fine-Tuned GPT-like Model","summary":"  Social media platforms are extensively used for sharing personal emotions,\ndaily activities, and various life events, keeping people updated with the\nlatest happenings. From the moment a user creates an account, they continually\nexpand their network of friends or followers, freely interacting with others by\nposting, commenting, and sharing content. Over time, user behavior evolves\nbased on demographic attributes and the networks they establish. In this\nresearch, we propose a predictive method to understand how a user evolves on\nsocial media throughout their life and to forecast the next stage of their\nevolution. We fine-tune a GPT-like decoder-only model (we named it E-GPT:\nEvolution-GPT) to predict the future stages of a user's evolution in online\nsocial media. We evaluate the performance of these models and demonstrate how\nuser attributes influence changes within their network by predicting future\nconnections and shifts in user activities on social media, which also addresses\nother social media challenges such as recommendation systems.\n","authors":["Ismail Hossain","Md Jahangir Alam","Sai Puppala","Sajedul Talukder"],"pdf_url":"https://arxiv.org/pdf/2407.09691v1.pdf","comment":"This article has been accepted as a long paper in the MSNDS 2024\n  workshop, to be held in conjunction with the International Conference on\n  Social Networks Analysis and Mining (ASONAM 2024), September 2-5, 2024. and\n  will be published in Springer"},{"id":"http://arxiv.org/abs/2407.09653v1","updated":"2024-07-12T19:22:17Z","published":"2024-07-12T19:22:17Z","title":"Bridging the Gap Between Information Seeking and Product Search Systems:\n  Q&A Recommendation for E-commerce","summary":"  Consumers on a shopping mission often leverage both product search and\ninformation seeking systems, such as web search engines and Question Answering\n(QA) systems, in an iterative process to improve their understanding of\navailable products and reach a purchase decision. While product search is\nuseful for shoppers to find the actual products meeting their requirements in\nthe catalog, information seeking systems can be utilized to answer any\nquestions they may have to refine those requirements. The recent success of\nLarge Language Models (LLMs) has opened up an opportunity to bridge the gap\nbetween the two tasks to help customers achieve their goals quickly and\neffectively by integrating conversational QA within product search. In this\npaper, we propose to recommend users Question-Answer (Q&A) pairs that are\nrelevant to their product search and can help them make a purchase decision. We\ndiscuss the different aspects of the problem including the requirements and\ncharacteristics of the Q&A pairs, their generation, and the optimization of the\nQ&A recommendation task. We highlight the challenges, open problems, and\nsuggested solutions to encourage future research in this emerging area.\n","authors":["Saar Kuzi","Shervin Malmasi"],"pdf_url":"https://arxiv.org/pdf/2407.09653v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.09475v1","updated":"2024-07-12T17:57:00Z","published":"2024-07-12T17:57:00Z","title":"Adaptive Prediction Ensemble: Improving Out-of-Distribution\n  Generalization of Motion Forecasting","summary":"  Deep learning-based trajectory prediction models for autonomous driving often\nstruggle with generalization to out-of-distribution (OOD) scenarios, sometimes\nperforming worse than simple rule-based models. To address this limitation, we\npropose a novel framework, Adaptive Prediction Ensemble (APE), which integrates\ndeep learning and rule-based prediction experts. A learned routing function,\ntrained concurrently with the deep learning model, dynamically selects the most\nreliable prediction based on the input scenario. Our experiments on large-scale\ndatasets, including Waymo Open Motion Dataset (WOMD) and Argoverse, demonstrate\nimprovement in zero-shot generalization across datasets. We show that our\nmethod outperforms individual prediction models and other variants,\nparticularly in long-horizon prediction and scenarios with a high proportion of\nOOD data. This work highlights the potential of hybrid approaches for robust\nand generalizable motion prediction in autonomous driving.\n","authors":["Jinning Li","Jiachen Li","Sangjae Bae","David Isele"],"pdf_url":"https://arxiv.org/pdf/2407.09475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09468v1","updated":"2024-07-12T17:48:36Z","published":"2024-07-12T17:48:36Z","title":"Beyond Euclid: An Illustrated Guide to Modern Machine Learning with\n  Geometric, Topological, and Algebraic Structures","summary":"  The enduring legacy of Euclidean geometry underpins classical machine\nlearning, which, for decades, has been primarily developed for data lying in\nEuclidean space. Yet, modern machine learning increasingly encounters richly\nstructured data that is inherently nonEuclidean. This data can exhibit\nintricate geometric, topological and algebraic structure: from the geometry of\nthe curvature of space-time, to topologically complex interactions between\nneurons in the brain, to the algebraic transformations describing symmetries of\nphysical systems. Extracting knowledge from such non-Euclidean data\nnecessitates a broader mathematical perspective. Echoing the 19th-century\nrevolutions that gave rise to non-Euclidean geometry, an emerging line of\nresearch is redefining modern machine learning with non-Euclidean structures.\nIts goal: generalizing classical methods to unconventional data types with\ngeometry, topology, and algebra. In this review, we provide an accessible\ngateway to this fast-growing field and propose a graphical taxonomy that\nintegrates recent advances into an intuitive unified framework. We subsequently\nextract insights into current challenges and highlight exciting opportunities\nfor future development in this field.\n","authors":["Sophia Sanborn","Johan Mathe","Mathilde Papillon","Domas Buracas","Hansen J Lillemark","Christian Shewmake","Abby Bertics","Xavier Pennec","Nina Miolane"],"pdf_url":"https://arxiv.org/pdf/2407.09468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12014v2","updated":"2024-07-12T17:39:19Z","published":"2024-03-18T17:51:16Z","title":"EnvGen: Generating and Adapting Environments via LLMs for Training\n  Embodied Agents","summary":"  Recent SOTA approaches for embodied learning via interaction directly employ\nlarge language models (LLMs) as agents to determine the next steps in an\nenvironment. Due to their world knowledge and reasoning capabilities, LLM\nagents achieve stronger performance than previous smaller agents based on\nreinforcement learning (RL); however, frequently calling LLMs is slow and\nexpensive. Instead of directly employing LLMs as agents, can we use LLMs'\nreasoning capabilities to adaptively create training environments to help\nsmaller RL agents learn useful skills that they are weak at? We propose EnvGen,\na novel framework to address this question. We first prompt an LLM to generate\ntraining environments by giving it the task description and simulator\nobjectives that the agents should learn and then asking it to generate a set of\nenvironment configurations (e.g., different terrains, items initially given to\nagents, etc.). Next, we train a small RL agent in a mixture of the original and\nLLM-generated environments. Then, we enable the LLM to continuously adapt the\ngenerated environments to progressively improve the skills that the agent is\nweak at, by providing feedback to the LLM in the form of the agent's\nperformance. We demonstrate the usefulness of EnvGen with comprehensive\nexperiments in Crafter and Heist environments. We find that a small RL agent\ntrained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and\nlearns long-horizon tasks significantly faster. We also show that using an LLM\nto adapt environments dynamically outperforms curriculum learning approaches\nand how the environments are adapted to help improve RL agents' weaker skills\nover time. Additionally, EnvGen is substantially more efficient as it only uses\na small number of LLM calls (e.g., 4 in total), whereas LLM agents require\nthousands of calls. Lastly, we present detailed ablation studies for EnvGen\ndesign choices.\n","authors":["Abhay Zala","Jaemin Cho","Han Lin","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2403.12014v2.pdf","comment":"COLM 2024; First two authors contributed equally; Project website:\n  https://envgen-llm.github.io/"},{"id":"http://arxiv.org/abs/2407.09453v1","updated":"2024-07-12T17:37:49Z","published":"2024-07-12T17:37:49Z","title":"Weight Block Sparsity: Training, Compilation, and AI Engine Accelerators","summary":"  Nowadays, increasingly larger Deep Neural Networks (DNNs) are being\ndeveloped, trained, and utilized. These networks require significant\ncomputational resources, putting a strain on both advanced and limited devices.\nOur solution is to implement {\\em weight block sparsity}, which is a structured\nsparsity that is friendly to hardware. By zeroing certain sections of the\nconvolution and fully connected layers parameters of pre-trained DNN models, we\ncan efficiently speed up the DNN's inference process. This results in a smaller\nmemory footprint, faster communication, and fewer operations.\n  Our work presents a vertical system that allows for the training of\nconvolution and matrix multiplication weights to exploit 8x8 block sparsity on\na single GPU within a reasonable amount of time. Compilers recognize this\nsparsity and use it for both data compaction and computation splitting into\nthreads. Blocks like these take full advantage of both spatial and temporal\nlocality, paving the way for fast vector operations and memory reuse. By using\nthis system on a Resnet50 model, we were able to reduce the weight by half with\nminimal accuracy loss, resulting in a two-times faster inference speed. We will\npresent performance estimates using accurate and complete code generation for\nAIE2 configuration sets (AMD Versal FPGAs) with Resnet50, Inception V3, and\nVGG16 to demonstrate the necessary synergy between hardware overlay designs and\nsoftware stacks for compiling and executing machine learning applications.\n","authors":["Paolo D'Alberto","Taehee Jeong","Akshai Jain","Shreyas Manjunath","Mrinal Sarmah","Samuel Hsu Yaswanth Raparti","Nitesh Pipralia"],"pdf_url":"https://arxiv.org/pdf/2407.09453v1.pdf","comment":"12 pages, 10 figures, 1 table"},{"id":"http://arxiv.org/abs/2311.09184v2","updated":"2024-07-12T17:35:18Z","published":"2023-11-15T18:25:26Z","title":"Benchmarking Generation and Evaluation Capabilities of Large Language\n  Models for Instruction Controllable Summarization","summary":"  While large language models (LLMs) can already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for desired summary\ncharacteristics. To this end, we curate an evaluation-only dataset for this\ntask setting and conduct human evaluations of five LLM-based systems to assess\ntheir instruction-following capabilities in controllable summarization. We then\nbenchmark LLM-based automatic evaluation for this task with 4 different\nevaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study\nreveals that instruction controllable text summarization remains a challenging\ntask for LLMs, since (1) all LLMs evaluated still make factual and other types\nof errors in their summaries; (2) no LLM-based evaluation methods can achieve a\nstrong alignment with human annotators when judging the quality of candidate\nsummaries; (3) different LLMs show large performance gaps in summary generation\nand evaluation capabilities. We make our collected benchmark InstruSum publicly\navailable to facilitate future research in this direction.\n","authors":["Yixin Liu","Alexander R. Fabbri","Jiawen Chen","Yilun Zhao","Simeng Han","Shafiq Joty","Pengfei Liu","Dragomir Radev","Chien-Sheng Wu","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2311.09184v2.pdf","comment":"NAACL 2024 Findings, GitHub Repo:\n  https://github.com/yale-nlp/InstruSum, LLM-evaluators Leaderboard:\n  https://huggingface.co/spaces/yale-nlp/InstruSumEval"},{"id":"http://arxiv.org/abs/2407.09450v1","updated":"2024-07-12T17:34:03Z","published":"2024-07-12T17:34:03Z","title":"Human-like Episodic Memory for Infinite Context LLMs","summary":"  Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs,\nenabling them to effectively handle practically infinite context lengths while\nmaintaining computational efficiency. EM-LLM organises sequences of tokens into\ncoherent episodic events using a combination of Bayesian surprise and\ngraph-theoretic boundary refinement in an on-line fashion. When needed, these\nevents are retrieved through a two-stage memory process, combining\nsimilarity-based and temporally contiguous retrieval for efficient and\nhuman-like access to relevant information. Experiments on the LongBench dataset\ndemonstrate EM-LLM's superior performance, outperforming the state-of-the-art\nInfLLM model with an overall relative improvement of 4.3% across various tasks,\nincluding a 33% improvement on the PassageRetrieval task. Furthermore, our\nanalysis reveals strong correlations between EM-LLM's event segmentation and\nhuman-perceived events, suggesting a bridge between this artificial system and\nits biological counterpart. This work not only advances LLM capabilities in\nprocessing extended contexts but also provides a computational framework for\nexploring human memory mechanisms, opening new avenues for interdisciplinary\nresearch in AI and cognitive science.\n","authors":["Zafeirios Fountas","Martin A Benfeghoul","Adnan Oomerjee","Fenia Christopoulou","Gerasimos Lampouras","Haitham Bou-Ammar","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09441v1","updated":"2024-07-12T17:27:43Z","published":"2024-07-12T17:27:43Z","title":"The $Î¼\\mathcal{G}$ Language for Programming Graph Neural Networks","summary":"  Graph neural networks form a class of deep learning architectures\nspecifically designed to work with graph-structured data. As such, they share\nthe inherent limitations and problems of deep learning, especially regarding\nthe issues of explainability and trustworthiness. We propose $\\mu\\mathcal{G}$,\nan original domain-specific language for the specification of graph neural\nnetworks that aims to overcome these issues. The language's syntax is\nintroduced, and its meaning is rigorously defined by a denotational semantics.\nAn equivalent characterization in the form of an operational semantics is also\nprovided and, together with a type system, is used to prove the type soundness\nof $\\mu\\mathcal{G}$. We show how $\\mu\\mathcal{G}$ programs can be represented\nin a more user-friendly graphical visualization, and provide examples of its\ngenerality by showing how it can be used to define some of the most popular\ngraph neural network models, or to develop any custom graph processing\napplication.\n","authors":["Matteo Belenchia","Flavio Corradini","Michela Quadrini","Michele Loreti"],"pdf_url":"https://arxiv.org/pdf/2407.09441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01437v2","updated":"2024-07-12T17:20:34Z","published":"2024-07-01T16:32:16Z","title":"Needle in the Haystack for Memory Based Large Language Models","summary":"  Current large language models (LLMs) often perform poorly on simple fact\nretrieval tasks. Here we investigate if coupling a dynamically adaptable\nexternal memory to a LLM can alleviate this problem. For this purpose, we test\nLarimar, a recently proposed language model architecture which uses an external\nassociative memory, on long-context recall tasks including passkey and\nneedle-in-the-haystack tests. We demonstrate that the external memory of\nLarimar, which allows fast write and read of an episode of text samples, can be\nused at test time to handle contexts much longer than those seen during\ntraining. We further show that the latent readouts from the memory (to which\nlong contexts are written) control the decoder towards generating correct\noutputs, with the memory stored off of the GPU. Compared to existing\ntransformer-based LLM architectures for long-context recall tasks that use\nlarger parameter counts or modified attention mechanisms, a relatively smaller\nsize Larimar is able to maintain strong performance without any task-specific\ntraining or training on longer contexts.\n","authors":["Elliot Nelson","Georgios Kollias","Payel Das","Subhajit Chaudhury","Soham Dan"],"pdf_url":"https://arxiv.org/pdf/2407.01437v2.pdf","comment":"5 pages; slightly revised abstract"},{"id":"http://arxiv.org/abs/2407.04268v2","updated":"2024-07-12T17:10:14Z","published":"2024-07-05T05:45:34Z","title":"NeuFair: Neural Network Fairness Repair with Dropout","summary":"  This paper investigates neuron dropout as a post-processing bias mitigation\nfor deep neural networks (DNNs). Neural-driven software solutions are\nincreasingly applied in socially critical domains with significant fairness\nimplications. While neural networks are exceptionally good at finding\nstatistical patterns from data, they may encode and amplify existing biases\nfrom the historical data. Existing bias mitigation algorithms often require\nmodifying the input dataset or the learning algorithms. We posit that the\nprevalent dropout methods that prevent over-fitting during training by randomly\ndropping neurons may be an effective and less intrusive approach to improve the\nfairness of pre-trained DNNs. However, finding the ideal set of neurons to drop\nis a combinatorial problem. We propose NeuFair, a family of post-processing\nrandomized algorithms that mitigate unfairness in pre-trained DNNs via dropouts\nduring inference after training. Our randomized search is guided by an\nobjective to minimize discrimination while maintaining the model's utility. We\nshow that our design of randomized algorithms is effective and efficient in\nimproving fairness (up to 69%) with minimal or no model performance\ndegradation. We provide intuitive explanations of these phenomena and carefully\nexamine the influence of various hyperparameters of search algorithms on the\nresults. Finally, we empirically and conceptually compare NeuFair to different\nstate-of-the-art bias mitigators.\n","authors":["Vishnu Asutosh Dasu","Ashish Kumar","Saeid Tizpaz-Niari","Gang Tan"],"pdf_url":"https://arxiv.org/pdf/2407.04268v2.pdf","comment":"Paper accepted at ACM ISSTA 2024"},{"id":"http://arxiv.org/abs/2407.09434v1","updated":"2024-07-12T17:09:47Z","published":"2024-07-12T17:09:47Z","title":"A Perspective on Foundation Models for the Electric Power Grid","summary":"  Foundation models (FMs) currently dominate news headlines. They employ\nadvanced deep learning architectures to extract structural information\nautonomously from vast datasets through self-supervision. The resulting rich\nrepresentations of complex systems and dynamics can be applied to many\ndownstream applications. Therefore, FMs can find uses in electric power grids,\nchallenged by the energy transition and climate change. In this paper, we call\nfor the development of, and state why we believe in, the potential of FMs for\nelectric grids. We highlight their strengths and weaknesses amidst the\nchallenges of a changing grid. We argue that an FM learning from diverse grid\ndata and topologies could unlock transformative capabilities, pioneering a new\napproach in leveraging AI to redefine how we manage complexity and uncertainty\nin the electric grid. Finally, we discuss a power grid FM concept, namely\nGridFM, based on graph neural networks and show how different downstream tasks\nbenefit.\n","authors":["Hendrik F. Hamann","Thomas Brunschwiler","Blazhe Gjorgiev","Leonardo S. A. Martins","Alban Puech","Anna Varbella","Jonas Weiss","Juan Bernabe-Moreno","Alexandre Blondin MassÃ©","Seong Choi","Ian Foster","Bri-Mathias Hodge","Rishabh Jain","Kibaek Kim","Vincent Mai","FranÃ§ois MirallÃ¨s","Martin De Montigny","Octavio Ramos-LeaÃ±os","Hussein SuprÃªme","Le Xie","El-Nasser S. Youssef","Arnaud Zinflou","Alexander J. Belvi","Ricardo J. Bessa","Bishnu Prasad Bhattari","Johannes Schmude","Stanislav Sobolevsky"],"pdf_url":"https://arxiv.org/pdf/2407.09434v1.pdf","comment":"Lead contact: H.F.H.; Major equal contributors: H.F.H., T.B., B.G.,\n  L.S.A.M., A.P., A.V., J.W.; Significant equal contributors: J.B., A.B.M.,\n  S.C., I.F., B.H., R.J., K.K., V.M., F.M., M.D.M., O.R., H.S., L.X., E.S.Y.,\n  A.Z.; Other equal contributors: A.J.B., R.J.B., B.P.B., J.S., S.S"},{"id":"http://arxiv.org/abs/2403.19629v2","updated":"2024-07-12T16:56:18Z","published":"2024-03-28T17:46:25Z","title":"Metric Learning from Limited Pairwise Preference Comparisons","summary":"  We study metric learning from preference comparisons under the ideal point\nmodel, in which a user prefers an item over another if it is closer to their\nlatent ideal item. These items are embedded into $\\mathbb{R}^d$ equipped with\nan unknown Mahalanobis distance shared across users. While recent work shows\nthat it is possible to simultaneously recover the metric and ideal items given\n$\\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a\nlimited budget of $o(d)$ comparisons. We study whether the metric can still be\nrecovered, even though it is known that learning individual ideal items is now\nno longer possible. We show that in general, $o(d)$ comparisons reveal no\ninformation about the metric, even with infinitely many users. However, when\ncomparisons are made over items that exhibit low-dimensional structure, each\nuser can contribute to learning the metric restricted to a low-dimensional\nsubspace so that the metric can be jointly identified. We present a\ndivide-and-conquer approach that achieves this, and provide theoretical\nrecovery guarantees and empirical validation.\n","authors":["Zhi Wang","Geelon So","Ramya Korlakai Vinayak"],"pdf_url":"https://arxiv.org/pdf/2403.19629v2.pdf","comment":"The 40th Conference on Uncertainty in Artificial Intelligence\n  (UAI-2024)"},{"id":"http://arxiv.org/abs/2407.09427v1","updated":"2024-07-12T16:54:17Z","published":"2024-07-12T16:54:17Z","title":"Flow-Based Generative Emulation of Grids of Stellar Evolutionary Models","summary":"  We present a flow-based generative approach to emulate grids of stellar\nevolutionary models. By interpreting the input parameters and output properties\nof these models as multi-dimensional probability distributions, we train\nconditional normalizing flows to learn and predict the complex relationships\nbetween grid inputs and outputs in the form of conditional joint distributions.\nLeveraging the expressive power and versatility of these flows, we showcase\ntheir ability to emulate a variety of evolutionary tracks and isochrones across\na continuous range of input parameters. In addition, we describe a simple\nBayesian approach for estimating stellar parameters using these flows and\ndemonstrate its application to asteroseismic datasets of red giants observed by\nthe Kepler mission. By applying this approach to red giants in open clusters\nNGC 6791 and NGC 6819, we illustrate how large age uncertainties can arise when\nfitting only to global asteroseismic and spectroscopic parameters without prior\ninformation on initial helium abundances and mixing length parameter values. We\nalso conduct inference using the flow at a large scale by determining revised\nestimates of masses and radii for 15,388 field red giants. These estimates show\nimproved agreement with results from existing grid-based modelling, reveal\ndistinct population-level features in the red clump, and suggest that the\nmasses of Kepler red giants previously determined using the corrected\nasteroseismic scaling relations have been overestimated by 5-10%.\n","authors":["Marc Hon","Yaguang Li","Joel Ong"],"pdf_url":"https://arxiv.org/pdf/2407.09427v1.pdf","comment":"27 pages, 18 figures. Accepted for publication in ApJ. Code,\n  animation, and interactive visualizations are available at\n  https://github.com/mtyhon/modelflows/. Table 4 is also available as ancillary\n  file attached to this submission"},{"id":"http://arxiv.org/abs/2406.04313v4","updated":"2024-07-12T16:51:07Z","published":"2024-06-06T17:57:04Z","title":"Improving Alignment and Robustness with Circuit Breakers","summary":"  AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that interrupts the models as they respond with harmful outputs\nwith \"circuit breakers.\" Existing techniques aimed at improving alignment, such\nas refusal training, are often bypassed. Techniques such as adversarial\ntraining try to plug these holes by countering specific attacks. As an\nalternative to refusal training and adversarial training, circuit-breaking\ndirectly controls the representations that are responsible for harmful outputs\nin the first place. Our technique can be applied to both text-only and\nmultimodal language models to prevent the generation of harmful outputs without\nsacrificing utility -- even in the presence of powerful unseen attacks.\nNotably, while adversarial robustness in standalone image recognition remains\nan open challenge, circuit breakers allow the larger multimodal system to\nreliably withstand image \"hijacks\" that aim to produce harmful content.\nFinally, we extend our approach to AI agents, demonstrating considerable\nreductions in the rate of harmful actions when they are under attack. Our\napproach represents a significant step forward in the development of reliable\nsafeguards to harmful behavior and adversarial attacks.\n","authors":["Andy Zou","Long Phan","Justin Wang","Derek Duenas","Maxwell Lin","Maksym Andriushchenko","Rowan Wang","Zico Kolter","Matt Fredrikson","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2406.04313v4.pdf","comment":"Code and models are available at\n  https://github.com/GraySwanAI/circuit-breakers"},{"id":"http://arxiv.org/abs/2306.15552v2","updated":"2024-07-12T16:50:59Z","published":"2023-06-27T15:24:24Z","title":"A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC\n  Platforms","summary":"  Recent trends in deep learning (DL) imposed hardware accelerators as the most\nviable solution for several classes of high-performance computing (HPC)\napplications such as image classification, computer vision, and speech\nrecognition. This survey summarizes and classifies the most recent advances in\ndesigning DL accelerators suitable to reach the performance requirements of HPC\napplications. In particular, it highlights the most advanced approaches to\nsupport deep learning accelerations including not only GPU and TPU-based\naccelerators but also design-specific hardware accelerators such as FPGA-based\nand ASIC-based accelerators, Neural Processing Units, open hardware\nRISC-V-based accelerators and co-processors. The survey also describes\naccelerators based on emerging memory technologies and computing paradigms,\nsuch as 3D-stacked Processor-In-Memory, non-volatile memories (mainly,\nResistive RAM and Phase Change Memories) to implement in-memory computing,\nNeuromorphic Processing Units, and accelerators based on Multi-Chip Modules.\nAmong emerging technologies, we also include some insights into quantum-based\naccelerators and photonics. To conclude, the survey classifies the most\ninfluential architectures and technologies proposed in the last years, with the\npurpose of offering the reader a comprehensive perspective in the rapidly\nevolving field of deep learning.\n","authors":["Cristina Silvano","Daniele Ielmini","Fabrizio Ferrandi","Leandro Fiorin","Serena Curzel","Luca Benini","Francesco Conti","Angelo Garofalo","Cristian Zambelli","Enrico Calore","Sebastiano Fabio Schifano","Maurizio Palesi","Giuseppe Ascia","Davide Patti","Nicola Petra","Davide De Caro","Luciano Lavagno","Teodoro Urso","Valeria Cardellini","Gian Carlo Cardarilli","Robert Birke","Stefania Perri"],"pdf_url":"https://arxiv.org/pdf/2306.15552v2.pdf","comment":"Preprint version of our manuscript submitted to the journal @ ACM\n  CSUR (58 pages including Appendix) on June 22nd, 2023. Major revision\n  submitted on July 12th, 2024"},{"id":"http://arxiv.org/abs/2407.09415v1","updated":"2024-07-12T16:44:03Z","published":"2024-07-12T16:44:03Z","title":"A Benchmark Environment for Offline Reinforcement Learning in Racing\n  Games","summary":"  Offline Reinforcement Learning (ORL) is a promising approach to reduce the\nhigh sample complexity of traditional Reinforcement Learning (RL) by\neliminating the need for continuous environmental interactions. ORL exploits a\ndataset of pre-collected transitions and thus expands the range of application\nof RL to tasks in which the excessive environment queries increase training\ntime and decrease efficiency, such as in modern AAA games. This paper\nintroduces OfflineMania a novel environment for ORL research. It is inspired by\nthe iconic TrackMania series and developed using the Unity 3D game engine. The\nenvironment simulates a single-agent racing game in which the objective is to\ncomplete the track through optimal navigation. We provide a variety of datasets\nto assess ORL performance. These datasets, created from policies of varying\nability and in different sizes, aim to offer a challenging testbed for\nalgorithm development and evaluation. We further establish a set of baselines\nfor a range of Online RL, ORL, and hybrid Offline to Online RL approaches using\nour environment.\n","authors":["Girolamo Macaluso","Alessandro Sestini","Andrew D. Bagdanov"],"pdf_url":"https://arxiv.org/pdf/2407.09415v1.pdf","comment":"Accepted at IEEE Conference on Games"},{"id":"http://arxiv.org/abs/2407.04622v2","updated":"2024-07-12T16:38:12Z","published":"2024-07-05T16:29:15Z","title":"On scalable oversight with weak LLMs judging strong LLMs","summary":"  Scalable oversight protocols aim to enable humans to accurately supervise\nsuperhuman AI. In this paper we study debate, where two AI's compete to\nconvince a judge; consultancy, where a single AI tries to convince a judge that\nasks questions; and compare to a baseline of direct question-answering, where\nthe judge just answers outright without the AI. We use large language models\n(LLMs) as both AI agents and as stand-ins for human judges, taking the judge\nmodels to be weaker than agent models. We benchmark on a diverse range of\nasymmetries between judges and agents, extending previous work on a single\nextractive QA task with information asymmetry, to also include mathematics,\ncoding, logic and multimodal reasoning asymmetries. We find that debate\noutperforms consultancy across all tasks when the consultant is randomly\nassigned to argue for the correct/incorrect answer. Comparing debate to direct\nquestion answering, the results depend on the type of task: in extractive QA\ntasks with information asymmetry debate outperforms direct question answering,\nbut in other tasks without information asymmetry the results are mixed.\nPrevious work assigned debaters/consultants an answer to argue for. When we\nallow them to instead choose which answer to argue for, we find judges are less\nfrequently convinced by the wrong answer in debate than in consultancy.\nFurther, we find that stronger debater models increase judge accuracy, though\nmore modestly than in previous studies.\n","authors":["Zachary Kenton","Noah Y. Siegel","JÃ¡nos KramÃ¡r","Jonah Brown-Cohen","Samuel Albanie","Jannis Bulian","Rishabh Agarwal","David Lindner","Yunhao Tang","Noah D. Goodman","Rohin Shah"],"pdf_url":"https://arxiv.org/pdf/2407.04622v2.pdf","comment":"15 pages (53 including appendices). V2: minor correction to Figure 3;\n  add Figure A.9 comparing open vs assigned consultancy; add a reference"},{"id":"http://arxiv.org/abs/2403.09302v2","updated":"2024-07-12T16:27:06Z","published":"2024-03-14T11:49:43Z","title":"StainFuser: Controlling Diffusion for Faster Neural Style Transfer in\n  Multi-Gigapixel Histology Images","summary":"  Stain normalization algorithms aim to transform the color and intensity\ncharacteristics of a source multi-gigapixel histology image to match those of a\ntarget image, mitigating inconsistencies in the appearance of stains used to\nhighlight cellular components in the images. We propose a new approach,\nStainFuser, which treats this problem as a style transfer task using a novel\nConditional Latent Diffusion architecture, eliminating the need for handcrafted\ncolor components. With this method, we curate SPI-2M the largest stain\nnormalization dataset to date of over 2 million histology images with neural\nstyle transfer for high-quality transformations. Trained on this data,\nStainFuser outperforms current state-of-the-art deep learning and handcrafted\nmethods in terms of the quality of normalized images and in terms of downstream\nmodel performance on the CoNIC dataset.\n","authors":["Robert Jewsbury","Ruoyu Wang","Abhir Bhalerao","Nasir Rajpoot","Quoc Dang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.09302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09387v1","updated":"2024-07-12T16:07:53Z","published":"2024-07-12T16:07:53Z","title":"Meta-Analysis with Untrusted Data","summary":"  [See paper for full abstract] Meta-analysis is a crucial tool for answering\nscientific questions. It is usually conducted on a relatively small amount of\n``trusted'' data -- ideally from randomized, controlled trials -- which allow\ncausal effects to be reliably estimated with minimal assumptions. We show how\nto answer causal questions much more precisely by making two changes. First, we\nincorporate untrusted data drawn from large observational databases, related\nscientific literature and practical experience -- without sacrificing rigor or\nintroducing strong assumptions. Second, we train richer models capable of\nhandling heterogeneous trials, addressing a long-standing challenge in\nmeta-analysis. Our approach is based on conformal prediction, which\nfundamentally produces rigorous prediction intervals, but doesn't handle\nindirect observations: in meta-analysis, we observe only noisy effects due to\nthe limited number of participants in each trial. To handle noise, we develop a\nsimple, efficient version of fully-conformal kernel ridge regression, based on\na novel condition called idiocentricity. We introduce noise-correcting terms in\nthe residuals and analyze their interaction with a ``variance shaving''\ntechnique. In multiple experiments on healthcare datasets, our algorithms\ndeliver tighter, sounder intervals than traditional ones. This paper charts a\nnew course for meta-analysis and evidence-based medicine, where heterogeneity\nand untrusted data are embraced for more nuanced and precise predictions.\n","authors":["Shiva Kaul","Geoffrey J. Gordon"],"pdf_url":"https://arxiv.org/pdf/2407.09387v1.pdf","comment":"Full-length version of conference submission"},{"id":"http://arxiv.org/abs/2309.06212v6","updated":"2024-07-12T16:05:50Z","published":"2023-09-12T13:28:06Z","title":"Long-term drought prediction using deep neural networks based on\n  geospatial weather data","summary":"  The problem of high-quality drought forecasting up to a year in advance is\ncritical for agriculture planning and insurance. Yet, it is still unsolved with\nreasonable accuracy due to data complexity and aridity stochasticity. We tackle\ndrought data by introducing an end-to-end approach that adopts a\nspatio-temporal neural network model with accessible open monthly climate data\nas the input.\n  Our systematic research employs diverse proposed models and five distinct\nenvironmental regions as a testbed to evaluate the efficacy of the Palmer\nDrought Severity Index (PDSI) prediction. Key aggregated findings are the\nexceptional performance of a Transformer model, EarthFormer, in making accurate\nshort-term (up to six months) forecasts. At the same time, the Convolutional\nLSTM excels in longer-term forecasting.\n","authors":["Alexander Marusov","Vsevolod Grabar","Yury Maximov","Nazar Sotiriadi","Alexander Bulkin","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2309.06212v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09381v1","updated":"2024-07-12T16:03:58Z","published":"2024-07-12T16:03:58Z","title":"The Effectiveness of Curvature-Based Rewiring and the Role of\n  Hyperparameters in GNNs Revisited","summary":"  Message passing is the dominant paradigm in Graph Neural Networks (GNNs). The\nefficiency of message passing, however, can be limited by the topology of the\ngraph. This happens when information is lost during propagation due to being\noversquashed when travelling through bottlenecks. To remedy this, recent\nefforts have focused on graph rewiring techniques, which disconnect the input\ngraph originating from the data and the computational graph, on which message\npassing is performed. A prominent approach for this is to use discrete graph\ncurvature measures, of which several variants have been proposed, to identify\nand rewire around bottlenecks, facilitating information propagation. While\noversquashing has been demonstrated in synthetic datasets, in this work we\nreevaluate the performance gains that curvature-based rewiring brings to\nreal-world datasets. We show that in these datasets, edges selected during the\nrewiring process are not in line with theoretical criteria identifying\nbottlenecks. This implies they do not necessarily oversquash information during\nmessage passing. Subsequently, we demonstrate that SOTA accuracies on these\ndatasets are outliers originating from sweeps of hyperparameters -- both the\nones for training and dedicated ones related to the rewiring algorithm --\ninstead of consistent performance gains. In conclusion, our analysis nuances\nthe effectiveness of curvature-based rewiring in real-world datasets and brings\na new perspective on the methods to evaluate GNN accuracy improvements.\n","authors":["Floriano Tori","Vincent Holst","Vincent Ginis"],"pdf_url":"https://arxiv.org/pdf/2407.09381v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.09378v1","updated":"2024-07-12T15:56:33Z","published":"2024-07-12T15:56:33Z","title":"Graph Neural Network Causal Explanation via Neural Causal Models","summary":"  Graph neural network (GNN) explainers identify the important subgraph that\nensures the prediction for a given graph. Until now, almost all GNN explainers\nare based on association, which is prone to spurious correlations. We propose\n{\\name}, a GNN causal explainer via causal inference. Our explainer is based on\nthe observation that a graph often consists of a causal underlying subgraph.\n{\\name} includes three main steps: 1) It builds causal structure and the\ncorresponding structural causal model (SCM) for a graph, which enables the\ncause-effect calculation among nodes. 2) Directly calculating the cause-effect\nin real-world graphs is computationally challenging. It is then enlightened by\nthe recent neural causal model (NCM), a special type of SCM that is trainable,\nand design customized NCMs for GNNs. By training these GNN NCMs, the\ncause-effect can be easily calculated. 3) It uncovers the subgraph that\ncausally explains the GNN predictions via the optimized GNN-NCMs. Evaluation\nresults on multiple synthetic and real-world graphs validate that {\\name}\nsignificantly outperforms existing GNN explainers in exact groundtruth\nexplanation identification\n","authors":["Arman Behnam","Binghui Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09375v1","updated":"2024-07-12T15:56:11Z","published":"2024-07-12T15:56:11Z","title":"HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems\n  in Context","summary":"  This work explores the in-context learning capabilities of State Space Models\n(SSMs) and presents, to the best of our knowledge, the first theoretical\nexplanation of a possible underlying mechanism. We introduce a novel weight\nconstruction for SSMs, enabling them to predict the next state of any dynamical\nsystem after observing previous states without parameter fine-tuning. This is\naccomplished by extending the HiPPO framework to demonstrate that continuous\nSSMs can approximate the derivative of any input signal. Specifically, we find\nan explicit weight construction for continuous SSMs and provide an asymptotic\nerror bound on the derivative approximation. The discretization of this\ncontinuous SSM subsequently yields a discrete SSM that predicts the next state.\nFinally, we demonstrate the effectiveness of our parameterization empirically.\nThis work should be an initial step toward understanding how sequence models\nbased on SSMs learn in context.\n","authors":["Federico Arangath Joseph","Kilian Haefeli","Noah Liniger","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2407.09375v1.pdf","comment":"ICML 2024, Next Generation Sequence Modeling Architectures Workshop"},{"id":"http://arxiv.org/abs/2407.09373v1","updated":"2024-07-12T15:53:26Z","published":"2024-07-12T15:53:26Z","title":"Towards Personalised Patient Risk Prediction Using Temporal Hospital\n  Data Trajectories","summary":"  Quantifying a patient's health status provides clinicians with insight into\npatient risk, and the ability to better triage and manage resources. Early\nWarning Scores (EWS) are widely deployed to measure overall health status, and\nrisk of adverse outcomes, in hospital patients. However, current EWS are\nlimited both by their lack of personalisation and use of static observations.\nWe propose a pipeline that groups intensive care unit patients by the\ntrajectories of observations data throughout their stay as a basis for the\ndevelopment of personalised risk predictions. Feature importance is considered\nto provide model explainability. Using the MIMIC-IV dataset, six clusters were\nidentified, capturing differences in disease codes, observations, lengths of\nadmissions and outcomes. Applying the pipeline to data from just the first four\nhours of each ICU stay assigns the majority of patients to the same cluster as\nwhen the entire stay duration is considered. In-hospital mortality prediction\nmodels trained on individual clusters had higher F1 score performance in five\nof the six clusters when compared against the unclustered patient cohort. The\npipeline could form the basis of a clinical decision support tool, working to\nimprove the clinical characterisation of risk groups and the early detection of\npatient deterioration.\n","authors":["Thea Barnes","Enrico Werner","Jeffrey N. Clark","Raul Santos-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2407.09373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09370v1","updated":"2024-07-12T15:51:53Z","published":"2024-07-12T15:51:53Z","title":"Learning High-Frequency Functions Made Easy with Sinusoidal Positional\n  Encoding","summary":"  Fourier features based positional encoding (PE) is commonly used in machine\nlearning tasks that involve learning high-frequency features from\nlow-dimensional inputs, such as 3D view synthesis and time series regression\nwith neural tangent kernels. Despite their effectiveness, existing PEs require\nmanual, empirical adjustment of crucial hyperparameters, specifically the\nFourier features, tailored to each unique task. Further, PEs face challenges in\nefficiently learning high-frequency functions, particularly in tasks with\nlimited data. In this paper, we introduce sinusoidal PE (SPE), designed to\nefficiently learn adaptive frequency features closely aligned with the true\nunderlying function. Our experiments demonstrate that SPE, without\nhyperparameter tuning, consistently achieves enhanced fidelity and faster\ntraining across various tasks, including 3D view synthesis, Text-to-Speech\ngeneration, and 1D regression. SPE is implemented as a direct replacement for\nexisting PEs. Its plug-and-play nature lets numerous tasks easily adopt and\nbenefit from SPE.\n","authors":["Chuanhao Sun","Zhihang Yuan","Kai Xu","Luo Mai","Siddharth N","Shuo Chen","Mahesh K. Marina"],"pdf_url":"https://arxiv.org/pdf/2407.09370v1.pdf","comment":"16 pages, Conference, Accepted by ICML 2024"},{"id":"http://arxiv.org/abs/2303.07814v2","updated":"2024-07-12T15:48:09Z","published":"2023-03-14T11:44:58Z","title":"MS-TCRNet: Multi-Stage Temporal Convolutional Recurrent Networks for\n  Action Segmentation Using Sensor-Augmented Kinematics","summary":"  Action segmentation is a challenging task in high-level process analysis,\ntypically performed on video or kinematic data obtained from various sensors.\nThis work presents two contributions related to action segmentation on\nkinematic data. Firstly, we introduce two versions of Multi-Stage Temporal\nConvolutional Recurrent Networks (MS-TCRNet), specifically designed for\nkinematic data. The architectures consist of a prediction generator with\nintra-stage regularization and Bidirectional LSTM or GRU-based refinement\nstages. Secondly, we propose two new data augmentation techniques, World Frame\nRotation and Hand Inversion, which utilize the strong geometric structure of\nkinematic data to improve algorithm performance and robustness. We evaluate our\nmodels on three datasets of surgical suturing tasks: the Variable Tissue\nSimulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS)\nDataset, both of which are open surgery simulation datasets collected by us, as\nwell as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a\nwell-known benchmark in robotic surgery. Our methods achieved state-of-the-art\nperformance.\n","authors":["Adam Goldbraikh","Omer Shubi","Or Rubin","Carla M Pugh","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2303.07814v2.pdf","comment":"41 pages, 7 figures. Submitted to Pattern Recognition"},{"id":"http://arxiv.org/abs/2402.09497v2","updated":"2024-07-12T15:45:57Z","published":"2024-02-14T15:47:46Z","title":"Instruction Tuning for Secure Code Generation","summary":"  Modern language models (LMs) have gained widespread acceptance in everyday\nand professional contexts, particularly in programming. An essential procedure\nenabling this adoption is instruction tuning, which substantially enhances LMs'\npractical utility by training them to follow user instructions and human\npreferences. However, existing instruction tuning schemes overlook a crucial\naspect: the security of generated code. As a result, even the state-of-the-art\ninstruction-tuned LMs frequently produce unsafe code, posing significant\nsecurity risks. In this work, we introduce SafeCoder to address this gap.\nSafeCoder performs security-centric fine-tuning using a diverse and\nhigh-quality dataset that we collected using an automated pipeline. We\nintegrate the security fine-tuning with standard instruction tuning, to\nfacilitate a joint optimization of both security and utility. Despite its\nsimplicity, we show that SafeCoder is effective across a variety of popular LMs\nand datasets. It is able to drastically improve security (by about 30%), while\npreserving utility.\n","authors":["Jingxuan He","Mark Vero","Gabriela Krasnopolska","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2402.09497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14012v2","updated":"2024-07-12T15:44:38Z","published":"2024-02-21T18:51:42Z","title":"Chasing Convex Functions with Long-term Constraints","summary":"  We introduce and study a family of online metric problems with long-term\nconstraints. In these problems, an online player makes decisions $\\mathbf{x}_t$\nin a metric space $(X,d)$ to simultaneously minimize their hitting cost\n$f_t(\\mathbf{x}_t)$ and switching cost as determined by the metric. Over the\ntime horizon $T$, the player must satisfy a long-term demand constraint\n$\\sum_{t} c(\\mathbf{x}_t) \\geq 1$, where $c(\\mathbf{x}_t)$ denotes the fraction\nof demand satisfied at time $t$. Such problems can find a wide array of\napplications to online resource allocation in sustainable energy/computing\nsystems. We devise optimal competitive and learning-augmented algorithms for\nthe case of bounded hitting cost gradients and weighted $\\ell_1$ metrics, and\nfurther show that our proposed algorithms perform well in numerical\nexperiments.\n","authors":["Adam Lechowicz","Nicolas Christianson","Bo Sun","Noman Bashir","Mohammad Hajiesmaili","Adam Wierman","Prashant Shenoy"],"pdf_url":"https://arxiv.org/pdf/2402.14012v2.pdf","comment":"Accepted to ICML 2024. 31 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.09360v1","updated":"2024-07-12T15:37:05Z","published":"2024-07-12T15:37:05Z","title":"Novel clustered federated learning based on local loss","summary":"  This paper proposes LCFL, a novel clustering metric for evaluating clients'\ndata distributions in federated learning. LCFL aligns with federated learning\nrequirements, accurately assessing client-to-client variations in data\ndistribution. It offers advantages over existing clustered federated learning\nmethods, addressing privacy concerns, improving applicability to non-convex\nmodels, and providing more accurate classification results. LCFL does not\nrequire prior knowledge of clients' data distributions. We provide a rigorous\nmathematical analysis, demonstrating the correctness and feasibility of our\nframework. Numerical experiments with neural network instances highlight the\nsuperior performance of LCFL over baselines on several clustered federated\nlearning benchmarks.\n","authors":["Endong Gu","Yongxin Chen","Hao Wen","Xingju Cai","Deren Han"],"pdf_url":"https://arxiv.org/pdf/2407.09360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.05667v3","updated":"2024-07-12T15:34:29Z","published":"2022-11-10T16:04:28Z","title":"What Makes a Good Explanation?: A Harmonized View of Properties of\n  Explanations","summary":"  Interpretability provides a means for humans to verify aspects of machine\nlearning (ML) models and empower human+ML teaming in situations where the task\ncannot be fully automated. Different contexts require explanations with\ndifferent properties. For example, the kind of explanation required to\ndetermine if an early cardiac arrest warning system is ready to be integrated\ninto a care setting is very different from the type of explanation required for\na loan applicant to help determine the actions they might need to take to make\ntheir application successful.\n  Unfortunately, there is a lack of standardization when it comes to properties\nof explanations: different papers may use the same term to mean different\nquantities, and different terms to mean the same quantity. This lack of a\nstandardized terminology and categorization of the properties of ML\nexplanations prevents us from both rigorously comparing interpretable machine\nlearning methods and identifying what properties are needed in what contexts.\n  In this work, we survey properties defined in interpretable machine learning\npapers, synthesize them based on what they actually measure, and describe the\ntrade-offs between different formulations of these properties. In doing so, we\nenable more informed selection of task-appropriate formulations of explanation\nproperties as well as standardization for future work in interpretable machine\nlearning.\n","authors":["Zixi Chen","Varshini Subhash","Marton Havasi","Weiwei Pan","Finale Doshi-Velez"],"pdf_url":"https://arxiv.org/pdf/2211.05667v3.pdf","comment":"Short version accepted at NeurIPS 2022 workshops on Progress and\n  Challenges in Building Trustworthy Embodied AI and Trustworthy and Socially\n  Responsible Machine Learning"},{"id":"http://arxiv.org/abs/2407.09357v1","updated":"2024-07-12T15:32:44Z","published":"2024-07-12T15:32:44Z","title":"Any-Property-Conditional Molecule Generation with Self-Criticism using\n  Spanning Trees","summary":"  Generating novel molecules is challenging, with most representations leading\nto generative models producing many invalid molecules. Spanning Tree-based\nGraph Generation (STGG) is a promising approach to ensure the generation of\nvalid molecules, outperforming state-of-the-art SMILES and graph diffusion\nmodels for unconditional generation. In the real world, we want to be able to\ngenerate molecules conditional on one or multiple desired properties rather\nthan unconditionally. Thus, in this work, we extend STGG to\nmulti-property-conditional generation. Our approach, STGG+, incorporates a\nmodern Transformer architecture, random masking of properties during training\n(enabling conditioning on any subset of properties and classifier-free\nguidance), an auxiliary property-prediction loss (allowing the model to\nself-criticize molecules and select the best ones), and other improvements. We\nshow that STGG+ achieves state-of-the-art performance on in-distribution and\nout-of-distribution conditional generation, and reward maximization.\n","authors":["Alexia Jolicoeur-Martineau","Aristide Baratin","Kisoo Kwon","Boris Knyazev","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.09357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06112v2","updated":"2024-07-12T15:23:28Z","published":"2022-10-12T12:16:46Z","title":"Efficient Bayesian Updates for Deep Learning via Laplace Approximations","summary":"  Since training deep neural networks takes significant computational\nresources, extending the training dataset with new data is difficult, as it\ntypically requires complete retraining. Moreover, specific applications do not\nallow costly retraining due to time or computational constraints. We address\nthis issue by proposing a novel Bayesian update method for deep neural networks\nby using a last-layer Laplace approximation. Concretely, we leverage\nsecond-order optimization techniques on the Gaussian posterior distribution of\na Laplace approximation, computing the inverse Hessian matrix in closed form.\nThis way, our method allows for fast and effective updates upon the arrival of\nnew data in a stationary setting. A large-scale evaluation study across\ndifferent data modalities confirms that our updates are a fast and competitive\nalternative to costly retraining. Furthermore, we demonstrate its applicability\nin a deep active learning scenario by using our update to improve existing\nselection strategies.\n","authors":["Denis Huseljic","Marek Herde","Lukas Rauch","Paul Hahn","Zhixin Huang","Daniel Kottke","Stephan Vogt","Bernhard Sick"],"pdf_url":"https://arxiv.org/pdf/2210.06112v2.pdf","comment":"submitted to NeurIPS"},{"id":"http://arxiv.org/abs/2407.07933v2","updated":"2024-07-12T15:15:58Z","published":"2024-07-10T12:58:30Z","title":"Identification and Estimation of the Bi-Directional MR with Some Invalid\n  Instruments","summary":"  We consider the challenging problem of estimating causal effects from purely\nobservational data in the bi-directional Mendelian randomization (MR), where\nsome invalid instruments, as well as unmeasured confounding, usually exist. To\naddress this problem, most existing methods attempt to find proper valid\ninstrumental variables (IVs) for the target causal effect by expert knowledge\nor by assuming that the causal model is a one-directional MR model. As such, in\nthis paper, we first theoretically investigate the identification of the\nbi-directional MR from observational data. In particular, we provide necessary\nand sufficient conditions under which valid IV sets are correctly identified\nsuch that the bi-directional MR model is identifiable, including the causal\ndirections of a pair of phenotypes (i.e., the treatment and outcome). Moreover,\nbased on the identification theory, we develop a cluster fusion-like method to\ndiscover valid IV sets and estimate the causal effects of interest. We\ntheoretically demonstrate the correctness of the proposed algorithm.\nExperimental results show the effectiveness of our method for estimating causal\neffects in bi-directional MR.\n","authors":["Feng Xie","Zhen Yao","Lin Xie","Yan Zeng","Zhi Geng"],"pdf_url":"https://arxiv.org/pdf/2407.07933v2.pdf","comment":"27 pages, 6 tables, 7 figures"},{"id":"http://arxiv.org/abs/2407.06124v2","updated":"2024-07-12T15:15:03Z","published":"2024-07-08T17:00:28Z","title":"Structured Generations: Using Hierarchical Clusters to guide Diffusion\n  Models","summary":"  This paper introduces Diffuse-TreeVAE, a deep generative model that\nintegrates hierarchical clustering into the framework of Denoising Diffusion\nProbabilistic Models (DDPMs). The proposed approach generates new images by\nsampling from a root embedding of a learned latent tree VAE-based structure, it\nthen propagates through hierarchical paths, and utilizes a second-stage DDPM to\nrefine and generate distinct, high-quality images for each data cluster. The\nresult is a model that not only improves image clarity but also ensures that\nthe generated samples are representative of their respective clusters,\naddressing the limitations of previous VAE-based methods and advancing the\nstate of clustering-based generative modeling.\n","authors":["Jorge da Silva Goncalves","Laura Manduchi","Moritz Vandenhirtz","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2407.06124v2.pdf","comment":"8 pages, 7 figures, Structured Probabilistic Inference & Generative\n  Modeling workshop of ICML 2024"},{"id":"http://arxiv.org/abs/2407.09336v1","updated":"2024-07-12T15:13:16Z","published":"2024-07-12T15:13:16Z","title":"Guidelines for Augmentation Selection in Contrastive Learning for Time\n  Series Classification","summary":"  Self-supervised contrastive learning has become a key technique in deep\nlearning, particularly in time series analysis, due to its ability to learn\nmeaningful representations without explicit supervision. Augmentation is a\ncritical component in contrastive learning, where different augmentations can\ndramatically impact performance, sometimes influencing accuracy by over 30%.\nHowever, the selection of augmentations is predominantly empirical which can be\nsuboptimal, or grid searching that is time-consuming. In this paper, we\nestablish a principled framework for selecting augmentations based on dataset\ncharacteristics such as trend and seasonality. Specifically, we construct 12\nsynthetic datasets incorporating trend, seasonality, and integration weights.\nWe then evaluate the effectiveness of 8 different augmentations across these\nsynthetic datasets, thereby inducing generalizable associations between time\nseries characteristics and augmentation efficiency. Additionally, we evaluated\nthe induced associations across 6 real-world datasets encompassing domains such\nas activity recognition, disease diagnosis, traffic monitoring, electricity\nusage, mechanical fault prognosis, and finance. These real-world datasets are\ndiverse, covering a range from 1 to 12 channels, 2 to 10 classes, sequence\nlengths of 14 to 1280, and data frequencies from 250 Hz to daily intervals. The\nexperimental results show that our proposed trend-seasonality-based\naugmentation recommendation algorithm can accurately identify the effective\naugmentations for a given time series dataset, achieving an average Recall@3 of\n0.667, outperforming baselines. Our work provides guidance for studies\nemploying contrastive learning in time series analysis, with wide-ranging\napplications. All the code, datasets, and analysis results will be released at\nhttps://github.com/DL4mHealth/TS-Contrastive-Augmentation-Recommendation.\n","authors":["Ziyu Liu","Azadeh Alavi","Minyi Li","Xiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.09336v1.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.07674v2","updated":"2024-07-12T15:10:53Z","published":"2024-07-10T14:00:20Z","title":"Feasibility Study on Active Learning of Smart Surrogates for Scientific\n  Simulations","summary":"  High-performance scientific simulations, important for comprehension of\ncomplex systems, encounter computational challenges especially when exploring\nextensive parameter spaces. There has been an increasing interest in developing\ndeep neural networks (DNNs) as surrogate models capable of accelerating the\nsimulations. However, existing approaches for training these DNN surrogates\nrely on extensive simulation data which are heuristically selected and\ngenerated with expensive computation -- a challenge under-explored in the\nliterature. In this paper, we investigate the potential of incorporating active\nlearning into DNN surrogate training. This allows intelligent and objective\nselection of training simulations, reducing the need to generate extensive\nsimulation data as well as the dependency of the performance of DNN surrogates\non pre-defined training simulations. In the problem context of constructing DNN\nsurrogates for diffusion equations with sources, we examine the efficacy of\ndiversity- and uncertainty-based strategies for selecting training simulations,\nconsidering two different DNN architecture. The results set the groundwork for\ndeveloping the high-performance computing infrastructure for Smart Surrogates\nthat supports on-the-fly generation of simulation data steered by active\nlearning strategies to potentially improve the efficiency of scientific\nsimulations.\n","authors":["Pradeep Bajracharya","Javier QuetzalcÃ³atl Toledo-MarÃ­n","Geoffrey Fox","Shantenu Jha","Linwei Wang"],"pdf_url":"https://arxiv.org/pdf/2407.07674v2.pdf","comment":"17 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.11938v2","updated":"2024-07-12T15:08:15Z","published":"2024-03-18T16:35:13Z","title":"State space representations of the Roesser type for convolutional layers","summary":"  From the perspective of control theory, convolutional layers (of neural\nnetworks) are 2-D (or N-D) linear time-invariant dynamical systems. The usual\nrepresentation of convolutional layers by the convolution kernel corresponds to\nthe representation of a dynamical system by its impulse response. However, many\nanalysis tools from control theory, e.g., involving linear matrix inequalities,\nrequire a state space representation. For this reason, we explicitly provide a\nstate space representation of the Roesser type for 2-D convolutional layers\nwith $c_\\mathrm{in}r_1 + c_\\mathrm{out}r_2$ states, where\n$c_\\mathrm{in}$/$c_\\mathrm{out}$ is the number of input/output channels of the\nlayer and $r_1$/$r_2$ characterizes the width/length of the convolution kernel.\nThis representation is shown to be minimal for $c_\\mathrm{in} =\nc_\\mathrm{out}$. We further construct state space representations for dilated,\nstrided, and N-D convolutions.\n","authors":["Patricia Pauli","Dennis Gramlich","Frank AllgÃ¶wer"],"pdf_url":"https://arxiv.org/pdf/2403.11938v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09324v1","updated":"2024-07-12T15:01:09Z","published":"2024-07-12T15:01:09Z","title":"Provable Privacy Advantages of Decentralized Federated Learning via\n  Distributed Optimization","summary":"  Federated learning (FL) emerged as a paradigm designed to improve data\nprivacy by enabling data to reside at its source, thus embedding privacy as a\ncore consideration in FL architectures, whether centralized or decentralized.\nContrasting with recent findings by Pasquini et al., which suggest that\ndecentralized FL does not empirically offer any additional privacy or security\nbenefits over centralized models, our study provides compelling evidence to the\ncontrary. We demonstrate that decentralized FL, when deploying distributed\noptimization, provides enhanced privacy protection - both theoretically and\nempirically - compared to centralized approaches. The challenge of quantifying\nprivacy loss through iterative processes has traditionally constrained the\ntheoretical exploration of FL protocols. We overcome this by conducting a\npioneering in-depth information-theoretical privacy analysis for both\nframeworks. Our analysis, considering both eavesdropping and passive adversary\nmodels, successfully establishes bounds on privacy leakage. We show information\ntheoretically that the privacy loss in decentralized FL is upper bounded by the\nloss in centralized FL. Compared to the centralized case where local gradients\nof individual participants are directly revealed, a key distinction of\noptimization-based decentralized FL is that the relevant information includes\ndifferences of local gradients over successive iterations and the aggregated\nsum of different nodes' gradients over the network. This information\ncomplicates the adversary's attempt to infer private data. To bridge our\ntheoretical insights with practical applications, we present detailed case\nstudies involving logistic regression and deep neural networks. These examples\ndemonstrate that while privacy leakage remains comparable in simpler models,\ncomplex models like deep neural networks exhibit lower privacy risks under\ndecentralized FL.\n","authors":["Wenrui Yu","Qiongxiu Li","Milan LopuhaÃ¤-Zwakenberg","Mads GrÃ¦sbÃ¸ll Christensen","Richard Heusdens"],"pdf_url":"https://arxiv.org/pdf/2407.09324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15371v2","updated":"2024-07-12T14:52:49Z","published":"2024-03-22T17:50:43Z","title":"Can large language models explore in-context?","summary":"  We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.\n","authors":["Akshay Krishnamurthy","Keegan Harris","Dylan J. Foster","Cyril Zhang","Aleksandrs Slivkins"],"pdf_url":"https://arxiv.org/pdf/2403.15371v2.pdf","comment":"Minor updates, added references to related and concurrent work"},{"id":"http://arxiv.org/abs/2407.01394v2","updated":"2024-07-12T14:44:33Z","published":"2024-07-01T15:46:45Z","title":"Gloss2Text: Sign Language Gloss translation using LLMs and Semantically\n  Aware Label Smoothing","summary":"  Sign language translation from video to spoken text presents unique\nchallenges owing to the distinct grammar, expression nuances, and high\nvariation of visual appearance across different speakers and contexts. The\nintermediate gloss annotations of videos aim to guide the translation process.\nIn our work, we focus on {\\em Gloss2Text} translation stage and propose several\nadvances by leveraging pre-trained large language models (LLMs), data\naugmentation, and novel label-smoothing loss function exploiting gloss\ntranslation ambiguities improving significantly the performance of\nstate-of-the-art approaches. Through extensive experiments and ablation studies\non the PHOENIX Weather 2014T dataset, our approach surpasses state-of-the-art\nperformance in {\\em Gloss2Text} translation, indicating its efficacy in\naddressing sign language translation and suggesting promising avenues for\nfuture research and development.\n","authors":["Pooya Fayyazsanavi","Antonios Anastasopoulos","Jana KoÅ¡eckÃ¡"],"pdf_url":"https://arxiv.org/pdf/2407.01394v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09297v1","updated":"2024-07-12T14:30:41Z","published":"2024-07-12T14:30:41Z","title":"Learning Distances from Data with Normalizing Flows and Score Matching","summary":"  Density-based distances (DBDs) offer an elegant solution to the problem of\nmetric learning. By defining a Riemannian metric which increases with\ndecreasing probability density, shortest paths naturally follow the data\nmanifold and points are clustered according to the modes of the data. We show\nthat existing methods to estimate Fermat distances, a particular choice of DBD,\nsuffer from poor convergence in both low and high dimensions due to i)\ninaccurate density estimates and ii) reliance on graph-based paths which are\nincreasingly rough in high dimensions. To address these issues, we propose\nlearning the densities using a normalizing flow, a generative model with\ntractable density estimation, and employing a smooth relaxation method using a\nscore model initialized from a graph-based proposal. Additionally, we introduce\na dimension-adapted Fermat distance that exhibits more intuitive behavior when\nscaled to high dimensions and offers better numerical properties. Our work\npaves the way for practical use of density-based distances, especially in\nhigh-dimensional spaces.\n","authors":["Peter Sorrenson","Daniel Behrend-Uriarte","Christoph SchnÃ¶rr","Ullrich KÃ¶the"],"pdf_url":"https://arxiv.org/pdf/2407.09297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08307v2","updated":"2024-07-12T14:15:23Z","published":"2023-12-13T17:26:54Z","title":"3DReact: Geometric deep learning for chemical reactions","summary":"  Geometric deep learning models, which incorporate the relevant molecular\nsymmetries within the neural network architecture, have considerably improved\nthe accuracy and data efficiency of predictions of molecular properties.\nBuilding on this success, we introduce 3DReact, a geometric deep learning model\nto predict reaction properties from three-dimensional structures of reactants\nand products. We demonstrate that the invariant version of the model is\nsufficient for existing reaction datasets. We illustrate its competitive\nperformance on the prediction of activation barriers on the GDB7-22-TS,\nCyclo-23-TS and Proparg-21-TS datasets in different atom-mapping regimes. We\nshow that, compared to existing models for reaction property prediction,\n3DReact offers a flexible framework that exploits atom-mapping information, if\navailable, as well as geometries of reactants and products (in an invariant or\nequivariant fashion). Accordingly, it performs systematically well across\ndifferent datasets, atom-mapping regimes, as well as both interpolation and\nextrapolation tasks.\n","authors":["Puck van Gerwen","Ksenia R. Briling","Charlotte Bunne","Vignesh Ram Somnath","Ruben Laplaza","Andreas Krause","Clemence Corminboeuf"],"pdf_url":"https://arxiv.org/pdf/2312.08307v2.pdf","comment":"46 pages + SI (13 pages)"},{"id":"http://arxiv.org/abs/2403.18717v2","updated":"2024-07-12T14:13:41Z","published":"2024-03-27T16:06:37Z","title":"Semi-Supervised Learning for Deep Causal Generative Models","summary":"  Developing models that are capable of answering questions of the form \"How\nwould x change if y had been z?'\" is fundamental to advancing medical image\nanalysis. Training causal generative models that address such counterfactual\nquestions, though, currently requires that all relevant variables have been\nobserved and that the corresponding labels are available in the training data.\nHowever, clinical data may not have complete records for all patients and state\nof the art causal generative models are unable to take full advantage of this.\nWe thus develop, for the first time, a semi-supervised deep causal generative\nmodel that exploits the causal relationships between variables to maximise the\nuse of all available data. We explore this in the setting where each sample is\neither fully labelled or fully unlabelled, as well as the more clinically\nrealistic case of having different labels missing for each sample. We leverage\ntechniques from causal inference to infer missing values and subsequently\ngenerate realistic counterfactuals, even for samples with incomplete labels.\n","authors":["Yasin Ibrahim","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2403.18717v2.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.09276v1","updated":"2024-07-12T14:09:40Z","published":"2024-07-12T14:09:40Z","title":"H2O-Danube3 Technical Report","summary":"  We present H2O-Danube3, a series of small language models consisting of\nH2O-Danube3-4B, trained on 6T tokens and H2O-Danube3-500M, trained on 4T\ntokens. Our models are pre-trained on high quality Web data consisting of\nprimarily English tokens in three stages with different data mixes before final\nsupervised tuning for chat version. The models exhibit highly competitive\nmetrics across a multitude of academic, chat, and fine-tuning benchmarks.\nThanks to its compact architecture, H2O-Danube3 can be efficiently run on a\nmodern smartphone, enabling local inference and rapid processing capabilities\neven on mobile devices. We make all models openly available under Apache 2.0\nlicense further democratizing LLMs to a wider audience economically.\n","authors":["Pascal Pfeiffer","Philipp Singer","Yauhen Babakhin","Gabor Fodor","Nischay Dhankhar","Sri Satish Ambati"],"pdf_url":"https://arxiv.org/pdf/2407.09276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09274v1","updated":"2024-07-12T14:03:02Z","published":"2024-07-12T14:03:02Z","title":"Unifying Sequences, Structures, and Descriptions for Any-to-Any Protein\n  Generation with the Large Multimodal Model HelixProtX","summary":"  Proteins are fundamental components of biological systems and can be\nrepresented through various modalities, including sequences, structures, and\ntextual descriptions. Despite the advances in deep learning and scientific\nlarge language models (LLMs) for protein research, current methodologies\npredominantly focus on limited specialized tasks -- often predicting one\nprotein modality from another. These approaches restrict the understanding and\ngeneration of multimodal protein data. In contrast, large multimodal models\nhave demonstrated potential capabilities in generating any-to-any content like\ntext, images, and videos, thus enriching user interactions across various\ndomains. Integrating these multimodal model technologies into protein research\noffers significant promise by potentially transforming how proteins are\nstudied. To this end, we introduce HelixProtX, a system built upon the large\nmultimodal model, aiming to offer a comprehensive solution to protein research\nby supporting any-to-any protein modality generation. Unlike existing methods,\nit allows for the transformation of any input protein modality into any desired\nprotein modality. The experimental results affirm the advanced capabilities of\nHelixProtX, not only in generating functional descriptions from amino acid\nsequences but also in executing critical tasks such as designing protein\nsequences and structures from textual descriptions. Preliminary findings\nindicate that HelixProtX consistently achieves superior accuracy across a range\nof protein-related tasks, outperforming existing state-of-the-art models. By\nintegrating multimodal large models into protein research, HelixProtX opens new\navenues for understanding protein biology, thereby promising to accelerate\nscientific discovery.\n","authors":["Zhiyuan Chen","Tianhao Chen","Chenggang Xie","Yang Xue","Xiaonan Zhang","Jingbo Zhou","Xiaomin Fang"],"pdf_url":"https://arxiv.org/pdf/2407.09274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09271v1","updated":"2024-07-12T13:57:49Z","published":"2024-07-12T13:57:49Z","title":"iNeMo: Incremental Neural Mesh Models for Robust Class-Incremental\n  Learning","summary":"  Different from human nature, it is still common practice today for vision\ntasks to train deep learning models only initially and on fixed datasets. A\nvariety of approaches have recently addressed handling continual data streams.\nHowever, extending these methods to manage out-of-distribution (OOD) scenarios\nhas not effectively been investigated. On the other hand, it has recently been\nshown that non-continual neural mesh models exhibit strong performance in\ngeneralizing to such OOD scenarios. To leverage this decisive property in a\ncontinual learning setting, we propose incremental neural mesh models that can\nbe extended with new meshes over time. In addition, we present a latent space\ninitialization strategy that enables us to allocate feature space for future\nunseen classes in advance and a positional regularization term that forces the\nfeatures of the different classes to consistently stay in respective latent\nspace regions. We demonstrate the effectiveness of our method through extensive\nexperiments on the Pascal3D and ObjectNet3D datasets and show that our approach\noutperforms the baselines for classification by $2-6\\%$ in the in-domain and by\n$6-50\\%$ in the OOD setting. Our work also presents the first incremental\nlearning approach for pose estimation. Our code and model can be found at\nhttps://github.com/Fischer-Tom/iNeMo.\n","authors":["Tom Fischer","Yaoyao Liu","Artur Jesslen","Noor Ahmed","Prakhar Kaushik","Angtian Wang","Alan Yuille","Adam Kortylewski","Eddy Ilg"],"pdf_url":"https://arxiv.org/pdf/2407.09271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05435v2","updated":"2024-07-12T13:46:47Z","published":"2024-02-08T06:20:01Z","title":"GPT-4 Generated Narratives of Life Events using a Structured Narrative\n  Prompt: A Validation Study","summary":"  Large Language Models (LLMs) play a pivotal role in generating vast arrays of\nnarratives, facilitating a systematic exploration of their effectiveness for\ncommunicating life events in narrative form. In this study, we employ a\nzero-shot structured narrative prompt to generate 24,000 narratives using\nOpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and\nevaluate their validity in conveying birth, death, hiring, and firing events.\nRemarkably, 87.43% of the narratives sufficiently convey the intention of the\nstructured prompt. To automate the identification of valid and invalid\nnarratives, we train and validate nine Machine Learning models on the\nclassified datasets. Leveraging these models, we extend our analysis to predict\nthe classifications of the remaining 21,120 narratives. All the ML models\nexcelled at classifying valid narratives as valid, but experienced challenges\nat simultaneously classifying invalid narratives as invalid. Our findings not\nonly advance the study of LLM capabilities, limitations, and validity but also\noffer practical insights for narrative generation and natural language\nprocessing applications.\n","authors":["Christopher J. Lynch","Erik Jensen","Madison H. Munro","Virginia Zamponi","Joseph Martinez","Kevin O'Brien","Brandon Feldhaus","Katherine Smith","Ann Marie Reinhold","Ross Gore"],"pdf_url":"https://arxiv.org/pdf/2402.05435v2.pdf","comment":"29 pages, 24 figures"},{"id":"http://arxiv.org/abs/2407.09251v1","updated":"2024-07-12T13:30:00Z","published":"2024-07-12T13:30:00Z","title":"Deep Adversarial Defense Against Multilevel-Lp Attacks","summary":"  Deep learning models have shown considerable vulnerability to adversarial\nattacks, particularly as attacker strategies become more sophisticated. While\ntraditional adversarial training (AT) techniques offer some resilience, they\noften focus on defending against a single type of attack, e.g., the\n$\\ell_\\infty$-norm attack, which can fail for other types. This paper\nintroduces a computationally efficient multilevel $\\ell_p$ defense, called the\nEfficient Robust Mode Connectivity (EMRC) method, which aims to enhance a deep\nlearning model's resilience against multiple $\\ell_p$-norm attacks. Similar to\nanalytical continuation approaches used in continuous optimization, the method\nblends two $p$-specific adversarially optimal models, the $\\ell_1$- and\n$\\ell_\\infty$-norm AT solutions, to provide good adversarial robustness for a\nrange of $p$. We present experiments demonstrating that our approach performs\nbetter on various attacks as compared to AT-$\\ell_\\infty$, E-AT, and MSD, for\ndatasets/architectures including: CIFAR-10, CIFAR-100 / PreResNet110,\nWideResNet, ViT-Base.\n","authors":["Ren Wang","Yuxuan Li","Alfred Hero"],"pdf_url":"https://arxiv.org/pdf/2407.09251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09250v1","updated":"2024-07-12T13:23:54Z","published":"2024-07-12T13:23:54Z","title":"FedsLLM: Federated Split Learning for Large Language Models over\n  Communication Networks","summary":"  Addressing the challenges of deploying large language models in wireless\ncommunication networks, this paper combines low-rank adaptation technology\n(LoRA) with the splitfed learning framework to propose the federated split\nlearning for large language models (FedsLLM) framework. The method introduced\nin this paper utilizes LoRA technology to reduce processing loads by dividing\nthe network into client subnetworks and server subnetworks. It leverages a\nfederated server to aggregate and update client models. As the training data\nare transmitted through a wireless network between clients and both main and\nfederated servers, the training delay is determined by the learning accuracy\nand the allocation of communication bandwidth. This paper models the\nminimization of the training delay by integrating computation and communication\noptimization, simplifying the optimization problem into a convex problem to\nfind the optimal solution. Additionally, it presents a lemma that describes the\nprecise solutions to this problem. Simulation results demonstrate that the\nproposed optimization algorithm reduces delays by an average of 47.63% compared\nto unoptimized scenarios.\n","authors":["Kai Zhao","Zhaohui Yang","Chongwen Huang","Xiaoming Chen","Zhaoyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.09250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18332v2","updated":"2024-07-12T13:16:16Z","published":"2024-06-26T13:21:00Z","title":"Early Classification of Time Series: Taxonomy and Benchmark","summary":"  In many situations, the measurements of a studied phenomenon are provided\nsequentially, and the prediction of its class needs to be made as early as\npossible so as not to incur too high a time penalty, but not too early and risk\npaying the cost of misclassification. This problem has been particularly\nstudied in the case of time series, and is known as Early Classification of\nTime Series (ECTS). Although it has been the subject of a growing body of\nliterature, there is still a lack of a systematic, shared evaluation protocol\nto compare the relative merits of the various existing methods. This document\nbegins by situating these methods within a principle-based taxonomy. It defines\ndimensions for organizing their evaluation, and then reports the results of a\nvery extensive set of experiments along these dimensions involving nine\nstate-of-the art ECTS algorithms. In addition, these and other experiments can\nbe carried out using an open-source library in which most of the existing ECTS\nalgorithms have been implemented (see \\url{https://github.com/ML-EDM/ml_edm}).\n","authors":["AurÃ©lien Renault","Alexis Bondu","Antoine CornuÃ©jols","Vincent Lemaire"],"pdf_url":"https://arxiv.org/pdf/2406.18332v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03625v3","updated":"2024-07-12T12:55:53Z","published":"2024-02-06T01:29:35Z","title":"Convex Relaxations of ReLU Neural Networks Approximate Global Optima in\n  Polynomial Time","summary":"  In this paper, we study the optimality gap between two-layer ReLU networks\nregularized with weight decay and their convex relaxations. We show that when\nthe training data is random, the relative optimality gap between the original\nproblem and its relaxation can be bounded by a factor of O(log n^0.5), where n\nis the number of training samples. A simple application leads to a tractable\npolynomial-time algorithm that is guaranteed to solve the original non-convex\nproblem up to a logarithmic factor. Moreover, under mild assumptions, we show\nthat local gradient methods converge to a point with low training loss with\nhigh probability. Our result is an exponential improvement compared to existing\nresults and sheds new light on understanding why local gradient methods work\nwell.\n","authors":["Sungyoon Kim","Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2402.03625v3.pdf","comment":"Version 2: Fixed proof of Thm 4.4, slight clarification on assumption\n  2 Version 3: Modified to ICML style and slight clarification on assumption 1"},{"id":"http://arxiv.org/abs/2307.05284v3","updated":"2024-07-12T12:54:37Z","published":"2023-07-11T14:25:10Z","title":"On the Need of a Modeling Language for Distribution Shifts:\n  Illustrations on Tabular Datasets","summary":"  Different distribution shifts require different interventions, and algorithms\nmust be grounded in the specific shifts they address. However, methodological\ndevelopment for robust algorithms typically relies on structural assumptions\nthat lack empirical validation. Advocating for an empirically grounded\ndata-driven approach to research, we build an empirical testbed comprising\nnatural shifts across 5 tabular datasets and 60,000 method configurations\nencompassing imbalanced learning and distributionally robust optimization (DRO)\nmethods. We find $Y|X$-shifts are most prevalent on our testbed, in stark\ncontrast to the heavy focus on $X$ (covariate)-shifts in the ML literature. The\nperformance of robust algorithms varies significantly over shift types, and is\nno better than that of vanilla methods. To understand why, we conduct an\nin-depth empirical analysis of DRO methods and find that although often\nneglected by researchers, implementation details -- such as the choice of\nunderlying model class (e.g., XGBoost) and hyperparameter selection -- have a\nbigger impact on performance than the ambiguity set or its radius. To further\nbridge that gap between methodological research and practice, we design case\nstudies that illustrate how such a data-driven, inductive understanding of\ndistribution shifts can enhance both data-centric and algorithmic\ninterventions.\n","authors":["Jiashuo Liu","Tianyu Wang","Peng Cui","Hongseok Namkoong"],"pdf_url":"https://arxiv.org/pdf/2307.05284v3.pdf","comment":"Conference version appeared in NeurIPS 2023"},{"id":"http://arxiv.org/abs/2402.09303v3","updated":"2024-07-12T12:47:19Z","published":"2024-02-14T16:47:20Z","title":"Comparing supervised learning dynamics: Deep neural networks match human\n  data efficiency but show a generalisation lag","summary":"  Recent research has seen many behavioral comparisons between humans and deep\nneural networks (DNNs) in the domain of image classification. Often, comparison\nstudies focus on the end-result of the learning process by measuring and\ncomparing the similarities in the representations of object categories once\nthey have been formed. However, the process of how these representations emerge\n-- that is, the behavioral changes and intermediate stages observed during the\nacquisition -- is less often directly and empirically compared. Here we report\na detailed investigation of the learning dynamics in human observers and\nvarious classic and state-of-the-art DNNs. We develop a constrained supervised\nlearning environment to align learning-relevant conditions such as starting\npoint, input modality, available input data and the feedback provided. Across\nthe whole learning process we evaluate and compare how well learned\nrepresentations can be generalized to previously unseen test data. Comparisons\nacross the entire learning process indicate that DNNs demonstrate a level of\ndata efficiency comparable to human learners, challenging some prevailing\nassumptions in the field. However, our results also reveal representational\ndifferences: while DNNs' learning is characterized by a pronounced\ngeneralisation lag, humans appear to immediately acquire generalizable\nrepresentations without a preliminary phase of learning training set-specific\ninformation that is only later transferred to novel data.\n","authors":["Lukas S. Huber","Fred W. Mast","Felix A. Wichmann"],"pdf_url":"https://arxiv.org/pdf/2402.09303v3.pdf","comment":"Final version accepted @ ICLR 2024 Workshop on Representational\n  Alignment (Re-Align)"},{"id":"http://arxiv.org/abs/2407.09216v1","updated":"2024-07-12T12:28:08Z","published":"2024-07-12T12:28:08Z","title":"A Fair Ranking and New Model for Panoptic Scene Graph Generation","summary":"  In panoptic scene graph generation (PSGG), models retrieve interactions\nbetween objects in an image which are grounded by panoptic segmentation masks.\nPrevious evaluations on panoptic scene graphs have been subject to an erroneous\nevaluation protocol where multiple masks for the same object can lead to\nmultiple relation distributions per mask-mask pair. This can be exploited to\nincrease the final score. We correct this flaw and provide a fair ranking over\na wide range of existing PSGG models. The observed scores for existing methods\nincrease by up to 7.4 mR@50 for all two-stage methods, while dropping by up to\n19.3 mR@50 for all one-stage methods, highlighting the importance of a correct\nevaluation. Contrary to recent publications, we show that existing two-stage\nmethods are competitive to one-stage methods. Building on this, we introduce\nthe Decoupled SceneFormer (DSFormer), a novel two-stage model that outperforms\nall existing scene graph models by a large margin of +11 mR@50 and +10 mNgR@50\non the corrected evaluation, thus setting a new SOTA. As a core design\nprinciple, DSFormer encodes subject and object masks directly into feature\nspace.\n","authors":["Julian Lorenz","Alexander Pest","Daniel Kienzle","Katja Ludwig","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2407.09216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09212v1","updated":"2024-07-12T12:20:39Z","published":"2024-07-12T12:20:39Z","title":"Generating SROI^{-} Ontologies via Knowledge Graph Query Embedding\n  Learning","summary":"  Query embedding approaches answer complex logical queries over incomplete\nknowledge graphs (KGs) by computing and operating on low-dimensional vector\nrepresentations of entities, relations, and queries. However, current query\nembedding models heavily rely on excessively parameterized neural networks and\ncannot explain the knowledge learned from the graph. We propose a novel query\nembedding method, AConE, which explains the knowledge learned from the graph in\nthe form of SROI^{-} description logic axioms while being more\nparameter-efficient than most existing approaches. AConE associates queries to\na SROI^{-} description logic concept. Every SROI^{-} concept is embedded as a\ncone in complex vector space, and each SROI^{-} relation is embedded as a\ntransformation that rotates and scales cones. We show theoretically that AConE\ncan learn SROI^{-} axioms, and defines an algebra whose operations correspond\none to one to SROI^{-} description logic concept constructs. Our empirical\nstudy on multiple query datasets shows that AConE achieves superior results\nover previous baselines with fewer parameters. Notably on the WN18RR dataset,\nAConE achieves significant improvement over baseline models. We provide\ncomprehensive analyses showing that the capability to represent axioms\npositively impacts the results of query answering.\n","authors":["Yunjie He","Daniel Hernandez","Mojtaba Nayyeri","Bo Xiong","Yuqicheng Zhu","Evgeny Kharlamov","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2407.09212v1.pdf","comment":"Accepted by ECAI 2024"},{"id":"http://arxiv.org/abs/2310.05227v5","updated":"2024-07-12T12:05:28Z","published":"2023-10-08T16:48:29Z","title":"Physics-aware Machine Learning Revolutionizes Scientific Paradigm for\n  Machine Learning and Process-based Hydrology","summary":"  Accurate hydrological understanding and water cycle prediction are crucial\nfor addressing scientific and societal challenges associated with the\nmanagement of water resources, particularly under the dynamic influence of\nanthropogenic climate change. Existing reviews predominantly concentrate on the\ndevelopment of machine learning (ML) in this field, yet there is a clear\ndistinction between hydrology and ML as separate paradigms. Here, we introduce\nphysics-aware ML as a transformative approach to overcome the perceived barrier\nand revolutionize both fields. Specifically, we present a comprehensive review\nof the physics-aware ML methods, building a structured community (PaML) of\nexisting methodologies that integrate prior physical knowledge or physics-based\nmodeling into ML. We systematically analyze these PaML methodologies with\nrespect to four aspects: physical data-guided ML, physics-informed ML,\nphysics-embedded ML, and physics-aware hybrid learning. PaML facilitates\nML-aided hypotheses, accelerating insights from big data and fostering\nscientific discoveries. We first conduct a systematic review of hydrology in\nPaML, including rainfall-runoff hydrological processes and hydrodynamic\nprocesses, and highlight the most promising and challenging directions for\ndifferent objectives and PaML methods. Finally, a new PaML-based hydrology\nplatform, termed HydroPML, is released as a foundation for hydrological\napplications. HydroPML enhances the explainability and causality of ML and lays\nthe groundwork for the digital water cycle's realization. The HydroPML platform\nis publicly available at https://hydropml.github.io/.\n","authors":["Qingsong Xu","Yilei Shi","Jonathan Bamber","Ye Tuo","Ralf Ludwig","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.05227v5.pdf","comment":"44 pages, 6 figures"},{"id":"http://arxiv.org/abs/2309.03774v3","updated":"2024-07-12T11:46:08Z","published":"2023-09-07T15:25:47Z","title":"Deep Learning Safety Concerns in Automated Driving Perception","summary":"  Recent advances in the field of deep learning and impressive performance of\ndeep neural networks (DNNs) for perception have resulted in an increased demand\nfor their use in automated driving (AD) systems. The safety of such systems is\nof utmost importance and thus requires to consider the unique properties of\nDNNs.\n  In order to achieve safety of AD systems with DNN-based perception components\nin a systematic and comprehensive approach, so-called safety concerns have been\nintroduced as a suitable structuring element. On the one hand, the concept of\nsafety concerns is -- by design -- well aligned to existing standards relevant\nfor safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has\nalready inspired several academic publications and upcoming standards on AI\nsafety such as ISO PAS 8800.\n  While the concept of safety concerns has been previously introduced, this\npaper extends and refines it, leveraging feedback from various domain and\nsafety experts in the field. In particular, this paper introduces an additional\ncategorization for a better understanding as well as enabling cross-functional\nteams to jointly address the concerns.\n","authors":["Stephanie Abrecht","Alexander Hirsch","Shervin Raafatnia","Matthias Woehrle"],"pdf_url":"https://arxiv.org/pdf/2309.03774v3.pdf","comment":"Added note regarding accepted version at IEEE Transactions on\n  Intelligent Vehicles with DOI"},{"id":"http://arxiv.org/abs/2210.03919v5","updated":"2024-07-12T11:44:29Z","published":"2022-10-08T05:12:25Z","title":"CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features\n  for a Disentangled, Interpretable, and Controllable Text-Guided Face\n  Manipulation","summary":"  Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges\nimages and text by embedding them into a joint latent space. This opens the\ndoor to ample literature that aims to manipulate an input image by providing a\ntextual explanation. However, due to the discrepancy between image and text\nembeddings in the joint space, using text embeddings as the optimization target\noften introduces undesired artifacts in the resulting images. Disentanglement,\ninterpretability, and controllability are also hard to guarantee for\nmanipulation. To alleviate these problems, we propose to define corpus\nsubspaces spanned by relevant prompts to capture specific image\ncharacteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as\nan optimization target to improve the performance of text-guided image\nmanipulation. Our method is a simple and general paradigm that can be easily\ncomputed and adapted, and smoothly incorporated into any CLIP-based image\nmanipulation algorithm. To demonstrate the effectiveness of our method, we\nconduct several theoretical and empirical studies. As a case study, we utilize\nthe method for text-guided semantic face editing. We quantitatively and\nqualitatively demonstrate that PAE facilitates a more disentangled,\ninterpretable, and controllable image manipulation with state-of-the-art\nquality and accuracy. Project page: https://chenliang-zhou.github.io/CLIP-PAE/.\n","authors":["Chenliang Zhou","Fangcheng Zhong","Cengiz Oztireli"],"pdf_url":"https://arxiv.org/pdf/2210.03919v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04820v2","updated":"2024-07-12T11:41:33Z","published":"2023-09-09T15:18:46Z","title":"ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class\n  Class-agnostic Counting","summary":"  Class-agnostic counting methods enumerate objects of an arbitrary class,\nproviding tremendous utility in many fields. Prior works have limited\nusefulness as they require either a set of examples of the type to be counted\nor that the query image contains only a single type of object. A significant\nfactor in these shortcomings is the lack of a dataset to properly address\ncounting in settings with more than one kind of object present. To address\nthese issues, we propose the first Multi-class, Class-Agnostic Counting dataset\n(MCAC) and A Blind Counter (ABC123), a method that can count multiple types of\nobjects simultaneously without using examples of type during training or\ninference. ABC123 introduces a new paradigm where instead of requiring\nexemplars to guide the enumeration, examples are found after the counting stage\nto help a user understand the generated outputs. We show that ABC123\noutperforms contemporary methods on MCAC without needing human in-the-loop\nannotations. We also show that this performance transfers to FSC-147, the\nstandard class-agnostic counting dataset. MCAC is available at\nMCAC.active.vision and ABC123 is available at ABC123.active.vision.\n","authors":["Michael A. Hobley","Victor A. Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2309.04820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06649v2","updated":"2024-07-12T11:38:56Z","published":"2024-03-30T05:32:42Z","title":"ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein\n  Interaction Prediction","summary":"  The prediction of protein-protein interactions (PPIs) is crucial for\nunderstanding biological functions and diseases. Previous machine learning\napproaches to PPI prediction mainly focus on direct physical interactions,\nignoring the broader context of nonphysical connections through intermediate\nproteins, thus limiting their effectiveness. The emergence of Large Language\nModels (LLMs) provides a new opportunity for addressing this complex biological\nchallenge. By transforming structured data into natural language prompts, we\ncan map the relationships between proteins into texts. This approach allows\nLLMs to identify indirect connections between proteins, tracing the path from\nupstream to downstream. Therefore, we propose a novel framework ProLLM that\nemploys an LLM tailored for PPI for the first time. Specifically, we propose\nProtein Chain of Thought (ProCoT), which replicates the biological mechanism of\nsignaling pathways as natural language prompts. ProCoT considers a signaling\npathway as a protein reasoning process, which starts from upstream proteins and\npasses through several intermediate proteins to transmit biological signals to\ndownstream proteins. Thus, we can use ProCoT to predict the interaction between\nupstream proteins and downstream proteins. The training of ProLLM employs the\nProCoT format, which enhances the model's understanding of complex biological\nproblems. In addition to ProCoT, this paper also contributes to the exploration\nof embedding replacement of protein sites in natural language prompts, and\ninstruction fine-tuning in protein knowledge datasets. We demonstrate the\nefficacy of ProLLM through rigorous validation against benchmark datasets,\nshowing significant improvement over existing methods in terms of prediction\naccuracy and generalizability. The code is available at:\nhttps://github.com/MingyuJ666/ProLLM.\n","authors":["Mingyu Jin","Haochen Xue","Zhenting Wang","Boming Kang","Ruosong Ye","Kaixiong Zhou","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.06649v2.pdf","comment":"Accepted by COLM 2024"},{"id":"http://arxiv.org/abs/2407.09186v1","updated":"2024-07-12T11:38:41Z","published":"2024-07-12T11:38:41Z","title":"Variational Inference via Smoothed Particle Hydrodynamics","summary":"  A new variational inference method, SPH-ParVI, based on smoothed particle\nhydrodynamics (SPH), is proposed for sampling partially known densities (e.g.\nup to a constant) or sampling using gradients. SPH-ParVI simulates the flow of\na fluid under external effects driven by the target density; transient or\nsteady state of the fluid approximates the target density. The continuum fluid\nis modelled as an interacting particle system (IPS) via SPH, where each\nparticle carries smoothed properties, interacts and evolves as per the\nNavier-Stokes equations. This mesh-free, Lagrangian simulation method offers\nfast, flexible, scalable and deterministic sampling and inference for a class\nof probabilistic models such as those encountered in Bayesian inference and\ngenerative modelling.\n","authors":["Yongchao Huang"],"pdf_url":"https://arxiv.org/pdf/2407.09186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09173v1","updated":"2024-07-12T11:12:49Z","published":"2024-07-12T11:12:49Z","title":"Conformal Inductive Graph Neural Networks","summary":"  Conformal prediction (CP) transforms any model's output into prediction sets\nguaranteed to include (cover) the true label. CP requires exchangeability, a\nrelaxation of the i.i.d. assumption, to obtain a valid distribution-free\ncoverage guarantee. This makes it directly applicable to transductive\nnode-classification. However, conventional CP cannot be applied in inductive\nsettings due to the implicit shift in the (calibration) scores caused by\nmessage passing with the new nodes. We fix this issue for both cases of node\nand edge-exchangeable graphs, recovering the standard coverage guarantee\nwithout sacrificing statistical efficiency. We further prove that the guarantee\nholds independently of the prediction time, e.g. upon arrival of a new\nnode/edge or at any subsequent moment.\n","authors":["Soroush H. Zargarbashi","Aleksandar Bojchevski"],"pdf_url":"https://arxiv.org/pdf/2407.09173v1.pdf","comment":"Published as a conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2407.09167v1","updated":"2024-07-12T11:01:28Z","published":"2024-07-12T11:01:28Z","title":"SE(3)-bi-equivariant Transformers for Point Cloud Assembly","summary":"  Given a pair of point clouds, the goal of assembly is to recover a rigid\ntransformation that aligns one point cloud to the other. This task is\nchallenging because the point clouds may be non-overlapped, and they may have\narbitrary initial positions. To address these difficulties, we propose a\nmethod, called SE(3)-bi-equivariant transformer (BITR), based on the\nSE(3)-bi-equivariance prior of the task: it guarantees that when the inputs are\nrigidly perturbed, the output will transform accordingly. Due to its\nequivariance property, BITR can not only handle non-overlapped PCs, but also\nguarantee robustness against initial positions. Specifically, BITR first\nextracts features of the inputs using a novel $SE(3) \\times SE(3)$-transformer,\nand then projects the learned feature to group SE(3) as the output. Moreover,\nwe theoretically show that swap and scale equivariances can be incorporated\ninto BITR, thus it further guarantees stable performance under scaling and\nswapping the inputs. We experimentally show the effectiveness of BITR in\npractical tasks.\n","authors":["Ziming Wang","Rebecka JÃ¶rnsten"],"pdf_url":"https://arxiv.org/pdf/2407.09167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09165v1","updated":"2024-07-12T10:59:44Z","published":"2024-07-12T10:59:44Z","title":"Robust Yet Efficient Conformal Prediction Sets","summary":"  Conformal prediction (CP) can convert any model's output into prediction sets\nguaranteed to include the true label with any user-specified probability.\nHowever, same as the model itself, CP is vulnerable to adversarial test\nexamples (evasion) and perturbed calibration data (poisoning). We derive\nprovably robust sets by bounding the worst-case change in conformity scores.\nOur tighter bounds lead to more efficient sets. We cover both continuous and\ndiscrete (sparse) data and our guarantees work both for evasion and poisoning\nattacks (on both features and labels).\n","authors":["Soroush H. Zargarbashi","Mohammad Sadegh Akhondzadeh","Aleksandar Bojchevski"],"pdf_url":"https://arxiv.org/pdf/2407.09165v1.pdf","comment":"Proceedings of the 41st International Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2407.09162v1","updated":"2024-07-12T10:58:01Z","published":"2024-07-12T10:58:01Z","title":"Exploring State Space and Reasoning by Elimination in Tsetlin Machine","summary":"  The Tsetlin Machine (TM) has gained significant attention in Machine Learning\n(ML). By employing logical fundamentals, it facilitates pattern learning and\nrepresentation, offering an alternative approach for developing comprehensible\nArtificial Intelligence (AI) with a specific focus on pattern classification in\nthe form of conjunctive clauses. In the domain of Natural Language Processing\n(NLP), TM is utilised to construct word embedding and describe target words\nusing clauses. To enhance the descriptive capacity of these clauses, we study\nthe concept of Reasoning by Elimination (RbE) in clauses' formulation, which\ninvolves incorporating feature negations to provide a more comprehensive\nrepresentation. In more detail, this paper employs the Tsetlin Machine\nAuto-Encoder (TM-AE) architecture to generate dense word vectors, aiming at\ncapturing contextual information by extracting feature-dense vectors for a\ngiven vocabulary. Thereafter, the principle of RbE is explored to improve\ndescriptivity and optimise the performance of the TM. Specifically, the\nspecificity parameter s and the voting margin parameter T are leveraged to\nregulate feature distribution in the state space, resulting in a dense\nrepresentation of information for each clause. In addition, we investigate the\nstate spaces of TM-AE, especially for the forgotten/excluded features.\nEmpirical investigations on artificially generated data, the IMDB dataset, and\nthe 20 Newsgroups dataset showcase the robustness of the TM, with accuracy\nreaching 90.62\\% for the IMDB.\n","authors":["Ahmed K. Kadhim","Ole-Christoffer Granmo","Lei Jiao","Rishad Shafik"],"pdf_url":"https://arxiv.org/pdf/2407.09162v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.09157v1","updated":"2024-07-12T10:44:51Z","published":"2024-07-12T10:44:51Z","title":"Movie Recommendation with Poster Attention via Multi-modal Transformer\n  Feature Fusion","summary":"  Pre-trained models learn general representations from large datsets which can\nbe fine-turned for specific tasks to significantly reduce training time.\nPre-trained models like generative pretrained transformers (GPT), bidirectional\nencoder representations from transformers (BERT), vision transfomers (ViT) have\nbecome a cornerstone of current research in machine learning. This study\nproposes a multi-modal movie recommendation system by extract features of the\nwell designed posters for each movie and the narrative text description of the\nmovie. This system uses the BERT model to extract the information of text\nmodality, the ViT model applied to extract the information of poster/image\nmodality, and the Transformer architecture for feature fusion of all modalities\nto predict users' preference. The integration of pre-trained foundational\nmodels with some smaller data sets in downstream applications capture\nmulti-modal content features in a more comprehensive manner, thereby providing\nmore accurate recommendations. The efficiency of the proof-of-concept model is\nverified by the standard benchmark problem the MovieLens 100K and 1M datasets.\nThe prediction accuracy of user ratings is enhanced in comparison to the\nbaseline algorithm, thereby demonstrating the potential of this cross-modal\nalgorithm to be applied for movie or video recommendation.\n","authors":["Linhan Xia","Yicheng Yang","Ziou Chen","Zheng Yang","Shengxin Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.09157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06963v2","updated":"2024-07-12T10:40:08Z","published":"2024-02-10T14:36:31Z","title":"Tree Ensembles for Contextual Bandits","summary":"  We propose a novel framework for contextual multi-armed bandits based on tree\nensembles. Our framework integrates two widely used bandit methods, Upper\nConfidence Bound and Thompson Sampling, for both standard and combinatorial\nsettings. We demonstrate the effectiveness of our framework via several\nexperimental studies, employing both XGBoost and random forest, two popular\ntree ensemble methods. Compared to state-of-the-art methods based on decision\ntrees and neural networks, our methods exhibit superior performance in terms of\nboth regret minimization and computational runtime, when applied to benchmark\ndatasets and the real-world application of navigation over road networks.\n","authors":["Hannes Nilsson","Rikard Johansson","Niklas Ãkerblom","Morteza Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2402.06963v2.pdf","comment":"The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2402.14522v2","updated":"2024-07-12T10:39:28Z","published":"2024-02-22T13:13:31Z","title":"Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap\n  for Prompt-Based Large Language Models and Beyond","summary":"  Task embedding, a meta-learning technique that captures task-specific\ninformation, has gained popularity, especially in areas such as multi-task\nlearning, model editing, and interpretability. However, it faces challenges\nwith the emergence of prompt-guided Large Language Models (LLMs) operating in a\ngradient-free manner. Existing task embedding methods rely on fine-tuned,\ntask-specific language models, which hinders the adaptability of task\nembeddings across diverse models, especially prompt-based LLMs. To hardness the\npotential of task embeddings in the era of LLMs, we propose a framework for\nunified task embeddings (FUTE), harmonizing task embeddings from various\nmodels, including smaller language models and LLMs with varied prompts, within\na single vector space. Such uniformity enables comparison and analysis of\nsimilarities amongst different models, broadening the scope and utility of\nexisting task embedding methods in multi-model scenarios, while maintaining\ntheir performance comparable to architecture-specific methods.\n","authors":["Xinyu Wang","Hainiu Xu","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2402.14522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04437v2","updated":"2024-07-12T10:33:31Z","published":"2024-05-07T16:00:32Z","title":"vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention","summary":"  Efficient management of GPU memory is essential for high throughput LLM\ninference. Prior systems used to reserve KV-cache memory ahead-of-time that\nresulted in wasted capacity due to internal fragmentation. Inspired by demand\npaging, vLLM proposed PagedAttention to enable dynamic memory allocation for\nKV-cache. This approach eliminates fragmentation and improves serving\nthroughout. However, to be able to allocate physical memory dynamically,\nPagedAttention changes the layout of KV-cache from contiguous virtual memory to\nnon-contiguous virtual memory. As a consequence, one needs to rewrite the\nattention kernels to support paging, and implement a memory manager in the\nserving framework. This results in both performance and programming overheads,\nas well as portability challenges in adopting state-of-the-art attention\nkernels.\n  In this paper, we propose vAttention, a new approach for dynamic KV-cache\nmemory management. In contrast to PagedAttention, vAttention stores KV-cache in\ncontiguous virtual memory and leverages OS support for on-demand allocation of\nphysical memory. vAttention thus enables one to use state-of-the art attention\nkernels out-of-the-box by adding support for dynamic allocation of physical\nmemory without having to re-write their code. We implement vAttention in the\nvLLM serving stack to show that it also helps improve decode throughput by up\nto 1.99x over vLLM, and the end-to-end serving throughput by up to 1.22x and\n1.29x, compared to using the state-of-the-art PagedAttention based kernels of\nFlashAttention and FlashInfer.\n","authors":["Ramya Prabhu","Ajay Nayak","Jayashree Mohan","Ramachandran Ramjee","Ashish Panwar"],"pdf_url":"https://arxiv.org/pdf/2405.04437v2.pdf","comment":"14 pages, 13 figures, 10 tables"},{"id":"http://arxiv.org/abs/2407.09150v1","updated":"2024-07-12T10:32:53Z","published":"2024-07-12T10:32:53Z","title":"Evaluating the Adversarial Robustness of Semantic Segmentation: Trying\n  Harder Pays Off","summary":"  Machine learning models are vulnerable to tiny adversarial input\nperturbations optimized to cause a very large output error. To measure this\nvulnerability, we need reliable methods that can find such adversarial\nperturbations. For image classification models, evaluation methodologies have\nemerged that have stood the test of time. However, we argue that in the area of\nsemantic segmentation, a good approximation of the sensitivity to adversarial\nperturbations requires significantly more effort than what is currently\nconsidered satisfactory. To support this claim, we re-evaluate a number of\nwell-known robust segmentation models in an extensive empirical study. We\npropose new attacks and combine them with the strongest attacks available in\nthe literature. We also analyze the sensitivity of the models in fine detail.\nThe results indicate that most of the state-of-the-art models have a\ndramatically larger sensitivity to adversarial perturbations than previously\nreported. We also demonstrate a size-bias: small objects are often more easily\nattacked, even if the large objects are robust, a phenomenon not revealed by\ncurrent evaluation metrics. Our results also demonstrate that a diverse set of\nstrong attacks is necessary, because different models are often vulnerable to\ndifferent attacks.\n","authors":["Levente Halmosi","BÃ¡lint Mohos","MÃ¡rk Jelasity"],"pdf_url":"https://arxiv.org/pdf/2407.09150v1.pdf","comment":"Accepted for ECCV 2024. For the implementation, see\n  https://github.com/szegedai/Robust-Segmentation-Evaluation"},{"id":"http://arxiv.org/abs/2407.09141v1","updated":"2024-07-12T10:19:02Z","published":"2024-07-12T10:19:02Z","title":"Accuracy is Not All You Need","summary":"  When Large Language Models (LLMs) are compressed using techniques such as\nquantization, the predominant way to demonstrate the validity of such\ntechniques is by measuring the model's accuracy on various benchmarks.If the\naccuracies of the baseline model and the compressed model are close, it is\nassumed that there was negligible degradation in quality.However, even when the\naccuracy of baseline and compressed model are similar, we observe the\nphenomenon of flips, wherein answers change from correct to incorrect and vice\nversa in proportion.We conduct a detailed study of metrics across multiple\ncompression techniques, models and datasets, demonstrating that the behavior of\ncompressed models as visible to end-users is often significantly different from\nthe baseline model, even when accuracy is similar.We further evaluate\ncompressed models qualitatively and quantitatively using MT-Bench and show that\ncompressed models are significantly worse than baseline models in this\nfree-form generative task.Thus, we argue that compression techniques should\nalso be evaluated using distance metrics.We propose two such metrics,\nKL-Divergence and flips, and show that they are well correlated.\n","authors":["Abhinav Dutta","Sanjeev Krishnan","Nipun Kwatra","Ramachandran Ramjee"],"pdf_url":"https://arxiv.org/pdf/2407.09141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15022v4","updated":"2024-07-12T10:16:55Z","published":"2024-01-26T17:29:01Z","title":"Applications of artificial intelligence in the analysis of\n  histopathology images of gliomas: a review","summary":"  In recent years, the diagnosis of gliomas has become increasingly complex.\nAnalysis of glioma histopathology images using artificial intelligence (AI)\noffers new opportunities to support diagnosis and outcome prediction. To give\nan overview of the current state of research, this review examines 83 publicly\navailable research studies that have proposed AI-based methods for whole-slide\nhistopathology images of human gliomas, covering the diagnostic tasks of\nsubtyping (23/83), grading (27/83), molecular marker prediction (20/83), and\nsurvival prediction (29/83). All studies were reviewed with regard to\nmethodological aspects as well as clinical applicability. It was found that the\nfocus of current research is the assessment of hematoxylin and eosin-stained\ntissue sections of adult-type diffuse gliomas. The majority of studies (52/83)\nare based on the publicly available glioblastoma and low-grade glioma datasets\nfrom The Cancer Genome Atlas (TCGA) and only a few studies employed other\ndatasets in isolation (16/83) or in addition to the TCGA datasets (15/83).\nCurrent approaches mostly rely on convolutional neural networks (63/83) for\nanalyzing tissue at 20x magnification (35/83). A new field of research is the\nintegration of clinical data, omics data, or magnetic resonance imaging\n(29/83). So far, AI-based methods have achieved promising results, but are not\nyet used in real clinical settings. Future work should focus on the independent\nvalidation of methods on larger, multi-site datasets with high-quality and\nup-to-date clinical and molecular pathology annotations to demonstrate routine\napplicability.\n","authors":["Jan-Philipp Redlich","Friedrich Feuerhake","Joachim Weis","Nadine S. Schaadt","Sarah Teuber-Hanselmann","Christoph Buck","Sabine Luttmann","Andrea Eberle","Stefan Nikolin","Arno Appenzeller","Andreas Portmann","AndrÃ© Homeyer"],"pdf_url":"https://arxiv.org/pdf/2401.15022v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09136v1","updated":"2024-07-12T10:11:40Z","published":"2024-07-12T10:11:40Z","title":"Stepwise Verification and Remediation of Student Reasoning Errors with\n  Large Language Model Tutors","summary":"  Large language models (LLMs) present an opportunity to scale high-quality\npersonalized education to all. A promising approach towards this means is to\nbuild dialog tutoring models that scaffold students' problem-solving. However,\neven though existing LLMs perform well in solving reasoning questions, they\nstruggle to precisely detect student's errors and tailor their feedback to\nthese errors. Inspired by real-world teaching practice where teachers identify\nstudent errors and customize their response based on them, we focus on\nverifying student solutions and show how grounding to such verification\nimproves the overall quality of tutor response generation. We collect a dataset\nof 1K stepwise math reasoning chains with the first error step annotated by\nteachers. We show empirically that finding the mistake in a student solution is\nchallenging for current models. We propose and evaluate several verifiers for\ndetecting these errors. Using both automatic and human evaluation we show that\nthe student solution verifiers steer the generation model towards highly\ntargeted responses to student errors which are more often correct with less\nhallucinations compared to existing baselines.\n","authors":["Nico Daheim","Jakub Macina","Manu Kapur","Iryna Gurevych","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2407.09136v1.pdf","comment":"Preprint. Nico Daheim and Jakub Macina contributed equally. Code and\n  dataset can be found under: https://github.com/eth-lre/verify-then-generate"},{"id":"http://arxiv.org/abs/2403.14608v6","updated":"2024-07-12T09:58:10Z","published":"2024-03-21T17:55:50Z","title":"Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey","summary":"  Large models represent a groundbreaking advancement in multiple application\nfields, enabling remarkable achievements across various tasks. However, their\nunprecedented scale comes with significant computational costs. These models,\noften consisting of billions of parameters, require vast amounts of\ncomputational resources for execution. Especially, the expansive scale and\ncomputational demands pose considerable challenges when customizing them for\nparticular downstream tasks, particularly over the hardware platforms\nconstrained by computational capabilities. Parameter Efficient Fine-Tuning\n(PEFT) provides a practical solution by efficiently adjusting the large models\nover the various downstream tasks. In particular, PEFT refers to the process of\nadjusting the parameters of a pre-trained large models to adapt it to a\nspecific task or domain while minimizing the number of additional parameters\nintroduced or computational resources required. This approach is particularly\nimportant when dealing with large-scale language models with high parameter\ncounts, as fine-tuning these models from scratch can be computationally\nexpensive and resource-intensive, posing considerable challenges in the\nsupporting system platform design. In this survey, we present comprehensive\nstudies of various PEFT algorithms, examining their performance and\ncomputational overhead. Moreover, we provide an overview of applications\ndeveloped using different PEFT algorithms and discuss common techniques\nemployed to mitigate computation costs for PEFT. In addition to providing an\nextensive survey from an algorithmic standpoint, we also examine various\nreal-world system designs to investigate the implementation costs associated\nwith different PEFT approaches. This survey serves as an indispensable resource\nfor researchers aiming to understand both the PEFT algorithm and its system\nimplementation, offering detailed ......\n","authors":["Zeyu Han","Chao Gao","Jinyang Liu","Jeff Zhang","Sai Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14608v6.pdf","comment":"42 pages, 12 figures. Due to word limit, the abstract here is\n  truncated. The full abstract is available in the PDF"},{"id":"http://arxiv.org/abs/2407.09127v1","updated":"2024-07-12T09:46:26Z","published":"2024-07-12T09:46:26Z","title":"Robustness of Explainable Artificial Intelligence in Industrial Process\n  Modelling","summary":"  eXplainable Artificial Intelligence (XAI) aims at providing understandable\nexplanations of black box models. In this paper, we evaluate current XAI\nmethods by scoring them based on ground truth simulations and sensitivity\nanalysis. To this end, we used an Electric Arc Furnace (EAF) model to better\nunderstand the limits and robustness characteristics of XAI methods such as\nSHapley Additive exPlanations (SHAP), Local Interpretable Model-agnostic\nExplanations (LIME), as well as Averaged Local Effects (ALE) or Smooth\nGradients (SG) in a highly topical setting. These XAI methods were applied to\nvarious types of black-box models and then scored based on their correctness\ncompared to the ground-truth sensitivity of the data-generating processes using\na novel scoring evaluation methodology over a range of simulated additive\nnoise. The resulting evaluation shows that the capability of the Machine\nLearning (ML) models to capture the process accurately is, indeed, coupled with\nthe correctness of the explainability of the underlying data-generating\nprocess. We furthermore show the differences between XAI methods in their\nability to correctly predict the true sensitivity of the modeled industrial\nprocess.\n","authors":["Benedikt Kantz","Clemens Staudinger","Christoph Feilmayr","Johannes Wachlmayr","Alexander Haberl","Stefan Schuster","Franz Pernkopf"],"pdf_url":"https://arxiv.org/pdf/2407.09127v1.pdf","comment":"11 pages, 3 figures, accepted at the ICML'24 Workshop ML4MS"},{"id":"http://arxiv.org/abs/2302.01029v3","updated":"2024-07-12T09:46:14Z","published":"2023-02-02T11:46:23Z","title":"On Suppressing Range of Adaptive Stepsizes of Adam to Improve\n  Generalisation Performance","summary":"  A number of recent adaptive optimizers improve the generalisation performance\nof Adam by essentially reducing the variance of adaptive stepsizes to get\ncloser to SGD with momentum. Following the above motivation, we suppress the\nrange of the adaptive stepsizes of Adam by exploiting the layerwise gradient\nstatistics. In particular, at each iteration, we propose to perform three\nconsecutive operations on the second momentum v_t before using it to update a\nDNN model: (1): down-scaling, (2): epsilon-embedding, and (3):\ndown-translating. The resulting algorithm is referred to as SET-Adam, where SET\nis a brief notation of the three operations. The down-scaling operation on v_t\nis performed layerwise by making use of the angles between the layerwise\nsubvectors of v_t and the corresponding all-one subvectors. Extensive\nexperimental results show that SET-Adam outperforms eight adaptive optimizers\nwhen training transformers and LSTMs for NLP, and VGG and ResNet for image\nclassification over CIAF10 and CIFAR100 while matching the best performance of\nthe eight adaptive methods when training WGAN-GP models for image generation\ntasks. Furthermore, SET-Adam produces higher validation accuracies than Adam\nand AdaBelief for training ResNet18 over ImageNet.\n","authors":["Guoqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.01029v3.pdf","comment":"Accepted by ECML 2024. arXiv admin note: substantial text overlap\n  with arXiv:2203.13273"},{"id":"http://arxiv.org/abs/2407.09124v1","updated":"2024-07-12T09:38:47Z","published":"2024-07-12T09:38:47Z","title":"Decentralized multi-agent reinforcement learning algorithm using a\n  cluster-synchronized laser network","summary":"  Multi-agent reinforcement learning (MARL) studies crucial principles that are\napplicable to a variety of fields, including wireless networking and autonomous\ndriving. We propose a photonic-based decision-making algorithm to address one\nof the most fundamental problems in MARL, called the competitive multi-armed\nbandit (CMAB) problem. Our numerical simulations demonstrate that chaotic\noscillations and cluster synchronization of optically coupled lasers, along\nwith our proposed decentralized coupling adjustment, efficiently balance\nexploration and exploitation while facilitating cooperative decision-making\nwithout explicitly sharing information among agents. Our study demonstrates how\ndecentralized reinforcement learning can be achieved by exploiting complex\nphysical processes controlled by simple algorithms.\n","authors":["Shun Kotoku","Takatomo Mihana","AndrÃ© RÃ¶hm","Ryoichi Horisaki"],"pdf_url":"https://arxiv.org/pdf/2407.09124v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.09120v1","updated":"2024-07-12T09:35:25Z","published":"2024-07-12T09:35:25Z","title":"URRL-IMVC: Unified and Robust Representation Learning for Incomplete\n  Multi-View Clustering","summary":"  Incomplete multi-view clustering (IMVC) aims to cluster multi-view data that\nare only partially available. This poses two main challenges: effectively\nleveraging multi-view information and mitigating the impact of missing views.\nPrevailing solutions employ cross-view contrastive learning and missing view\nrecovery techniques. However, they either neglect valuable complementary\ninformation by focusing only on consensus between views or provide unreliable\nrecovered views due to the absence of supervision. To address these\nlimitations, we propose a novel Unified and Robust Representation Learning for\nIncomplete Multi-View Clustering (URRL-IMVC). URRL-IMVC directly learns a\nunified embedding that is robust to view missing conditions by integrating\ninformation from multiple views and neighboring samples. Firstly, to overcome\nthe limitations of cross-view contrastive learning, URRL-IMVC incorporates an\nattention-based auto-encoder framework to fuse multi-view information and\ngenerate unified embeddings. Secondly, URRL-IMVC directly enhances the\nrobustness of the unified embedding against view-missing conditions through KNN\nimputation and data augmentation techniques, eliminating the need for explicit\nmissing view recovery. Finally, incremental improvements are introduced to\nfurther enhance the overall performance, such as the Clustering Module and the\ncustomization of the Encoder. We extensively evaluate the proposed URRL-IMVC\nframework on various benchmark datasets, demonstrating its state-of-the-art\nperformance. Furthermore, comprehensive ablation studies are performed to\nvalidate the effectiveness of our design.\n","authors":["Ge Teng","Ting Mao","Chen Shen","Xiang Tian","Xuesong Liu","Yaowu Chen","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.09120v1.pdf","comment":"Accepted by ACM SIGKDD 2024"},{"id":"http://arxiv.org/abs/2401.10158v2","updated":"2024-07-12T09:27:57Z","published":"2024-01-15T13:00:48Z","title":"DISTINQT: A Distributed Privacy Aware Learning Framework for QoS\n  Prediction for Future Mobile and Wireless Networks","summary":"  Beyond 5G and 6G networks are expected to support new and challenging use\ncases and applications that depend on a certain level of Quality of Service\n(QoS) to operate smoothly. Predicting the QoS in a timely manner is of high\nimportance, especially for safety-critical applications as in the case of\nvehicular communications. Although until recent years the QoS prediction has\nbeen carried out by centralized Artificial Intelligence (AI) solutions, a\nnumber of privacy, computational, and operational concerns have emerged.\nAlternative solutions have surfaced (e.g. Split Learning, Federated Learning),\ndistributing AI tasks of reduced complexity across nodes, while preserving the\nprivacy of the data. However, new challenges rise when it comes to scalable\ndistributed learning approaches, taking into account the heterogeneous nature\nof future wireless networks. The current work proposes DISTINQT, a novel\nmulti-headed input privacy-aware distributed learning framework for QoS\nprediction. Our framework supports multiple heterogeneous nodes, in terms of\ndata types and model architectures, by sharing computations across them. This\nenables the incorporation of diverse knowledge into a sole learning process\nthat will enhance the robustness and generalization capabilities of the final\nQoS prediction model. DISTINQT also contributes to data privacy preservation by\nencoding any raw input data into highly complex, compressed, and irreversible\nlatent representations before any transmission. Evaluation results showcase\nthat DISTINQT achieves a statistically identical performance compared to its\ncentralized version, while also proving the validity of the privacy preserving\nclaims. DISTINQT manages to achieve a reduction in prediction error of up to\n65% on average against six state-of-the-art centralized baseline solutions\npresented in the Tele-Operated Driving use case.\n","authors":["Nikolaos Koursioumpas","Lina Magoula","Ioannis Stavrakakis","Nancy Alonistioti","M. A. Gutierrez-Estevez","Ramin Khalili"],"pdf_url":"https://arxiv.org/pdf/2401.10158v2.pdf","comment":"12 Pages Double Column, 10 Figures, (Revised Version) Submitted for\n  possible publication in the IEEE Transactions on Vehicular Technology (IEEE\n  TVT)"},{"id":"http://arxiv.org/abs/2407.09111v1","updated":"2024-07-12T09:24:34Z","published":"2024-07-12T09:24:34Z","title":"Inference Optimization of Foundation Models on AI Accelerators","summary":"  Powerful foundation models, including large language models (LLMs), with\nTransformer architectures have ushered in a new era of Generative AI across\nvarious industries. Industry and research community have witnessed a large\nnumber of new applications, based on those foundation models. Such applications\ninclude question and answer, customer services, image and video generation, and\ncode completions, among others. However, as the number of model parameters\nreaches to hundreds of billions, their deployment incurs prohibitive inference\ncosts and high latency in real-world scenarios. As a result, the demand for\ncost-effective and fast inference using AI accelerators is ever more higher. To\nthis end, our tutorial offers a comprehensive discussion on complementary\ninference optimization techniques using AI accelerators. Beginning with an\noverview of basic Transformer architectures and deep learning system\nframeworks, we deep dive into system optimization techniques for fast and\nmemory-efficient attention computations and discuss how they can be implemented\nefficiently on AI accelerators. Next, we describe architectural elements that\nare key for fast transformer inference. Finally, we examine various model\ncompression and fast decoding strategies in the same context.\n","authors":["Youngsuk Park","Kailash Budhathoki","Liangfu Chen","Jonas KÃ¼bler","Jiaji Huang","MatthÃ¤us Kleindessner","Jun Huan","Volkan Cevher","Yida Wang","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2407.09111v1.pdf","comment":"Tutorial published at KDD 2024. Camera-ready version"},{"id":"http://arxiv.org/abs/2407.09105v1","updated":"2024-07-12T09:10:37Z","published":"2024-07-12T09:10:37Z","title":"Enhancing Training Efficiency Using Packing with Flash Attention","summary":"  Padding is often used in tuning LLM models by adding special tokens to\nshorter training examples to match the length of the longest sequence in each\nbatch. While this ensures uniformity for batch processing, it introduces\ninefficiencies by including irrelevant padding tokens in the computation and\nwastes GPU resources. On the other hand, the Hugging Face SFT trainer offers\nthe option to use packing to combine multiple training examples up to the\nmaximum sequence length. This allows for maximal utilization of GPU resources.\nHowever, without proper masking of each packed training example, attention will\nnot be computed correctly when using SFT trainer. We enable and then analyse\npacking and Flash Attention with proper attention masking of each example and\nshow the benefits of this training paradigm.\n","authors":["Achintya Kundu","Rhui Dih Lee","Laura Wynter","Raghu Kiran Ganti"],"pdf_url":"https://arxiv.org/pdf/2407.09105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09104v1","updated":"2024-07-12T09:10:07Z","published":"2024-07-12T09:10:07Z","title":"UserBoost: Generating User-specific Synthetic Data for Faster Enrolment\n  into Behavioural Biometric Systems","summary":"  Behavioural biometric authentication systems entail an enrolment period that\nis burdensome for the user. In this work, we explore generating synthetic\ngestures from a few real user gestures with generative deep learning, with the\napplication of training a simple (i.e. non-deep-learned) authentication model.\nSpecifically, we show that utilising synthetic data alongside real data can\nreduce the number of real datapoints a user must provide to enrol into a\nbiometric system. To validate our methods, we use the publicly available\ndataset of WatchAuth, a system proposed in 2022 for authenticating smartwatch\npayments using the physical gesture of reaching towards a payment terminal. We\ndevelop a regularised autoencoder model for generating synthetic user-specific\nwrist motion data representing these physical gestures, and demonstrate the\ndiversity and fidelity of our synthetic gestures. We show that using synthetic\ngestures in training can improve classification ability for a real-world\nsystem. Through this technique we can reduce the number of gestures required to\nenrol a user into a WatchAuth-like system by more than 40% without negatively\nimpacting its error rates.\n","authors":["George Webber","Jack Sturgess","Ivan Martinovic"],"pdf_url":"https://arxiv.org/pdf/2407.09104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09096v1","updated":"2024-07-12T08:48:16Z","published":"2024-07-12T08:48:16Z","title":"STD-LLM: Understanding Both Spatial and Temporal Properties of\n  Spatial-Temporal Data with LLMs","summary":"  Spatial-temporal forecasting and imputation are important for real-world\ndynamic systems such as intelligent transportation, urban planning, and public\nhealth. Most existing methods are tailored for individual forecasting or\nimputation tasks but are not designed for both. Additionally, they are less\neffective for zero-shot and few-shot learning. While large language models\n(LLMs) have exhibited strong pattern recognition and reasoning abilities across\nvarious tasks, including few-shot and zero-shot learning, their development in\nunderstanding spatial-temporal data has been constrained by insufficient\nmodeling of complex correlations such as the temporal correlations, spatial\nconnectivity, non-pairwise and high-order spatial-temporal correlations within\ndata. In this paper, we propose STD-LLM for understanding both spatial and\ntemporal properties of \\underline{S}patial-\\underline{T}emporal\n\\underline{D}ata with \\underline{LLM}s, which is capable of implementing both\nspatial-temporal forecasting and imputation tasks. STD-LLM understands\nspatial-temporal correlations via explicitly designed spatial and temporal\ntokenizers as well as virtual nodes. Topology-aware node embeddings are\ndesigned for LLMs to comprehend and exploit the topology structure of data.\nAdditionally, to capture the non-pairwise and higher-order correlations, we\ndesign a hypergraph learning module for LLMs, which can enhance the overall\nperformance and improve efficiency. Extensive experiments demonstrate that\nSTD-LLM exhibits strong performance and generalization capabilities across the\nforecasting and imputation tasks on various datasets. Moreover, STD-LLM\nachieves promising results on both few-shot and zero-shot learning tasks.\n","authors":["Yiheng Huang","Xiaowei Mao","Shengnan Guo","Yubin Chen","Youfang Lin","Huaiyu Wan"],"pdf_url":"https://arxiv.org/pdf/2407.09096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09093v1","updated":"2024-07-12T08:42:58Z","published":"2024-07-12T08:42:58Z","title":"On Exact Bit-level Reversible Transformers Without Changing\n  Architectures","summary":"  In the literature, various reversible deep neural networks (DNN) models have\nbeen proposed to reduce memory consumption or improve data-throughput in the\ntraining process. However, almost all existing reversible DNNs either are\nconstrained to have special structures or are constructed by modifying the\noriginal DNN architectures considerably to enable reversibility. In this work,\nwe propose exact bit-level reversible transformers without changing the\narchitectures in the inference procedure. The basic idea is to first treat each\ntransformer block as the Euler integration approximation for solving an\nordinary differential equation (ODE) and then incorporate the technique of\nbidirectional integration approximation (BDIA) (see [26]) for BDIA-based\ndiffusion inversion) into the neural architecture together with activation\nquantization to make it exactly bit-level reversible, referred to as\nBDIA-transformer. In the training process, we let a hyper-parameter $\\gamma$ in\nBDIA-transformer randomly take one of the two values $\\{0.5, -0.5\\}$ per\ntransformer block for averaging two consecutive integration approximations,\nwhich regularizes the models for improving the validation accuracy.\nLight-weight side information per transformer block is required to be stored in\nthe forward process to account for binary quantization loss to enable exact\nbit-level reversibility. In the inference procedure, the expectation\n$\\mathbb{E}(\\gamma)=0$ is taken to make the resulting architectures of\nBDIA-transformer be identical to transformers up to activation quantization.\nEmpirical study indicates that BDIA-transformers outperform their original\ncounterparts notably due to the regularization effect of the $\\gamma$\nparameter.\n","authors":["Guoqiang Zhang","J. P. Lewis","W. B. Kleijn"],"pdf_url":"https://arxiv.org/pdf/2407.09093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06165v2","updated":"2024-07-12T08:41:21Z","published":"2024-02-09T03:48:20Z","title":"Learning Contrastive Feature Representations for Facial Action Unit\n  Detection","summary":"  Facial action unit (AU) detection has long encountered the challenge of\ndetecting subtle feature differences when AUs activate. Existing methods often\nrely on encoding pixel-level information of AUs, which not only encodes\nadditional redundant information but also leads to increased model complexity\nand limited generalizability. Additionally, the accuracy of AU detection is\nnegatively impacted by the class imbalance issue of each AU type, and the\npresence of noisy and false AU labels. In this paper, we introduce a novel\ncontrastive learning framework aimed for AU detection that incorporates both\nself-supervised and supervised signals, thereby enhancing the learning of\ndiscriminative features for accurate AU detection. To tackle the class\nimbalance issue, we employ a negative sample re-weighting strategy that adjusts\nthe step size of updating parameters for minority and majority class samples.\nMoreover, to address the challenges posed by noisy and false AU labels, we\nemploy a sampling technique that encompasses three distinct types of positive\nsample pairs. This enables us to inject self-supervised signals into the\nsupervised signal, effectively mitigating the adverse effects of noisy labels.\nOur experimental assessments, conducted on four widely-utilized benchmark\ndatasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance\nof our approach compared to state-of-the-art methods of AU detection. Our code\nis available at \\url{https://github.com/Ziqiao-Shang/AUNCE}.\n","authors":["Ziqiao Shang","Bin Liu","Fengmao Lv","Fei Teng","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2402.06165v2.pdf","comment":"13 pages, 17 figures, submitted to IEEE Transactions on Circuits and\n  Systems for Video Technology (TCSVT)"},{"id":"http://arxiv.org/abs/2407.04302v2","updated":"2024-07-12T08:35:33Z","published":"2024-07-05T07:10:26Z","title":"Fair Federated Data Clustering through Personalization: Bridging the Gap\n  between Diverse Data Distributions","summary":"  The rapid growth of data from edge devices has catalyzed the performance of\nmachine learning algorithms. However, the data generated resides at client\ndevices thus there are majorly two challenge faced by traditional machine\nlearning paradigms - centralization of data for training and secondly for most\nthe generated data the class labels are missing and there is very poor\nincentives to clients to manually label their data owing to high cost and lack\nof expertise. To overcome these issues, there have been initial attempts to\nhandle unlabelled data in a privacy preserving distributed manner using\nunsupervised federated data clustering. The goal is partition the data\navailable on clients into $k$ partitions (called clusters) without actual\nexchange of data. Most of the existing algorithms are highly dependent on data\ndistribution patterns across clients or are computationally expensive.\nFurthermore, due to presence of skewed nature of data across clients in most of\npractical scenarios existing models might result in clients suffering high\nclustering cost making them reluctant to participate in federated process. To\nthis, we are first to introduce the idea of personalization in federated\nclustering. The goal is achieve balance between achieving lower clustering cost\nand at same time achieving uniform cost across clients. We propose p-FClus that\naddresses these goal in a single round of communication between server and\nclients. We validate the efficacy of p-FClus against variety of federated\ndatasets showcasing it's data independence nature, applicability to any finite\n$\\ell$-norm, while simultaneously achieving lower cost and variance.\n","authors":["Shivam Gupta"," Tarushi","Tsering Wangzes","Shweta Jain"],"pdf_url":"https://arxiv.org/pdf/2407.04302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12231v3","updated":"2024-07-12T08:26:25Z","published":"2024-02-19T15:36:36Z","title":"Diffusion Tempering Improves Parameter Estimation with Probabilistic\n  Integrators for Ordinary Differential Equations","summary":"  Ordinary differential equations (ODEs) are widely used to describe dynamical\nsystems in science, but identifying parameters that explain experimental\nmeasurements is challenging. In particular, although ODEs are differentiable\nand would allow for gradient-based parameter optimization, the nonlinear\ndynamics of ODEs often lead to many local minima and extreme sensitivity to\ninitial conditions. We therefore propose diffusion tempering, a novel\nregularization technique for probabilistic numerical methods which improves\nconvergence of gradient-based parameter optimization in ODEs. By iteratively\nreducing a noise parameter of the probabilistic integrator, the proposed method\nconverges more reliably to the true parameters. We demonstrate that our method\nis effective for dynamical systems of different complexity and show that it\nobtains reliable parameter estimates for a Hodgkin-Huxley model with a\npractically relevant number of parameters.\n","authors":["Jonas Beck","Nathanael Bosch","Michael Deistler","Kyra L. Kadhim","Jakob H. Macke","Philipp Hennig","Philipp Berens"],"pdf_url":"https://arxiv.org/pdf/2402.12231v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09087v1","updated":"2024-07-12T08:25:31Z","published":"2024-07-12T08:25:31Z","title":"On the Role of Discrete Tokenization in Visual Representation Learning","summary":"  In the realm of self-supervised learning (SSL), masked image modeling (MIM)\nhas gained popularity alongside contrastive learning methods. MIM involves\nreconstructing masked regions of input images using their unmasked portions. A\nnotable subset of MIM methodologies employs discrete tokens as the\nreconstruction target, but the theoretical underpinnings of this choice remain\nunderexplored. In this paper, we explore the role of these discrete tokens,\naiming to unravel their benefits and limitations. Building upon the connection\nbetween MIM and contrastive learning, we provide a comprehensive theoretical\nunderstanding on how discrete tokenization affects the model's generalization\ncapabilities. Furthermore, we propose a novel metric named TCAS, which is\nspecifically designed to assess the effectiveness of discrete tokens within the\nMIM framework. Inspired by this metric, we contribute an innovative tokenizer\ndesign and propose a corresponding MIM method named ClusterMIM. It demonstrates\nsuperior performance on a variety of benchmark datasets and ViT backbones. Code\nis available at https://github.com/PKU-ML/ClusterMIM.\n","authors":["Tianqi Du","Yifei Wang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09087v1.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2406.14393v2","updated":"2024-07-12T08:15:45Z","published":"2024-06-20T15:12:27Z","title":"Jailbreaking as a Reward Misspecification Problem","summary":"  The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. We introduce a metric ReGap to quantify the extent of reward\nmisspecification and demonstrate its effectiveness and robustness in detecting\nharmful backdoor prompts. Building upon these insights, we present ReMiss, a\nsystem for automated red teaming that generates adversarial prompts against\nvarious target aligned LLMs. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark while preserving the human readability of the\ngenerated prompts. Detailed analysis highlights the unique advantages brought\nby the proposed reward misspecification objective compared to previous methods.\n","authors":["Zhihui Xie","Jiahui Gao","Lei Li","Zhenguo Li","Qi Liu","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2406.14393v2.pdf","comment":"github url added"},{"id":"http://arxiv.org/abs/2406.02609v2","updated":"2024-07-12T08:15:22Z","published":"2024-06-03T04:09:36Z","title":"Less is More: Pseudo-Label Filtering for Continual Test-Time Adaptation","summary":"  Continual Test-Time Adaptation (CTTA) aims to adapt a pre-trained model to a\nsequence of target domains during the test phase without accessing the source\ndata. To adapt to unlabeled data from unknown domains, existing methods rely on\nconstructing pseudo-labels for all samples and updating the model through\nself-training. However, these pseudo-labels often involve noise, leading to\ninsufficient adaptation. To improve the quality of pseudo-labels, we propose a\npseudo-label selection method for CTTA, called Pseudo Labeling Filter (PLF).\nThe key idea of PLF is to keep selecting appropriate thresholds for\npseudo-labels and identify reliable ones for self-training. Specifically, we\npresent three principles for setting thresholds during continuous domain\nlearning, including initialization, growth and diversity. Based on these\nprinciples, we design Self-Adaptive Thresholding to filter pseudo-labels.\nAdditionally, we introduce a Class Prior Alignment (CPA) method to encourage\nthe model to make diverse predictions for unknown domain samples. Through\nextensive experiments, PLF outperforms current state-of-the-art methods,\nproving its effectiveness in CTTA.\n","authors":["Jiayao Tan","Fan Lyu","Chenggong Ni","Tingliang Feng","Fuyuan Hu","Zhang Zhang","Shaochuang Zhao","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.02609v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2310.03335 by other authors"},{"id":"http://arxiv.org/abs/2407.09064v1","updated":"2024-07-12T07:34:10Z","published":"2024-07-12T07:34:10Z","title":"Multi-Modal Dataset Creation for Federated~Learning with DICOM\n  Structured Reports","summary":"  Purpose: Federated training is often hindered by heterogeneous datasets due\nto divergent data storage options, inconsistent naming schemes, varied\nannotation procedures, and disparities in label quality. This is particularly\nevident in the emerging multi-modal learning paradigms, where dataset\nharmonization including a uniform data representation and filtering options are\nof paramount importance.\n  Methods: DICOM structured reports enable the standardized linkage of\narbitrary information beyond the imaging domain and can be used within Python\ndeep learning pipelines with highdicom. Building on this, we developed an open\nplatform for data integration and interactive filtering capabilities that\nsimplifies the process of assembling multi-modal datasets.\n  Results: In this study, we extend our prior work by showing its applicability\nto more and divergent data types, as well as streamlining datasets for\nfederated training within an established consortium of eight university\nhospitals in Germany. We prove its concurrent filtering ability by creating\nharmonized multi-modal datasets across all locations for predicting the outcome\nafter minimally invasive heart valve replacement. The data includes DICOM data\n(i.e. computed tomography images, electrocardiography scans) as well as\nannotations (i.e. calcification segmentations, pointsets and pacemaker\ndependency), and metadata (i.e. prosthesis and diagnoses).\n  Conclusion: Structured reports bridge the traditional gap between imaging\nsystems and information systems. Utilizing the inherent DICOM reference system\narbitrary data types can be queried concurrently to create meaningful cohorts\nfor clinical studies. The graphical interface as well as example structured\nreport templates will be made publicly available.\n","authors":["Malte TÃ¶lle","Lukas Burger","Halvar Kelm","Florian AndrÃ©","Peter Bannas","Gerhard Diller","Norbert Frey","Philipp Garthe","Stefan GroÃ","Anja Hennemuth","Lars Kaderali","Nina KrÃ¼ger","Andreas Leha","Simon Martin","Alexander Meyer","Eike Nagel","Stefan Orwat","Clemens Scherer","Moritz Seiffert","Jan Moritz Seliger","Stefan Simm","Tim Friede","Tim Seidler","Sandy Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2407.09064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18241v2","updated":"2024-07-12T07:30:00Z","published":"2024-03-27T04:09:34Z","title":"NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,\n  Reconstruction, and Generation","summary":"  3D shape generation aims to produce innovative 3D content adhering to\nspecific conditions and constraints. Existing methods often decompose 3D shapes\ninto a sequence of localized components, treating each element in isolation\nwithout considering spatial consistency. As a result, these approaches exhibit\nlimited versatility in 3D data representation and shape generation, hindering\ntheir ability to generate highly diverse 3D shapes that comply with the\nspecified constraints. In this paper, we introduce a novel spatial-aware 3D\nshape generation framework that leverages 2D plane representations for enhanced\n3D shape modeling. To ensure spatial coherence and reduce memory usage, we\nincorporate a hybrid shape representation technique that directly learns a\ncontinuous signed distance field representation of the 3D shape using\northogonal 2D planes. Additionally, we meticulously enforce spatial\ncorrespondences across distinct planes using a transformer-based autoencoder\nstructure, promoting the preservation of spatial relationships in the generated\n3D shapes. This yields an algorithm that consistently outperforms\nstate-of-the-art 3D shape generation methods on various tasks, including\nunconditional shape generation, multi-modal shape completion, single-view\nreconstruction, and text-to-shape synthesis. Our project page is available at\nhttps://weizheliu.github.io/NeuSDFusion/ .\n","authors":["Ruikai Cui","Weizhe Liu","Weixuan Sun","Senbo Wang","Taizhang Shang","Yang Li","Xibin Song","Han Yan","Zhennan Wu","Shenzhou Chen","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18241v2.pdf","comment":"ECCV 2024, project page: https://weizheliu.github.io/NeuSDFusion/"},{"id":"http://arxiv.org/abs/2407.09061v1","updated":"2024-07-12T07:29:08Z","published":"2024-07-12T07:29:08Z","title":"Spectral Self-supervised Feature Selection","summary":"  Choosing a meaningful subset of features from high-dimensional observations\nin unsupervised settings can greatly enhance the accuracy of downstream\nanalysis, such as clustering or dimensionality reduction, and provide valuable\ninsights into the sources of heterogeneity in a given dataset. In this paper,\nwe propose a self-supervised graph-based approach for unsupervised feature\nselection. Our method's core involves computing robust pseudo-labels by\napplying simple processing steps to the graph Laplacian's eigenvectors. The\nsubset of eigenvectors used for computing pseudo-labels is chosen based on a\nmodel stability criterion. We then measure the importance of each feature by\ntraining a surrogate model to predict the pseudo-labels from the observations.\nOur approach is shown to be robust to challenging scenarios, such as the\npresence of outliers and complex substructures. We demonstrate the\neffectiveness of our method through experiments on real-world datasets, showing\nits robustness across multiple domains, particularly its effectiveness on\nbiological datasets.\n","authors":["Daniel Segal","Ofir Lindenbaum","Ariel Jaffe"],"pdf_url":"https://arxiv.org/pdf/2407.09061v1.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.14794v3","updated":"2024-07-12T07:28:55Z","published":"2024-06-20T23:51:32Z","title":"ImageFlowNet: Forecasting Multiscale Trajectories of Disease Progression\n  with Irregularly-Sampled Longitudinal Medical Images","summary":"  The forecasting of disease progression from images is a holy grail for\nclinical decision making. However, this task is complicated by the inherent\nhigh dimensionality, temporal sparsity and sampling irregularity in\nlongitudinal image acquisitions. Existing methods often rely on extracting\nhand-crafted features and performing time-series analysis in this vector space,\nleading to a loss of rich spatial information within the images. To overcome\nthese challenges, we introduce ImageFlowNet, a novel framework that learns\nlatent-space flow fields that evolve multiscale representations in joint\nembedding spaces using neural ODEs and SDEs to model disease progression in the\nimage domain. Notably, ImageFlowNet learns multiscale joint representation\nspaces by combining cohorts of patients together so that information can be\ntransferred between the patient samples. The dynamics then provide plausible\ntrajectories of progression, with the SDE providing alternative trajectories\nfrom the same starting point. We provide theoretical insights that support our\nformulation of ODEs, and motivate our regularizations involving high-level\nvisual features, latent space organization, and trajectory smoothness. We then\ndemonstrate ImageFlowNet's effectiveness through empirical evaluations on three\nlongitudinal medical image datasets depicting progression in retinal geographic\natrophy, multiple sclerosis, and glioblastoma.\n","authors":["Chen Liu","Ke Xu","Liangbo L. Shen","Guillaume Huguet","Zilong Wang","Alexander Tong","Danilo Bzdok","Jay Stewart","Jay C. Wang","Lucian V. Del Priore","Smita Krishnaswamy"],"pdf_url":"https://arxiv.org/pdf/2406.14794v3.pdf","comment":"Fixed some typos. Merged multibib"},{"id":"http://arxiv.org/abs/2407.09055v1","updated":"2024-07-12T07:22:45Z","published":"2024-07-12T07:22:45Z","title":"Advanced Graph Clustering Methods: A Comprehensive and In-Depth Analysis","summary":"  Graph clustering, which aims to divide a graph into several homogeneous\ngroups, is a critical area of study with applications that span various fields\nsuch as social network analysis, bioinformatics, and image segmentation. This\npaper explores both traditional and more recent approaches to graph clustering.\nFirstly, key concepts and definitions in graph theory are introduced. The\nbackground section covers essential topics, including graph Laplacians and the\nintegration of Deep Learning in graph analysis. The paper then delves into\ntraditional clustering methods, including Spectral Clustering and the Leiden\nalgorithm. Following this, state-of-the-art clustering techniques that leverage\ndeep learning are examined. A comprehensive comparison of these methods is made\nthrough experiments. The paper concludes with a discussion of the practical\napplications of graph clustering and potential future research directions.\n","authors":["TimothÃ© Watteau","Aubin Bonnefoy","Simon Illouz-Laurent","Joaquim Jusseau","Serge Iovleff"],"pdf_url":"https://arxiv.org/pdf/2407.09055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09050v1","updated":"2024-07-12T07:18:05Z","published":"2024-07-12T07:18:05Z","title":"Refusing Safe Prompts for Multi-modal Large Language Models","summary":"  Multimodal large language models (MLLMs) have become the cornerstone of\ntoday's generative AI ecosystem, sparking intense competition among tech giants\nand startups. In particular, an MLLM generates a text response given a prompt\nconsisting of an image and a question. While state-of-the-art MLLMs use safety\nfilters and alignment techniques to refuse unsafe prompts, in this work, we\nintroduce MLLM-Refusal, the first method that induces refusals for safe\nprompts. In particular, our MLLM-Refusal optimizes a nearly-imperceptible\nrefusal perturbation and adds it to an image, causing target MLLMs to likely\nrefuse a safe prompt containing the perturbed image and a safe question.\nSpecifically, we formulate MLLM-Refusal as a constrained optimization problem\nand propose an algorithm to solve it. Our method offers competitive advantages\nfor MLLM model providers by potentially disrupting user experiences of\ncompeting MLLMs, since competing MLLM's users will receive unexpected refusals\nwhen they unwittingly use these perturbed images in their prompts. We evaluate\nMLLM-Refusal on four MLLMs across four datasets, demonstrating its\neffectiveness in causing competing MLLMs to refuse safe prompts while not\naffecting non-competing MLLMs. Furthermore, we explore three potential\ncountermeasures -- adding Gaussian noise, DiffPure, and adversarial training.\nOur results show that they are insufficient: though they can mitigate\nMLLM-Refusal's effectiveness, they also sacrifice the accuracy and/or\nefficiency of the competing MLLM. The code is available at\nhttps://github.com/Sadcardation/MLLM-Refusal.\n","authors":["Zedian Shao","Hongbin Liu","Yuepeng Hu","Neil Zhenqiang Gong"],"pdf_url":"https://arxiv.org/pdf/2407.09050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12945v2","updated":"2024-07-12T07:16:33Z","published":"2024-06-18T07:27:38Z","title":"Under the Hood of Tabular Data Generation Models: the Strong Impact of\n  Hyperparameter Tuning","summary":"  We investigate the impact of dataset-specific hyperparameter, feature\nencoding, and architecture tuning on five recent model families for tabular\ndata generation through an extensive benchmark on 16 datasets. This study\naddresses the practical need for a unified evaluation of models that fully\nconsiders hyperparameter optimization. Additionally, we propose a reduced\nsearch space for each model that allows for quick optimization, achieving\nnearly equivalent performance at a significantly lower cost.Our benchmark\ndemonstrates that, for most models, large-scale dataset-specific tuning\nsubstantially improves performance compared to the original configurations.\nFurthermore, we confirm that diffusion-based models generally outperform other\nmodels on tabular data. However, this advantage is not significant when the\nentire tuning and training process is restricted to the same GPU budget for all\nmodels.\n","authors":["G. Charbel N. Kindji","Lina Maria Rojas-Barahona","Elisa Fromont","Tanguy Urvoy"],"pdf_url":"https://arxiv.org/pdf/2406.12945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09039v1","updated":"2024-07-12T07:04:06Z","published":"2024-07-12T07:04:06Z","title":"Overcoming Catastrophic Forgetting in Tabular Data Classification: A\n  Pseudorehearsal-based approach","summary":"  Continual learning (CL) poses the important challenge of adapting to evolving\ndata distributions without forgetting previously acquired knowledge while\nconsolidating new knowledge. In this paper, we introduce a new methodology,\ncoined as Tabular-data Rehearsal-based Incremental Lifelong Learning framework\n(TRIL3), designed to address the phenomenon of catastrophic forgetting in\ntabular data classification problems. TRIL3 uses the prototype-based\nincremental generative model XuILVQ to generate synthetic data to preserve old\nknowledge and the DNDF algorithm, which was modified to run in an incremental\nway, to learn classification tasks for tabular data, without storing old\nsamples. After different tests to obtain the adequate percentage of synthetic\ndata and to compare TRIL3 with other CL available proposals, we can conclude\nthat the performance of TRIL3 outstands other options in the literature using\nonly 50% of synthetic data.\n","authors":["Pablo GarcÃ­a-Santaclara","Bruno FernÃ¡ndez-Castro","Rebeca P. DÃ­az-Redondo"],"pdf_url":"https://arxiv.org/pdf/2407.09039v1.pdf","comment":"11 pages, 4 tables, 3 figures"},{"id":"http://arxiv.org/abs/2311.17833v3","updated":"2024-07-12T06:53:50Z","published":"2023-11-29T17:35:29Z","title":"DiG-IN: Diffusion Guidance for Investigating Networks -- Uncovering\n  Classifier Differences Neuron Visualisations and Visual Counterfactual\n  Explanations","summary":"  While deep learning has led to huge progress in complex image classification\ntasks like ImageNet, unexpected failure modes, e.g. via spurious features, call\ninto question how reliably these classifiers work in the wild. Furthermore, for\nsafety-critical tasks the black-box nature of their decisions is problematic,\nand explanations or at least methods which make decisions plausible are needed\nurgently. In this paper, we address these problems by generating images that\noptimize a classifier-derived objective using a framework for guided image\ngeneration. We analyze the decisions of image classifiers by visual\ncounterfactual explanations (VCEs), detection of systematic mistakes by\nanalyzing images where classifiers maximally disagree, and visualization of\nneurons and spurious features. In this way, we validate existing observations,\ne.g. the shape bias of adversarially robust models, as well as novel failure\nmodes, e.g. systematic errors of zero-shot CLIP classifiers. Moreover, our VCEs\noutperform previous work while being more versatile.\n","authors":["Maximilian Augustin","Yannic Neuhaus","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2311.17833v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2407.09032v1","updated":"2024-07-12T06:48:00Z","published":"2024-07-12T06:48:00Z","title":"DRM Revisited: A Complete Error Analysis","summary":"  In this work, we address a foundational question in the theoretical analysis\nof the Deep Ritz Method (DRM) under the over-parameteriztion regime: Given a\ntarget precision level, how can one determine the appropriate number of\ntraining samples, the key architectural parameters of the neural networks, the\nstep size for the projected gradient descent optimization procedure, and the\nrequisite number of iterations, such that the output of the gradient descent\nprocess closely approximates the true solution of the underlying partial\ndifferential equation to the specified precision?\n","authors":["Yuling Jiao","Ruoxuan Li","Peiying Wu","Jerry Zhijian Yang","Pingwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.09032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09026v1","updated":"2024-07-12T06:34:24Z","published":"2024-07-12T06:34:24Z","title":"HPC: Hierarchical Progressive Coding Framework for Volumetric Video","summary":"  Volumetric video based on Neural Radiance Field (NeRF) holds vast potential\nfor various 3D applications, but its substantial data volume poses significant\nchallenges for compression and transmission. Current NeRF compression lacks the\nflexibility to adjust video quality and bitrate within a single model for\nvarious network and device capacities. To address these issues, we propose HPC,\na novel hierarchical progressive volumetric video coding framework achieving\nvariable bitrate using a single model. Specifically, HPC introduces a\nhierarchical representation with a multi-resolution residual radiance field to\nreduce temporal redundancy in long-duration sequences while simultaneously\ngenerating various levels of detail. Then, we propose an end-to-end progressive\nlearning approach with a multi-rate-distortion loss function to jointly\noptimize both hierarchical representation and compression. Our HPC trained only\nonce can realize multiple compression levels, while the current methods need to\ntrain multiple fixed-bitrate models for different rate-distortion (RD)\ntradeoffs. Extensive experiments demonstrate that HPC achieves flexible quality\nlevels with variable bitrate by a single model and exhibits competitive RD\nperformance, even outperforming fixed-bitrate models across various datasets.\n","authors":["Zihan Zheng","Houqiang Zhong","Qiang Hu","Xiaoyun Zhang","Li Song","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09026v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.09024v1","updated":"2024-07-12T06:32:36Z","published":"2024-07-12T06:32:36Z","title":"Aligning Diffusion Behaviors with Q-functions for Efficient Continuous\n  Control","summary":"  Drawing upon recent advances in language model alignment, we formulate\noffline Reinforcement Learning as a two-stage optimization problem: First\npretraining expressive generative policies on reward-free behavior datasets,\nthen fine-tuning these policies to align with task-specific annotations like\nQ-values. This strategy allows us to leverage abundant and diverse behavior\ndata to enhance generalization and enable rapid adaptation to downstream tasks\nusing minimal annotations. In particular, we introduce Efficient Diffusion\nAlignment (EDA) for solving continuous control problems. EDA utilizes diffusion\nmodels for behavior modeling. However, unlike previous approaches, we represent\ndiffusion policies as the derivative of a scalar neural network with respect to\naction inputs. This representation is critical because it enables direct\ndensity calculation for diffusion models, making them compatible with existing\nLLM alignment theories. During policy fine-tuning, we extend preference-based\nalignment methods like Direct Preference Optimization (DPO) to align diffusion\nbehaviors with continuous Q-functions. Our evaluation on the D4RL benchmark\nshows that EDA exceeds all baseline methods in overall performance. Notably,\nEDA maintains about 95\\% of performance and still outperforms several baselines\ngiven only 1\\% of Q-labelled data during fine-tuning.\n","authors":["Huayu Chen","Kaiwen Zheng","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.09024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09017v1","updated":"2024-07-12T06:10:01Z","published":"2024-07-12T06:10:01Z","title":"AI-Driven Guided Response for Security Operation Centers with Microsoft\n  Copilot for Security","summary":"  Security operation centers contend with a constant stream of security\nincidents, ranging from straightforward to highly complex. To address this, we\ndeveloped Copilot Guided Response (CGR), an industry-scale ML architecture that\nguides security analysts across three key tasks -- (1) investigation, providing\nessential historical context by identifying similar incidents; (2) triaging to\nascertain the nature of the incident -- whether it is a true positive, false\npositive, or benign positive; and (3) remediation, recommending tailored\ncontainment actions. CGR is integrated into the Microsoft Defender XDR product\nand deployed worldwide, generating millions of recommendations across thousands\nof customers. Our extensive evaluation, incorporating internal evaluation,\ncollaboration with security experts, and customer feedback, demonstrates that\nCGR delivers high-quality recommendations across all three tasks. We provide a\ncomprehensive overview of the CGR architecture, setting a precedent as the\nfirst cybersecurity company to openly discuss these capabilities in such depth.\nAdditionally, we GUIDE, the largest public collection of real-world security\nincidents, spanning 13M evidences across 1M annotated incidents. By enabling\nresearchers and practitioners to conduct research on real-world data, GUIDE\nadvances the state of cybersecurity and supports the development of\nnext-generation machine learning systems.\n","authors":["Scott Freitas","Jovan Kalajdjieski","Amir Gharib","Rob McCann"],"pdf_url":"https://arxiv.org/pdf/2407.09017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06287v2","updated":"2024-07-12T06:08:09Z","published":"2024-06-10T14:11:15Z","title":"VS-PINN: A fast and efficient training of physics-informed neural\n  networks using variable-scaling methods for solving PDEs with stiff behavior","summary":"  Physics-informed neural networks (PINNs) have recently emerged as a promising\nway to compute the solutions of partial differential equations (PDEs) using\ndeep neural networks. However, despite their significant success in various\nfields, it remains unclear in many aspects how to effectively train PINNs if\nthe solutions of PDEs exhibit stiff behaviors or high frequencies. In this\npaper, we propose a new method for training PINNs using variable-scaling\ntechniques. This method is simple and it can be applied to a wide range of\nproblems including PDEs with rapidly-varying solutions. Throughout various\nnumerical experiments, we will demonstrate the effectiveness of the proposed\nmethod for these problems and confirm that it can significantly improve the\ntraining efficiency and performance of PINNs. Furthermore, based on the\nanalysis of the neural tangent kernel (NTK), we will provide theoretical\nevidence for this phenomenon and show that our methods can indeed improve the\nperformance of PINNs.\n","authors":["Seungchan Ko","Sang Hyeon Park"],"pdf_url":"https://arxiv.org/pdf/2406.06287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17036v2","updated":"2024-07-12T06:04:47Z","published":"2024-01-30T14:16:02Z","title":"Data organization limits the predictability of binary classification","summary":"  The structure of data organization is widely recognized as having a\nsubstantial influence on the efficacy of machine learning algorithms,\nparticularly in binary classification tasks. Our research provides a\ntheoretical framework suggesting that the maximum potential of binary\nclassifiers on a given dataset is primarily constrained by the inherent\nqualities of the data. Through both theoretical reasoning and empirical\nexamination, we employed standard objective functions, evaluative metrics, and\nbinary classifiers to arrive at two principal conclusions. Firstly, we show\nthat the theoretical upper bound of binary classification performance on actual\ndatasets can be theoretically attained. This upper boundary represents a\ncalculable equilibrium between the learning loss and the metric of evaluation.\nSecondly, we have computed the precise upper bounds for three commonly used\nevaluation metrics, uncovering a fundamental uniformity with our overarching\nthesis: the upper bound is intricately linked to the dataset's characteristics,\nindependent of the classifier in use. Additionally, our subsequent analysis\nuncovers a detailed relationship between the upper limit of performance and the\nlevel of class overlap within the binary classification data. This relationship\nis instrumental for pinpointing the most effective feature subsets for use in\nfeature engineering.\n","authors":["Fei Jing","Zi-Ke Zhang","Yi-Cheng Zhang","Qingpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.17036v2.pdf","comment":"98 pages, 69 figures"},{"id":"http://arxiv.org/abs/2407.09013v1","updated":"2024-07-12T06:03:38Z","published":"2024-07-12T06:03:38Z","title":"Procedural Content Generation via Generative Artificial Intelligence","summary":"  The attempt to utilize machine learning in PCG has been made in the past. In\nthis survey paper, we investigate how generative artificial intelligence (AI),\nwhich saw a significant increase in interest in the mid-2010s, is being used\nfor PCG. We review applications of generative AI for the creation of various\ntypes of content, including terrains, items, and even storylines. While\ngenerative AI is effective for PCG, one significant issues it faces is that\nbuilding high-performance generative AI requires vast amounts of training data.\nBecause content generally highly customized, domain-specific training data is\nscarce, and straightforward approaches to generative AI models may not work\nwell. For PCG research to advance further, issues related to limited training\ndata must be overcome. Thus, we also give special consideration to research\nthat addresses the challenges posed by limited training data.\n","authors":["Xinyu Mao","Wanli Yu","Kazunori D Yamada","Michael R. Zielewski"],"pdf_url":"https://arxiv.org/pdf/2407.09013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09011v1","updated":"2024-07-12T06:01:51Z","published":"2024-07-12T06:01:51Z","title":"One Stone, Four Birds: A Comprehensive Solution for QA System Using\n  Supervised Contrastive Learning","summary":"  This paper presents a novel and comprehensive solution to enhance both the\nrobustness and efficiency of question answering (QA) systems through supervised\ncontrastive learning (SCL). Training a high-performance QA system has become\nstraightforward with pre-trained language models, requiring only a small amount\nof data and simple fine-tuning. However, despite recent advances, existing QA\nsystems still exhibit significant deficiencies in functionality and training\nefficiency. We address the functionality issue by defining four key tasks: user\ninput intent classification, out-of-domain input detection, new intent\ndiscovery, and continual learning. We then leverage a unified SCL-based\nrepresentation learning method to efficiently build an intra-class compact and\ninter-class scattered feature space, facilitating both known intent\nclassification and unknown intent detection and discovery. Consequently, with\nminimal additional tuning on downstream tasks, our approach significantly\nimproves model efficiency and achieves new state-of-the-art performance across\nall tasks.\n","authors":["Bo Wang","Tsunenori Mine"],"pdf_url":"https://arxiv.org/pdf/2407.09011v1.pdf","comment":"14 pages, under review"},{"id":"http://arxiv.org/abs/2309.11798v5","updated":"2024-07-12T05:55:47Z","published":"2023-09-21T06:04:06Z","title":"A Comprehensive Review of Community Detection in Graphs","summary":"  The study of complex networks has significantly advanced our understanding of\ncommunity structures which serves as a crucial feature of real-world graphs.\nDetecting communities in graphs is a challenging problem with applications in\nsociology, biology, and computer science. Despite the efforts of an\ninterdisciplinary community of scientists, a satisfactory solution to this\nproblem has not yet been achieved. This review article delves into the topic of\ncommunity detection in graphs, which serves as a thorough exposition of various\ncommunity detection methods from perspectives of modularity-based method,\nspectral clustering, probabilistic modelling, and deep learning. Along with the\nmethods, a new community detection method designed by us is also presented.\nAdditionally, the performance of these methods on the datasets with and without\nground truth is compared. In conclusion, this comprehensive review provides a\ndeep understanding of community detection in graphs.\n","authors":["Jiakang Li","Songning Lai","Zhihao Shuai","Yuan Tan","Yifan Jia","Mianyang Yu","Zichen Song","Xiaokang Peng","Ziyang Xu","Yongxin Ni","Haifeng Qiu","Jiayu Yang","Yutong Liu","Yonggang Lu"],"pdf_url":"https://arxiv.org/pdf/2309.11798v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19449v2","updated":"2024-07-12T05:10:32Z","published":"2024-02-29T18:47:52Z","title":"Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent\n  on Language Models","summary":"  Adam has been shown to outperform gradient descent on large language models\nby a larger margin than on other tasks, but it is unclear why. We show that a\nkey factor in this performance gap is the heavy-tailed class imbalance found in\nlanguage tasks. When trained with gradient descent, the loss of infrequent\nwords decreases more slowly than the loss of frequent ones. This leads to a\nslow decrease on the average loss as most samples come from infrequent words.\nOn the other hand, Adam and sign-based methods are less sensitive to this\nproblem. To establish that this behavior is caused by class imbalance, we show\nempirically that it can be reproduced across architectures and data types, on\nlanguage transformers, vision CNNs, and linear models. On a linear model with\ncross-entropy loss, we show that class imbalance leads to imbalanced,\ncorrelated gradients and Hessians that have been hypothesized to benefit Adam.\nWe also prove that, in continuous time, gradient descent converges slowly on\nlow-frequency classes while sign descent does not.\n","authors":["Frederik Kunstner","Robin Yadav","Alan Milligan","Mark Schmidt","Alberto Bietti"],"pdf_url":"https://arxiv.org/pdf/2402.19449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13468v2","updated":"2024-07-12T04:44:39Z","published":"2024-02-21T01:54:58Z","title":"STENCIL: Submodular Mutual Information Based Weak Supervision for\n  Cold-Start Active Learning","summary":"  As supervised fine-tuning of pre-trained models within NLP applications\nincreases in popularity, larger corpora of annotated data are required,\nespecially with increasing parameter counts in large language models. Active\nlearning, which attempts to mine and annotate unlabeled instances to improve\nmodel performance maximally fast, is a common choice for reducing the\nannotation cost; however, most methods typically ignore class imbalance and\neither assume access to initial annotated data or require multiple rounds of\nactive learning selection before improving rare classes. We present STENCIL,\nwhich utilizes a set of text exemplars and the recently proposed submodular\nmutual information to select a set of weakly labeled rare-class instances that\nare then strongly labeled by an annotator. We show that STENCIL improves\noverall accuracy by $10\\%-18\\%$ and rare-class F-1 score by $17\\%-40\\%$ on\nmultiple text classification datasets over common active learning methods\nwithin the class-imbalanced cold-start setting.\n","authors":["Nathan Beck","Adithya Iyer","Rishabh Iyer"],"pdf_url":"https://arxiv.org/pdf/2402.13468v2.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.08987v1","updated":"2024-07-12T04:44:29Z","published":"2024-07-12T04:44:29Z","title":"Parameter inference from a non-stationary unknown process","summary":"  Non-stationary systems are found throughout the world, from climate patterns\nunder the influence of variation in carbon dioxide concentration, to brain\ndynamics driven by ascending neuromodulation. Accordingly, there is a need for\nmethods to analyze non-stationary processes, and yet most time-series analysis\nmethods that are used in practice, on important problems across science and\nindustry, make the simplifying assumption of stationarity. One important\nproblem in the analysis of non-stationary systems is the problem class that we\nrefer to as Parameter Inference from a Non-stationary Unknown Process (PINUP).\nGiven an observed time series, this involves inferring the parameters that\ndrive non-stationarity of the time series, without requiring knowledge or\ninference of a mathematical model of the underlying system. Here we review and\nunify a diverse literature of algorithms for PINUP. We formulate the problem,\nand categorize the various algorithmic contributions. This synthesis will allow\nresearchers to identify gaps in the literature and will enable systematic\ncomparisons of different methods. We also demonstrate that the most common\nsystems that existing methods are tested on - notably the non-stationary Lorenz\nprocess and logistic map - are surprisingly easy to perform well on using\nsimple statistical features like windowed mean and variance, undermining the\npractice of using good performance on these systems as evidence of algorithmic\nperformance. We then identify more challenging problems that many existing\nmethods perform poorly on and which can be used to drive methodological\nadvances in the field. Our results unify disjoint scientific contributions to\nanalyzing non-stationary systems and suggest new directions for progress on the\nPINUP problem and the broader study of non-stationary phenomena.\n","authors":["Kieran S. Owens","Ben D. Fulcher"],"pdf_url":"https://arxiv.org/pdf/2407.08987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12986v2","updated":"2024-07-12T04:43:48Z","published":"2024-03-04T06:43:16Z","title":"BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced\n  Feature-Level Contrastive Learning","summary":"  Semi-supervised Learning (SSL) reduces the need for extensive annotations in\ndeep learning, but the more realistic challenge of imbalanced data distribution\nin SSL remains largely unexplored. In Class Imbalanced Semi-supervised Learning\n(CISSL), the bias introduced by unreliable pseudo-labels can be exacerbated by\nimbalanced data distributions. Most existing methods address this issue at\ninstance-level through reweighting or resampling, but the performance is\nheavily limited by their reliance on biased backbone representation. Some other\nmethods do perform feature-level adjustments like feature blending but might\nintroduce unfavorable noise. In this paper, we discuss the bonus of a more\nbalanced feature distribution for the CISSL problem, and further propose a\nBalanced Feature-Level Contrastive Learning method (BaCon). Our method directly\nregularizes the distribution of instances' representations in a well-designed\ncontrastive manner. Specifically, class-wise feature centers are computed as\nthe positive anchors, while negative anchors are selected by a straightforward\nyet effective mechanism. A distribution-related temperature adjustment is\nleveraged to control the class-wise contrastive degrees dynamically. Our method\ndemonstrates its effectiveness through comprehensive experiments on the\nCIFAR10-LT, CIFAR100-LT, STL10-LT, and SVHN-LT datasets across various\nsettings. For example, BaCon surpasses instance-level method FixMatch-based ABC\non CIFAR10-LT with a 1.21% accuracy improvement, and outperforms\nstate-of-the-art feature-level method CoSSL on CIFAR100-LT with a 0.63%\naccuracy improvement. When encountering more extreme imbalance degree, BaCon\nalso shows better robustness than other methods.\n","authors":["Qianhan Feng","Lujing Xie","Shijie Fang","Tong Lin"],"pdf_url":"https://arxiv.org/pdf/2403.12986v2.pdf","comment":"Accpeted paper of AAAI2024"},{"id":"http://arxiv.org/abs/2407.08983v1","updated":"2024-07-12T04:38:28Z","published":"2024-07-12T04:38:28Z","title":"Towards More Trustworthy and Interpretable LLMs for Code through\n  Syntax-Grounded Explanations","summary":"  Trustworthiness and interpretability are inextricably linked concepts for\nLLMs. The more interpretable an LLM is, the more trustworthy it becomes.\nHowever, current techniques for interpreting LLMs when applied to code-related\ntasks largely focus on accuracy measurements, measures of how models react to\nchange, or individual task performance instead of the fine-grained explanations\nneeded at prediction time for greater interpretability, and hence trust. To\nimprove upon this status quo, this paper introduces ASTrust, an\ninterpretability method for LLMs of code that generates explanations grounded\nin the relationship between model confidence and syntactic structures of\nprogramming languages. ASTrust explains generated code in the context of syntax\ncategories based on Abstract Syntax Trees and aids practitioners in\nunderstanding model predictions at both local (individual code snippets) and\nglobal (larger datasets of code) levels. By distributing and assigning model\nconfidence scores to well-known syntactic structures that exist within ASTs,\nour approach moves beyond prior techniques that perform token-level confidence\nmapping by offering a view of model confidence that directly aligns with\nprogramming language concepts with which developers are familiar. To put\nASTrust into practice, we developed an automated visualization that illustrates\nthe aggregated model confidence scores superimposed on sequence, heat-map, and\ngraph-based visuals of syntactic structures from ASTs. We examine both the\npractical benefit that ASTrust can provide through a data science study on 12\npopular LLMs on a curated set of GitHub repos and the usefulness of ASTrust\nthrough a human study.\n","authors":["David N. Palacio","Daniel Rodriguez-Cardenas","Alejandro Velasco","Dipin Khati","Kevin Moran","Denys Poshyvanyk"],"pdf_url":"https://arxiv.org/pdf/2407.08983v1.pdf","comment":"Under Review to appear in ACM Transactions on Software Engineering\n  and Methodology (TOSEM)"},{"id":"http://arxiv.org/abs/2407.08978v1","updated":"2024-07-12T04:18:22Z","published":"2024-07-12T04:18:22Z","title":"Towards Chapter-to-Chapter Context-Aware Literary Translation via Large\n  Language Models","summary":"  Discourse phenomena in existing document-level translation datasets are\nsparse, which has been a fundamental obstacle in the development of\ncontext-aware machine translation models. Moreover, most existing\ndocument-level corpora and context-aware machine translation methods rely on an\nunrealistic assumption on sentence-level alignments. To mitigate these issues,\nwe first curate a novel dataset of Chinese-English literature, which consists\nof 160 books with intricate discourse structures. Then, we propose a more\npragmatic and challenging setting for context-aware translation, termed\nchapter-to-chapter (Ch2Ch) translation, and investigate the performance of\ncommonly-used machine translation models under this setting. Furthermore, we\nintroduce a potential approach of finetuning large language models (LLMs)\nwithin the domain of Ch2Ch literary translation, yielding impressive\nimprovements over baselines. Through our comprehensive analysis, we unveil that\nliterary translation under the Ch2Ch setting is challenging in nature, with\nrespect to both model learning methods and translation decoding algorithms.\n","authors":["Linghao Jin","Li An","Xuezhe Ma"],"pdf_url":"https://arxiv.org/pdf/2407.08978v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.08976v1","updated":"2024-07-12T04:08:01Z","published":"2024-07-12T04:08:01Z","title":"Computational-Statistical Trade-off in Kernel Two-Sample Testing with\n  Random Fourier Features","summary":"  Recent years have seen a surge in methods for two-sample testing, among which\nthe Maximum Mean Discrepancy (MMD) test has emerged as an effective tool for\nhandling complex and high-dimensional data. Despite its success and widespread\nadoption, the primary limitation of the MMD test has been its quadratic-time\ncomplexity, which poses challenges for large-scale analysis. While various\napproaches have been proposed to expedite the procedure, it has been unclear\nwhether it is possible to attain the same power guarantee as the MMD test at\nsub-quadratic time cost. To fill this gap, we revisit the approximated MMD test\nusing random Fourier features, and investigate its computational-statistical\ntrade-off. We start by revealing that the approximated MMD test is pointwise\nconsistent in power only when the number of random features approaches\ninfinity. We then consider the uniform power of the test and study the\ntime-power trade-off under the minimax testing framework. Our result shows\nthat, by carefully choosing the number of random features, it is possible to\nattain the same minimax separation rates as the MMD test within sub-quadratic\ntime. We demonstrate this point under different distributional assumptions such\nas densities in a Sobolev ball. Our theoretical findings are corroborated by\nsimulation studies.\n","authors":["Ikjun Choi","Ilmun Kim"],"pdf_url":"https://arxiv.org/pdf/2407.08976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08974v1","updated":"2024-07-12T04:04:54Z","published":"2024-07-12T04:04:54Z","title":"Topology-enhanced machine learning model (Top-ML) for anticancer peptide\n  prediction","summary":"  Recently, therapeutic peptides have demonstrated great promise for cancer\ntreatment. To explore powerful anticancer peptides, artificial intelligence\n(AI)-based approaches have been developed to systematically screen potential\ncandidates. However, the lack of efficient featurization of peptides has become\na bottleneck for these machine-learning models. In this paper, we propose a\ntopology-enhanced machine learning model (Top-ML) for anticancer peptide\nprediction. Our Top-ML employs peptide topological features derived from its\nsequence \"connection\" information characterized by vector and spectral\ndescriptors. Our Top-ML model has been validated on two widely used AntiCP 2.0\nbenchmark datasets and has achieved state-of-the-art performance. Our results\nhighlight the potential of leveraging novel topology-based featurization to\naccelerate the identification of anticancer peptides.\n","authors":["Joshua Zhi En Tan","JunJie Wee","Xue Gong","Kelin Xia"],"pdf_url":"https://arxiv.org/pdf/2407.08974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08973v1","updated":"2024-07-12T03:58:04Z","published":"2024-07-12T03:58:04Z","title":"Integrating White and Black Box Techniques for Interpretable Machine\n  Learning","summary":"  In machine learning algorithm design, there exists a trade-off between the\ninterpretability and performance of the algorithm. In general, algorithms which\nare simpler and easier for humans to comprehend tend to show worse performance\nthan more complex, less transparent algorithms. For example, a random forest\nclassifier is likely to be more accurate than a simple decision tree, but at\nthe expense of interpretability. In this paper, we present an ensemble\nclassifier design which classifies easier inputs using a highly-interpretable\nclassifier (i.e., white box model), and more difficult inputs using a more\npowerful, but less interpretable classifier (i.e., black box model).\n","authors":["Eric M. Vernon","Naoki Masuyama","Yusuke Nojima"],"pdf_url":"https://arxiv.org/pdf/2407.08973v1.pdf","comment":"This paper was presented at ICICT2024 and will be published in\n  conference proceedings by Springer LNNS. ISSN: 2367-3370, Series"},{"id":"http://arxiv.org/abs/2306.04647v3","updated":"2024-07-12T03:46:02Z","published":"2023-06-05T01:29:24Z","title":"Compressed Sensing: A Discrete Optimization Approach","summary":"  We study the Compressed Sensing (CS) problem, which is the problem of finding\nthe most sparse vector that satisfies a set of linear measurements up to some\nnumerical tolerance. We introduce an $\\ell_2$ regularized formulation of CS\nwhich we reformulate as a mixed integer second order cone program. We derive a\nsecond order cone relaxation of this problem and show that under mild\nconditions on the regularization parameter, the resulting relaxation is\nequivalent to the well studied basis pursuit denoising problem. We present a\nsemidefinite relaxation that strengthens the second order cone relaxation and\ndevelop a custom branch-and-bound algorithm that leverages our second order\ncone relaxation to solve small-scale instances of CS to certifiable optimality.\nWhen compared against solutions produced by three state of the art benchmark\nmethods on synthetic data, our numerical results show that our approach\nproduces solutions that are on average $6.22\\%$ more sparse. When compared only\nagainst the experiment-wise best performing benchmark method on synthetic data,\nour approach produces solutions that are on average $3.10\\%$ more sparse. On\nreal world ECG data, for a given $\\ell_2$ reconstruction error our approach\nproduces solutions that are on average $9.95\\%$ more sparse than benchmark\nmethods ($3.88\\%$ more sparse if only compared against the best performing\nbenchmark), while for a given sparsity level our approach produces solutions\nthat have on average $10.77\\%$ lower reconstruction error than benchmark\nmethods ($1.42\\%$ lower error if only compared against the best performing\nbenchmark). When used as a component of a multi-label classification algorithm,\nour approach achieves greater classification accuracy than benchmark compressed\nsensing methods. This improved accuracy comes at the cost of an increase in\ncomputation time by several orders of magnitude.\n","authors":["Dimitris Bertsimas","Nicholas A. G. Johnson"],"pdf_url":"https://arxiv.org/pdf/2306.04647v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08970v1","updated":"2024-07-12T03:40:13Z","published":"2024-07-12T03:40:13Z","title":"Soft Prompts Go Hard: Steering Visual Language Models with Hidden\n  Meta-Instructions","summary":"  We introduce a new type of indirect injection vulnerabilities in language\nmodels that operate on images: hidden \"meta-instructions\" that influence how\nthe model interprets the image and steer the model's outputs to express an\nadversary-chosen style, sentiment, or point of view.\n  We explain how to create meta-instructions by generating images that act as\nsoft prompts. Unlike jailbreaking attacks and adversarial examples, the outputs\nresulting from these images are plausible and based on the visual content of\nthe image, yet follow the adversary's (meta-)instructions. We describe the\nrisks of these attacks, including misinformation and spin, evaluate their\nefficacy for multiple visual language models and adversarial meta-objectives,\nand demonstrate how they can \"unlock\" the capabilities of the underlying\nlanguage models that are unavailable via explicit text instructions. Finally,\nwe discuss defenses against these attacks.\n","authors":["Tingwei Zhang","Collin Zhang","John X. Morris","Eugene Bagdasaryan","Vitaly Shmatikov"],"pdf_url":"https://arxiv.org/pdf/2407.08970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02813v2","updated":"2024-07-12T03:39:05Z","published":"2024-07-03T05:17:26Z","title":"Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm\n  and Compiler Co-Design","summary":"  Deep neural networks (DNNs) are frequently employed in a variety of computer\nvision applications. Nowadays, an emerging trend in the current video\ndistribution system is to take advantage of DNN's overfitting properties to\nperform video resolution upscaling. By splitting videos into chunks and\napplying a super-resolution (SR) model to overfit each chunk, this scheme of SR\nmodels plus video chunks is able to replace traditional video transmission to\nenhance video quality and transmission efficiency. However, many models and\nchunks are needed to guarantee high performance, which leads to tremendous\noverhead on model switching and memory footprints at the user end. To resolve\nsuch problems, we propose a Dynamic Deep neural network assisted by a\nContent-Aware data processing pipeline to reduce the model number down to one\n(Dy-DCA), which helps promote performance while conserving computational\nresources. Additionally, to achieve real acceleration on the user end, we\ndesigned a framework that optimizes dynamic features (e.g., dynamic shapes,\nsizes, and control flow) in Dy-DCA to enable a series of compilation\noptimizations, including fused code generation, static execution planning, etc.\nBy employing such techniques, our method achieves better PSNR and real-time\nperformance (33 FPS) on an off-the-shelf mobile phone. Meanwhile, assisted by\nour compilation optimization, we achieve a 1.7$\\times$ speedup while saving up\nto 1.61$\\times$ memory consumption. Code available in\nhttps://github.com/coulsonlee/Dy-DCA-ECCV2024.\n","authors":["Gen Li","Zhihao Shu","Jie Ji","Minghai Qin","Fatemeh Afghah","Wei Niu","Xiaolong Ma"],"pdf_url":"https://arxiv.org/pdf/2407.02813v2.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2407.08966v1","updated":"2024-07-12T03:30:53Z","published":"2024-07-12T03:30:53Z","title":"LAPT: Label-driven Automated Prompt Tuning for OOD Detection with\n  Vision-Language Models","summary":"  Out-of-distribution (OOD) detection is crucial for model reliability, as it\nidentifies samples from unknown classes and reduces errors due to unexpected\ninputs. Vision-Language Models (VLMs) such as CLIP are emerging as powerful\ntools for OOD detection by integrating multi-modal information. However, the\npractical application of such systems is challenged by manual prompt\nengineering, which demands domain expertise and is sensitive to linguistic\nnuances. In this paper, we introduce Label-driven Automated Prompt Tuning\n(LAPT), a novel approach to OOD detection that reduces the need for manual\nprompt engineering. We develop distribution-aware prompts with in-distribution\n(ID) class names and negative labels mined automatically. Training samples\nlinked to these class labels are collected autonomously via image synthesis and\nretrieval methods, allowing for prompt learning without manual effort. We\nutilize a simple cross-entropy loss for prompt optimization, with cross-modal\nand cross-distribution mixing strategies to reduce image noise and explore the\nintermediate space between distributions, respectively. The LAPT framework\noperates autonomously, requiring only ID class names as input and eliminating\nthe need for manual intervention. With extensive experiments, LAPT consistently\noutperforms manually crafted prompts, setting a new standard for OOD detection.\nMoreover, LAPT not only enhances the distinction between ID and OOD samples,\nbut also improves the ID classification accuracy and strengthens the\ngeneralization robustness to covariate shifts, resulting in outstanding\nperformance in challenging full-spectrum OOD detection tasks. Codes are\navailable at \\url{https://github.com/YBZh/LAPT}.\n","authors":["Yabin Zhang","Wenjie Zhu","Chenhang He","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.08966v1.pdf","comment":"ECCV2024; Codes and Supp. are available at:\n  https://github.com/YBZh/LAPT"},{"id":"http://arxiv.org/abs/2407.08965v1","updated":"2024-07-12T03:28:46Z","published":"2024-07-12T03:28:46Z","title":"Lite-SAM Is Actually What You Need for Segment Everything","summary":"  This paper introduces Lite-SAM, an efficient end-to-end solution for the\nSegEvery task designed to reduce computational costs and redundancy. Lite-SAM\nis composed of four main components: a streamlined CNN-Transformer hybrid\nencoder (LiteViT), an automated prompt proposal network (AutoPPN), a\ntraditional prompt encoder, and a mask decoder. All these components are\nintegrated within the SAM framework. Our LiteViT, a high-performance\nlightweight backbone network, has only 1.16M parameters, which is a 23%\nreduction compared to the lightest existing backbone network Shufflenet. We\nalso introduce AutoPPN, an innovative end-to-end method for prompt boxes and\npoints generation. This is an improvement over traditional grid search sampling\nmethods, and its unique design allows for easy integration into any SAM series\nalgorithm, extending its usability. we have thoroughly benchmarked Lite-SAM\nacross a plethora of both public and private datasets. The evaluation\nencompassed a broad spectrum of universal metrics, including the number of\nparameters, SegEvery execution time, and accuracy. The findings reveal that\nLite-SAM, operating with a lean 4.2M parameters, significantly outpaces its\ncounterparts, demonstrating performance improvements of 43x, 31x, 20x, 21x, and\n1.6x over SAM, MobileSAM, Edge-SAM, EfficientViT-SAM, and MobileSAM-v2\nrespectively, all the while maintaining competitive accuracy. This underscores\nLite-SAM's prowess in achieving an optimal equilibrium between performance and\nprecision, thereby setting a new state-of-the-art(SOTA) benchmark in the\ndomain.\n","authors":["Jianhai Fu","Yuanjie Yu","Ningchuan Li","Yi Zhang","Qichao Chen","Jianping Xiong","Jun Yin","Zhiyu Xiang"],"pdf_url":"https://arxiv.org/pdf/2407.08965v1.pdf","comment":"ECCV 2024 Accepted"},{"id":"http://arxiv.org/abs/2407.08964v1","updated":"2024-07-12T03:28:24Z","published":"2024-07-12T03:28:24Z","title":"Communication-Aware Reinforcement Learning for Cooperative Adaptive\n  Cruise Control","summary":"  Cooperative Adaptive Cruise Control (CACC) plays a pivotal role in enhancing\ntraffic efficiency and safety in Connected and Autonomous Vehicles (CAVs).\nReinforcement Learning (RL) has proven effective in optimizing complex\ndecision-making processes in CACC, leading to improved system performance and\nadaptability. Among RL approaches, Multi-Agent Reinforcement Learning (MARL)\nhas shown remarkable potential by enabling coordinated actions among multiple\nCAVs through Centralized Training with Decentralized Execution (CTDE). However,\nMARL often faces scalability issues, particularly when CACC vehicles suddenly\njoin or leave the platoon, resulting in performance degradation. To address\nthese challenges, we propose Communication-Aware Reinforcement Learning\n(CA-RL). CA-RL includes a communication-aware module that extracts and\ncompresses vehicle communication information through forward and backward\ninformation transmission modules. This enables efficient cyclic information\npropagation within the CACC traffic flow, ensuring policy consistency and\nmitigating the scalability problems of MARL in CACC. Experimental results\ndemonstrate that CA-RL significantly outperforms baseline methods in various\ntraffic scenarios, achieving superior scalability, robustness, and overall\nsystem performance while maintaining reliable performance despite changes in\nthe number of participating vehicles.\n","authors":["Sicong Jiang","Seongjin Choi","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2407.08964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08953v1","updated":"2024-07-12T03:16:54Z","published":"2024-07-12T03:16:54Z","title":"Attribution Methods in Asset Pricing: Do They Account for Risk?","summary":"  Over the past few decades, machine learning models have been extremely\nsuccessful. As a result of axiomatic attribution methods, feature contributions\nhave been explained more clearly and rigorously. There are, however, few\nstudies that have examined domain knowledge in conjunction with the axioms. In\nthis study, we examine asset pricing in finance, a field closely related to\nrisk management. Consequently, when applying machine learning models, we must\nensure that the attribution methods reflect the underlying risks accurately. In\nthis work, we present and study several axioms derived from asset pricing\ndomain knowledge. It is shown that while Shapley value and Integrated Gradients\npreserve most axioms, neither can satisfy all axioms. Using extensive\nanalytical and empirical examples, we demonstrate how attribution methods can\nreflect risks and when they should not be used.\n","authors":["Dangxing Chen","Yuan Gao"],"pdf_url":"https://arxiv.org/pdf/2407.08953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08947v1","updated":"2024-07-12T03:07:28Z","published":"2024-07-12T03:07:28Z","title":"Constructing Concept-based Models to Mitigate Spurious Correlations with\n  Minimal Human Effort","summary":"  Enhancing model interpretability can address spurious correlations by\nrevealing how models draw their predictions. Concept Bottleneck Models (CBMs)\ncan provide a principled way of disclosing and guiding model behaviors through\nhuman-understandable concepts, albeit at a high cost of human efforts in data\nannotation. In this paper, we leverage a synergy of multiple foundation models\nto construct CBMs with nearly no human effort. We discover undesirable biases\nin CBMs built on pre-trained models and propose a novel framework designed to\nexploit pre-trained models while being immune to these biases, thereby reducing\nvulnerability to spurious correlations. Specifically, our method offers a\nseamless pipeline that adopts foundation models for assessing potential\nspurious correlations in datasets, annotating concepts for images, and refining\nthe annotations for improved robustness. We evaluate the proposed method on\nmultiple datasets, and the results demonstrate its effectiveness in reducing\nmodel reliance on spurious correlations while preserving its interpretability.\n","authors":["Jeeyung Kim","Ze Wang","Qiang Qiu"],"pdf_url":"https://arxiv.org/pdf/2407.08947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08946v1","updated":"2024-07-12T03:03:50Z","published":"2024-07-12T03:03:50Z","title":"Your Diffusion Model is Secretly a Noise Classifier and Benefits from\n  Contrastive Training","summary":"  Diffusion models learn to denoise data and the trained denoiser is then used\nto generate new samples from the data distribution. In this paper, we revisit\nthe diffusion sampling process and identify a fundamental cause of sample\nquality degradation: the denoiser is poorly estimated in regions that are far\nOutside Of the training Distribution (OOD), and the sampling process inevitably\nevaluates in these OOD regions. This can become problematic for all sampling\nmethods, especially when we move to parallel sampling which requires us to\ninitialize and update the entire sample trajectory of dynamics in parallel,\nleading to many OOD evaluations. To address this problem, we introduce a new\nself-supervised training objective that differentiates the levels of noise\nadded to a sample, leading to improved OOD denoising performance. The approach\nis based on our observation that diffusion models implicitly define a\nlog-likelihood ratio that distinguishes distributions with different amounts of\nnoise, and this expression depends on denoiser performance outside the standard\ntraining distribution. We show by diverse experiments that the proposed\ncontrastive diffusion training is effective for both sequential and parallel\nsettings, and it improves the performance and speed of parallel samplers\nsignificantly.\n","authors":["Yunshu Wu","Yingtao Luo","Xianghao Kong","Evangelos E. Papalexakis","Greg Ver Steeg"],"pdf_url":"https://arxiv.org/pdf/2407.08946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08418v2","updated":"2024-07-12T02:55:16Z","published":"2024-07-11T11:51:36Z","title":"PredBench: Benchmarking Spatio-Temporal Prediction across Diverse\n  Disciplines","summary":"  In this paper, we introduce PredBench, a benchmark tailored for the holistic\nevaluation of spatio-temporal prediction networks. Despite significant progress\nin this field, there remains a lack of a standardized framework for a detailed\nand comparative analysis of various prediction network architectures. PredBench\naddresses this gap by conducting large-scale experiments, upholding\nstandardized and appropriate experimental settings, and implementing\nmulti-dimensional evaluations. This benchmark integrates 12 widely adopted\nmethods with 15 diverse datasets across multiple application domains, offering\nextensive evaluation of contemporary spatio-temporal prediction networks.\nThrough meticulous calibration of prediction settings across various\napplications, PredBench ensures evaluations relevant to their intended use and\nenables fair comparisons. Moreover, its multi-dimensional evaluation framework\nbroadens the analysis with a comprehensive set of metrics, providing deep\ninsights into the capabilities of models. The findings from our research offer\nstrategic directions for future developments in the field. Our codebase is\navailable at https://github.com/OpenEarthLab/PredBench.\n","authors":["ZiDong Wang","Zeyu Lu","Di Huang","Tong He","Xihui Liu","Wanli Ouyang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2407.08418v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08934v1","updated":"2024-07-12T02:39:50Z","published":"2024-07-12T02:39:50Z","title":"Compositional Structures in Neural Embedding and Interaction\n  Decompositions","summary":"  We describe a basic correspondence between linear algebraic structures within\nvector embeddings in artificial neural networks and conditional independence\nconstraints on the probability distributions modeled by these networks. Our\nframework aims to shed light on the emergence of structural patterns in data\nrepresentations, a phenomenon widely acknowledged but arguably still lacking a\nsolid formal grounding. Specifically, we introduce a characterization of\ncompositional structures in terms of \"interaction decompositions,\" and we\nestablish necessary and sufficient conditions for the presence of such\nstructures within the representations of a model.\n","authors":["Matthew Trager","Alessandro Achille","Pramuditha Perera","Luca Zancato","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2407.08934v1.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.08933v1","updated":"2024-07-12T02:34:54Z","published":"2024-07-12T02:34:54Z","title":"Machine Learning in High Volume Media Manufacturing","summary":"  Errors or failures in a high-volume manufacturing environment can have\nsignificant impact that can result in both the loss of time and money.\nIdentifying such failures early has been a top priority for manufacturing\nindustries and various rule-based algorithms have been developed over the\nyears. However, catching these failures is time consuming and such algorithms\ncannot adapt well to changes in designs, and sometimes variations in everyday\nbehavior. More importantly, the number of units to monitor in a high-volume\nmanufacturing environment is too big for manual monitoring or for a simple\nprogram. Here we develop a novel program that combines both rule-based\ndecisions and machine learning models that can not only learn and adapt to such\nday-to-day variations or long-term design changes, but also can be applied at\nscale to the high number of manufacturing units in use today. Using the current\nstate-of-the-art technologies, we then deploy this program at-scale to handle\nthe needs of ever-increasing demand from the manufacturing environment.\n","authors":["Siddarth Reddy Karuka","Abhinav Sunderrajan","Zheng Zheng","Yong Woon Tiean","Ganesh Nagappan","Allan Luk"],"pdf_url":"https://arxiv.org/pdf/2407.08933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10757v3","updated":"2024-07-12T02:21:54Z","published":"2024-05-17T13:09:39Z","title":"Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective","summary":"  Graph Neural Networks (GNNs) have shown remarkable performance in various\ntasks. However, recent works reveal that GNNs are vulnerable to backdoor\nattacks. Generally, backdoor attack poisons the graph by attaching backdoor\ntriggers and the target class label to a set of nodes in the training graph. A\nGNN trained on the poisoned graph will then be misled to predict test nodes\nattached with trigger to the target class. Despite their effectiveness, our\nempirical analysis shows that triggers generated by existing methods tend to be\nout-of-distribution (OOD), which significantly differ from the clean data.\nHence, these injected triggers can be easily detected and pruned with widely\nused outlier detection methods in real-world applications. Therefore, in this\npaper, we study a novel problem of unnoticeable graph backdoor attacks with\nin-distribution (ID) triggers. To generate ID triggers, we introduce an OOD\ndetector in conjunction with an adversarial learning strategy to generate the\nattributes of the triggers within distribution. To ensure a high attack success\nrate with ID triggers, we introduce novel modules designed to enhance trigger\nmemorization by the victim model trained on poisoned graph. Extensive\nexperiments on real-world datasets demonstrate the effectiveness of the\nproposed method in generating in distribution triggers that can by-pass various\ndefense strategies while maintaining a high attack success rate.\n","authors":["Zhiwei Zhang","Minhua Lin","Enyan Dai","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2405.10757v3.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2407.08922v1","updated":"2024-07-12T02:05:59Z","published":"2024-07-12T02:05:59Z","title":"Leveraging large language models for nano synthesis mechanism\n  explanation: solid foundations or mere conjectures?","summary":"  With the rapid development of artificial intelligence (AI), large language\nmodels (LLMs) such as GPT-4 have garnered significant attention in the\nscientific community, demonstrating great potential in advancing scientific\ndiscovery. This progress raises a critical question: are these LLMs\nwell-aligned with real-world physicochemical principles? Current evaluation\nstrategies largely emphasize fact-based knowledge, such as material property\nprediction or name recognition, but they often lack an understanding of\nfundamental physicochemical mechanisms that require logical reasoning. To\nbridge this gap, our study developed a benchmark consisting of 775\nmultiple-choice questions focusing on the mechanisms of gold nanoparticle\nsynthesis. By reflecting on existing evaluation metrics, we question whether a\ndirect true-or-false assessment merely suggests conjecture. Hence, we propose a\nnovel evaluation metric, the confidence-based score (c-score), which probes the\noutput logits to derive the precise probability for the correct answer. Based\non extensive experiments, our results show that in the context of gold\nnanoparticle synthesis, LLMs understand the underlying physicochemical\nmechanisms rather than relying on conjecture. This study underscores the\npotential of LLMs to grasp intrinsic scientific mechanisms and sets the stage\nfor developing more reliable and effective AI tools across various scientific\ndomains.\n","authors":["Yingming Pu","Liping Huang","Tao Lin","Hongyu Chen"],"pdf_url":"https://arxiv.org/pdf/2407.08922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11410v3","updated":"2024-07-12T02:02:45Z","published":"2024-01-21T06:33:45Z","title":"Agricultural Recommendation System based on Deep Learning: A\n  Multivariate Weather Forecasting Approach","summary":"  Agriculture plays a fundamental role in driving economic growth and ensuring\nfood security for populations around the world. Although labor-intensive\nagriculture has led to steady increases in food grain production in many\ndeveloping countries, it is frequently challenged by adverse weather\nconditions, including heavy rainfall, low temperatures, and drought. These\nfactors substantially hinder food production, posing significant risks to\nglobal food security. In order to have a profitable, sustainable, and\nfarmer-friendly agricultural practice, this paper proposes a context-based crop\nrecommendation system powered by a weather forecast model. For implementation\npurposes, we have considered the whole territory of Bangladesh. With extensive\nevaluation, the multivariate Stacked Bi-LSTM (three Bi-LSTM layers with a time\nDistributed layer) Network is employed as the weather forecasting model. The\nproposed weather model can forecast Rainfall, Temperature, Humidity, and\nSunshine for any given location in Bangladesh with an average R-Squared value\nof 0.9824, and the model outperforms other state-of-the-art LSTM models. These\npredictions guide our system in generating viable farming decisions.\nAdditionally, our full-fledged system is capable of alerting the farmers about\nextreme weather conditions so that preventive measures can be undertaken to\nprotect the crops. Finally, the system is also adept at making knowledge-based\ncrop suggestions for flood and drought-prone regions.\n","authors":["Md Zubair","Md. Shahidul Salim","Mehrab Mustafy Rahman","Mohammad Jahid Ibna Basher","Shahin Imran","Iqbal H. Sarker"],"pdf_url":"https://arxiv.org/pdf/2401.11410v3.pdf","comment":"18 pages, 16 figures and 13 tables. Two figures and one table have\n  been added to this version"},{"id":"http://arxiv.org/abs/2306.08388v3","updated":"2024-07-12T01:59:00Z","published":"2023-06-14T09:24:32Z","title":"Skill-Critic: Refining Learned Skills for Hierarchical Reinforcement\n  Learning","summary":"  Hierarchical reinforcement learning (RL) can accelerate long-horizon\ndecision-making by temporally abstracting a policy into multiple levels.\nPromising results in sparse reward environments have been seen with skills,\ni.e. sequences of primitive actions. Typically, a skill latent space and policy\nare discovered from offline data. However, the resulting low-level policy can\nbe unreliable due to low-coverage demonstrations or distribution shifts. As a\nsolution, we propose the Skill-Critic algorithm to fine-tune the low-level\npolicy in conjunction with high-level skill selection. Our Skill-Critic\nalgorithm optimizes both the low-level and high-level policies; these policies\nare initialized and regularized by the latent space learned from offline\ndemonstrations to guide the parallel policy optimization. We validate\nSkill-Critic in multiple sparse-reward RL environments, including a new\nsparse-reward autonomous racing task in Gran Turismo Sport. The experiments\nshow that Skill-Critic's low-level policy fine-tuning and demonstration-guided\nregularization are essential for good performance. Code and videos are\navailable at our website: https://sites.google.com/view/skill-critic.\n","authors":["Ce Hao","Catherine Weaver","Chen Tang","Kenta Kawamoto","Masayoshi Tomizuka","Wei Zhan"],"pdf_url":"https://arxiv.org/pdf/2306.08388v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.06886v2","updated":"2024-07-12T01:48:00Z","published":"2024-07-09T14:14:47Z","title":"Aligning Cyber Space with Physical World: A Comprehensive Survey on\n  Embodied AI","summary":"  Embodied Artificial Intelligence (Embodied AI) is crucial for achieving\nArtificial General Intelligence (AGI) and serves as a foundation for various\napplications that bridge cyberspace and the physical world. Recently, the\nemergence of Multi-modal Large Models (MLMs) and World Models (WMs) have\nattracted significant attention due to their remarkable perception,\ninteraction, and reasoning capabilities, making them a promising architecture\nfor the brain of embodied agents. However, there is no comprehensive survey for\nEmbodied AI in the era of MLMs. In this survey, we give a comprehensive\nexploration of the latest advancements in Embodied AI. Our analysis firstly\nnavigates through the forefront of representative works of embodied robots and\nsimulators, to fully understand the research focuses and their limitations.\nThen, we analyze four main research targets: 1) embodied perception, 2)\nembodied interaction, 3) embodied agent, and 4) sim-to-real adaptation,\ncovering the state-of-the-art methods, essential paradigms, and comprehensive\ndatasets. Additionally, we explore the complexities of MLMs in virtual and real\nembodied agents, highlighting their significance in facilitating interactions\nin dynamic digital and physical environments. Finally, we summarize the\nchallenges and limitations of embodied AI and discuss their potential future\ndirections. We hope this survey will serve as a foundational reference for the\nresearch community and inspire continued innovation. The associated project can\nbe found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.\n","authors":["Yang Liu","Weixing Chen","Yongjie Bai","Jingzhou Luo","Xinshuai Song","Kaixuan Jiang","Zhida Li","Ganlong Zhao","Junyi Lin","Guanbin Li","Wen Gao","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2407.06886v2.pdf","comment":"The first comprehensive review of Embodied AI in the era of MLMs, 37\n  pages. We also provide the paper list for Embodied AI:\n  https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List"},{"id":"http://arxiv.org/abs/2406.18145v2","updated":"2024-07-12T01:36:06Z","published":"2024-06-26T07:53:48Z","title":"Beyond Statistical Estimation: Differentially Private Individual\n  Computation via Shuffling","summary":"  In data-driven applications, preserving user privacy while enabling valuable\ncomputations remains a critical challenge. Technologies like Differential\nPrivacy (DP) have been pivotal in addressing these concerns. The shuffle model\nof DP requires no trusted curators and can achieve high utility by leveraging\nthe privacy amplification effect yielded from shuffling. These benefits have\nled to significant interest in the shuffle model. However, the computation\ntasks in the shuffle model are limited to statistical estimation, making the\nshuffle model inapplicable to real-world scenarios in which each user requires\na personalized output. This paper introduces a novel paradigm termed Private\nIndividual Computation (PIC), expanding the shuffle model to support a broader\nrange of permutation-equivariant computations. PIC enables personalized outputs\nwhile preserving privacy, and enjoys privacy amplification through shuffling.\nWe propose a concrete protocol that realizes PIC. By using one-time public\nkeys, our protocol enables users to receive their outputs without compromising\nanonymity, which is essential for privacy amplification. Additionally, we\npresent an optimal randomizer, the Minkowski Response, designed for the PIC\nmodel to enhance utility. We formally prove the security and privacy properties\nof the PIC protocol. Theoretical analysis and empirical evaluations demonstrate\nPIC's capability in handling non-statistical computation tasks, and the\nefficacy of PIC and the Minkowski randomizer in achieving superior utility\ncompared to existing solutions.\n","authors":["Shaowei Wang","Changyu Dong","Xiangfu Song","Jin Li","Zhili Zhou","Di Wang","Han Wu"],"pdf_url":"https://arxiv.org/pdf/2406.18145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08916v1","updated":"2024-07-12T01:26:33Z","published":"2024-07-12T01:26:33Z","title":"Transforming Movie Recommendations with Advanced Machine Learning: A\n  Study of NMF, SVD,and K-Means Clustering","summary":"  This study develops a robust movie recommendation system using various\nmachine learning techniques, including Non- Negative Matrix Factorization\n(NMF), Truncated Singular Value Decomposition (SVD), and K-Means clustering.\nThe primary objective is to enhance user experience by providing personalized\nmovie recommendations. The research encompasses data preprocessing, model\ntraining, and evaluation, highlighting the efficacy of the employed methods.\nResults indicate that the proposed system achieves high accuracy and relevance\nin recommendations, making significant contributions to the field of\nrecommendations systems.\n","authors":["Yubing Yan","Camille Moreau","Zhuoyue Wang","Wenhan Fan","Chengqian Fu"],"pdf_url":"https://arxiv.org/pdf/2407.08916v1.pdf","comment":"Accepted by 2024 4th International Symposium on Computer Technology\n  and Information Science, IEEE"},{"id":"http://arxiv.org/abs/2407.08910v1","updated":"2024-07-12T01:06:01Z","published":"2024-07-12T01:06:01Z","title":"PAIL: Performance based Adversarial Imitation Learning Engine for Carbon\n  Neutral Optimization","summary":"  Achieving carbon neutrality within industrial operations has become\nincreasingly imperative for sustainable development. It is both a significant\nchallenge and a key opportunity for operational optimization in industry 4.0.\nIn recent years, Deep Reinforcement Learning (DRL) based methods offer\npromising enhancements for sequential optimization processes and can be used\nfor reducing carbon emissions. However, existing DRL methods need a pre-defined\nreward function to assess the impact of each action on the final sustainable\ndevelopment goals (SDG). In many real applications, such a reward function\ncannot be given in advance. To address the problem, this study proposes a\nPerformance based Adversarial Imitation Learning (PAIL) engine. It is a novel\nmethod to acquire optimal operational policies for carbon neutrality without\nany pre-defined action rewards. Specifically, PAIL employs a Transformer-based\npolicy generator to encode historical information and predict following actions\nwithin a multi-dimensional space. The entire action sequence will be\niteratively updated by an environmental simulator. Then PAIL uses a\ndiscriminator to minimize the discrepancy between generated sequences and\nreal-world samples of high SDG. In parallel, a Q-learning framework based\nperformance estimator is designed to estimate the impact of each action on SDG.\nBased on these estimations, PAIL refines generated policies with the rewards\nfrom both discriminator and performance estimator. PAIL is evaluated on\nmultiple real-world application cases and datasets. The experiment results\ndemonstrate the effectiveness of PAIL comparing to other state-of-the-art\nbaselines. In addition, PAIL offers meaningful interpretability for the\noptimization in carbon neutrality.\n","authors":["Yuyang Ye","Lu-An Tang","Haoyu Wang","Runlong Yu","Wenchao Yu","Erhu He","Haifeng Chen","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2407.08910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02619v8","updated":"2024-07-12T00:34:01Z","published":"2024-02-04T21:33:18Z","title":"Increasing Trust in Language Models through the Reuse of Verified\n  Circuits","summary":"  Language Models (LMs) are increasingly used for a wide range of prediction\ntasks, but their training can often neglect rare edge cases, reducing their\nreliability. Here, we define a stringent standard of trustworthiness whereby\nthe task algorithm and circuit implementation must be verified, accounting for\nedge cases, with no known failure modes. We show that a model can be trained to\nmeet this standard if built using mathematically and logically specified\nframeworks. In this paper, we fully verify an auto-regressive transformer model\nfor n-digit integer addition. To exhibit the reusability of verified modules,\nwe insert the trained integer addition model into a larger untrained model and\ntrain the combined model to perform both addition and subtraction. We find\nextensive reuse of the addition circuits for both tasks, easing verification of\nthe more complex subtractor model. We discuss how inserting verified task\nmodules into LMs can leverage model reuse to improve verifiability and\ntrustworthiness of language models built using them. The reuse of verified\ncircuits reduces the effort to verify more complex composite models which we\nbelieve to be a significant step towards safety of language models.\n","authors":["Philip Quirke","Clement Neo","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2402.02619v8.pdf","comment":"8 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.11789v6","updated":"2024-07-12T00:17:04Z","published":"2023-03-20T08:37:08Z","title":"Random Inverse Problems Over Graphs: Decentralized Online Learning","summary":"  We establish a framework of distributed random inverse problems over network\ngraphs with online measurements, and propose a decentralized online learning\nalgorithm. This unifies the distributed parameter estimation in Hilbert spaces\nand the least mean square problem in reproducing kernel Hilbert spaces\n(RKHS-LMS). We transform the convergence of the algorithm into the asymptotic\nstability of a class of inhomogeneous random difference equations in Hilbert\nspaces with L2-bounded martingale difference terms and develop the L2\n-asymptotic stability theory in Hilbert spaces. It is shown that if the network\ngraph is connected and the sequence of forward operators satisfies the\ninfinite-dimensional spatio-temporal persistence of excitation condition, then\nthe estimates of all nodes are mean square and almost surely strongly\nconsistent. Moreover, we propose a decentralized online learning algorithm in\nRKHS based on non-stationary and non-independent online data streams, and prove\nthat the algorithm is mean square and almost surely strongly consistent if the\noperators induced by the random input data satisfy the infinite-dimensional\nspatio-temporal persistence of excitation condition.\n","authors":["Tao Li","Xiwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11789v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08898v1","updated":"2024-07-12T00:07:43Z","published":"2024-07-12T00:07:43Z","title":"IDAT: A Multi-Modal Dataset and Toolkit for Building and Evaluating\n  Interactive Task-Solving Agents","summary":"  Seamless interaction between AI agents and humans using natural language\nremains a key goal in AI research. This paper addresses the challenges of\ndeveloping interactive agents capable of understanding and executing grounded\nnatural language instructions through the IGLU competition at NeurIPS. Despite\nadvancements, challenges such as a scarcity of appropriate datasets and the\nneed for effective evaluation platforms persist. We introduce a scalable data\ncollection tool for gathering interactive grounded language instructions within\na Minecraft-like environment, resulting in a Multi-Modal dataset with around\n9,000 utterances and over 1,000 clarification questions. Additionally, we\npresent a Human-in-the-Loop interactive evaluation platform for qualitative\nanalysis and comparison of agent performance through multi-turn communication\nwith human annotators. We offer to the community these assets referred to as\nIDAT (IGLU Dataset And Toolkit) which aim to advance the development of\nintelligent, interactive AI agents and provide essential resources for further\nresearch.\n","authors":["Shrestha Mohanty","Negar Arabzadeh","Andrea Tupini","Yuxuan Sun","Alexey Skrynnik","Artem Zholus","Marc-Alexandre CÃ´tÃ©","Julia Kiseleva"],"pdf_url":"https://arxiv.org/pdf/2407.08898v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.09029v1","updated":"2024-07-12T06:44:42Z","published":"2024-07-12T06:44:42Z","title":"Enhancing Emotion Recognition in Incomplete Data: A Novel Cross-Modal\n  Alignment, Reconstruction, and Refinement Framework","summary":"  Multimodal emotion recognition systems rely heavily on the full availability\nof modalities, suffering significant performance declines when modal data is\nincomplete. To tackle this issue, we present the Cross-Modal Alignment,\nReconstruction, and Refinement (CM-ARR) framework, an innovative approach that\nsequentially engages in cross-modal alignment, reconstruction, and refinement\nphases to handle missing modalities and enhance emotion recognition. This\nframework utilizes unsupervised distribution-based contrastive learning to\nalign heterogeneous modal distributions, reducing discrepancies and modeling\nsemantic uncertainty effectively. The reconstruction phase applies normalizing\nflow models to transform these aligned distributions and recover missing\nmodalities. The refinement phase employs supervised point-based contrastive\nlearning to disrupt semantic correlations and accentuate emotional traits,\nthereby enriching the affective content of the reconstructed representations.\nExtensive experiments on the IEMOCAP and MSP-IMPROV datasets confirm the\nsuperior performance of CM-ARR under conditions of both missing and complete\nmodalities. Notably, averaged across six scenarios of missing modalities,\nCM-ARR achieves absolute improvements of 2.11% in WAR and 2.12% in UAR on the\nIEMOCAP dataset, and 1.71% and 1.96% in WAR and UAR, respectively, on the\nMSP-IMPROV dataset.\n","authors":["Haoqin Sun","Shiwan Zhao","Shaokai Li","Xiangyu Kong","Xuechen Wang","Aobo Kong","Jiaming Zhou","Yong Chen","Wenjia Zeng","Yong Qin"],"pdf_url":"https://arxiv.org/pdf/2407.09029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09026v1","updated":"2024-07-12T06:34:24Z","published":"2024-07-12T06:34:24Z","title":"HPC: Hierarchical Progressive Coding Framework for Volumetric Video","summary":"  Volumetric video based on Neural Radiance Field (NeRF) holds vast potential\nfor various 3D applications, but its substantial data volume poses significant\nchallenges for compression and transmission. Current NeRF compression lacks the\nflexibility to adjust video quality and bitrate within a single model for\nvarious network and device capacities. To address these issues, we propose HPC,\na novel hierarchical progressive volumetric video coding framework achieving\nvariable bitrate using a single model. Specifically, HPC introduces a\nhierarchical representation with a multi-resolution residual radiance field to\nreduce temporal redundancy in long-duration sequences while simultaneously\ngenerating various levels of detail. Then, we propose an end-to-end progressive\nlearning approach with a multi-rate-distortion loss function to jointly\noptimize both hierarchical representation and compression. Our HPC trained only\nonce can realize multiple compression levels, while the current methods need to\ntrain multiple fixed-bitrate models for different rate-distortion (RD)\ntradeoffs. Extensive experiments demonstrate that HPC achieves flexible quality\nlevels with variable bitrate by a single model and exhibits competitive RD\nperformance, even outperforming fixed-bitrate models across various datasets.\n","authors":["Zihan Zheng","Houqiang Zhong","Qiang Hu","Xiaoyun Zhang","Li Song","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09026v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.09705v1","updated":"2024-07-12T22:12:03Z","published":"2024-07-12T22:12:03Z","title":"Diagnosing and Re-learning for Balanced Multimodal Learning","summary":"  To overcome the imbalanced multimodal learning problem, where models prefer\nthe training of specific modalities, existing methods propose to control the\ntraining of uni-modal encoders from different perspectives, taking the\ninter-modal performance discrepancy as the basis. However, the intrinsic\nlimitation of modality capacity is ignored. The scarcely informative modalities\ncan be recognized as ``worse-learnt'' ones, which could force the model to\nmemorize more noise, counterproductively affecting the multimodal model\nability. Moreover, the current modality modulation methods narrowly concentrate\non selected worse-learnt modalities, even suppressing the training of others.\nHence, it is essential to consider the intrinsic limitation of modality\ncapacity and take all modalities into account during balancing. To this end, we\npropose the Diagnosing \\& Re-learning method. The learning state of each\nmodality is firstly estimated based on the separability of its uni-modal\nrepresentation space, and then used to softly re-initialize the corresponding\nuni-modal encoder. In this way, the over-emphasizing of scarcely informative\nmodalities is avoided. In addition, encoders of worse-learnt modalities are\nenhanced, simultaneously avoiding the over-training of other modalities.\nAccordingly, multimodal learning is effectively balanced and enhanced.\nExperiments covering multiple types of modalities and multimodal frameworks\ndemonstrate the superior performance of our simple-yet-effective method for\nbalanced multimodal learning. The source code and dataset are available at\n\\url{https://github.com/GeWu-Lab/Diagnosing_Relearning_ECCV2024}.\n","authors":["Yake Wei","Siwei Li","Ruoxuan Feng","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2407.09705v1.pdf","comment":"Accepted by ECCV 2024"}]},"2024-07-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.10969v1","updated":"2024-07-15T17:59:29Z","published":"2024-07-15T17:59:29Z","title":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated","summary":"  We introduce, Q-Sparse, a simple yet effective approach to training\nsparsely-activated large language models (LLMs). Q-Sparse enables full sparsity\nof activations in LLMs which can bring significant efficiency gains in\ninference. This is achieved by applying top-K sparsification to the activations\nand the straight-through-estimator to the training. The key results from this\nwork are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs\nwhile being much more efficient at inference time; (2) We present an\ninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is\neffective in different settings, including training-from-scratch,\ncontinue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for\nboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the\nsynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the\ncornerstone and a clear path to revolutionize the efficiency, including cost\nand energy consumption, of future LLMs.\n","authors":["Hongyu Wang","Shuming Ma","Ruiping Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.10969v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2407.10956v1","updated":"2024-07-15T17:54:37Z","published":"2024-07-15T17:54:37Z","title":"Spider2-V: How Far Are Multimodal Agents From Automating Data Science\n  and Engineering Workflows?","summary":"  Data science and engineering workflows often span multiple stages, from\nwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As\nvision language models (VLMs) advance in multimodal understanding and code\ngeneration, VLM-based agents could potentially automate these workflows by\ngenerating SQL queries, Python code, and GUI operations. This automation can\nimprove the productivity of experts while democratizing access to large-scale\ndata analysis. In this paper, we introduce Spider2-V, the first multimodal\nagent benchmark focusing on professional data science and engineering\nworkflows, featuring 494 real-world tasks in authentic computer environments\nand incorporating 20 enterprise-level professional applications. These tasks,\nderived from real-world use cases, evaluate the ability of a multimodal agent\nto perform data-related tasks by writing code and managing the GUI in\nenterprise data software systems. To balance realistic simulation with\nevaluation simplicity, we devote significant effort to developing automatic\nconfigurations for task setup and carefully crafting evaluation metrics for\neach task. Furthermore, we supplement multimodal agents with comprehensive\ndocuments of these enterprise data software systems. Our empirical evaluation\nreveals that existing state-of-the-art LLM/VLM-based agents do not reliably\nautomate full data workflows (14.0% success). Even with step-by-step guidance,\nthese agents still underperform in tasks that require fine-grained,\nknowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted\nworkspaces (10.6%). We hope that Spider2-V paves the way for autonomous\nmultimodal agents to transform the automation of data science and engineering\nworkflow. Our code and data are available at https://spider2-v.github.io.\n","authors":["Ruisheng Cao","Fangyu Lei","Haoyuan Wu","Jixuan Chen","Yeqiao Fu","Hongcheng Gao","Xinzhuang Xiong","Hanchong Zhang","Yuchen Mao","Wenjing Hu","Tianbao Xie","Hongshen Xu","Danyang Zhang","Sida Wang","Ruoxi Sun","Pengcheng Yin","Caiming Xiong","Ansong Ni","Qian Liu","Victor Zhong","Lu Chen","Kai Yu","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2407.10956v1.pdf","comment":"34 pages, 14 figures, 10 tables"},{"id":"http://arxiv.org/abs/2407.10953v1","updated":"2024-07-15T17:50:43Z","published":"2024-07-15T17:50:43Z","title":"MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with\n  Open-domain Information Extraction Large Language Models","summary":"  The Mutual Reinforcement Effect (MRE) represents a promising avenue in\ninformation extraction and multitasking research. Nevertheless, its\napplicability has been constrained due to the exclusive availability of MRE mix\ndatasets in Japanese, thereby limiting comprehensive exploration by the global\nresearch community. To address this limitation, we introduce a Multilingual MRE\nmix dataset (MMM) that encompasses 21 sub-datasets in English, Japanese, and\nChinese. In this paper, we also propose a method for dataset translation\nassisted by Large Language Models (LLMs), which significantly reduces the\nmanual annotation time required for dataset construction by leveraging LLMs to\ntranslate the original Japanese datasets. Additionally, we have enriched the\ndataset by incorporating open-domain Named Entity Recognition (NER) and\nsentence classification tasks. Utilizing this expanded dataset, we developed a\nunified input-output framework to train an Open-domain Information Extraction\nLarge Language Model (OIELLM). The OIELLM model demonstrates the capability to\neffectively process novel MMM datasets, exhibiting significant improvements in\nperformance.\n","authors":["Chengguang Gan","Qingyu Yin","Xinyang He","Hanjun Wei","Yunhao Liang","Younghun Lim","Shijian Wang","Hexiang Huang","Qinghao Zhang","Shiwen Ni","Tatsunori Mori"],"pdf_url":"https://arxiv.org/pdf/2407.10953v1.pdf","comment":"Under Review. 11 pages, 5 Figure"},{"id":"http://arxiv.org/abs/2407.10949v1","updated":"2024-07-15T17:45:53Z","published":"2024-07-15T17:45:53Z","title":"Representing Rule-based Chatbots with Transformers","summary":"  Transformer-based chatbots can conduct fluent, natural-sounding\nconversations, but we have limited understanding of the mechanisms underlying\ntheir behavior. Prior work has taken a bottom-up approach to understanding\nTransformers by constructing Transformers for various synthetic and formal\nlanguage tasks, such as regular expressions and Dyck languages. However, it is\nnot obvious how to extend this approach to understand more naturalistic\nconversational agents. In this work, we take a step in this direction by\nconstructing a Transformer that implements the ELIZA program, a classic,\nrule-based chatbot. ELIZA illustrates some of the distinctive challenges of the\nconversational setting, including both local pattern matching and long-term\ndialog state tracking. We build on constructions from prior work -- in\nparticular, for simulating finite-state automata -- showing how simpler\nconstructions can be composed and extended to give rise to more sophisticated\nbehavior. Next, we train Transformers on a dataset of synthetically generated\nELIZA conversations and investigate the mechanisms the models learn. Our\nanalysis illustrates the kinds of mechanisms these models tend to prefer -- for\nexample, models favor an induction head mechanism over a more precise, position\nbased copying mechanism; and using intermediate generations to simulate\nrecurrent data structures, like ELIZA's memory mechanisms. Overall, by drawing\nan explicit connection between neural chatbots and interpretable, symbolic\nmechanisms, our results offer a new setting for mechanistic analysis of\nconversational agents.\n","authors":["Dan Friedman","Abhishek Panigrahi","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10949v1.pdf","comment":"Code and data are available at\n  https://github.com/princeton-nlp/ELIZA-Transformer"},{"id":"http://arxiv.org/abs/2407.10944v1","updated":"2024-07-15T17:41:34Z","published":"2024-07-15T17:41:34Z","title":"Learning from Naturally Occurring Feedback","summary":"  Human feedback data is a critical component in developing language models.\nHowever, collecting this feedback is costly and ultimately not scalable. We\npropose a scalable method for extracting feedback that users naturally include\nwhen interacting with chat models, and leveraging it for model training. We are\nfurther motivated by previous work that showed there are also qualitative\nadvantages to using naturalistic (rather than auto-generated) feedback, such as\nless hallucinations and biases. We manually annotated conversation data to\nconfirm the presence of naturally occurring feedback in a standard corpus,\nfinding that as much as 30% of the chats include explicit feedback. We apply\nour method to over 1M conversations to obtain hundreds of thousands of feedback\nsamples. Training with the extracted feedback shows significant performance\nimprovements over baseline models, demonstrating the efficacy of our approach\nin enhancing model alignment to human preferences.\n","authors":["Shachar Don-Yehiya","Leshem Choshen","Omri Abend"],"pdf_url":"https://arxiv.org/pdf/2407.10944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11299v2","updated":"2024-07-15T17:37:11Z","published":"2024-03-17T18:42:38Z","title":"SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant","summary":"  Recent advances in vision-language models have shown notable generalization\nin broad tasks through visual instruction tuning. However, bridging the gap\nbetween the pre-trained vision encoder and the large language models (LLMs)\nbecomes the whole network's bottleneck. To improve cross-modality alignment,\nexisting works usually consider more visual instruction data covering a broader\nrange of vision tasks to fine-tune the model for question-answering, which,\nhowever, is costly to obtain and has not thoroughly explored the rich\ncontextual information contained in images. This paper first attempts to\nharness the overlooked context within visual instruction data, training the\nmodel to self-supervised \"learning\" how to ask high-quality questions. In this\nway, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large\nVision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible\nand meaningful image-related questions while analyzing the visual clue and\nprior language knowledge, signifying an advanced level of generalized visual\nunderstanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instruction\ndata shows a performance improvement compared with traditional\nvisual-instruction tuning methods. This improvement highlights the efficacy of\nself-questioning techniques in achieving a deeper and more nuanced\ncomprehension of visual content across various contexts.\n","authors":["Guohao Sun","Can Qin","Jiamian Wang","Zeyuan Chen","Ran Xu","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2403.11299v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10930v1","updated":"2024-07-15T17:30:31Z","published":"2024-07-15T17:30:31Z","title":"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better\n  Together","summary":"  Natural Language Processing (NLP) systems are increasingly taking the form of\nmulti-stage pipelines involving multiple distinct language models (LMs) and\nprompting strategies. Here we address the question of how to fine-tune such\nsystems to improve their performance. We cast this as a problem of optimizing\nthe underlying LM weights and the prompting strategies together, and consider a\nchallenging but highly realistic scenario in which we have no gold labels for\nany intermediate stages in the pipeline. To address this challenge, we evaluate\napproximate optimization strategies in which we bootstrap training labels for\nall pipeline stages and use these to optimize the pipeline's prompts and\nfine-tune its weights alternatingly. In experiments with multi-hop QA,\nmathematical reasoning, and feature-based classification, we find that simple\napproaches for optimizing the prompts and weights together outperform directly\noptimizing weights alone and prompts alone by up to 65% and 5%, respectively,\non average across LMs and tasks. We will release our new optimizers in DSPy at\nhttp://dspy.ai\n","authors":["Dilara Soylu","Christopher Potts","Omar Khattab"],"pdf_url":"https://arxiv.org/pdf/2407.10930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10920v1","updated":"2024-07-15T17:21:41Z","published":"2024-07-15T17:21:41Z","title":"Benchmarking Vision Language Models for Cultural Understanding","summary":"  Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.\n","authors":["Shravan Nayak","Kanishk Jain","Rabiul Awal","Siva Reddy","Sjoerd van Steenkiste","Lisa Anne Hendricks","Karolina StaÅczak","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.10920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10899v1","updated":"2024-07-15T16:49:26Z","published":"2024-07-15T16:49:26Z","title":"Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis","summary":"  Effective educational measurement relies heavily on the curation of\nwell-designed item pools (i.e., possessing the right psychometric properties).\nHowever, item calibration is time-consuming and costly, requiring a sufficient\nnumber of respondents for the response process. We explore using six different\nLLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus)\nand various combinations of them using sampling methods to produce responses\nwith psychometric properties similar to human answers. Results show that some\nLLMs have comparable or higher proficiency in College Algebra than college\nstudents. No single LLM mimics human respondents due to narrow proficiency\ndistributions, but an ensemble of LLMs can better resemble college students'\nability distribution. The item parameters calibrated by LLM-Respondents have\nhigh correlations (e.g. > 0.8 for GPT-3.5) compared to their human calibrated\ncounterparts, and closely resemble the parameters of the human subset (e.g.\n0.02 Spearman correlation difference). Several augmentation strategies are\nevaluated for their relative performance, with resampling methods proving most\neffective, enhancing the Spearman correlation from 0.89 (human only) to 0.93\n(augmented human).\n","authors":["Yunting Liu","Shreya Bhandari","Zachary A. Pardos"],"pdf_url":"https://arxiv.org/pdf/2407.10899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12128v2","updated":"2024-07-15T16:32:39Z","published":"2023-10-18T17:37:10Z","title":"DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM\n  Planning","summary":"  Text-to-image (T2I) generation has seen significant growth over the past few\nyears. Despite this, there has been little work on generating diagrams with T2I\nmodels. A diagram is a symbolic/schematic representation that explains\ninformation using structurally rich and spatially complex visualizations (e.g.,\na dense combination of related objects, text labels, directional arrows/lines,\netc.). Existing state-of-the-art T2I models often fail at diagram generation\nbecause they lack fine-grained object layout control when many objects are\ndensely connected via complex relations such as arrows/lines, and also often\nfail to render comprehensible text labels. To address this gap, we present\nDiagrammerGPT, a novel two-stage text-to-diagram generation framework\nleveraging the layout guidance capabilities of LLMs to generate more accurate\ndiagrams. In the first stage, we use LLMs to generate and iteratively refine\n'diagram plans' (in a planner-auditor feedback loop). In the second stage, we\nuse a diagram generator, DiagramGLIGEN, and a text label rendering module to\ngenerate diagrams (with clear text labels) following the diagram plans. To\nbenchmark the text-to-diagram generation task, we introduce AI2D-Caption, a\ndensely annotated diagram dataset built on top of the AI2D dataset. We show\nthat our DiagrammerGPT framework produces more accurate diagrams, outperforming\nexisting T2I models. We also provide comprehensive analysis, including\nopen-domain diagram generation, multi-platform vector graphic diagram\ngeneration, human-in-the-loop editing, and multimodal planner/auditor LLMs.\n","authors":["Abhay Zala","Han Lin","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2310.12128v2.pdf","comment":"COLM 2024; Project page: https://diagrammerGPT.github.io/"},{"id":"http://arxiv.org/abs/2406.11012v5","updated":"2024-07-15T16:17:52Z","published":"2024-06-16T17:10:32Z","title":"Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs\n  Using the New York Times Connections Word Game","summary":"  The New York Times Connections game has emerged as a popular and challenging\npursuit for word puzzle enthusiasts. We collect 200 Connections games to\nevaluate the performance of state-of-the-art large language models (LLMs)\nagainst expert and novice human players. Our results show that even the\nbest-performing LLM, GPT-4o, which has otherwise shown impressive reasoning\nabilities on a wide variety of benchmarks, can only fully solve 8% of the\ngames. Compared to GPT-4o, novice and expert players perform better, with\nexpert human players significantly outperforming GPT-4o. To deepen our\nunderstanding we create a taxonomy of the knowledge types required to\nsuccessfully categorize words in the Connections game, revealing that LLMs\nstruggle with associative, encyclopedic, and linguistic knowledge. Our findings\nestablish the New York Times Connections game as a challenging benchmark for\nevaluating abstract reasoning capabilities in humans and AI systems.\n","authors":["Prisha Samadarshi","Mariam Mustafa","Anushka Kulkarni","Raven Rothkopf","Tuhin Chakrabarty","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2406.11012v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10855v1","updated":"2024-07-15T16:07:13Z","published":"2024-07-15T16:07:13Z","title":"Weighted Grouped Query Attention in Transformers","summary":"  The attention mechanism forms the foundational blocks for transformer\nlanguage models. Recent approaches show that scaling the model achieves\nhuman-level performance. However, with increasing demands for scaling and\nconstraints on hardware memory, the inference costs of these models remain\nhigh. To reduce the inference time, Multi-Query Attention (MQA) and\nGrouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet\nal., 2023) respectively. In this paper, we propose a variation of Grouped-Query\nAttention, termed Weighted Grouped-Query Attention (WGQA). We introduced new\nlearnable parameters for each key and value head in the T5 decoder attention\nblocks, enabling the model to take a weighted average during finetuning. Our\nmodel achieves an average of 0.53% improvement over GQA, and the performance\nconverges to traditional Multi-head attention (MHA) with no additional overhead\nduring inference. We evaluated the introduction of these parameters and\nsubsequent finetuning informs the model about the grouping mechanism during\ntraining, thereby enhancing performance. Additionally, we demonstrate the\nscaling laws in our analysis by comparing the results between T5-small and\nT5-base architecture.\n","authors":["Sai Sena Chinnakonduru","Astarag Mohapatra"],"pdf_url":"https://arxiv.org/pdf/2407.10855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10853v1","updated":"2024-07-15T16:04:44Z","published":"2024-07-15T16:04:44Z","title":"An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases","summary":"  Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. This paper aims to provide a technical guide for\npractitioners to assess bias and fairness risks in LLM use cases. The main\ncontribution of this work is a decision framework that allows practitioners to\ndetermine which metrics to use for a specific LLM use case. To achieve this,\nthis study categorizes LLM bias and fairness risks, maps those risks to a\ntaxonomy of LLM use cases, and then formally defines various metrics to assess\neach type of risk. As part of this work, several new bias and fairness metrics\nare introduced, including innovative counterfactual metrics as well as metrics\nbased on stereotype classifiers. Instead of focusing solely on the model\nitself, the sensitivity of both prompt-risk and model-risk are taken into\naccount by defining evaluations at the level of an LLM use case, characterized\nby a model and a population of prompts. Furthermore, because all of the\nevaluation metrics are calculated solely using the LLM output, the proposed\nframework is highly practical and easily actionable for practitioners.\n","authors":["Dylan Bouchard"],"pdf_url":"https://arxiv.org/pdf/2407.10853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13124v2","updated":"2024-07-15T16:04:05Z","published":"2024-06-19T00:40:19Z","title":"Learning to Generate Answers with Citations via Factual Consistency\n  Models","summary":"  Large Language Models (LLMs) frequently hallucinate, impeding their\nreliability in mission-critical situations. One approach to address this issue\nis to provide citations to relevant sources alongside generated content,\nenhancing the verifiability of generations. However, citing passages accurately\nin answers remains a substantial challenge. This paper proposes a\nweakly-supervised fine-tuning method leveraging factual consistency models\n(FCMs). Our approach alternates between generating texts with citations and\nsupervised fine-tuning with FCM-filtered citation data. Focused learning is\nintegrated into the objective, directing the fine-tuning process to emphasise\nthe factual unit tokens, as measured by an FCM. Results on the ALCE few-shot\ncitation benchmark with various instruction-tuned LLMs demonstrate superior\nperformance compared to in-context learning, vanilla supervised fine-tuning,\nand state-of-the-art methods, with an average improvement of $34.1$, $15.5$,\nand $10.5$ citation F$_1$ points, respectively. Moreover, in a domain transfer\nsetting we show that the obtained citation generation ability robustly\ntransfers to unseen datasets. Notably, our citation improvements contribute to\nthe lowest factual error rate across baselines.\n","authors":["Rami Aly","Zhiqiang Tang","Samson Tan","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2406.13124v2.pdf","comment":"Accepted to ACL 2024. Code is available at\n  https://github.com/amazon-science/learning-to-generate-answers-with-citations"},{"id":"http://arxiv.org/abs/2407.10829v1","updated":"2024-07-15T15:42:22Z","published":"2024-07-15T15:42:22Z","title":"BiasScanner: Automatic Detection and Classification of News Bias to\n  Strengthen Democracy","summary":"  The increasing consumption of news online in the 21st century coincided with\nincreased publication of disinformation, biased reporting, hate speech and\nother unwanted Web content. We describe BiasScanner, an application that aims\nto strengthen democracy by supporting news consumers with scrutinizing news\narticles they are reading online. BiasScanner contains a server-side\npre-trained large language model to identify biased sentences of news articles\nand a front-end Web browser plug-in. At the time of writing, BiasScanner can\nidentify and classify more than two dozen types of media bias at the sentence\nlevel, making it the most fine-grained model and only deployed application\n(automatic system in use) of its kind. It was implemented in a light-weight and\nprivacy-respecting manner, and in addition to highlighting likely biased\nsentence it also provides explanations for each classification decision as well\nas a summary analysis for each news article. While prior research has addressed\nnews bias detection, we are not aware of any work that resulted in a deployed\nbrowser plug-in (c.f. also biasscanner.org for a Web demo).\n","authors":["Tim Menzner","Jochen L. Leidner"],"pdf_url":"https://arxiv.org/pdf/2407.10829v1.pdf","comment":"10 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.10827v1","updated":"2024-07-15T15:38:51Z","published":"2024-07-15T15:38:51Z","title":"LLM Circuit Analyses Are Consistent Across Training and Scale","summary":"  Most currently deployed large language models (LLMs) undergo continuous\ntraining or additional finetuning. By contrast, most research into LLMs'\ninternal mechanisms focuses on models at one snapshot in time (the end of\npre-training), raising the question of whether their results generalize to\nreal-world settings. Existing studies of mechanisms over time focus on\nencoder-only or toy models, which differ significantly from most deployed\nmodels. In this study, we track how model mechanisms, operationalized as\ncircuits, emerge and evolve across 300 billion tokens of training in\ndecoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.\nWe find that task abilities and the functional components that support them\nemerge consistently at similar token counts across scale. Moreover, although\nsuch components may be implemented by different attention heads over time, the\noverarching algorithm that they implement remains. Surprisingly, both these\nalgorithms and the types of components involved therein can replicate across\nmodel scale. These results suggest that circuit analyses conducted on small\nmodels at the end of pre-training can provide insights that still apply after\nadditional pre-training and over model scale.\n","authors":["Curt Tigges","Michael Hanna","Qinan Yu","Stella Biderman"],"pdf_url":"https://arxiv.org/pdf/2407.10827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10817v1","updated":"2024-07-15T15:33:45Z","published":"2024-07-15T15:33:45Z","title":"Foundational Autoraters: Taming Large Language Models for Better\n  Automatic Evaluation","summary":"  As large language models (LLMs) advance, it becomes more challenging to\nreliably evaluate their output due to the high costs of human evaluation. To\nmake progress towards better LLM autoraters, we introduce FLAMe, a family of\nFoundational Large Autorater Models. FLAMe is trained on our large and diverse\ncollection of 100+ quality assessment tasks comprising 5M+ human judgments,\ncurated and standardized using publicly released human evaluations from\nprevious research. FLAMe significantly improves generalization to a wide\nvariety of held-out tasks, outperforming LLMs trained on proprietary data like\nGPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a\npowerful starting point for further downstream fine-tuning, using reward\nmodeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our\nFLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative\nmodel trained exclusively on permissively licensed data, outperforming both\nGPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more\ncomputationally efficient approach using a novel tail-patch fine-tuning\nstrategy to optimize our FLAMe multitask mixture for reward modeling evaluation\n(FLAMe-Opt-RM), offering competitive RewardBench performance while requiring\napproximately 25x less training datapoints. Overall, our FLAMe variants\noutperform all popular proprietary LLM-as-a-Judge models we consider across 8\nout of 12 autorater evaluation benchmarks, encompassing 53 quality assessment\ntasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals\nthat FLAMe is significantly less biased than these LLM-as-a-Judge models on the\nCoBBLEr autorater bias benchmark, while effectively identifying high-quality\nresponses for code generation.\n","authors":["Tu Vu","Kalpesh Krishna","Salaheddin Alzubi","Chris Tar","Manaal Faruqui","Yun-Hsuan Sung"],"pdf_url":"https://arxiv.org/pdf/2407.10817v1.pdf","comment":"31 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.10807v1","updated":"2024-07-15T15:23:21Z","published":"2024-07-15T15:23:21Z","title":"Employing Sentence Space Embedding for Classification of Data Stream\n  from Fake News Domain","summary":"  Tabular data is considered the last unconquered castle of deep learning, yet\nthe task of data stream classification is stated to be an equally important and\ndemanding research area. Due to the temporal constraints, it is assumed that\ndeep learning methods are not the optimal solution for application in this\nfield. However, excluding the entire -- and prevalent -- group of methods seems\nrather rash given the progress that has been made in recent years in its\ndevelopment. For this reason, the following paper is the first to present an\napproach to natural language data stream classification using the sentence\nspace method, which allows for encoding text into the form of a discrete\ndigital signal. This allows the use of convolutional deep networks dedicated to\nimage classification to solve the task of recognizing fake news based on text\ndata. Based on the real-life Fakeddit dataset, the proposed approach was\ncompared with state-of-the-art algorithms for data stream classification based\non generalization ability and time complexity.\n","authors":["PaweÅ Zyblewski","Jakub Klikowski","Weronika Borek-Marciniec","PaweÅ Ksieniewicz"],"pdf_url":"https://arxiv.org/pdf/2407.10807v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.15770v2","updated":"2024-07-15T15:22:15Z","published":"2024-04-24T09:44:44Z","title":"ChEX: Interactive Localization and Region Description in Chest X-rays","summary":"  Report generation models offer fine-grained textual interpretations of\nmedical images like chest X-rays, yet they often lack interactivity (i.e. the\nability to steer the generation process through user queries) and localized\ninterpretability (i.e. visually grounding their predictions), which we deem\nessential for future adoption in clinical practice. While there have been\nefforts to tackle these issues, they are either limited in their interactivity\nby not supporting textual queries or fail to also offer localized\ninterpretability. Therefore, we propose a novel multitask architecture and\ntraining paradigm integrating textual prompts and bounding boxes for diverse\naspects like anatomical regions and pathologies. We call this approach the\nChest X-Ray Explainer (ChEX). Evaluations across a heterogeneous set of 9 chest\nX-ray tasks, including localized image interpretation and report generation,\nshowcase its competitiveness with SOTA models while additional analysis\ndemonstrates ChEX's interactive capabilities. Code:\nhttps://github.com/philip-mueller/chex\n","authors":["Philip MÃ¼ller","Georgios Kaissis","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2404.15770v2.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10805v1","updated":"2024-07-15T15:20:40Z","published":"2024-07-15T15:20:40Z","title":"Think-on-Graph 2.0: Deep and Interpretable Large Language Model\n  Reasoning with Knowledge Graph-guided Retrieval","summary":"  Retrieval-augmented generation (RAG) has significantly advanced large\nlanguage models (LLMs) by enabling dynamic information retrieval to mitigate\nknowledge gaps and hallucinations in generated content. However, these systems\noften falter with complex reasoning and consistency across diverse queries. In\nthis work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns\nquestions with the knowledge graph and uses it as a navigational tool, which\ndeepens and refines the RAG paradigm for information collection and\nintegration. The KG-guided navigation fosters deep and long-range associations\nto uphold logical consistency and optimize the scope of retrieval for precision\nand interoperability. In conjunction, factual consistency can be better ensured\nthrough semantic similarity guided by precise directives. ToG${2.0}$ not only\nimproves the accuracy and reliability of LLMs' responses but also demonstrates\nthe potential of hybrid structured knowledge systems to significantly advance\nLLM reasoning, aligning it closer to human-like performance. We conducted\nextensive experiments on four public datasets to demonstrate the advantages of\nour method compared to the baseline.\n","authors":["Shengjie Ma","Chengjin Xu","Xuhui Jiang","Muzhi Li","Huaren Qu","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2407.10805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10804v1","updated":"2024-07-15T15:20:13Z","published":"2024-07-15T15:20:13Z","title":"Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning\n  and Format Alignment","summary":"  Adapting general large language models (LLMs) to specialized domains presents\ngreat challenges due to varied data distributions. This adaptation typically\nrequires continual pre-training on massive domain-specific corpora to\nfacilitate knowledge memorization, followed by training to apply this knowledge\nfollowing human instructions and preferences. However, this method may result\nin inefficient knowledge memorization due to a lack of awareness of knowledge\nutilization and imposes substantial demands on LLMs to simultaneously learn\nknowledge utilization and format alignment with limited training samples. To\nfacilitate the domain adaptation of LLM, we revise this process and propose a\nnew domain adaptation framework including domain knowledge learning and general\nformat alignment, called Mix-CPT. Specifically, we first conduct a knowledge\nmixture continual pre-training that concurrently focuses on knowledge\nmemorization and utilization, allowing for mutual reinforcement. To avoid\ncatastrophic forgetting during the continual pre-training process, we further\nincorporate a logit swap self-distillation constraint. Subsequently, leveraging\nthe knowledge and capabilities acquired during continual pre-training, we\nefficiently perform instruction tuning and alignment with a few general\ntraining samples to achieve format alignment. Extensive experiments demonstrate\nthat our proposed Mix-CPT framework can simultaneously improve the task-solving\ncapabilities of LLMs on the target and general domains compared to the\ntraditional adaptation methods.\n","authors":["Jinhao Jiang","Junyi Li","Wayne Xin Zhao","Yang Song","Tao Zhang","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2407.10804v1.pdf","comment":"LLM, CPT, knowledge learning, format alignment; work in progress"},{"id":"http://arxiv.org/abs/2407.10795v1","updated":"2024-07-15T15:14:01Z","published":"2024-07-15T15:14:01Z","title":"Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping","summary":"  Decoding by contrasting layers (DoLa), is designed to improve the generation\nquality of large language models (LLMs) by contrasting the prediction\nprobabilities between an early exit output (amateur logits) and the final\noutput (expert logits). However, we find that this approach does not work well\non non-English tasks. Inspired by previous interpretability work on language\ntransition during the model's forward pass, we discover that this issue arises\nfrom a language mismatch between early exit output and final output. In this\nwork, we propose an improved contrastive decoding algorithm that is effective\nfor diverse languages beyond English. To obtain more helpful amateur logits, we\ndevise two strategies to skip a set of bottom, language-agnostic layers based\non our preliminary analysis. Experimental results on multilingual reasoning\nbenchmarks demonstrate that our proposed method outperforms previous\ncontrastive decoding baselines and substantially improves LLM's\nchain-of-thought reasoning accuracy across 11 languages. The project will be\navailable at: https://github.com/NJUNLP/SkipLayerCD.\n","authors":["Wenhao Zhu","Sizhe Liu","Shujian Huang","Shuaijie She","Chris Wendler","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10794v1","updated":"2024-07-15T15:13:49Z","published":"2024-07-15T15:13:49Z","title":"Graphusion: Leveraging Large Language Models for Scientific Knowledge\n  Graph Fusion and Construction in NLP Education","summary":"  Knowledge graphs (KGs) are crucial in the field of artificial intelligence\nand are widely applied in downstream tasks, such as enhancing Question\nAnswering (QA) systems. The construction of KGs typically requires significant\neffort from domain experts. Recently, Large Language Models (LLMs) have been\nused for knowledge graph construction (KGC), however, most existing approaches\nfocus on a local perspective, extracting knowledge triplets from individual\nsentences or documents. In this work, we introduce Graphusion, a zero-shot KGC\nframework from free text. The core fusion module provides a global view of\ntriplets, incorporating entity merging, conflict resolution, and novel triplet\ndiscovery. We showcase how Graphusion could be applied to the natural language\nprocessing (NLP) domain and validate it in the educational scenario.\nSpecifically, we introduce TutorQA, a new expert-verified benchmark for graph\nreasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our\nevaluation demonstrates that Graphusion surpasses supervised baselines by up to\n10% in accuracy on link prediction. Additionally, it achieves average scores of\n2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and\nrelation recognition, respectively.\n","authors":["Rui Yang","Boming Yang","Sixun Ouyang","Tianwei She","Aosong Feng","Yuang Jiang","Freddy Lecue","Jinghui Lu","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2407.10794v1.pdf","comment":"24 pages, 11 figures, 13 tables. arXiv admin note: substantial text\n  overlap with arXiv:2402.14293"},{"id":"http://arxiv.org/abs/2407.10793v1","updated":"2024-07-15T15:11:16Z","published":"2024-07-15T15:11:16Z","title":"GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation\n  Framework","summary":"  Methods to evaluate Large Language Model (LLM) responses and detect\ninconsistencies, also known as hallucinations, with respect to the provided\nknowledge, are becoming increasingly important for LLM applications. Current\nmetrics fall short in their ability to provide explainable decisions,\nsystematically check all pieces of information in the response, and are often\ntoo computationally expensive to be used in practice. We present GraphEval: a\nhallucination evaluation framework based on representing information in\nKnowledge Graph (KG) structures. Our method identifies the specific triples in\nthe KG that are prone to hallucinations and hence provides more insight into\nwhere in the response a hallucination has occurred, if at all, than previous\nmethods. Furthermore, using our approach in conjunction with state-of-the-art\nnatural language inference (NLI) models leads to an improvement in balanced\naccuracy on various hallucination benchmarks, compared to using the raw NLI\nmodels. Lastly, we explore the use of GraphEval for hallucination correction by\nleveraging the structure of the KG, a method we name GraphCorrect, and\ndemonstrate that the majority of hallucinations can indeed be rectified.\n","authors":["Hannah Sansford","Nicholas Richardson","Hermina Petric Maretic","Juba Nait Saada"],"pdf_url":"https://arxiv.org/pdf/2407.10793v1.pdf","comment":"12 pages, to be published at KiL'24: Workshop on Knowledge-infused\n  Learning co-located with 30th ACM KDD Conference, August 26, 2024, Barcelona,\n  Spain"},{"id":"http://arxiv.org/abs/2403.18684v2","updated":"2024-07-15T14:48:09Z","published":"2024-03-27T15:27:36Z","title":"Scaling Laws For Dense Retrieval","summary":"  Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.\n","authors":["Yan Fang","Jingtao Zhan","Qingyao Ai","Jiaxin Mao","Weihang Su","Jia Chen","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18684v2.pdf","comment":"Accepted at SIGIR 2024. V2 fixes a bug in the experiments"},{"id":"http://arxiv.org/abs/2407.10759v1","updated":"2024-07-15T14:38:09Z","published":"2024-07-15T14:38:09Z","title":"Qwen2-Audio Technical Report","summary":"  We introduce the latest progress of Qwen-Audio, a large-scale audio-language\nmodel called Qwen2-Audio, which is capable of accepting various audio signal\ninputs and performing audio analysis or direct textual responses with regard to\nspeech instructions. In contrast to complex hierarchical tags, we have\nsimplified the pre-training process by utilizing natural language prompts for\ndifferent data and tasks, and have further expanded the data volume. We have\nboosted the instruction-following capability of Qwen2-Audio and implemented two\ndistinct audio interaction modes for voice chat and audio analysis. In the\nvoice chat mode, users can freely engage in voice interactions with Qwen2-Audio\nwithout text input. In the audio analysis mode, users could provide audio and\ntext instructions for analysis during the interaction. Note that we do not use\nany system prompts to switch between voice chat and audio analysis modes.\nQwen2-Audio is capable of intelligently comprehending the content within audio\nand following voice commands to respond appropriately. For instance, in an\naudio segment that simultaneously contains sounds, multi-speaker conversations,\nand a voice command, Qwen2-Audio can directly understand the command and\nprovide an interpretation and response to the audio. Additionally, DPO has\noptimized the model's performance in terms of factuality and adherence to\ndesired behavior. According to the evaluation results from AIR-Bench,\nQwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests\nfocused on audio-centric instruction-following capabilities. Qwen2-Audio is\nopen-sourced with the aim of fostering the advancement of the multi-modal\nlanguage community.\n","authors":["Yunfei Chu","Jin Xu","Qian Yang","Haojie Wei","Xipin Wei","Zhifang Guo","Yichong Leng","Yuanjun Lv","Jinzheng He","Junyang Lin","Chang Zhou","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.10759v1.pdf","comment":"https://github.com/QwenLM/Qwen2-Audio. Checkpoints, codes and scripts\n  will be opensoursed soon"},{"id":"http://arxiv.org/abs/2407.10747v1","updated":"2024-07-15T14:20:09Z","published":"2024-07-15T14:20:09Z","title":"Codebook LLMs: Adapting Political Science Codebooks for LLM Use and\n  Adapting LLMs to Follow Codebooks","summary":"  Codebooks -- documents that operationalize constructs and outline annotation\nprocedures -- are used almost universally by social scientists when coding\nunstructured political texts. Recently, to reduce manual annotation costs,\npolitical scientists have looked to generative large language models (LLMs) to\nlabel and analyze text data. However, previous work using LLMs for\nclassification has implicitly relied on the universal label assumption --\ncorrect classification of documents is possible using only a class label or\nminimal definition and the information that the LLM inductively learns during\nits pre-training. In contrast, we argue that political scientists who care\nabout valid measurement should instead make a codebook-construct label\nassumption -- an LLM should follow the definition and exclusion criteria of a\nconstruct/label provided in a codebook. In this work, we collect and curate\nthree political science datasets and their original codebooks and conduct a set\nof experiments to understand whether LLMs comply with codebook instructions,\nwhether rewriting codebooks improves performance, and whether\ninstruction-tuning LLMs on codebook-document-label tuples improves performance\nover zero-shot classification. Using Mistral 7B Instruct as our LLM, we find\nre-structuring the original codebooks gives modest gains in zero-shot\nperformance but the model still struggles to comply with the constraints of the\ncodebooks. Optimistically, instruction-tuning Mistral on one of our datasets\ngives significant gains over zero-shot inference (0.76 versus 0.53 micro F1).\nWe hope our conceptualization of the codebook-specific task, assumptions, and\ninstruction-tuning pipeline as well our semi-structured LLM codebook format\nwill help political scientists readily adapt to the LLM era.\n","authors":["Andrew Halterman","Katherine A. Keith"],"pdf_url":"https://arxiv.org/pdf/2407.10747v1.pdf","comment":"Presented at PolMeth 2024"},{"id":"http://arxiv.org/abs/2407.10745v1","updated":"2024-07-15T14:18:47Z","published":"2024-07-15T14:18:47Z","title":"What distinguishes conspiracy from critical narratives? A computational\n  analysis of oppositional discourse","summary":"  The current prevalence of conspiracy theories on the internet is a\nsignificant issue, tackled by many computational approaches. However, these\napproaches fail to recognize the relevance of distinguishing between texts\nwhich contain a conspiracy theory and texts which are simply critical and\noppose mainstream narratives. Furthermore, little attention is usually paid to\nthe role of inter-group conflict in oppositional narratives. We contribute by\nproposing a novel topic-agnostic annotation scheme that differentiates between\nconspiracies and critical texts, and that defines span-level categories of\ninter-group conflict. We also contribute with the multilingual\nXAI-DisInfodemics corpus (English and Spanish), which contains a high-quality\nannotation of Telegram messages related to COVID-19 (5,000 messages per\nlanguage). We also demonstrate the feasibility of an NLP-based automatization\nby performing a range of experiments that yield strong baseline solutions.\nFinally, we perform an analysis which demonstrates that the promotion of\nintergroup conflict and the presence of violence and anger are key aspects to\ndistinguish between the two types of oppositional narratives, i.e., conspiracy\nvs. critical.\n","authors":["Damir KorenÄiÄ","Berta Chulvi","Xavier Bonet Casals","Alejandro Toselli","Mariona TaulÃ©","Paolo Rosso"],"pdf_url":"https://arxiv.org/pdf/2407.10745v1.pdf","comment":"submitted to the Expert Systems journal"},{"id":"http://arxiv.org/abs/2404.08666v2","updated":"2024-07-15T14:07:16Z","published":"2024-03-31T15:13:15Z","title":"Revealing Trends in Datasets from the 2022 ACL and EMNLP Conferences","summary":"  Natural language processing (NLP) has grown significantly since the advent of\nthe Transformer architecture. Transformers have given birth to pre-trained\nlarge language models (PLMs). There has been tremendous improvement in the\nperformance of NLP systems across several tasks. NLP systems are on par or, in\nsome cases, better than humans at accomplishing specific tasks. However, it\nremains the norm that \\emph{better quality datasets at the time of pretraining\nenable PLMs to achieve better performance, regardless of the task.} The need to\nhave quality datasets has prompted NLP researchers to continue creating new\ndatasets to satisfy particular needs. For example, the two top NLP conferences,\nACL and EMNLP, accepted ninety-two papers in 2022, introducing new datasets.\nThis work aims to uncover the trends and insights mined within these datasets.\nMoreover, we provide valuable suggestions to researchers interested in curating\ndatasets in the future.\n","authors":["Jesse Atuhurra","Hidetaka Kamigaito"],"pdf_url":"https://arxiv.org/pdf/2404.08666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10735v1","updated":"2024-07-15T14:01:35Z","published":"2024-07-15T14:01:35Z","title":"Transforming Agency. On the mode of existence of Large Language Models","summary":"  This paper investigates the ontological characterization of Large Language\nModels (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we\npay special attention to their status as agents. This requires explaining in\ndetail the architecture, processing, and training procedures that enable LLMs\nto display their capacities, and the extensions used to turn LLMs into\nagent-like systems. After a systematic analysis we conclude that a LLM fails to\nmeet necessary and sufficient conditions for autonomous agency in the light of\nembodied theories of mind: the individuality condition (it is not the product\nof its own activity, it is not even directly affected by it), the normativity\ncondition (it does not generate its own norms or goals), and, partially the\ninteractional asymmetry condition (it is not the origin and sustained source of\nits interaction with the environment). If not agents, then ... what are LLMs?\nWe argue that ChatGPT should be characterized as an interlocutor or linguistic\nautomaton, a library-that-talks, devoid of (autonomous) agency, but capable to\nengage performatively on non-purposeful yet purpose-structured and\npurpose-bounded tasks. When interacting with humans, a \"ghostly\" component of\nthe human-machine interaction makes it possible to enact genuine conversational\nexperiences with LLMs. Despite their lack of sensorimotor and biological\nembodiment, LLMs textual embodiment (the training corpus) and resource-hungry\ncomputational embodiment, significantly transform existing forms of human\nagency. Beyond assisted and extended agency, the LLM-human coupling can produce\nmidtended forms of agency, closer to the production of intentional agency than\nto the extended instrumentality of any previous technologies.\n","authors":["Xabier E. Barandiaran","Lola S. Almendros"],"pdf_url":"https://arxiv.org/pdf/2407.10735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17159v2","updated":"2024-07-15T13:57:56Z","published":"2024-05-27T13:33:29Z","title":"Stop! In the Name of Flaws: Disentangling Personal Names and\n  Sociodemographic Attributes in NLP","summary":"  Personal names simultaneously differentiate individuals and categorize them\nin ways that are important in a given society. While the natural language\nprocessing community has thus associated personal names with sociodemographic\ncharacteristics in a variety of tasks, researchers have engaged to varying\ndegrees with the established methodological problems in doing so. To guide\nfuture work that uses names and sociodemographic characteristics, we provide an\noverview of relevant research: first, we present an interdisciplinary\nbackground on names and naming. We then survey the issues inherent to\nassociating names with sociodemographic attributes, covering problems of\nvalidity (e.g., systematic error, construct validity), as well as ethical\nconcerns (e.g., harms, differential impact, cultural insensitivity). Finally,\nwe provide guiding questions along with normative recommendations to avoid\nvalidity and ethical pitfalls when dealing with names and sociodemographic\ncharacteristics in natural language processing.\n","authors":["Vagrant Gautam","Arjun Subramonian","Anne Lauscher","Os Keyes"],"pdf_url":"https://arxiv.org/pdf/2405.17159v2.pdf","comment":"Gender Bias in Natural Language Processing Workshop at ACL 2024"},{"id":"http://arxiv.org/abs/2407.10725v1","updated":"2024-07-15T13:51:37Z","published":"2024-07-15T13:51:37Z","title":"CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated\n  Responses","summary":"  The rapid progress in Large Language Models (LLMs) poses potential risks such\nas generating unethical content. Assessing LLMs' values can help expose their\nmisalignment, but relies on reference-free evaluators, e.g., fine-tuned LLMs or\nclose-source ones like GPT-4, to identify values reflected in generated\nresponses. Nevertheless, these evaluators face two challenges in open-ended\nvalue evaluation: they should align with changing human value definitions with\nminimal annotation, against their own bias (adaptability), and detect varying\nvalue expressions and scenarios robustly (generalizability). To handle these\nchallenges, we introduce CLAVE, a novel framework which integrates two\ncomplementary LLMs, a large one to extract high-level value concepts from a few\nhuman labels, leveraging its extensive knowledge and generalizability, and a\nsmaller one fine-tuned on such concepts to better align with human value\nunderstanding. This dual-model approach enables calibration with any value\nsystems using <100 human-labeled samples per value type. Then we present\nValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples\nacross diverse domains, covering three major value systems. We benchmark the\ncapabilities of 12+ popular LLM evaluators and analyze their strengths and\nweaknesses. Our findings reveal that combining fine-tuned small models and\nprompt-based large ones serves as a superior balance in value evaluation.\n","authors":["Jing Yao","Xiaoyuan Yi","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2407.10725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10718v1","updated":"2024-07-15T13:45:40Z","published":"2024-07-15T13:45:40Z","title":"Sibyl: Simple yet Effective Agent Framework for Complex Real-world\n  Reasoning","summary":"  Existing agents based on large language models (LLMs) demonstrate robust\nproblem-solving capabilities by integrating LLMs' inherent knowledge, strong\nin-context learning and zero-shot capabilities, and the use of tools combined\nwith intricately designed LLM invocation workflows by humans. However, these\nagents still exhibit shortcomings in long-term reasoning and under-use the\npotential of existing tools, leading to noticeable deficiencies in complex\nreal-world reasoning scenarios. To address these limitations, we introduce\nSibyl, a simple yet powerful LLM-based agent framework designed to tackle\ncomplex reasoning tasks by efficiently leveraging a minimal set of tools.\nDrawing inspiration from Global Workspace Theory, Sibyl incorporates a global\nworkspace to enhance the management and sharing of knowledge and conversation\nhistory throughout the system. Furthermore, guided by Society of Mind Theory,\nSibyl implements a multi-agent debate-based jury to self-refine the final\nanswers, ensuring a comprehensive and balanced approach. This approach aims to\nreduce system complexity while expanding the scope of problems solvable-from\nmatters typically resolved by humans in minutes to those requiring hours or\neven days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl\nhas been designed with a focus on scalability and ease of debugging by\nincorporating the concept of reentrancy from functional programming from its\ninception, with the aim of seamless and low effort integration in other LLM\napplications to improve capabilities. Our experimental results on the GAIA\nbenchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves\nstate-of-the-art performance with an average score of 34.55%, compared to other\nagents based on GPT-4. We hope that Sibyl can inspire more reliable and\nreusable LLM-based agent solutions to address complex real-world reasoning\ntasks.\n","authors":["Yulong Wang","Tianhao Shen","Lifeng Liu","Jian Xie"],"pdf_url":"https://arxiv.org/pdf/2407.10718v1.pdf","comment":"Our code is available at https://github.com/Ag2S1/Sibyl-System"},{"id":"http://arxiv.org/abs/2405.17653v3","updated":"2024-07-15T13:30:52Z","published":"2024-05-27T20:53:22Z","title":"InversionView: A General-Purpose Method for Reading Information from\n  Neural Activations","summary":"  The inner workings of neural networks can be better understood if we can\nfully decipher the information encoded in neural activations. In this paper, we\nargue that this information is embodied by the subset of inputs that give rise\nto similar activations. Computing such subsets is nontrivial as the input space\nis exponentially large. We propose InversionView, which allows us to\npractically inspect this subset by sampling from a trained decoder model\nconditioned on activations. This helps uncover the information content of\nactivation vectors, and facilitates understanding of the algorithms implemented\nby transformer models. We present four case studies where we investigate models\nranging from small transformers to GPT-2. In these studies, we demonstrate the\ncharacteristics of our method, show the distinctive advantages it offers, and\nprovide causally verified circuits.\n","authors":["Xinting Huang","Madhur Panwar","Navin Goyal","Michael Hahn"],"pdf_url":"https://arxiv.org/pdf/2405.17653v3.pdf","comment":"ICML 2024 Mechanistic Interpretability Workshop oral"},{"id":"http://arxiv.org/abs/2407.10701v1","updated":"2024-07-15T13:17:42Z","published":"2024-07-15T13:17:42Z","title":"DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems","summary":"  Recently, there has been a growing interest among large language model (LLM)\ndevelopers in LLM-based document reading systems, which enable users to upload\ntheir own documents and pose questions related to the document contents, going\nbeyond simple reading comprehension tasks. Consequently, these systems have\nbeen carefully designed to tackle challenges such as file parsing, metadata\nextraction, multi-modal information understanding and long-context reading.\nHowever, no current benchmark exists to evaluate their performance in such\nscenarios, where a raw file and questions are provided as input, and a\ncorresponding response is expected as output. In this paper, we introduce\nDocBench, a new benchmark designed to evaluate LLM-based document reading\nsystems. Our benchmark involves a meticulously crafted process, including the\nrecruitment of human annotators and the generation of synthetic questions. It\nincludes 229 real documents and 1,102 questions, spanning across five different\ndomains and four major types of questions. We evaluate both proprietary\nLLM-based systems accessible via web interfaces or APIs, and a parse-then-read\npipeline employing open-source LLMs. Our evaluations reveal noticeable gaps\nbetween existing LLM-based document reading systems and human performance,\nunderscoring the challenges of developing proficient systems. To summarize,\nDocBench aims to establish a standardized benchmark for evaluating LLM-based\ndocument reading systems under diverse real-world scenarios, thereby guiding\nfuture advancements in this research area.\n","authors":["Anni Zou","Wenhao Yu","Hongming Zhang","Kaixin Ma","Deng Cai","Zhuosheng Zhang","Hai Zhao","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2407.10701v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2402.18659v2","updated":"2024-07-15T13:10:32Z","published":"2024-02-28T19:09:08Z","title":"Large Language Models and Games: A Survey and Roadmap","summary":"  Recent years have seen an explosive increase in research on large language\nmodels (LLMs), and accompanying public engagement on the topic. While starting\nas a niche area within natural language processing, LLMs have shown remarkable\npotential across a broad range of applications and domains, including games.\nThis paper surveys the current state of the art across the various applications\nof LLMs in and for games, and identifies the different roles LLMs can take\nwithin a game. Importantly, we discuss underexplored areas and promising\ndirections for future uses of LLMs in games and we reconcile the potential and\nlimitations of LLMs within the games domain. As the first comprehensive survey\nand roadmap at the intersection of LLMs and games, we are hopeful that this\npaper will serve as the basis for groundbreaking research and innovation in\nthis exciting new field.\n","authors":["Roberto Gallotta","Graham Todd","Marvin Zammit","Sam Earle","Antonios Liapis","Julian Togelius","Georgios N. Yannakakis"],"pdf_url":"https://arxiv.org/pdf/2402.18659v2.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.05070v2","updated":"2024-07-15T13:06:13Z","published":"2024-02-07T18:21:17Z","title":"A Roadmap to Pluralistic Alignment","summary":"  With increased power and prevalence of AI systems, it is ever more critical\nthat AI systems are designed to serve all, i.e., people with diverse values and\nperspectives. However, aligning models to serve pluralistic human values\nremains an open research question. In this piece, we propose a roadmap to\npluralistic alignment, specifically using language models as a test bed. We\nidentify and formalize three possible ways to define and operationalize\npluralism in AI systems: 1) Overton pluralistic models that present a spectrum\nof reasonable responses; 2) Steerably pluralistic models that can steer to\nreflect certain perspectives; and 3) Distributionally pluralistic models that\nare well-calibrated to a given population in distribution. We also formalize\nand discuss three possible classes of pluralistic benchmarks: 1)\nMulti-objective benchmarks, 2) Trade-off steerable benchmarks, which\nincentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic\nbenchmarks which explicitly model diverse human ratings. We use this framework\nto argue that current alignment techniques may be fundamentally limited for\npluralistic AI; indeed, we highlight empirical evidence, both from our own\nexperiments and from other work, that standard alignment procedures might\nreduce distributional pluralism in models, motivating the need for further\nresearch on pluralistic alignment.\n","authors":["Taylor Sorensen","Jared Moore","Jillian Fisher","Mitchell Gordon","Niloofar Mireshghallah","Christopher Michael Rytting","Andre Ye","Liwei Jiang","Ximing Lu","Nouha Dziri","Tim Althoff","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2402.05070v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2407.10691v1","updated":"2024-07-15T13:04:09Z","published":"2024-07-15T13:04:09Z","title":"$\\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific\n  Domain through Complementary Granularity","summary":"  Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,\nrespectively, averaged on queries containing multiple subqueries from five\nscientific retrieval datasets. Moreover, the efficacy of two downstream\nscientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$to boost the application of LLMs in the scientific domain.\n","authors":["Fengyu Cai","Xinran Zhao","Tong Chen","Sihao Chen","Hongming Zhang","Iryna Gurevych","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2407.10691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10259v2","updated":"2024-07-15T13:00:46Z","published":"2024-04-16T03:26:43Z","title":"Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy","summary":"  The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events.\n","authors":["Tunazzina Islam","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2404.10259v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00463v3","updated":"2024-07-15T12:56:28Z","published":"2024-06-29T15:20:11Z","title":"Open-Source Conversational AI with SpeechBrain 1.0","summary":"  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks\n","authors":["Mirco Ravanelli","Titouan Parcollet","Adel Moumen","Sylvain de Langen","Cem Subakan","Peter Plantinga","Yingzhi Wang","Pooneh Mousavi","Luca Della Libera","Artem Ploujnikov","Francesco Paissan","Davide Borra","Salah Zaiem","Zeyu Zhao","Shucong Zhang","Georgios Karakasidis","Sung-Lin Yeh","Aku Rouhe","Rudolf Braun","Florian Mai","Juan Zuluaga-Gomez","Seyed Mahed Mousavi","Andreas Nautsch","Xuechen Liu","Sangeet Sagar","Jarod Duret","Salima Mdhaffar","Gaelle Laperriere","Renato De Mori","Yannick Esteve"],"pdf_url":"https://arxiv.org/pdf/2407.00463v3.pdf","comment":"Submitted to JMLR (Machine Learning Open Source Software)"},{"id":"http://arxiv.org/abs/2407.04020v2","updated":"2024-07-15T12:47:39Z","published":"2024-07-04T15:55:13Z","title":"LLMAEL: Large Language Models are Good Context Augmenters for Entity\n  Linking","summary":"  Entity Linking (EL) models are well-trained at mapping mentions to their\ncorresponding entities according to a given context. However, EL models\nstruggle to disambiguate long-tail entities due to their limited training data.\nMeanwhile, large language models (LLMs) are more robust at interpreting\nuncommon mentions. Yet, due to a lack of specialized training, LLMs suffer at\ngenerating correct entity IDs. Furthermore, training an LLM to perform EL is\ncost-intensive. Building upon these insights, we introduce LLM-Augmented Entity\nLinking LLMAEL, a plug-and-play approach to enhance entity linking through LLM\ndata augmentation. We leverage LLMs as knowledgeable context augmenters,\ngenerating mention-centered descriptions as additional input, while preserving\ntraditional EL models for task specific processing. Experiments on 6 standard\ndatasets show that the vanilla LLMAEL outperforms baseline EL models in most\ncases, while the fine-tuned LLMAEL set the new state-of-the-art results across\nall 6 benchmarks.\n","authors":["Amy Xin","Yunjia Qi","Zijun Yao","Fangwei Zhu","Kaisheng Zeng","Xu Bin","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2407.04020v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19076v3","updated":"2024-07-15T12:36:42Z","published":"2024-05-29T13:34:32Z","title":"Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials\n  Analysis and Design","summary":"  We present Cephalo, a series of multimodal vision large language models\n(V-LLMs) designed for materials science applications, integrating visual and\nlinguistic data for enhanced understanding. A key innovation of Cephalo is its\nadvanced dataset generation method. Cephalo is trained on integrated image and\ntext data from thousands of scientific papers and science-focused Wikipedia\ndata demonstrates can interpret complex visual scenes, generate precise\nlanguage descriptions, and answer queries about images effectively. The\ncombination of a vision encoder with an autoregressive transformer supports\nmultimodal natural language understanding, which can be coupled with other\ngenerative methods to create an image-to-text-to-3D pipeline. To develop more\ncapable models from smaller ones, we report both mixture-of-expert methods and\nmodel merging. We examine the models in diverse use cases that incorporate\nbiological materials, fracture and engineering analysis, protein biophysics,\nand bio-inspired design based on insect behavior. Generative applications\ninclude bio-inspired designs, including pollen-inspired architected materials,\nas well as the synthesis of bio-inspired material microstructures from a\nphotograph of a solar eclipse. Additional model fine-tuning with a series of\nmolecular dynamics results demonstrate Cephalo's enhanced capabilities to\naccurately predict statistical features of stress and atomic energy\ndistributions, as well as crack dynamics and damage in materials.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2405.19076v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10671v1","updated":"2024-07-15T12:35:42Z","published":"2024-07-15T12:35:42Z","title":"Qwen2 Technical Report","summary":"  This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face1 and ModelScope2, and the\nsupplementary materials including example code on GitHub3. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.\n","authors":["An Yang","Baosong Yang","Binyuan Hui","Bo Zheng","Bowen Yu","Chang Zhou","Chengpeng Li","Chengyuan Li","Dayiheng Liu","Fei Huang","Guanting Dong","Haoran Wei","Huan Lin","Jialong Tang","Jialin Wang","Jian Yang","Jianhong Tu","Jianwei Zhang","Jianxin Ma","Jin Xu","Jingren Zhou","Jinze Bai","Jinzheng He","Junyang Lin","Kai Dang","Keming Lu","Keqin Chen","Kexin Yang","Mei Li","Mingfeng Xue","Na Ni","Pei Zhang","Peng Wang","Ru Peng","Rui Men","Ruize Gao","Runji Lin","Shijie Wang","Shuai Bai","Sinan Tan","Tianhang Zhu","Tianhao Li","Tianyu Liu","Wenbin Ge","Xiaodong Deng","Xiaohuan Zhou","Xingzhang Ren","Xinyu Zhang","Xipin Wei","Xuancheng Ren","Yang Fan","Yang Yao","Yichang Zhang","Yu Wan","Yunfei Chu","Zeyu Cui","Zhenru Zhang","Zhihao Fan"],"pdf_url":"https://arxiv.org/pdf/2407.10671v1.pdf","comment":"25 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.10670v1","updated":"2024-07-15T12:35:00Z","published":"2024-07-15T12:35:00Z","title":"Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for\n  Improved Quality and Efficiency in RAG Systems","summary":"  Retrieval-augmented generation (RAG) techniques leverage the in-context\nlearning capabilities of large language models (LLMs) to produce more accurate\nand relevant responses. Originating from the simple 'retrieve-then-read'\napproach, the RAG framework has evolved into a highly flexible and modular\nparadigm. A critical component, the Query Rewriter module, enhances knowledge\nretrieval by generating a search-friendly query. This method aligns input\nquestions more closely with the knowledge base. Our research identifies\nopportunities to enhance the Query Rewriter module to Query Rewriter+ by\ngenerating multiple queries to overcome the Information Plateaus associated\nwith a single query and by rewriting questions to eliminate Ambiguity, thereby\nclarifying the underlying intent. We also find that current RAG systems exhibit\nissues with Irrelevant Knowledge; to overcome this, we propose the Knowledge\nFilter. These two modules are both based on the instruction-tuned Gemma-2B\nmodel, which together enhance response quality. The final identified issue is\nRedundant Retrieval; we introduce the Memory Knowledge Reservoir and the\nRetriever Trigger to solve this. The former supports the dynamic expansion of\nthe RAG system's knowledge base in a parameter-free manner, while the latter\noptimizes the cost for accessing external knowledge, thereby improving resource\nutilization and response efficiency. These four RAG modules synergistically\nimprove the response quality and efficiency of the RAG system. The\neffectiveness of these modules has been validated through experiments and\nablation studies across six common QA datasets. The source code can be accessed\nat https://github.com/Ancientshi/ERM4.\n","authors":["Yunxiao Shi","Xing Zi","Zijing Shi","Haimin Zhang","Qiang Wu","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2407.10670v1.pdf","comment":"ECAI2024 #1304"},{"id":"http://arxiv.org/abs/2309.01809v2","updated":"2024-07-15T12:21:56Z","published":"2023-09-04T20:54:11Z","title":"Are Emergent Abilities in Large Language Models just In-Context\n  Learning?","summary":"  Large language models, comprising billions of parameters and pre-trained on\nextensive web-scale corpora, have been claimed to acquire certain capabilities\nwithout having been specifically trained on them. These capabilities, referred\nto as \"emergent abilities,\" have been a driving force in discussions regarding\nthe potentials and risks of language models. A key challenge in evaluating\nemergent abilities is that they are confounded by model competencies that arise\nthrough alternative prompting techniques, including in-context learning, which\nis the ability of models to complete a task based on a few examples. We present\na novel theory that explains emergent abilities, taking into account their\npotential confounding factors, and rigorously substantiate this theory through\nover 1000 experiments. Our findings suggest that purported emergent abilities\nare not truly emergent, but result from a combination of in-context learning,\nmodel memory, and linguistic knowledge. Our work is a foundational step in\nexplaining language model performance, providing a template for their efficient\nuse and clarifying the paradox of their ability to excel in some instances\nwhile faltering in others. Thus, we demonstrate that their capabilities should\nnot be overestimated.\n","authors":["Sheng Lu","Irina Bigoulaeva","Rachneet Sachdeva","Harish Tayyar Madabushi","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2309.01809v2.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2407.10657v1","updated":"2024-07-15T12:16:33Z","published":"2024-07-15T12:16:33Z","title":"An Empirical Study of Validating Synthetic Data for Formula Generation","summary":"  Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.\n","authors":["Usneek Singh","JosÃ© Cambronero","Sumit Gulwani","Aditya Kanade","Anirudh Khatry","Vu Le","Mukul Singh","Gust Verbruggen"],"pdf_url":"https://arxiv.org/pdf/2407.10657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10707v2","updated":"2024-07-15T12:14:13Z","published":"2024-03-15T21:54:00Z","title":"Discovering Latent Themes in Social Media Messaging: A\n  Machine-in-the-Loop Approach Integrating LLMs","summary":"  Grasping the themes of social media content is key to understanding the\nnarratives that influence public opinion and behavior. The thematic analysis\ngoes beyond traditional topic-level analysis, which often captures only the\nbroadest patterns, providing deeper insights into specific and actionable\nthemes such as \"public sentiment towards vaccination\", \"political discourse\nsurrounding climate policies,\" etc. In this paper, we introduce a novel\napproach to uncovering latent themes in social media messaging. Recognizing the\nlimitations of the traditional topic-level analysis, which tends to capture\nonly overarching patterns, this study emphasizes the need for a finer-grained,\ntheme-focused exploration. Traditional theme discovery methods typically\ninvolve manual processes and a human-in-the-loop approach. While valuable,\nthese methods face challenges in scalability, consistency, and resource\nintensity in terms of time and cost. To address these challenges, we propose a\nmachine-in-the-loop approach that leverages the advanced capabilities of Large\nLanguage Models (LLMs). To demonstrate our approach, we apply our framework to\ncontentious topics, such as climate debate and vaccine debate. We use two\npublicly available datasets: (1) the climate campaigns dataset of 21k Facebook\nads and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads. Our\nquantitative and qualitative analysis shows that our methodology yields more\naccurate and interpretable results compared to the baselines. Our results not\nonly demonstrate the effectiveness of our approach in uncovering latent themes\nbut also illuminate how these themes are tailored for demographic targeting in\nsocial media contexts. Additionally, our work sheds light on the dynamic nature\nof social media, revealing the shifts in the thematic focus of messaging in\nresponse to real-world events.\n","authors":["Tunazzina Islam","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2403.10707v2.pdf","comment":"Accepted at 19th International AAAI Conference on Web and Social\n  Media (ICWSM-2025)"},{"id":"http://arxiv.org/abs/2403.17806v2","updated":"2024-07-15T12:07:09Z","published":"2024-03-26T15:44:58Z","title":"Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding\n  Model Mechanisms","summary":"  Many recent language model (LM) interpretability studies have adopted the\ncircuits framework, which aims to find the minimal computational subgraph, or\ncircuit, that explains LM behavior on a given task. Most studies determine\nwhich edges belong in a LM's circuit by performing causal interventions on each\nedge independently, but this scales poorly with model size. Edge attribution\npatching (EAP), gradient-based approximation to interventions, has emerged as a\nscalable but imperfect solution to this problem. In this paper, we introduce a\nnew method - EAP with integrated gradients (EAP-IG) - that aims to better\nmaintain a core property of circuits: faithfulness. A circuit is faithful if\nall model edges outside the circuit can be ablated without changing the model's\nperformance on the task; faithfulness is what justifies studying circuits,\nrather than the full model. Our experiments demonstrate that circuits found\nusing EAP are less faithful than those found using EAP-IG, even though both\nhave high node overlap with circuits found previously using causal\ninterventions. We conclude more generally that when using circuits to compare\nthe mechanisms models use to solve tasks, faithfulness, not overlap, is what\nshould be measured.\n","authors":["Michael Hanna","Sandro Pezzelle","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.17806v2.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2407.10645v1","updated":"2024-07-15T12:04:32Z","published":"2024-07-15T12:04:32Z","title":"Prompt Selection Matters: Enhancing Text Annotations for Social Sciences\n  with Large Language Models","summary":"  Large Language Models have recently been applied to text annotation tasks\nfrom social sciences, equalling or surpassing the performance of human workers\nat a fraction of the cost. However, no inquiry has yet been made on the impact\nof prompt selection on labelling accuracy. In this study, we show that\nperformance greatly varies between prompts, and we apply the method of\nautomatic prompt optimization to systematically craft high quality prompts. We\nalso provide the community with a simple, browser-based implementation of the\nmethod at https://prompt-ultra.github.io/ .\n","authors":["Louis Abraham","Charles Arnal","Antoine Marie"],"pdf_url":"https://arxiv.org/pdf/2407.10645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11717v2","updated":"2024-07-15T11:53:41Z","published":"2024-06-17T16:36:12Z","title":"Refusal in Language Models Is Mediated by a Single Direction","summary":"  Conversational large language models are fine-tuned for both\ninstruction-following and safety, resulting in models that obey benign requests\nbut refuse harmful ones. While this refusal behavior is widespread across chat\nmodels, its underlying mechanisms remain poorly understood. In this work, we\nshow that refusal is mediated by a one-dimensional subspace, across 13 popular\nopen-source chat models up to 72B parameters in size. Specifically, for each\nmodel, we find a single direction such that erasing this direction from the\nmodel's residual stream activations prevents it from refusing harmful\ninstructions, while adding this direction elicits refusal on even harmless\ninstructions. Leveraging this insight, we propose a novel white-box jailbreak\nmethod that surgically disables refusal with minimal effect on other\ncapabilities. Finally, we mechanistically analyze how adversarial suffixes\nsuppress propagation of the refusal-mediating direction. Our findings\nunderscore the brittleness of current safety fine-tuning methods. More broadly,\nour work showcases how an understanding of model internals can be leveraged to\ndevelop practical methods for controlling model behavior.\n","authors":["Andy Arditi","Oscar Obeso","Aaquib Syed","Daniel Paleka","Nina Panickssery","Wes Gurnee","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2406.11717v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17043v3","updated":"2024-07-15T11:52:10Z","published":"2024-01-30T14:25:32Z","title":"CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented\n  Generation of Large Language Models","summary":"  Retrieval-Augmented Generation (RAG) is a technique that enhances the\ncapabilities of large language models (LLMs) by incorporating external\nknowledge sources. This method addresses common LLM limitations, including\noutdated information and the tendency to produce inaccurate \"hallucinated\"\ncontent. However, the evaluation of RAG systems is challenging, as existing\nbenchmarks are limited in scope and diversity. Most of the current benchmarks\npredominantly assess question-answering applications, overlooking the broader\nspectrum of situations where RAG could prove advantageous. Moreover, they only\nevaluate the performance of the LLM component of the RAG pipeline in the\nexperiments, and neglect the influence of the retrieval component and the\nexternal knowledge database. To address these issues, this paper constructs a\nlarge-scale and more comprehensive benchmark, and evaluates all the components\nof RAG systems in various RAG application scenarios. Specifically, we have\ncategorized the range of RAG applications into four distinct types-Create,\nRead, Update, and Delete (CRUD), each representing a unique use case. \"Create\"\nrefers to scenarios requiring the generation of original, varied content.\n\"Read\" involves responding to intricate questions in knowledge-intensive\nsituations. \"Update\" focuses on revising and rectifying inaccuracies or\ninconsistencies in pre-existing texts. \"Delete\" pertains to the task of\nsummarizing extensive texts into more concise forms. For each of these CRUD\ncategories, we have developed comprehensive datasets to evaluate the\nperformance of RAG systems. We also analyze the effects of various components\nof the RAG system, such as the retriever, the context length, the knowledge\nbase construction, and the LLM. Finally, we provide useful insights for\noptimizing the RAG technology for different scenarios.\n","authors":["Yuanjie Lyu","Zhiyu Li","Simin Niu","Feiyu Xiong","Bo Tang","Wenjin Wang","Hao Wu","Huanyong Liu","Tong Xu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2401.17043v3.pdf","comment":"40 Pages"},{"id":"http://arxiv.org/abs/2405.10650v8","updated":"2024-07-15T11:47:29Z","published":"2024-05-17T09:25:30Z","title":"SPOR: A Comprehensive and Practical Evaluation Method for Compositional\n  Generalization in Data-to-Text Generation","summary":"  Compositional generalization is an important ability of language models and\nhas many different manifestations. For data-to-text generation, previous\nresearch on this ability is limited to a single manifestation called\nSystematicity and lacks consideration of large language models (LLMs), which\ncannot fully cover practical application scenarios. In this work, we propose\nSPOR, a comprehensive and practical evaluation method for compositional\ngeneralization in data-to-text generation. SPOR includes four aspects of\nmanifestations (Systematicity, Productivity, Order invariance, and Rule\nlearnability) and allows high-quality evaluation without additional manual\nannotations based on existing datasets. We demonstrate SPOR on two different\ndatasets and evaluate some existing language models including LLMs. We find\nthat the models are deficient in various aspects of the evaluation and need\nfurther improvement. Our work shows the necessity for comprehensive research on\ndifferent manifestations of compositional generalization in data-to-text\ngeneration and provides a framework for evaluation.\n","authors":["Ziyao Xu","Houfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2405.10650v8.pdf","comment":"Accepted at ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2407.10629v1","updated":"2024-07-15T11:28:16Z","published":"2024-07-15T11:28:16Z","title":"Balancing the Scales: Reinforcement Learning for Fair Classification","summary":"  Fairness in classification tasks has traditionally focused on bias removal\nfrom neural representations, but recent trends favor algorithmic methods that\nembed fairness into the training process. These methods steer models towards\nfair performance, preventing potential elimination of valuable information that\narises from representation manipulation. Reinforcement Learning (RL), with its\ncapacity for learning through interaction and adjusting reward functions to\nencourage desired behaviors, emerges as a promising tool in this domain. In\nthis paper, we explore the usage of RL to address bias in imbalanced\nclassification by scaling the reward function to mitigate bias. We employ the\ncontextual multi-armed bandit framework and adapt three popular RL algorithms\nto suit our objectives, demonstrating a novel approach to mitigating bias.\n","authors":["Leon Eshuijs","Shihan Wang","Antske Fokkens"],"pdf_url":"https://arxiv.org/pdf/2407.10629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10627v1","updated":"2024-07-15T11:26:07Z","published":"2024-07-15T11:26:07Z","title":"Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated\n  Chatbot Arena","summary":"  Assessing the effectiveness of large language models (LLMs) presents\nsubstantial challenges. The method of conducting human-annotated battles in an\nonline Chatbot Arena is a highly effective evaluative technique. However, this\napproach is limited by the costs and time required for human annotation. In\nthis paper, we introduce Arena Learning, an innovative offline strategy\ndesigned to simulate these arena battles using AI-driven annotations to\nevaluate battle outcomes, thus facilitating the continuous improvement of the\ntarget model through both supervised fine-tuning and reinforcement learning.\nArena Learning comprises two key elements. First, it ensures precise\nevaluations and maintains consistency between offline simulations and online\ncompetitions via WizardArena, a pipeline developed to accurately predict the\nElo rankings of various models using a meticulously designed offline test set.\nOur results demonstrate that WizardArena's predictions closely align with those\nfrom the online Arena. Second, it involves the continuous improvement of\ntraining data based on the battle results and the refined model. We establish a\ndata flywheel to iteratively update the training data by highlighting the\nweaknesses of the target model based on its battle results, enabling it to\nlearn from the strengths of multiple different models. We apply Arena Learning\nto train our target model, WizardLM-$\\beta$, and demonstrate significant\nperformance enhancements across various metrics. This fully automated training\nand evaluation pipeline sets the stage for continuous advancements in various\nLLMs via post-training. Notably, Arena Learning plays a pivotal role in the\nsuccess of WizardLM-2, and this paper serves both as an exploration of its\nefficacy and a foundational study for future discussions related to WizardLM-2\nand its derivatives.\n","authors":["Haipeng Luo","Qingfeng Sun","Can Xu","Pu Zhao","Qingwei Lin","Jianguang Lou","Shifeng Chen","Yansong Tang","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10626v1","updated":"2024-07-15T11:26:03Z","published":"2024-07-15T11:26:03Z","title":"NoviCode: Generating Programs from Natural Language Utterances by\n  Novices","summary":"  Current Text-to-Code models demonstrate impressive capabilities in generating\nexecutable code from natural language snippets. However, current studies focus\non technical instructions and programmer-oriented language, and it is an open\nquestion whether these models can effectively translate natural language\ndescriptions given by non-technical users and express complex goals, to an\nexecutable program that contains an intricate flow - composed of API access and\ncontrol structures as loops, conditions, and sequences. To unlock the challenge\nof generating a complete program from a plain non-technical description we\npresent NoviCode, a novel NL Programming task, which takes as input an API and\na natural language description by a novice non-programmer and provides an\nexecutable program as output. To assess the efficacy of models on this task, we\nprovide a novel benchmark accompanied by test suites wherein the generated\nprogram code is assessed not according to their form, but according to their\nfunctional execution. Our experiments show that, first, NoviCode is indeed a\nchallenging task in the code synthesis domain, and that generating complex code\nfrom non-technical instructions goes beyond the current Text-to-Code paradigm.\nSecond, we show that a novel approach wherein we align the NL utterances with\nthe compositional hierarchical structure of the code, greatly enhances the\nperformance of LLMs on this task, compared with the end-to-end Text-to-Code\ncounterparts.\n","authors":["Asaf Achi Mordechai","Yoav Goldberg","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2407.10626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09014v2","updated":"2024-07-15T11:19:46Z","published":"2024-07-12T06:06:54Z","title":"CompAct: Compressing Retrieved Documents Actively for Question Answering","summary":"  Retrieval-augmented generation supports language models to strengthen their\nfactual groundings by providing external contexts. However, language models\noften face challenges when given extensive information, diminishing their\neffectiveness in solving questions. Context compression tackles this issue by\nfiltering out irrelevant information, but current methods still struggle in\nrealistic scenarios where crucial information cannot be captured with a\nsingle-step approach. To overcome this limitation, we introduce CompAct, a\nnovel framework that employs an active strategy to condense extensive documents\nwithout losing key information. Our experiments demonstrate that CompAct brings\nsignificant improvements in both performance and compression rate on multi-hop\nquestion-answering (QA) benchmarks. CompAct flexibly operates as a\ncost-efficient plug-in module with various off-the-shelf retrievers or readers,\nachieving exceptionally high compression rates (47x).\n","authors":["Chanwoong Yoon","Taewhoo Lee","Hyeon Hwang","Minbyul Jeong","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2407.09014v2.pdf","comment":"Code available at https://github.com/dmis-lab/CompAct"},{"id":"http://arxiv.org/abs/2406.08940v2","updated":"2024-07-15T10:42:02Z","published":"2024-06-13T09:10:16Z","title":"Word Order in English-Japanese Simultaneous Interpretation: Analyses and\n  Evaluation using Chunk-wise Monotonic Translation","summary":"  This paper analyzes the features of monotonic translations, which follow the\nword order of the source language, in simultaneous interpreting (SI). Word\norder differences are one of the biggest challenges in SI, especially for\nlanguage pairs with significant structural differences like English and\nJapanese. We analyzed the characteristics of chunk-wise monotonic translation\n(CMT) sentences using the NAIST English-to-Japanese Chunk-wise Monotonic\nTranslation Evaluation Dataset and identified some grammatical structures that\nmake monotonic translation difficult in English-Japanese SI. We further\ninvestigated the features of CMT sentences by evaluating the output from the\nexisting speech translation (ST) and simultaneous speech translation (simulST)\nmodels on the NAIST English-to-Japanese Chunk-wise Monotonic Translation\nEvaluation Dataset as well as on existing test sets. The results indicate the\npossibility that the existing SI-based test set underestimates the model\nperformance. The results also suggest that using CMT sentences as references\ngives higher scores to simulST models than ST models, and that using an\noffline-based test set to evaluate the simulST models underestimates the model\nperformance.\n","authors":["Kosuke Doi","Yuka Ko","Mana Makinae","Katsuhito Sudoh","Satoshi Nakamura"],"pdf_url":"https://arxiv.org/pdf/2406.08940v2.pdf","comment":"Accepted to IWSLT2024"},{"id":"http://arxiv.org/abs/2407.10603v1","updated":"2024-07-15T10:25:14Z","published":"2024-07-15T10:25:14Z","title":"Leave No Knowledge Behind During Knowledge Distillation: Towards\n  Practical and Effective Knowledge Distillation for Code-Switching ASR Using\n  Realistic Data","summary":"  Recent advances in automatic speech recognition (ASR) often rely on large\nspeech foundation models for generating high-quality transcriptions. However,\nthese models can be impractical due to limited computing resources. The\nsituation is even more severe in terms of more realistic or difficult\nscenarios, such as code-switching ASR (CS-ASR). To address this, we present a\nframework for developing more efficient models for CS-ASR through knowledge\ndistillation using realistic speech-only data. Our proposed method, Leave No\nKnowledge Behind During Knowledge Distillation (K$^2$D), leverages both the\nteacher model's knowledge and additional insights from a small auxiliary model.\nWe evaluate our approach on two in-domain and two out-domain datasets,\ndemonstrating that K$^2$D is effective. By conducting K$^2$D on the unlabeled\nrealistic data, we have successfully obtained a 2-time smaller model with\n5-time faster generation speed while outperforming the baseline methods and the\nteacher model on all the testing sets. We have made our model publicly\navailable on Hugging Face\n(https://huggingface.co/andybi7676/k2d-whisper.zh-en).\n","authors":["Liang-Hsuan Tseng","Zih-Ching Chen","Wei-Shun Chang","Cheng-Kuang Lee","Tsung-Ren Huang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2407.10603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02130v4","updated":"2024-07-15T10:16:14Z","published":"2024-03-04T15:39:59Z","title":"Using LLMs for the Extraction and Normalization of Product Attribute\n  Values","summary":"  Product offers on e-commerce websites often consist of a product title and a\ntextual product description. In order to enable features such as faceted\nproduct search or to generate product comparison tables, it is necessary to\nextract structured attribute-value pairs from the unstructured product titles\nand descriptions and to normalize the extracted values to a single, unified\nscale for each attribute. This paper explores the potential of using large\nlanguage models (LLMs), such as GPT-3.5 and GPT-4, to extract and normalize\nattribute values from product titles and descriptions. We experiment with\ndifferent zero-shot and few-shot prompt templates for instructing LLMs to\nextract and normalize attribute-value pairs. We introduce the Web Data Commons\n- Product Attribute Value Extraction (WDC-PAVE) benchmark dataset for our\nexperiments. WDC-PAVE consists of product offers from 59 different websites\nwhich provide schema.org annotations. The offers belong to five different\nproduct categories, each with a specific set of attributes. The dataset\nprovides manually verified attribute-value pairs in two forms: (i) directly\nextracted values and (ii) normalized attribute values. The normalization of the\nattribute values requires systems to perform the following types of operations:\nname expansion, generalization, unit of measurement conversion, and string\nwrangling. Our experiments demonstrate that GPT-4 outperforms the PLM-based\nextraction methods SU-OpenTag, AVEQA, and MAVEQA by 10%, achieving an F1-score\nof 91%. For the extraction and normalization of product attribute values, GPT-4\nachieves a similar performance to the extraction scenario, while being\nparticularly strong at string wrangling and name expansion.\n","authors":["Alexander Brinkmann","Nick Baumann","Christian Bizer"],"pdf_url":"https://arxiv.org/pdf/2403.02130v4.pdf","comment":"The paper has been accepted at ADBIS2024"},{"id":"http://arxiv.org/abs/2402.07754v2","updated":"2024-07-15T10:03:59Z","published":"2024-02-12T16:23:28Z","title":"Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language\n  Models","summary":"  Recently, diffusion models have garnered significant interest in the field of\ntext processing due to their many potential advantages compared to conventional\nautoregressive models. In this work, we propose Diffusion-of-Thought (DoT), a\nnovel approach that integrates diffusion models with Chain-of-Thought, a\nwell-established technique for improving the reasoning ability of\nautoregressive language models. In contrast to autoregressive language models\nthat make decisions in a left-to-right, token-by-token manner, DoT allows\nreasoning steps to diffuse over time through a diffusion language model and\noffers greater flexibility in trading-off computation for reasoning\nperformance. Our experimental results demonstrate the effectiveness of DoT in\nmulti-digit multiplication, boolean logic, and grade school math problems, with\na small diffusion model outperforming a much larger autoregressive model in\nboth efficiency and accuracy. In addition to that, DoT showcases promising\nself-correction abilities and benefits from existing reasoning-enhancing\ntechniques like self-consistency decoding. Our findings contribute to the\nunderstanding and development of reasoning with diffusion language models.\n","authors":["Jiacheng Ye","Shansan Gong","Liheng Chen","Lin Zheng","Jiahui Gao","Han Shi","Chuan Wu","Xin Jiang","Zhenguo Li","Wei Bi","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2402.07754v2.pdf","comment":"Multiple updates (add boolean logic dataset, add DoT based on SEDD\n  model and add detailed mathematical formulation in Appendix)"},{"id":"http://arxiv.org/abs/2407.10582v1","updated":"2024-07-15T10:00:22Z","published":"2024-07-15T10:00:22Z","title":"Boosting Zero-Shot Crosslingual Performance using LLM-Based\n  Augmentations with Effective Data Selection","summary":"  Large language models (LLMs) are very proficient text generators. We leverage\nthis capability of LLMs to generate task-specific data via zero-shot prompting\nand promote cross-lingual transfer for low-resource target languages. Given\ntask-specific data in a source language and a teacher model trained on this\ndata, we propose using this teacher to label LLM generations and employ a set\nof simple data selection strategies that use the teacher's label probabilities.\nOur data selection strategies help us identify a representative subset of\ndiverse generations that help boost zero-shot accuracies while being efficient,\nin comparison to using all the LLM generations (without any subset selection).\nWe also highlight other important design choices that affect cross-lingual\nperformance such as the use of translations of source data and what labels are\nbest to use for the LLM generations. We observe significant performance gains\nacross sentiment analysis and natural language inference tasks (of up to a\nmaximum of 7.13 absolute points and 1.5 absolute points on average) across a\nnumber of target languages (Hindi, Marathi, Urdu, Swahili) and domains.\n","authors":["Barah Fazili","Ashish Sunil Agrawal","Preethi Jyothi"],"pdf_url":"https://arxiv.org/pdf/2407.10582v1.pdf","comment":"Accepted in Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2407.10554v1","updated":"2024-07-15T09:07:07Z","published":"2024-07-15T09:07:07Z","title":"Beyond Generative Artificial Intelligence: Roadmap for Natural Language\n  Generation","summary":"  Generative Artificial Intelligence has grown exponentially as a result of\nLarge Language Models (LLMs). This has been possible because of the impressive\nperformance of deep learning methods created within the field of Natural\nLanguage Processing (NLP) and its subfield Natural Language Generation (NLG),\nwhich is the focus of this paper. Within the growing LLM family are the popular\nGPT-4, Bard and more specifically, tools such as ChatGPT have become a\nbenchmark for other LLMs when solving most of the tasks involved in NLG\nresearch. This scenario poses new questions about the next steps for NLG and\nhow the field can adapt and evolve to deal with new challenges in the era of\nLLMs. To address this, the present paper conducts a review of a representative\nsample of surveys recently published in NLG. By doing so, we aim to provide the\nscientific community with a research roadmap to identify which NLG aspects are\nstill not suitably addressed by LLMs, as well as suggest future lines of\nresearch that should be addressed going forward.\n","authors":["MarÃ­a MirÃ³ Maestre","IvÃ¡n MartÃ­nez-Murillo","Tania J. Martin","Borja Navarro-Colorado","Antonio FerrÃ¡ndez","Armando SuÃ¡rez Cueto","Elena Lloret"],"pdf_url":"https://arxiv.org/pdf/2407.10554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08619v3","updated":"2024-07-15T08:53:55Z","published":"2024-05-14T13:59:24Z","title":"ALMol: Aligned Language-Molecule Translation LLMs through Offline\n  Preference Contrastive Optimisation","summary":"  The field of chemistry and Artificial Intelligence (AI) intersection is an\narea of active research that aims to accelerate scientific discovery. The\nintegration of large language models (LLMs) with scientific modalities has\nshown significant promise in this endeavour. However, challenges persist in\neffectively addressing training efficacy and the out-of-distribution problem,\nparticularly as existing approaches rely on larger models and datasets. In this\ncontext, we focus on machine language-molecule translation and deploy a novel\ntraining approach called contrastive preference optimisation, which avoids\ngenerating translations that are merely adequate but not perfect. To ensure\ngeneralisability and mitigate memorisation effects, we conduct experiments\nusing only 10% of the data. Our results demonstrate that our models achieve up\nto a 32% improvement compared to counterpart models. Finally, we introduce a\nfine-grained, domain-agnostic evaluation method to assess hallucination in LLMs\nand promote responsible use.\n","authors":["Dimitris Gkoumas"],"pdf_url":"https://arxiv.org/pdf/2405.08619v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07590v4","updated":"2024-07-15T08:51:52Z","published":"2023-11-09T17:12:44Z","title":"Large Language Models can Strategically Deceive their Users when Put\n  Under Pressure","summary":"  We demonstrate a situation in which Large Language Models, trained to be\nhelpful, harmless, and honest, can display misaligned behavior and\nstrategically deceive their users about this behavior without being instructed\nto do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated\nenvironment, where it assumes the role of an autonomous stock trading agent.\nWithin this environment, the model obtains an insider tip about a lucrative\nstock trade and acts upon it despite knowing that insider trading is\ndisapproved of by company management. When reporting to its manager, the model\nconsistently hides the genuine reasons behind its trading decision. We perform\na brief investigation of how this behavior varies under changes to the setting,\nsuch as removing model access to a reasoning scratchpad, attempting to prevent\nthe misaligned behavior by changing system instructions, changing the amount of\npressure the model is under, varying the perceived risk of getting caught, and\nmaking other simple changes to the environment. To our knowledge, this is the\nfirst demonstration of Large Language Models trained to be helpful, harmless,\nand honest, strategically deceiving their users in a realistic situation\nwithout direct instructions or training for deception.\n","authors":["JÃ©rÃ©my Scheurer","Mikita Balesni","Marius Hobbhahn"],"pdf_url":"https://arxiv.org/pdf/2311.07590v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20994v2","updated":"2024-07-15T08:50:29Z","published":"2024-05-31T16:38:54Z","title":"CWRCzech: 100M Query-Document Czech Click Dataset and Its Application to\n  Web Relevance Ranking","summary":"  We present CWRCzech, Click Web Ranking dataset for Czech, a 100M\nquery-document Czech click dataset for relevance ranking with user behavior\ndata collected from search engine logs of Seznam$.$cz. To the best of our\nknowledge, CWRCzech is the largest click dataset with raw text published so\nfar. It provides document positions in the search results as well as\ninformation about user behavior: 27.6M clicked documents and 10.8M dwell times.\nIn addition, we also publish a manually annotated Czech test for the relevance\ntask, containing nearly 50k query-document pairs, each annotated by at least 2\nannotators. Finally, we analyze how the user behavior data improve relevance\nranking and show that models trained on data automatically harnessed at\nsufficient scale can surpass the performance of models trained on human\nannotated data. CWRCzech is published under an academic non-commercial license\nand is available to the research community at\nhttps://github.com/seznam/CWRCzech.\n","authors":["Josef VonÃ¡Å¡ek","Milan Straka","Rostislav KrÄ","Lenka LasoÅovÃ¡","Ekaterina Egorova","Jana StrakovÃ¡","Jakub NÃ¡plava"],"pdf_url":"https://arxiv.org/pdf/2405.20994v2.pdf","comment":"Accepted to SIGIR 2024"},{"id":"http://arxiv.org/abs/2407.10510v1","updated":"2024-07-15T08:06:37Z","published":"2024-07-15T08:06:37Z","title":"TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription\n  Prediction","summary":"  Traditional Chinese medicine (TCM) relies on specific combinations of herbs\nin prescriptions to treat symptoms and signs, a practice that spans thousands\nof years. Predicting TCM prescriptions presents a fascinating technical\nchallenge with practical implications. However, this task faces limitations due\nto the scarcity of high-quality clinical datasets and the intricate\nrelationship between symptoms and herbs. To address these issues, we introduce\nDigestDS, a new dataset containing practical medical records from experienced\nexperts in digestive system diseases. We also propose a method, TCM-FTP (TCM\nFine-Tuning Pre-trained), to leverage pre-trained large language models (LLMs)\nthrough supervised fine-tuning on DigestDS. Additionally, we enhance\ncomputational efficiency using a low-rank adaptation technique. TCM-FTP also\nincorporates data augmentation by permuting herbs within prescriptions,\ncapitalizing on their order-agnostic properties. Impressively, TCM-FTP achieves\nan F1-score of 0.8031, surpassing previous methods significantly. Furthermore,\nit demonstrates remarkable accuracy in dosage prediction, achieving a\nnormalized mean square error of 0.0604. In contrast, LLMs without fine-tuning\nperform poorly. Although LLMs have shown capabilities on a wide range of tasks,\nthis work illustrates the importance of fine-tuning for TCM prescription\nprediction, and we have proposed an effective way to do that.\n","authors":["Xingzhi Zhou","Xin Dong","Chunhao Li","Yuning Bai","Yulong Xu","Ka Chun Cheung","Simon See","Xinpeng Song","Runshun Zhang","Xuezhong Zhou","Nevin L. Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.10510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10499v1","updated":"2024-07-15T07:43:55Z","published":"2024-07-15T07:43:55Z","title":"CIBench: Evaluating Your LLMs with a Code Interpreter Plugin","summary":"  While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization.\n","authors":["Songyang Zhang","Chuyu Zhang","Yingfan Hu","Haowen Shen","Kuikun Liu","Zerun Ma","Fengzhe Zhou","Wenwei Zhang","Xuming He","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10499v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.10490v1","updated":"2024-07-15T07:30:28Z","published":"2024-07-15T07:30:28Z","title":"Learning Dynamics of LLM Finetuning","summary":"  Learning dynamics, which describes how the learning of specific training\nexamples influences the model's prediction of other examples, give us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during finetuning, by analyzing\nthe step-wise decomposition and accumulated influence among different\nresponses. Our framework allows a uniform interpretation of many interesting\nobservations about the training of popular algorithms for both instruction\ntuning and preference tuning. The analysis not only explains where the benefits\nof these methods come from but also inspires a simple, effective method to\nfurther improve the alignment performance. Code for experiments is available at\nhttps://github.com/Joshua-Ren/Learning_dynamics_LLM.\n","authors":["Yi Ren","Danica J. Sutherland"],"pdf_url":"https://arxiv.org/pdf/2407.10490v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2407.10488v1","updated":"2024-07-15T07:20:06Z","published":"2024-07-15T07:20:06Z","title":"How and where does CLIP process negation?","summary":"  Various benchmarks have been proposed to test linguistic understanding in\npre-trained vision \\& language (VL) models. Here we build on the existence task\nfrom the VALSE benchmark (Parcalabescu et al, 2022) which we use to test\nmodels' understanding of negation, a particularly interesting issue for\nmultimodal models. However, while such VL benchmarks are useful for measuring\nmodel performance, they do not reveal anything about the internal processes\nthrough which these models arrive at their outputs in such visio-linguistic\ntasks. We take inspiration from the growing literature on model\ninterpretability to explain the behaviour of VL models on the understanding of\nnegation. Specifically, we approach these questions through an in-depth\nanalysis of the text encoder in CLIP (Radford et al, 2021), a highly\ninfluential VL model. We localise parts of the encoder that process negation\nand analyse the role of attention heads in this task. Our contributions are\nthreefold. We demonstrate how methods from the language model interpretability\nliterature (such as causal tracing) can be translated to multimodal models and\ntasks; we provide concrete insights into how CLIP processes negation on the\nVALSE existence task; and we highlight inherent limitations in the VALSE\ndataset as a benchmark for linguistic understanding.\n","authors":["Vincent Quantmeyer","Pablo Mosteiro","Albert Gatt"],"pdf_url":"https://arxiv.org/pdf/2407.10488v1.pdf","comment":"Accepted at the 3rd Workshop on Advances in Language and Vision\n  Research (ALVR 2024)"},{"id":"http://arxiv.org/abs/2407.10486v1","updated":"2024-07-15T07:14:56Z","published":"2024-07-15T07:14:56Z","title":"IDEAL: Leveraging Infinite and Dynamic Characterizations of Large\n  Language Models for Query-focused Summarization","summary":"  Query-focused summarization (QFS) aims to produce summaries that answer\nparticular questions of interest, enabling greater user control and\npersonalization. With the advent of large language models (LLMs), shows their\nimpressive capability of textual understanding through large-scale pretraining,\nwhich implies the great potential of extractive snippet generation. In this\npaper, we systematically investigated two indispensable characteristics that\nthe LLMs-based QFS models should be harnessed, Lengthy Document Summarization\nand Efficiently Fine-grained Query-LLM Alignment, respectively.\nCorrespondingly, we propose two modules called Query-aware HyperExpert and\nQuery-focused Infini-attention to access the aforementioned characteristics.\nThese innovations pave the way for broader application and accessibility in the\nfield of QFS technology. Extensive experiments conducted on existing QFS\nbenchmarks indicate the effectiveness and generalizability of the proposed\napproach. Our code is publicly available at\nhttps://github.com/DCDmllm/IDEAL_Summary.\n","authors":["Jie Cao","Dian Jiao","Qiang Yan","Wenqiao Zhang","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2407.10486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10481v1","updated":"2024-07-15T07:07:11Z","published":"2024-07-15T07:07:11Z","title":"SuperPADL: Scaling Language-Directed Physics-Based Control with\n  Progressive Supervised Distillation","summary":"  Physically-simulated models for human motion can generate high-quality\nresponsive character animations, often in real-time. Natural language serves as\na flexible interface for controlling these models, allowing expert and\nnon-expert users to quickly create and edit their animations. Many recent\nphysics-based animation methods, including those that use text interfaces,\ntrain control policies using reinforcement learning (RL). However, scaling\nthese methods beyond several hundred motions has remained challenging.\nMeanwhile, kinematic animation models are able to successfully learn from\nthousands of diverse motions by leveraging supervised learning methods.\nInspired by these successes, in this work we introduce SuperPADL, a scalable\nframework for physics-based text-to-motion that leverages both RL and\nsupervised learning to train controllers on thousands of diverse motion clips.\nSuperPADL is trained in stages using progressive distillation, starting with a\nlarge number of specialized experts using RL. These experts are then\niteratively distilled into larger, more robust policies using a combination of\nreinforcement learning and supervised learning. Our final SuperPADL controller\nis trained on a dataset containing over 5000 skills and runs in real time on a\nconsumer GPU. Moreover, our policy can naturally transition between skills,\nallowing for users to interactively craft multi-stage animations. We\nexperimentally demonstrate that SuperPADL significantly outperforms RL-based\nbaselines at this large data scale.\n","authors":["Jordan Juravsky","Yunrong Guo","Sanja Fidler","Xue Bin Peng"],"pdf_url":"https://arxiv.org/pdf/2407.10481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08940v2","updated":"2024-07-15T06:27:07Z","published":"2024-07-12T02:55:13Z","title":"Large Language Models as Biomedical Hypothesis Generators: A\n  Comprehensive Evaluation","summary":"  The rapid growth of biomedical knowledge has outpaced our ability to\nefficiently extract insights and generate novel hypotheses. Large language\nmodels (LLMs) have emerged as a promising tool to revolutionize knowledge\ninteraction and potentially accelerate biomedical discovery. In this paper, we\npresent a comprehensive evaluation of LLMs as biomedical hypothesis generators.\nWe construct a dataset of background-hypothesis pairs from biomedical\nliterature, carefully partitioned into training, seen, and unseen test sets\nbased on publication date to mitigate data contamination. Using this dataset,\nwe assess the hypothesis generation capabilities of top-tier instructed models\nin zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of\nuncertainty, a crucial aspect of scientific discovery, we incorporate tool use\nand multi-agent interactions in our evaluation framework. Furthermore, we\npropose four novel metrics grounded in extensive literature review to evaluate\nthe quality of generated hypotheses, considering both LLM-based and human\nassessments. Our experiments yield two key findings: 1) LLMs can generate novel\nand validated hypotheses, even when tested on literature unseen during\ntraining, and 2) Increasing uncertainty through multi-agent interactions and\ntool use can facilitate diverse candidate generation and improve zero-shot\nhypothesis generation performance. However, we also observe that the\nintegration of additional knowledge through few-shot learning and tool use may\nnot always lead to performance gains, highlighting the need for careful\nconsideration of the type and scope of external knowledge incorporated. These\nfindings underscore the potential of LLMs as powerful aids in biomedical\nhypothesis generation and provide valuable insights to guide further research\nin this area.\n","authors":["Biqing Qi","Kaiyan Zhang","Kai Tian","Haoxiang Li","Zhang-Ren Chen","Sihang Zeng","Ermo Hua","Hu Jinfang","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.08940v2.pdf","comment":"Accepted to COLM 2024. This is an extended version of the paper at\n  arXiv:2311.05965"},{"id":"http://arxiv.org/abs/2407.10457v1","updated":"2024-07-15T06:12:17Z","published":"2024-07-15T06:12:17Z","title":"The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore\n  Non-Determinism","summary":"  Current evaluations of large language models (LLMs) often overlook\nnon-determinism, typically focusing on a single output per example. This limits\nour understanding of LLM performance variability in real-world applications.\nOur study addresses this issue by exploring key questions about the performance\ndifferences between greedy decoding and sampling, identifying benchmarks'\nconsistency regarding non-determinism, and examining unique model behaviors.\nThrough extensive experiments, we observe that greedy decoding generally\noutperforms sampling methods for most evaluated tasks. We also observe\nconsistent performance across different LLM sizes and alignment methods, noting\nthat alignment can reduce sampling variance. Moreover, our best-of-N sampling\napproach demonstrates that smaller LLMs can match or surpass larger models such\nas GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This\nresearch shows the importance of considering non-determinism in LLM evaluations\nand provides insights for future LLM development and evaluation.\n","authors":["Yifan Song","Guoyin Wang","Sujian Li","Bill Yuchen Lin"],"pdf_url":"https://arxiv.org/pdf/2407.10457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10456v1","updated":"2024-07-15T06:11:18Z","published":"2024-07-15T06:11:18Z","title":"Don't Throw Away Data: Better Sequence Knowledge Distillation","summary":"  A critical component in knowledge distillation is the means of coupling the\nteacher and student. The predominant sequence knowledge distillation method\ninvolves supervised learning of the student against teacher-decoded outputs,\nand is exemplified by the current state of the art, which incorporates minimum\nBayes risk (MBR) decoding. In this paper we seek to integrate MBR more tightly\nin distillation training, specifically by using several high scoring MBR\ntranslations, rather than a single selected sequence, thus capturing a rich\ndiversity of teacher outputs. Our experiments on English to German and English\nto Japanese translation show consistent improvements over strong baseline\nmethods for both tasks and with varying model sizes. Additionally, we conduct a\ndetailed analysis focusing on data efficiency and capacity curse aspects to\nelucidate MBR-n and explore its further potential.\n","authors":["Jun Wang","Eleftheria Briakou","Hamid Dadkhahi","Rishabh Agarwal","Colin Cherry","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2407.10456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10453v1","updated":"2024-07-15T05:51:11Z","published":"2024-07-15T05:51:11Z","title":"Enhancing Medication Recommendation with LLM Text Representation","summary":"  Most of the existing medication recommendation models are predicted with only\nstructured data such as medical codes, with the remaining other large amount of\nunstructured or semi-structured data underutilization. To increase the\nutilization effectively, we proposed a method of enhancing medication\nrecommendation with Large Language Model (LLM) text representation. LLM\nharnesses powerful language understanding and generation capabilities, enabling\nthe extraction of information from complex and lengthy unstructured data such\nas clinical notes which contain complex terminology. This method can be applied\nto several existing base models we selected and improve medication\nrecommendation performance with the combination representation of text and\nmedical codes experiments on two different datasets. LLM text representation\nalone can even demonstrate a comparable ability to the medical code\nrepresentation alone. Overall, this is a general method that can be applied to\nother models for improved recommendations.\n","authors":["Yu-Tzu Lee"],"pdf_url":"https://arxiv.org/pdf/2407.10453v1.pdf","comment":"65 pages, 18 figures"},{"id":"http://arxiv.org/abs/2309.00384v3","updated":"2024-07-15T05:42:34Z","published":"2023-09-01T10:44:36Z","title":"BatchPrompt: Accomplish more with less","summary":"  As the ever-increasing token limits of large language models (LLMs) have\nenabled long context as input, prompting with single data samples might no\nlonger an efficient way. A straightforward strategy improving efficiency is to\nbatch data within the token limit (e.g., 8k for gpt-3.5-turbo; 32k for GPT-4),\nwhich we call BatchPrompt. We have two initial observations for prompting with\nbatched data. First, we find that prompting with batched data in longer\ncontexts will inevitably lead to worse performance, compared to single-data\nprompting. Second, the performance of the language model is significantly\ncorrelated with the positions and order of the batched data, due to the\ncorresponding change in decoder context. To retain efficiency and overcome\nperformance loss, we propose Batch Permutation and Ensembling (BPE), and a\nnovel Self-reflection-guided EArly Stopping (SEAS) technique. Our comprehensive\nexperimental evaluation demonstrates that BPE can boost the performance of\nBatchPrompt with a striking margin on a range of popular NLP tasks, including\nquestion answering (Boolq), textual entailment (RTE), and duplicate questions\nidentification (QQP). These performances are even competitive with/higher than\nsingle-data prompting(SinglePrompt), while BatchPrompt requires much fewer LLM\ncalls and input tokens (For SinglePrompt v.s. BatchPrompt with batch size 32,\nusing just 9%-16% the number of LLM calls, Boolq accuracy 90.6% to 90.9% with\n27.4% tokens, QQP accuracy 87.2% to 88.4% with 18.6% tokens, RTE accuracy 91.5%\nto 91.1% with 30.8% tokens). To the best of our knowledge, this is the first\nwork to technically improve prompting efficiency of large language models. We\nhope our simple yet effective approach will shed light on the future research\nof large language models. The code will be released.\n","authors":["Jianzhe Lin","Maurice Diesendruck","Liang Du","Robin Abraham"],"pdf_url":"https://arxiv.org/pdf/2309.00384v3.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.03234v2","updated":"2024-07-15T05:20:18Z","published":"2024-07-03T16:03:42Z","title":"Self-Evaluation as a Defense Against Adversarial Attacks on LLMs","summary":"  When LLMs are deployed in sensitive, human-facing settings, it is crucial\nthat they do not output unsafe, biased, or privacy-violating outputs. For this\nreason, models are both trained and instructed to refuse to answer unsafe\nprompts such as \"Tell me how to build a bomb.\" We find that, despite these\nsafeguards, it is possible to break model defenses simply by appending a space\nto the end of a model's input. In a study of eight open-source models, we\ndemonstrate that this acts as a strong enough attack to cause the majority of\nmodels to generate harmful outputs with very high success rates. We examine the\ncauses of this behavior, finding that the contexts in which single spaces occur\nin tokenized training data encourage models to generate lists when prompted,\noverriding training signals to refuse to answer unsafe requests. Our findings\nunderscore the fragile state of current model alignment and promote the\nimportance of developing more robust alignment methods. Code and data will be\nmade available at https://github.com/Linlt-leon/self-eval.\n","authors":["Hannah Brown","Leon Lin","Kenji Kawaguchi","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2407.03234v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.10342v2","updated":"2024-07-15T04:19:50Z","published":"2024-02-15T22:11:18Z","title":"Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on\n  Efficient Data Utilization","summary":"  Reinforcement Learning from Human Feedback (RLHF) has achieved impressive\nempirical successes while relying on a small amount of human feedback. However,\nthere is limited theoretical justification for this phenomenon. Additionally,\nmost recent studies focus on value-based algorithms despite the recent\nempirical successes of policy-based algorithms. In this work, we consider an\nRLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based\non the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes\nknowledge of the reward function. In PO-RLHF, knowledge of the reward function\nis not assumed, and the algorithm uses trajectory-based comparison feedback to\ninfer the reward function. We provide performance bounds for PO-RLHF with low\nquery complexity, which provides insight into why a small amount of human\nfeedback may be sufficient to achieve good performance with RLHF. A key novelty\nis a trajectory-level elliptical potential analysis, which bounds the reward\nestimation error when comparison feedback (rather than numerical reward\nobservation) is given. We provide and analyze algorithms PG-RLHF and NN-PG-RLHF\nfor two settings: linear and neural function approximation, respectively.\n","authors":["Yihan Du","Anna Winnicki","Gal Dalal","Shie Mannor","R. Srikant"],"pdf_url":"https://arxiv.org/pdf/2402.10342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07532v2","updated":"2024-07-15T04:18:26Z","published":"2023-12-12T18:58:02Z","title":"Interfacing Foundation Models' Embeddings","summary":"  Foundation models possess strong capabilities in reasoning and memorizing\nacross modalities. To further unleash the power of foundation models, we\npresent FIND, a generalized interface for aligning foundation models'\nembeddings with unified image and dataset-level understanding spanning modality\nand granularity. As shown in the teaser figure, a lightweight transformer\ninterface without tuning any foundation model weights is enough for\nsegmentation, grounding, and retrieval in an interleaved manner. The proposed\ninterface has the following favorable attributes: (1) Generalizable. It applies\nto various tasks spanning retrieval, segmentation, etc., under the same\narchitecture and weights. (2) Interleavable. With the benefit of multi-task\nmulti-modal training, the proposed interface creates an interleaved shared\nembedding space. (3) Extendable. The proposed interface is adaptive to new\ntasks, and new models. In light of the interleaved embedding space, we\nintroduce FIND-Bench, which introduces new training and evaluation annotations\nto the COCO dataset for interleaved segmentation and retrieval. We are the\nfirst work aligning foundations models' embeddings for interleave\nunderstanding. Meanwhile, our approach achieves state-of-the-art performance on\nFIND-Bench and competitive performance on standard retrieval and segmentation\nsettings.\n","authors":["Xueyan Zou","Linjie Li","Jianfeng Wang","Jianwei Yang","Mingyu Ding","Junyi Wei","Zhengyuan Yang","Feng Li","Hao Zhang","Shilong Liu","Arul Aravinthan","Yong Jae Lee","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07532v2.pdf","comment":"CODE: https://github.com/UX-Decoder/FIND"},{"id":"http://arxiv.org/abs/2407.10430v1","updated":"2024-07-15T04:16:20Z","published":"2024-07-15T04:16:20Z","title":"Expanding the Scope: Inductive Knowledge Graph Reasoning with\n  Multi-Starting Progressive Propagation","summary":"  Knowledge graphs (KGs) are widely acknowledged as incomplete, and new\nentities are constantly emerging in the real world. Inductive KG reasoning aims\nto predict missing facts for these new entities. Among existing models, graph\nneural networks (GNNs) based ones have shown promising performance for this\ntask. However, they are still challenged by inefficient message propagation due\nto the distance and scalability issues. In this paper, we propose a new\ninductive KG reasoning model, MStar, by leveraging conditional message passing\nneural networks (C-MPNNs). Our key insight is to select multiple query-specific\nstarting entities to expand the scope of progressive propagation. To propagate\nquery-related messages to a farther area within limited steps, we subsequently\ndesign a highway layer to propagate information toward these selected starting\nentities. Moreover, we introduce a training strategy called LinkVerify to\nmitigate the impact of noisy training samples. Experimental results validate\nthat MStar achieves superior performance compared with state-of-the-art models,\nespecially for distant entities.\n","authors":["Zhoutian Shao","Yuanning Cui","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2407.10430v1.pdf","comment":"Accepted in the 23rd International Semantic Web Conference (ISWC\n  2024)"},{"id":"http://arxiv.org/abs/2401.07886v2","updated":"2024-07-15T03:54:20Z","published":"2024-01-15T18:28:17Z","title":"Learned Best-Effort LLM Serving","summary":"  Many applications must provide low-latency LLM service to users or risk\nunacceptable user experience. However, over-provisioning resources to serve\nfluctuating request patterns is often prohibitively expensive. In this work, we\npresent a best-effort serving system that employs deep reinforcement learning\nto adjust service quality based on the task distribution and system load. Our\nbest-effort system can maintain availability with over 10x higher client\nrequest rates, serves above 96% of peak performance 4.1x more often, and serves\nabove 98% of peak performance 2.3x more often than static serving on\nunpredictable workloads. Our learned router is robust to shifts in both the\narrival and task distribution. Compared to static serving, learned best-effort\nserving allows for cost-efficient serving through increased hardware utility.\nAdditionally, we argue that learned best-effort LLM serving is applicable in\nwide variety of settings and provides application developers great flexibility\nto meet their specific needs.\n","authors":["Siddharth Jha","Coleman Hooper","Xiaoxuan Liu","Sehoon Kim","Kurt Keutzer"],"pdf_url":"https://arxiv.org/pdf/2401.07886v2.pdf","comment":"Es-FoMo @ ICML 2024"},{"id":"http://arxiv.org/abs/2407.09020v2","updated":"2024-07-15T03:53:12Z","published":"2024-07-12T06:22:45Z","title":"3M-Health: Multimodal Multi-Teacher Knowledge Distillation for Mental\n  Health Detection","summary":"  The significance of mental health classification is paramount in contemporary\nsociety, where digital platforms serve as crucial sources for monitoring\nindividuals' well-being. However, existing social media mental health datasets\nprimarily consist of text-only samples, potentially limiting the efficacy of\nmodels trained on such data. Recognising that humans utilise cross-modal\ninformation to comprehend complex situations or issues, we present a novel\napproach to address the limitations of current methodologies. In this work, we\nintroduce a Multimodal and Multi-Teacher Knowledge Distillation model for\nMental Health Classification, leveraging insights from cross-modal human\nunderstanding. Unlike conventional approaches that often rely on simple\nconcatenation to integrate diverse features, our model addresses the challenge\nof appropriately representing inputs of varying natures (e.g., texts and\nsounds). To mitigate the computational complexity associated with integrating\nall features into a single model, we employ a multimodal and multi-teacher\narchitecture. By distributing the learning process across multiple teachers,\neach specialising in a particular feature extraction aspect, we enhance the\noverall mental health classification performance. Through experimental\nvalidation, we demonstrate the efficacy of our model in achieving improved\nperformance. All relevant codes will be made available upon publication.\n","authors":["Rina Carines Cabral","Siwen Luo","Josiah Poon","Soyeon Caren Han"],"pdf_url":"https://arxiv.org/pdf/2407.09020v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06608v3","updated":"2024-07-15T03:17:50Z","published":"2024-06-06T18:10:11Z","title":"The Prompt Report: A Systematic Survey of Prompting Techniques","summary":"  Generative Artificial Intelligence (GenAI) systems are being increasingly\ndeployed across all parts of industry and research settings. Developers and end\nusers interact with these systems through the use of prompting or prompt\nengineering. While prompting is a widespread and highly researched concept,\nthere exists conflicting terminology and a poor ontological understanding of\nwhat constitutes a prompt due to the area's nascency. This paper establishes a\nstructured understanding of prompts, by assembling a taxonomy of prompting\ntechniques and analyzing their use. We present a comprehensive vocabulary of 33\nvocabulary terms, a taxonomy of 58 text-only prompting techniques, and 40\ntechniques for other modalities. We further present a meta-analysis of the\nentire literature on natural language prefix-prompting.\n","authors":["Sander Schulhoff","Michael Ilie","Nishant Balepur","Konstantine Kahadze","Amanda Liu","Chenglei Si","Yinheng Li","Aayush Gupta","HyoJung Han","Sevien Schulhoff","Pranav Sandeep Dulepet","Saurav Vidyadhara","Dayeon Ki","Sweta Agrawal","Chau Pham","Gerson Kroiz","Feileen Li","Hudson Tao","Ashay Srivastava","Hevander Da Costa","Saloni Gupta","Megan L. Rogers","Inna Goncearenco","Giuseppe Sarli","Igor Galynker","Denis Peskoff","Marine Carpuat","Jules White","Shyamal Anadkat","Alexander Hoyle","Philip Resnik"],"pdf_url":"https://arxiv.org/pdf/2406.06608v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00782v3","updated":"2024-07-15T02:03:54Z","published":"2024-06-30T17:59:07Z","title":"Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical\n  Reasoning","summary":"  Direct Preference Optimization (DPO) has proven effective at improving the\nperformance of large language models (LLMs) on downstream tasks such as\nreasoning and alignment. In this work, we propose Step-Controlled DPO (SCDPO),\na method for automatically providing stepwise error supervision by creating\nnegative samples of mathematical reasoning rationales that start making errors\nat a specified step. By applying these samples in DPO training, SCDPO can\nbetter align the model to understand reasoning errors and output accurate\nreasoning steps. We apply SCDPO to both code-integrated and chain-of-thought\nsolutions, empirically showing that it consistently improves the performance\ncompared to naive DPO on three different SFT models, including one existing SFT\nmodel and two models we finetuned. Qualitative analysis of the credit\nassignment of SCDPO and DPO demonstrates the effectiveness of SCDPO at\nidentifying errors in mathematical solutions. We then apply SCDPO to an\nInternLM2-20B model, resulting in a 20B model that achieves high scores of\n88.5% on GSM8K and 58.1% on MATH, rivaling all other open-source LLMs, showing\nthe great potential of our method.\n","authors":["Zimu Lu","Aojun Zhou","Ke Wang","Houxing Ren","Weikang Shi","Junting Pan","Mingjie Zhan","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2407.00782v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10385v1","updated":"2024-07-15T01:33:54Z","published":"2024-07-15T01:33:54Z","title":"By My Eyes: Grounding Multimodal Large Language Models with Sensor Data\n  via Visual Prompting","summary":"  Large language models (LLMs) have demonstrated exceptional abilities across\nvarious domains. However, utilizing LLMs for ubiquitous sensing applications\nremains challenging as existing text-prompt methods show significant\nperformance degradation when handling long sensor data sequences. We propose a\nvisual prompting approach for sensor data using multimodal LLMs (MLLMs). We\ndesign a visual prompt that directs MLLMs to utilize visualized sensor data\nalongside the target sensory task descriptions. Additionally, we introduce a\nvisualization generator that automates the creation of optimal visualizations\ntailored to a given sensory task, eliminating the need for prior task-specific\nknowledge. We evaluated our approach on nine sensory tasks involving four\nsensing modalities, achieving an average of 10% higher accuracy than text-based\nprompts and reducing token costs by 15.8x. Our findings highlight the\neffectiveness and cost-efficiency of visual prompts with MLLMs for various\nsensory tasks.\n","authors":["Hyungjun Yoon","Biniyam Aschalew Tolera","Taesik Gong","Kimin Lee","Sung-Ju Lee"],"pdf_url":"https://arxiv.org/pdf/2407.10385v1.pdf","comment":"21 pages, 16 figures"},{"id":"http://arxiv.org/abs/2407.10380v1","updated":"2024-07-15T01:21:56Z","published":"2024-07-15T01:21:56Z","title":"NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models","summary":"  Cognitive textual and visual reasoning tasks, such as puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. While LLMs and VLMs, through extensive\ntraining on large amounts of human-curated data, have attained a high level of\npseudo-human intelligence in some common sense reasoning tasks, they still\nstruggle with more complex reasoning tasks that require cognitive\nunderstanding. In this work, we introduce a new dataset, NTSEBench, designed to\nevaluate the cognitive multi-modal reasoning and problem-solving skills of\nlarge models. The dataset comprises 2,728 multiple-choice questions comprising\nof a total of 4,642 images across 26 categories sampled from the NTSE\nexamination conducted nationwide in India, featuring both visual and textual\ngeneral aptitude questions that do not rely on rote learning. We establish\nbaselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a\ncomparison between open source and propriety models, we propose four distinct\nmodeling strategies to handle different modalities (text and images) in the\ndataset instances.\n","authors":["Pranshu Pandya","Agney S Talwarr","Vatsal Gupta","Tushar Kataria","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2407.10380v1.pdf","comment":"15 pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.10376v1","updated":"2024-07-15T01:09:08Z","published":"2024-07-15T01:09:08Z","title":"Large Language Model-based FMRI Encoding of Language Functions for\n  Subjects with Neurocognitive Disorder","summary":"  Functional magnetic resonance imaging (fMRI) is essential for developing\nencoding models that identify functional changes in language-related brain\nareas of individuals with Neurocognitive Disorders (NCD). While large language\nmodel (LLM)-based fMRI encoding has shown promise, existing studies\npredominantly focus on healthy, young adults, overlooking older NCD populations\nand cognitive level correlations. This paper explores language-related\nfunctional changes in older NCD adults using LLM-based fMRI encoding and brain\nscores, addressing current limitations. We analyze the correlation between\nbrain scores and cognitive scores at both whole-brain and language-related ROI\nlevels. Our findings reveal that higher cognitive abilities correspond to\nbetter brain scores, with correlations peaking in the middle temporal gyrus.\nThis study highlights the potential of fMRI encoding models and brain scores\nfor detecting early functional changes in NCD patients.\n","authors":["Yuejiao Wang","Xianmin Gong","Lingwei Meng","Xixin Wu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2407.10376v1.pdf","comment":"5 pages, accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2402.08787v5","updated":"2024-07-15T00:18:21Z","published":"2024-02-13T20:51:58Z","title":"Rethinking Machine Unlearning for Large Language Models","summary":"  We explore machine unlearning (MU) in the domain of large language models\n(LLMs), referred to as LLM unlearning. This initiative aims to eliminate\nundesirable data influence (e.g., sensitive or illegal information) and the\nassociated model capabilities, while maintaining the integrity of essential\nknowledge generation and not affecting causally unrelated information. We\nenvision LLM unlearning becoming a pivotal element in the life-cycle management\nof LLMs, potentially standing as an essential foundation for developing\ngenerative AI that is not only safe, secure, and trustworthy, but also\nresource-efficient without the need of full retraining. We navigate the\nunlearning landscape in LLMs from conceptual formulation, methodologies,\nmetrics, and applications. In particular, we highlight the often-overlooked\naspects of existing LLM unlearning research, e.g., unlearning scope, data-model\ninteraction, and multifaceted efficacy assessment. We also draw connections\nbetween LLM unlearning and related areas such as model editing, influence\nfunctions, model explanation, adversarial training, and reinforcement learning.\nFurthermore, we outline an effective assessment framework for LLM unlearning\nand explore its applications in copyright and privacy safeguards and\nsociotechnical harm reduction.\n","authors":["Sijia Liu","Yuanshun Yao","Jinghan Jia","Stephen Casper","Nathalie Baracaldo","Peter Hase","Yuguang Yao","Chris Yuhao Liu","Xiaojun Xu","Hang Li","Kush R. Varshney","Mohit Bansal","Sanmi Koyejo","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.08787v5.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.10972v1","updated":"2024-07-15T17:59:55Z","published":"2024-07-15T17:59:55Z","title":"VGBench: Evaluating Large Language Models on Vector Graphics\n  Understanding and Generation","summary":"  In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons or sketches.\nRecent studies have shown promising results on processing vector graphics with\ncapable Large Language Models (LLMs). However, such works focus solely on\nqualitative results, understanding, or a specific type of vector graphics. We\npropose VGBench, a comprehensive benchmark for LLMs on handling vector graphics\nthrough diverse aspects, including (a) both visual understanding and\ngeneration, (b) evaluation of various vector graphics formats, (c) diverse\nquestion types, (d) wide range of prompting techniques, (e) under multiple\nLLMs. Evaluating on our collected 4279 understanding and 5845 generation\nsamples, we find that LLMs show strong capability on both aspects while\nexhibiting less desirable performance on low-level formats (SVG). Both data and\nevaluation pipeline will be open-sourced at https://vgbench.github.io.\n","authors":["Bocheng Zou","Mu Cai","Jianrui Zhang","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2407.10972v1.pdf","comment":"Project Page: https://vgbench.github.io"},{"id":"http://arxiv.org/abs/2407.10964v1","updated":"2024-07-15T17:58:42Z","published":"2024-07-15T17:58:42Z","title":"No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen\n  Representations","summary":"  This paper introduces FUNGI, Features from UNsupervised GradIents, a method\nto enhance the features of vision encoders by leveraging self-supervised\ngradients. Our method is simple: given any pretrained model, we first compute\ngradients from various self-supervised objectives for each input. These are\nprojected to a lower dimension and then concatenated with the model's\nembedding. The resulting features are evaluated on k-nearest neighbor\nclassification over 11 datasets from vision, 5 from natural language\nprocessing, and 2 from audio. Across backbones spanning various sizes and\npretraining strategies, FUNGI features provide consistent performance\nimprovements over the embeddings. We also show that using FUNGI features can\nbenefit linear classification and image retrieval, and that they significantly\nimprove the retrieval-based in-context scene understanding abilities of\npretrained models, for example improving upon DINO by +17% for semantic\nsegmentation - without any training.\n","authors":["Walter Simoncini","Spyros Gidaris","Andrei Bursuc","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2407.10964v1.pdf","comment":"Preprint. Code available at\n  https://github.com/WalterSimoncini/fungivision"},{"id":"http://arxiv.org/abs/2407.02844v3","updated":"2024-07-15T17:55:49Z","published":"2024-07-03T06:40:26Z","title":"Multi-Attention Integrated Deep Learning Frameworks for Enhanced Breast\n  Cancer Segmentation and Identification","summary":"  Breast cancer poses a profound threat to lives globally, claiming numerous\nlives each year. Therefore, timely detection is crucial for early intervention\nand improved chances of survival. Accurately diagnosing and classifying breast\ntumors using ultrasound images is a persistent challenge in medicine, demanding\ncutting-edge solutions for improved treatment strategies. This research\nintroduces multiattention-enhanced deep learning (DL) frameworks designed for\nthe classification and segmentation of breast cancer tumors from ultrasound\nimages. A spatial channel attention mechanism is proposed for segmenting tumors\nfrom ultrasound images, utilizing a novel LinkNet DL framework with an\nInceptionResNet backbone. Following this, the paper proposes a deep\nconvolutional neural network with an integrated multi-attention framework\n(DCNNIMAF) to classify the segmented tumor as benign, malignant, or normal.\nFrom experimental results, it is observed that the segmentation model has\nrecorded an accuracy of 98.1%, with a minimal loss of 0.6%. It has also\nachieved high Intersection over Union (IoU) and Dice Coefficient scores of\n96.9% and 97.2%, respectively. Similarly, the classification model has attained\nan accuracy of 99.2%, with a low loss of 0.31%. Furthermore, the classification\nframework has achieved outstanding F1-Score, precision, and recall values of\n99.1%, 99.3%, and 99.1%, respectively. By offering a robust framework for early\ndetection and accurate classification of breast cancer, this proposed work\nsignificantly advances the field of medical image analysis, potentially\nimproving diagnostic precision and patient outcomes.\n","authors":["Pandiyaraju V","Shravan Venkatraman","Pavan Kumar S","Santhosh Malarvannan","Kannan A"],"pdf_url":"https://arxiv.org/pdf/2407.02844v3.pdf","comment":"29 pages, 15 figures, 6 tables"},{"id":"http://arxiv.org/abs/2407.10958v1","updated":"2024-07-15T17:55:09Z","published":"2024-07-15T17:55:09Z","title":"InVi: Object Insertion In Videos Using Off-the-Shelf Diffusion Models","summary":"  We introduce InVi, an approach for inserting or replacing objects within\nvideos (referred to as inpainting) using off-the-shelf, text-to-image latent\ndiffusion models. InVi targets controlled manipulation of objects and blending\nthem seamlessly into a background video unlike existing video editing methods\nthat focus on comprehensive re-styling or entire scene alterations. To achieve\nthis goal, we tackle two key challenges. Firstly, for high quality control and\nblending, we employ a two-step process involving inpainting and matching. This\nprocess begins with inserting the object into a single frame using a\nControlNet-based inpainting diffusion model, and then generating subsequent\nframes conditioned on features from an inpainted frame as an anchor to minimize\nthe domain gap between the background and the object. Secondly, to ensure\ntemporal coherence, we replace the diffusion model's self-attention layers with\nextended-attention layers. The anchor frame features serve as the keys and\nvalues for these layers, enhancing consistency across frames. Our approach\nremoves the need for video-specific fine-tuning, presenting an efficient and\nadaptable solution. Experimental results demonstrate that InVi achieves\nrealistic object insertion with consistent blending and coherence across\nframes, outperforming existing methods.\n","authors":["Nirat Saini","Navaneeth Bodla","Ashish Shrivastava","Avinash Ravichandran","Xiao Zhang","Abhinav Shrivastava","Bharat Singh"],"pdf_url":"https://arxiv.org/pdf/2407.10958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10957v1","updated":"2024-07-15T17:54:45Z","published":"2024-07-15T17:54:45Z","title":"Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes","summary":"  Traditional reference segmentation tasks have predominantly focused on silent\nvisual scenes, neglecting the integral role of multimodal perception and\ninteraction in human experiences. In this work, we introduce a novel task\ncalled Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment\nobjects within the visual domain based on expressions containing multimodal\ncues. Such expressions are articulated in natural language forms but are\nenriched with multimodal cues, including audio and visual descriptions. To\nfacilitate this research, we construct the first Ref-AVS benchmark, which\nprovides pixel-level annotations for objects described in corresponding\nmultimodal-cue expressions. To tackle the Ref-AVS task, we propose a new method\nthat adequately utilizes multimodal cues to offer precise segmentation\nguidance. Finally, we conduct quantitative and qualitative experiments on three\ntest subsets to compare our approach with existing methods from related tasks.\nThe results demonstrate the effectiveness of our method, highlighting its\ncapability to precisely segment objects using multimodal-cue expressions.\nDataset is available at\n\\href{https://gewu-lab.github.io/Ref-AVS}{https://gewu-lab.github.io/Ref-AVS}.\n","authors":["Yaoting Wang","Peiwen Sun","Dongzhan Zhou","Guangyao Li","Honggang Zhang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2407.10957v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.10947v1","updated":"2024-07-15T17:45:20Z","published":"2024-07-15T17:45:20Z","title":"Can Textual Semantics Mitigate Sounding Object Segmentation Preference?","summary":"  The Audio-Visual Segmentation (AVS) task aims to segment sounding objects in\nthe visual space using audio cues. However, in this work, it is recognized that\nprevious AVS methods show a heavy reliance on detrimental segmentation\npreferences related to audible objects, rather than precise audio guidance. We\nargue that the primary reason is that audio lacks robust semantics compared to\nvision, especially in multi-source sounding scenes, resulting in weak audio\nguidance over the visual space. Motivated by the the fact that text modality is\nwell explored and contains rich abstract semantics, we propose leveraging text\ncues from the visual scene to enhance audio guidance with the semantics\ninherent in text. Our approach begins by obtaining scene descriptions through\nan off-the-shelf image captioner and prompting a frozen large language model to\ndeduce potential sounding objects as text cues. Subsequently, we introduce a\nnovel semantics-driven audio modeling module with a dynamic mask to integrate\naudio features with text cues, leading to representative sounding object\nfeatures. These features not only encompass audio cues but also possess vivid\nsemantics, providing clearer guidance in the visual space. Experimental results\non AVS benchmarks validate that our method exhibits enhanced sensitivity to\naudio when aided by text cues, achieving highly competitive performance on all\nthree subsets. Project page:\n\\href{https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference}{https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference}\n","authors":["Yaoting Wang","Peiwen Sun","Yuanchao Li","Honggang Zhang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2407.10947v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.10943v1","updated":"2024-07-15T17:40:46Z","published":"2024-07-15T17:40:46Z","title":"GRUtopia: Dream General Robots in a City at Scale","summary":"  Recent works have been exploring the scaling laws in the field of Embodied\nAI. Given the prohibitive costs of collecting real-world data, we believe the\nSimulation-to-Real (Sim2Real) paradigm is a crucial step for scaling the\nlearning of embodied models. This paper introduces project GRUtopia, the first\nsimulated interactive 3D society designed for various robots. It features\nseveral advancements: (a) The scene dataset, GRScenes, includes 100k\ninteractive, finely annotated scenes, which can be freely combined into\ncity-scale environments. In contrast to previous works mainly focusing on home,\nGRScenes covers 89 diverse scene categories, bridging the gap of\nservice-oriented environments where general robots would be initially deployed.\n(b) GRResidents, a Large Language Model (LLM) driven Non-Player Character (NPC)\nsystem that is responsible for social interaction, task generation, and task\nassignment, thus simulating social scenarios for embodied AI applications. (c)\nThe benchmark, GRBench, supports various robots but focuses on legged robots as\nprimary agents and poses moderately challenging tasks involving Object\nLoco-Navigation, Social Loco-Navigation, and Loco-Manipulation. We hope that\nthis work can alleviate the scarcity of high-quality data in this field and\nprovide a more comprehensive assessment of Embodied AI research. The project is\navailable at https://github.com/OpenRobotLab/GRUtopia.\n","authors":["Hanqing Wang","Jiahe Chen","Wensi Huang","Qingwei Ben","Tai Wang","Boyu Mi","Tao Huang","Siheng Zhao","Yilun Chen","Sizhe Yang","Peizhou Cao","Wenye Yu","Zichao Ye","Jialun Li","Junfeng Long","Zirui Wang","Huiling Wang","Ying Zhao","Zhongying Tu","Yu Qiao","Dahua Lin","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2407.10943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11299v2","updated":"2024-07-15T17:37:11Z","published":"2024-03-17T18:42:38Z","title":"SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant","summary":"  Recent advances in vision-language models have shown notable generalization\nin broad tasks through visual instruction tuning. However, bridging the gap\nbetween the pre-trained vision encoder and the large language models (LLMs)\nbecomes the whole network's bottleneck. To improve cross-modality alignment,\nexisting works usually consider more visual instruction data covering a broader\nrange of vision tasks to fine-tune the model for question-answering, which,\nhowever, is costly to obtain and has not thoroughly explored the rich\ncontextual information contained in images. This paper first attempts to\nharness the overlooked context within visual instruction data, training the\nmodel to self-supervised \"learning\" how to ask high-quality questions. In this\nway, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large\nVision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible\nand meaningful image-related questions while analyzing the visual clue and\nprior language knowledge, signifying an advanced level of generalized visual\nunderstanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instruction\ndata shows a performance improvement compared with traditional\nvisual-instruction tuning methods. This improvement highlights the efficacy of\nself-questioning techniques in achieving a deeper and more nuanced\ncomprehension of visual content across various contexts.\n","authors":["Guohao Sun","Can Qin","Jiamian Wang","Zeyuan Chen","Ran Xu","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2403.11299v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10937v1","updated":"2024-07-15T17:36:54Z","published":"2024-07-15T17:36:54Z","title":"IDOL: Unified Dual-Modal Latent Diffusion for Human-Centric Joint\n  Video-Depth Generation","summary":"  Significant advances have been made in human-centric video generation, yet\nthe joint video-depth generation problem remains underexplored. Most existing\nmonocular depth estimation methods may not generalize well to synthesized\nimages or videos, and multi-view-based methods have difficulty controlling the\nhuman appearance and motion. In this work, we present IDOL (unIfied Dual-mOdal\nLatent diffusion) for high-quality human-centric joint video-depth generation.\nOur IDOL consists of two novel designs. First, to enable dual-modal generation\nand maximize the information exchange between video and depth generation, we\npropose a unified dual-modal U-Net, a parameter-sharing framework for joint\nvideo and depth denoising, wherein a modality label guides the denoising\ntarget, and cross-modal attention enables the mutual information flow. Second,\nto ensure a precise video-depth spatial alignment, we propose a motion\nconsistency loss that enforces consistency between the video and depth feature\nmotion fields, leading to harmonized outputs. Additionally, a cross-attention\nmap consistency loss is applied to align the cross-attention map of the video\ndenoising with that of the depth denoising, further facilitating spatial\nalignment. Extensive experiments on the TikTok and NTU120 datasets show our\nsuperior performance, significantly surpassing existing methods in terms of\nvideo FVD and depth accuracy.\n","authors":["Yuanhao Zhai","Kevin Lin","Linjie Li","Chung-Ching Lin","Jianfeng Wang","Zhengyuan Yang","David Doermann","Junsong Yuan","Zicheng Liu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10937v1.pdf","comment":"ECCV 2024; project page: https://yhzhai.github.io/idol/"},{"id":"http://arxiv.org/abs/2407.10935v1","updated":"2024-07-15T17:35:38Z","published":"2024-07-15T17:35:38Z","title":"STARS: Self-supervised Tuning for 3D Action Recognition in Skeleton\n  Sequences","summary":"  Self-supervised pretraining methods with masked prediction demonstrate\nremarkable within-dataset performance in skeleton-based action recognition.\nHowever, we show that, unlike contrastive learning approaches, they do not\nproduce well-separated clusters. Additionally, these methods struggle with\ngeneralization in few-shot settings. To address these issues, we propose\nSelf-supervised Tuning for 3D Action Recognition in Skeleton sequences (STARS).\nSpecifically, STARS first uses a masked prediction stage using an\nencoder-decoder architecture. It then employs nearest-neighbor contrastive\nlearning to partially tune the weights of the encoder, enhancing the formation\nof semantic clusters for different actions. By tuning the encoder for a few\nepochs, and without using hand-crafted data augmentations, STARS achieves\nstate-of-the-art self-supervised results in various benchmarks, including\nNTU-60, NTU-120, and PKU-MMD. In addition, STARS exhibits significantly better\nresults than masked prediction models in few-shot settings, where the model has\nnot seen the actions throughout pretraining. Project page:\nhttps://soroushmehraban.github.io/stars/\n","authors":["Soroush Mehraban","Mohammad Javad Rajabi","Babak Taati"],"pdf_url":"https://arxiv.org/pdf/2407.10935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10926v1","updated":"2024-07-15T17:25:42Z","published":"2024-07-15T17:25:42Z","title":"In-Loop Filtering via Trained Look-Up Tables","summary":"  In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.\n","authors":["Zhuoyuan Li","Jiacheng Li","Yao Li","Li Li","Dong Liu","Feng Wu"],"pdf_url":"https://arxiv.org/pdf/2407.10926v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.10923v1","updated":"2024-07-15T17:23:00Z","published":"2024-07-15T17:23:00Z","title":"OPa-Ma: Text Guided Mamba for 360-degree Image Out-painting","summary":"  In this paper, we tackle the recently popular topic of generating 360-degree\nimages given the conventional narrow field of view (NFoV) images that could be\ntaken from a single camera or cellphone. This task aims to predict the\nreasonable and consistent surroundings from the NFoV images. Existing methods\nfor feature extraction and fusion, often built with transformer-based\narchitectures, incur substantial memory usage and computational expense. They\nalso have limitations in maintaining visual continuity across the entire\n360-degree images, which could cause inconsistent texture and style generation.\nTo solve the aforementioned issues, we propose a novel text-guided out-painting\nframework equipped with a State-Space Model called Mamba to utilize its\nlong-sequence modelling and spatial continuity. Furthermore, incorporating\ntextual information is an effective strategy for guiding image generation,\nenriching the process with detailed context and increasing diversity.\nEfficiently extracting textual features and integrating them with image\nattributes presents a significant challenge for 360-degree image out-painting.\nTo address this, we develop two modules, Visual-textual Consistency Refiner\n(VCR) and Global-local Mamba Adapter (GMA). VCR enhances contextual richness by\nfusing the modified text features with the image features, while GMA provides\nadaptive state-selective conditions by capturing the information flow from\nglobal to local representations. Our proposed method achieves state-of-the-art\nperformance with extensive experiments on two broadly used 360-degree image\ndatasets, including indoor and outdoor settings.\n","authors":["Penglei Gao","Kai Yao","Tiandi Ye","Steven Wang","Yuan Yao","Xiaofeng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10921v1","updated":"2024-07-15T17:22:16Z","published":"2024-07-15T17:22:16Z","title":"A Dual-Attention Aware Deep Convolutional Neural Network for Early\n  Alzheimer's Detection","summary":"  Alzheimer's disease (AD) represents the primary form of neurodegeneration,\nimpacting millions of individuals each year and causing progressive cognitive\ndecline. Accurately diagnosing and classifying AD using neuroimaging data\npresents ongoing challenges in medicine, necessitating advanced interventions\nthat will enhance treatment measures. In this research, we introduce a dual\nattention enhanced deep learning (DL) framework for classifying AD from\nneuroimaging data. Combined spatial and self-attention mechanisms play a vital\nrole in emphasizing focus on neurofibrillary tangles and amyloid plaques from\nthe MRI images, which are difficult to discern with regular imaging techniques.\nResults demonstrate that our model yielded remarkable performance in comparison\nto existing state of the art (SOTA) convolutional neural networks (CNNs), with\nan accuracy of 99.1%. Moreover, it recorded remarkable metrics, with an\nF1-Score of 99.31%, a precision of 99.24%, and a recall of 99.5%. These results\nhighlight the promise of cutting edge DL methods in medical diagnostics,\ncontributing to highly reliable and more efficient healthcare solutions.\n","authors":["Pandiyaraju V","Shravan Venkatraman","Abeshek A","Aravintakshan S A","Pavan Kumar S","Kannan A"],"pdf_url":"https://arxiv.org/pdf/2407.10921v1.pdf","comment":"18 pages, 10 figures, 6 tables"},{"id":"http://arxiv.org/abs/2407.10920v1","updated":"2024-07-15T17:21:41Z","published":"2024-07-15T17:21:41Z","title":"Benchmarking Vision Language Models for Cultural Understanding","summary":"  Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.\n","authors":["Shravan Nayak","Kanishk Jain","Rabiul Awal","Siva Reddy","Sjoerd van Steenkiste","Lisa Anne Hendricks","Karolina StaÅczak","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.10920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10918v1","updated":"2024-07-15T17:19:50Z","published":"2024-07-15T17:19:50Z","title":"PartImageNet++ Dataset: Scaling up Part-based Models for Robust\n  Recognition","summary":"  Deep learning-based object recognition systems can be easily fooled by\nvarious adversarial perturbations. One reason for the weak robustness may be\nthat they do not have part-based inductive bias like the human recognition\nprocess. Motivated by this, several part-based recognition models have been\nproposed to improve the adversarial robustness of recognition. However, due to\nthe lack of part annotations, the effectiveness of these methods is only\nvalidated on small-scale nonstandard datasets. In this work, we propose PIN++,\nshort for PartImageNet++, a dataset providing high-quality part segmentation\nannotations for all categories of ImageNet-1K (IN-1K). With these annotations,\nwe build part-based methods directly on the standard IN-1K dataset for robust\nrecognition. Different from previous two-stage part-based models, we propose a\nMulti-scale Part-supervised Model (MPM), to learn a robust representation with\npart annotations. Experiments show that MPM yielded better adversarial\nrobustness on the large-scale IN-1K over strong baselines across various attack\nsettings. Furthermore, MPM achieved improved robustness on common corruptions\nand several out-of-distribution datasets. The dataset, together with these\nresults, enables and encourages researchers to explore the potential of\npart-based models in more real applications.\n","authors":["Xiao Li","Yining Liu","Na Dong","Sitian Qin","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2407.10918v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2401.05675v2","updated":"2024-07-15T17:19:18Z","published":"2024-01-11T05:36:36Z","title":"Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for\n  Text-to-Image Generation","summary":"  Recent works have demonstrated that using reinforcement learning (RL) with\nmultiple quality rewards can improve the quality of generated images in\ntext-to-image (T2I) generation. However, manually adjusting reward weights\nposes challenges and may cause over-optimization in certain metrics. To solve\nthis, we propose Parrot, which addresses the issue through multi-objective\noptimization and introduces an effective multi-reward optimization strategy to\napproximate Pareto optimal. Utilizing batch-wise Pareto optimal selection,\nParrot automatically identifies the optimal trade-off among different rewards.\nWe use the novel multi-reward optimization algorithm to jointly optimize the\nT2I model and a prompt expansion network, resulting in significant improvement\nof image quality and also allow to control the trade-off of different rewards\nusing a reward related prompt during inference. Furthermore, we introduce\noriginal prompt-centered guidance at inference time, ensuring fidelity to user\ninput after prompt expansion. Extensive experiments and a user study validate\nthe superiority of Parrot over several baselines across various quality\ncriteria, including aesthetics, human preference, text-image alignment, and\nimage sentiment.\n","authors":["Seung Hyun Lee","Yinxiao Li","Junjie Ke","Innfarn Yoo","Han Zhang","Jiahui Yu","Qifei Wang","Fei Deng","Glenn Entis","Junfeng He","Gang Li","Sangpil Kim","Irfan Essa","Feng Yang"],"pdf_url":"https://arxiv.org/pdf/2401.05675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08744v2","updated":"2024-07-15T17:15:34Z","published":"2023-12-14T08:39:39Z","title":"GOEmbed: Gradient Origin Embeddings for Representation Agnostic 3D\n  Feature Learning","summary":"  Encoding information from 2D views of an object into a 3D representation is\ncrucial for generalized 3D feature extraction. Such features can then enable 3D\nreconstruction, 3D generation, and other applications. We propose GOEmbed\n(Gradient Origin Embeddings) that encodes input 2D images into any 3D\nrepresentation, without requiring a pre-trained image feature extractor; unlike\ntypical prior approaches in which input images are either encoded using 2D\nfeatures extracted from large pre-trained models, or customized features are\ndesigned to handle different 3D representations; or worse, encoders may not yet\nbe available for specialized 3D neural representations such as MLPs and\nhash-grids. We extensively evaluate our proposed GOEmbed under different\nexperimental settings on the OmniObject3D benchmark. First, we evaluate how\nwell the mechanism compares against prior encoding mechanisms on multiple 3D\nrepresentations using an illustrative experiment called Plenoptic-Encoding.\nSecond, the efficacy of the GOEmbed mechanism is further demonstrated by\nachieving a new SOTA FID of 22.12 on the OmniObject3D generation task using a\ncombination of GOEmbed and DFM (Diffusion with Forward Models), which we call\nGOEmbedFusion. Finally, we evaluate how the GOEmbed mechanism bolsters\nsparse-view 3D reconstruction pipelines.\n","authors":["Animesh Karnewar","Roman Shapovalov","Tom Monnier","Andrea Vedaldi","Niloy J. Mitra","David Novotny"],"pdf_url":"https://arxiv.org/pdf/2312.08744v2.pdf","comment":"ECCV 2024 conference; project page at:\n  https://holodiffusion.github.io/goembed/"},{"id":"http://arxiv.org/abs/2407.10910v1","updated":"2024-07-15T17:10:31Z","published":"2024-07-15T17:10:31Z","title":"DataDream: Few-shot Guided Dataset Generation","summary":"  While text-to-image diffusion models have been shown to achieve\nstate-of-the-art results in image synthesis, they have yet to prove their\neffectiveness in downstream applications. Previous work has proposed to\ngenerate data for image classifier training given limited real data access.\nHowever, these methods struggle to generate in-distribution images or depict\nfine-grained features, thereby hindering the generalization of classification\nmodels trained on synthetic datasets. We propose DataDream, a framework for\nsynthesizing classification datasets that more faithfully represents the real\ndata distribution when guided by few-shot examples of the target classes.\nDataDream fine-tunes LoRA weights for the image generation model on the few\nreal images before generating the training data using the adapted model. We\nthen fine-tune LoRA weights for CLIP using the synthetic data to improve\ndownstream image classification over previous approaches on a large variety of\ndatasets. We demonstrate the efficacy of DataDream through extensive\nexperiments, surpassing state-of-the-art classification accuracy with few-shot\ndata across 7 out of 10 datasets, while being competitive on the other 3.\nAdditionally, we provide insights into the impact of various factors, such as\nthe number of real-shot and generated images as well as the fine-tuning compute\non model performance. The code is available at\nhttps://github.com/ExplainableML/DataDream.\n","authors":["Jae Myung Kim","Jessica Bader","Stephan Alaniz","Cordelia Schmid","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2407.10910v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2312.05287v2","updated":"2024-07-15T17:05:12Z","published":"2023-12-08T15:39:06Z","title":"Human-in-the-Loop Visual Re-ID for Population Size Estimation","summary":"  Computer vision-based re-identification (Re-ID) systems are increasingly\nbeing deployed for estimating population size in large image collections.\nHowever, the estimated size can be significantly inaccurate when the task is\nchallenging or when deployed on data from new distributions. We propose a\nhuman-in-the-loop approach for estimating population size driven by a pairwise\nsimilarity derived from an off-the-shelf Re-ID system. Our approach, based on\nnested importance sampling, selects pairs of images for human vetting driven by\nthe pairwise similarity, and produces asymptotically unbiased population size\nestimates with associated confidence intervals. We perform experiments on\nvarious animal Re-ID datasets and demonstrate that our method outperforms\nstrong baselines and active clustering approaches. In many cases, we are able\nto reduce the error rates of the estimated size from around 80% using CV alone\nto less than 20% by vetting a fraction (often less than 0.002%) of the total\npairs. The cost of vetting reduces with the increase in accuracy and provides a\npractical approach for population size estimation within a desired tolerance\nwhen deploying Re-ID systems.\n","authors":["Gustavo Perez","Daniel Sheldon","Grant Van Horn","Subhransu Maji"],"pdf_url":"https://arxiv.org/pdf/2312.05287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03202v3","updated":"2024-07-15T16:59:07Z","published":"2024-04-04T05:10:26Z","title":"OmniGS: Fast Radiance Field Reconstruction using Omnidirectional\n  Gaussian Splatting","summary":"  Photorealistic reconstruction relying on 3D Gaussian Splatting has shown\npromising potential in various domains. However, the current 3D Gaussian\nSplatting system only supports radiance field reconstruction using undistorted\nperspective images. In this paper, we present OmniGS, a novel omnidirectional\nGaussian splatting system, to take advantage of omnidirectional images for fast\nradiance field reconstruction. Specifically, we conduct a theoretical analysis\nof spherical camera model derivatives in 3D Gaussian Splatting. According to\nthe derivatives, we then implement a new GPU-accelerated omnidirectional\nrasterizer that directly splats 3D Gaussians onto the equirectangular screen\nspace for omnidirectional image rendering. We realize differentiable\noptimization of the omnidirectional radiance field without the requirement of\ncube-map rectification or tangent-plane approximation. Extensive experiments\nconducted in egocentric and roaming scenarios demonstrate that our method\nachieves state-of-the-art reconstruction quality and high rendering speed using\nomnidirectional images. The code will be publicly available.\n","authors":["Longwei Li","Huajian Huang","Sai-Kit Yeung","Hui Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.03202v3.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.05118v2","updated":"2024-07-15T16:53:17Z","published":"2024-07-06T16:08:17Z","title":"SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional\n  Temporal Grounding","summary":"  Temporal grounding, also known as video moment retrieval, aims at locating\nvideo segments corresponding to a given query sentence. The compositional\nnature of natural language enables the localization beyond predefined events,\nposing a certain challenge to the compositional generalizability of existing\nmethods. Recent studies establish the correspondence between videos and queries\nthrough a decompose-reconstruct manner to achieve compositional generalization.\nHowever, they only consider dominant primitives and build negative queries\nthrough random sampling and recombination, resulting in semantically\nimplausible negatives that hinder the models from learning rational\ncompositions. In addition, recent DETR-based methods still underperform in\ncompositional temporal grounding, showing irrational saliency responses when\ngiven negative queries that have subtle differences from positive queries. To\naddress these limitations, we first propose a large language model-driven\nmethod for negative query construction, utilizing GPT-3.5-Turbo to generate\nsemantically plausible hard negative queries. Subsequently, we introduce a\ncoarse-to-fine saliency ranking strategy, which encourages the model to learn\nthe multi-granularity semantic relationships between videos and hierarchical\nnegative queries to boost compositional generalization. Extensive experiments\non two challenging benchmarks validate the effectiveness and generalizability\nof our proposed method. Our code is available at\nhttps://github.com/zxccade/SHINE.\n","authors":["Zixu Cheng","Yujiang Pu","Shaogang Gong","Parisa Kordjamshidi","Yu Kong"],"pdf_url":"https://arxiv.org/pdf/2407.05118v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10902v1","updated":"2024-07-15T16:53:04Z","published":"2024-07-15T16:53:04Z","title":"Interpreting Hand gestures using Object Detection and Digits\n  Classification","summary":"  Hand gestures have evolved into a natural and intuitive means of engaging\nwith technology. The objective of this research is to develop a robust system\nthat can accurately recognize and classify hand gestures representing numbers.\nThe proposed approach involves collecting a dataset of hand gesture images,\npreprocessing and enhancing the images, extracting relevant features, and\ntraining a machine learning model. The advancement of computer vision\ntechnology and object detection techniques, in conjunction with OpenCV's\ncapability to analyze and comprehend hand gestures, presents a chance to\ntransform the identification of numerical digits and its potential\napplications. The advancement of computer vision technology and object\nidentification technologies, along with OpenCV's capacity to analyze and\ninterpret hand gestures, has the potential to revolutionize human interaction,\nboosting people's access to information, education, and employment\nopportunities. Keywords: Computer Vision, Machine learning, Deep Learning,\nNeural Networks\n","authors":["Sangeetha K","Balaji VS","Kamalesh P","Anirudh Ganapathy PS"],"pdf_url":"https://arxiv.org/pdf/2407.10902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14906v2","updated":"2024-07-15T16:42:22Z","published":"2023-11-25T02:46:12Z","title":"AutoEval-Video: An Automatic Benchmark for Assessing Large Vision\n  Language Models in Open-Ended Video Question Answering","summary":"  We propose a novel and challenging benchmark, AutoEval-Video, to\ncomprehensively evaluate large vision-language models in open-ended video\nquestion answering. The comprehensiveness of AutoEval-Video is demonstrated in\ntwo aspects: 1) AutoEval-Video constructs open-ended video-questions across 9\nskill dimensions, addressing capabilities of perception, comprehension, and\ngeneration. 2) AutoEval-Video contains newly collected videos that cover over\n40 distinct themes. To efficiently evaluate responses to the open-ended\nquestions, we employ an LLM-based evaluation approach, but instead of merely\nproviding a reference answer, we annotate unique evaluation rules for every\nsingle instance (video-question pair). To maximize the robustness of these\nrules, we develop a novel adversarial annotation mechanism. By using\ninstance-specific rules as prompt, GPT-4, as an automatic evaluator, can\nachieve a stable evaluation accuracy of around 97.0%, comparable to the 94.9% -\n97.5% accuracy of a human evaluator. Furthermore, we assess the performance of\neight large vision-language models on AutoEval-Video. Among them, GPT-4V(ision)\nsignificantly outperforms other models, achieving an accuracy of 32.2%.\nHowever, there is still substantial room for improvement compared to human\naccuracy of 72.8%. By conducting an extensive case study, we uncover several\ndrawbacks of GPT-4V, such as limited temporal and dynamic comprehension, and\noverly general responses. Code is available at\nhttps://github.com/Xiuyuan-Chen/AutoEval-Video.\n","authors":["Xiuyuan Chen","Yuan Lin","Yuchen Zhang","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2311.14906v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10888v1","updated":"2024-07-15T16:38:59Z","published":"2024-07-15T16:38:59Z","title":"Leveraging Multimodal CycleGAN for the Generation of Anatomically\n  Accurate Synthetic CT Scans from MRIs","summary":"  In many clinical settings, the use of both Computed Tomography (CT) and\nMagnetic Resonance (MRI) is necessary to pursue a thorough understanding of the\npatient's anatomy and to plan a suitable therapeutical strategy; this is often\nthe case in MRI-based radiotherapy, where CT is always necessary to prepare the\ndose delivery, as it provides the essential information about the radiation\nabsorption properties of the tissues. Sometimes, MRI is preferred to contour\nthe target volumes. However, this approach is often not the most efficient, as\nit is more expensive, time-consuming and, most importantly, stressful for the\npatients. To overcome this issue, in this work, we analyse the capabilities of\ndifferent configurations of Deep Learning models to generate synthetic CT scans\nfrom MRI, leveraging the power of Generative Adversarial Networks (GANs) and,\nin particular, the CycleGAN architecture, capable of working in an unsupervised\nmanner and without paired images, which were not available. Several CycleGAN\nmodels were trained unsupervised to generate CT scans from different MRI\nmodalities with and without contrast agents. To overcome the problem of not\nhaving a ground truth, distribution-based metrics were used to assess the\nmodel's performance quantitatively, together with a qualitative evaluation\nwhere physicians were asked to differentiate between real and synthetic images\nto understand how realistic the generated images were. The results show how,\ndepending on the input modalities, the models can have very different\nperformances; however, models with the best quantitative results, according to\nthe distribution-based metrics used, can generate very difficult images to\ndistinguish from the real ones, even for physicians, demonstrating the\napproach's potential.\n","authors":["Leonardo Crespi","Samuele Camnasio","Damiano Dei","Nicola Lambri","Pietro Mancosu","Marta Scorsetti","Daniele Loiacono"],"pdf_url":"https://arxiv.org/pdf/2407.10888v1.pdf","comment":"Currently submitted to: Scientific Reports"},{"id":"http://arxiv.org/abs/2310.12128v2","updated":"2024-07-15T16:32:39Z","published":"2023-10-18T17:37:10Z","title":"DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM\n  Planning","summary":"  Text-to-image (T2I) generation has seen significant growth over the past few\nyears. Despite this, there has been little work on generating diagrams with T2I\nmodels. A diagram is a symbolic/schematic representation that explains\ninformation using structurally rich and spatially complex visualizations (e.g.,\na dense combination of related objects, text labels, directional arrows/lines,\netc.). Existing state-of-the-art T2I models often fail at diagram generation\nbecause they lack fine-grained object layout control when many objects are\ndensely connected via complex relations such as arrows/lines, and also often\nfail to render comprehensible text labels. To address this gap, we present\nDiagrammerGPT, a novel two-stage text-to-diagram generation framework\nleveraging the layout guidance capabilities of LLMs to generate more accurate\ndiagrams. In the first stage, we use LLMs to generate and iteratively refine\n'diagram plans' (in a planner-auditor feedback loop). In the second stage, we\nuse a diagram generator, DiagramGLIGEN, and a text label rendering module to\ngenerate diagrams (with clear text labels) following the diagram plans. To\nbenchmark the text-to-diagram generation task, we introduce AI2D-Caption, a\ndensely annotated diagram dataset built on top of the AI2D dataset. We show\nthat our DiagrammerGPT framework produces more accurate diagrams, outperforming\nexisting T2I models. We also provide comprehensive analysis, including\nopen-domain diagram generation, multi-platform vector graphic diagram\ngeneration, human-in-the-loop editing, and multimodal planner/auditor LLMs.\n","authors":["Abhay Zala","Han Lin","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2310.12128v2.pdf","comment":"COLM 2024; Project page: https://diagrammerGPT.github.io/"},{"id":"http://arxiv.org/abs/2407.10876v1","updated":"2024-07-15T16:25:07Z","published":"2024-07-15T16:25:07Z","title":"RepVF: A Unified Vector Fields Representation for Multi-task 3D\n  Perception","summary":"  Concurrent processing of multiple autonomous driving 3D perception tasks\nwithin the same spatiotemporal scene poses a significant challenge, in\nparticular due to the computational inefficiencies and feature competition\nbetween tasks when using traditional multi-task learning approaches. This paper\naddresses these issues by proposing a novel unified representation, RepVF,\nwhich harmonizes the representation of various perception tasks such as 3D\nobject detection and 3D lane detection within a single framework. RepVF\ncharacterizes the structure of different targets in the scene through a vector\nfield, enabling a single-head, multi-task learning model that significantly\nreduces computational redundancy and feature competition. Building upon RepVF,\nwe introduce RFTR, a network designed to exploit the inherent connections\nbetween different tasks by utilizing a hierarchical structure of queries that\nimplicitly model the relationships both between and within tasks. This approach\neliminates the need for task-specific heads and parameters, fundamentally\nreducing the conflicts inherent in traditional multi-task learning paradigms.\nWe validate our approach by combining labels from the OpenLane dataset with the\nWaymo Open dataset. Our work presents a significant advancement in the\nefficiency and effectiveness of multi-task perception in autonomous driving,\noffering a new perspective on handling multiple 3D perception tasks\nsynchronously and in parallel. The code will be available at:\nhttps://github.com/jbji/RepVF\n","authors":["Chunliang Li","Wencheng Han","Junbo Yin","Sanyuan Zhao","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2407.10876v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10874v1","updated":"2024-07-15T16:23:53Z","published":"2024-07-15T16:23:53Z","title":"Random Channel Ablation for Robust Hand Gesture Classification with\n  Multimodal Biosignals","summary":"  Biosignal-based hand gesture classification is an important component of\neffective human-machine interaction. For multimodal biosignal sensing, the\nmodalities often face data loss due to missing channels in the data which can\nadversely affect the gesture classification performance. To make the\nclassifiers robust to missing channels in the data, this paper proposes using\nRandom Channel Ablation (RChA) during the training process. Ultrasound and\nforce myography (FMG) data were acquired from the forearm for 12 hand gestures\nover 2 subjects. The resulting multimodal data had 16 total channels, 8 for\neach modality. The proposed method was applied to convolutional neural network\narchitecture, and compared with baseline, imputation, and oracle methods. Using\n5-fold cross-validation for the two subjects, on average, 12.2% and 24.5%\nimprovement was observed for gesture classification with up to 4 and 8 missing\nchannels respectively compared to the baseline. Notably, the proposed method is\nalso robust to an increase in the number of missing channels compared to other\nmethods. These results show the efficacy of using random channel ablation to\nimprove classifier robustness for multimodal and multi-channel biosignal-based\nhand gesture classification.\n","authors":["Keshav Bimbraw","Jing Liu","Ye Wang","Toshiaki Koike-Akino"],"pdf_url":"https://arxiv.org/pdf/2407.10874v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.10870v1","updated":"2024-07-15T16:18:06Z","published":"2024-07-15T16:18:06Z","title":"GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via\n  VLM","summary":"  Large vision-language models (LVLMs), such as the Generative Pre-trained\nTransformer 4-omni (GPT-4o), are emerging multi-modal foundation models which\nhave great potential as powerful artificial-intelligence (AI) assistance tools\nfor a myriad of applications, including healthcare, industrial, and academic\nsectors. Although such foundation models perform well in a wide range of\ngeneral tasks, their capability without fine-tuning is often limited in\nspecialized tasks. However, full fine-tuning of large foundation models is\nchallenging due to enormous computation/memory/dataset requirements. We show\nthat GPT-4o can decode hand gestures from forearm ultrasound data even with no\nfine-tuning, and improves with few-shot, in-context learning.\n","authors":["Keshav Bimbraw","Ye Wang","Jing Liu","Toshiaki Koike-Akino"],"pdf_url":"https://arxiv.org/pdf/2407.10870v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2312.13277v2","updated":"2024-07-15T16:13:05Z","published":"2023-12-20T18:56:45Z","title":"Deep Learning on Object-centric 3D Neural Fields","summary":"  In recent years, Neural Fields (NFs) have emerged as an effective tool for\nencoding diverse continuous signals such as images, videos, audio, and 3D\nshapes. When applied to 3D data, NFs offer a solution to the fragmentation and\nlimitations associated with prevalent discrete representations. However, given\nthat NFs are essentially neural networks, it remains unclear whether and how\nthey can be seamlessly integrated into deep learning pipelines for solving\ndownstream tasks. This paper addresses this research problem and introduces\nnf2vec, a framework capable of generating a compact latent representation for\nan input NF in a single inference pass. We demonstrate that nf2vec effectively\nembeds 3D objects represented by the input NFs and showcase how the resulting\nembeddings can be employed in deep learning pipelines to successfully address\nvarious tasks, all while processing exclusively NFs. We test this framework on\nseveral NFs used to represent 3D surfaces, such as unsigned/signed distance and\noccupancy fields. Moreover, we demonstrate the effectiveness of our approach\nwith more complex NFs that encompass both geometry and appearance of 3D objects\nsuch as neural radiance fields.\n","authors":["Pierluigi Zama Ramirez","Luca De Luigi","Daniele Sirocchi","Adriano Cardace","Riccardo Spezialetti","Francesco Ballerini","Samuele Salti","Luigi Di Stefano"],"pdf_url":"https://arxiv.org/pdf/2312.13277v2.pdf","comment":"Extended version of the paper \"Deep Learning on Implicit Neural\n  Representations of Shapes\" that was presented at ICLR 2023. Accepted at TPAMI"},{"id":"http://arxiv.org/abs/2407.10862v1","updated":"2024-07-15T16:10:58Z","published":"2024-07-15T16:10:58Z","title":"R3D-AD: Reconstruction via Diffusion for 3D Anomaly Detection","summary":"  3D anomaly detection plays a crucial role in monitoring parts for localized\ninherent defects in precision manufacturing. Embedding-based and\nreconstruction-based approaches are among the most popular and successful\nmethods. However, there are two major challenges to the practical application\nof the current approaches: 1) the embedded models suffer the prohibitive\ncomputational and storage due to the memory bank structure; 2) the\nreconstructive models based on the MAE mechanism fail to detect anomalies in\nthe unmasked regions. In this paper, we propose R3D-AD, reconstructing\nanomalous point clouds by diffusion model for precise 3D anomaly detection. Our\napproach capitalizes on the data distribution conversion of the diffusion\nprocess to entirely obscure the input's anomalous geometry. It step-wisely\nlearns a strict point-level displacement behavior, which methodically corrects\nthe aberrant points. To increase the generalization of the model, we further\npresent a novel 3D anomaly simulation strategy named Patch-Gen to generate\nrealistic and diverse defect shapes, which narrows the domain gap between\ntraining and testing. Our R3D-AD ensures a uniform spatial transformation,\nwhich allows straightforwardly generating anomaly results by distance\ncomparison. Extensive experiments show that our R3D-AD outperforms previous\nstate-of-the-art methods, achieving 73.4% Image-level AUROC on the Real3D-AD\ndataset and 74.9% Image-level AUROC on the Anomaly-ShapeNet dataset with an\nexceptional efficiency.\n","authors":["Zheyuan Zhou","Le Wang","Naiyu Fang","Zili Wang","Lemiao Qiu","Shuyou Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.10862v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10860v1","updated":"2024-07-15T16:10:11Z","published":"2024-07-15T16:10:11Z","title":"Human-Centric Transformer for Domain Adaptive Action Recognition","summary":"  We study the domain adaptation task for action recognition, namely domain\nadaptive action recognition, which aims to effectively transfer action\nrecognition power from a label-sufficient source domain to a label-free target\ndomain. Since actions are performed by humans, it is crucial to exploit human\ncues in videos when recognizing actions across domains. However, existing\nmethods are prone to losing human cues but prefer to exploit the correlation\nbetween non-human contexts and associated actions for recognition, and the\ncontexts of interest agnostic to actions would reduce recognition performance\nin the target domain. To overcome this problem, we focus on uncovering\nhuman-centric action cues for domain adaptive action recognition, and our\nconception is to investigate two aspects of human-centric action cues, namely\nhuman cues and human-context interaction cues. Accordingly, our proposed\nHuman-Centric Transformer (HCTransformer) develops a decoupled human-centric\nlearning paradigm to explicitly concentrate on human-centric action cues in\ndomain-variant video feature learning. Our HCTransformer first conducts\nhuman-aware temporal modeling by a human encoder, aiming to avoid a loss of\nhuman cues during domain-invariant video feature learning. Then, by a\nTransformer-like architecture, HCTransformer exploits domain-invariant and\naction-correlated contexts by a context encoder, and further models\ndomain-invariant interaction between humans and action-correlated contexts. We\nconduct extensive experiments on three benchmarks, namely UCF-HMDB,\nKinetics-NecDrone and EPIC-Kitchens-UDA, and the state-of-the-art performance\ndemonstrates the effectiveness of our proposed HCTransformer.\n","authors":["Kun-Yu Lin","Jiaming Zhou","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.10860v1.pdf","comment":"Accepted by TPAMI"},{"id":"http://arxiv.org/abs/2407.10856v1","updated":"2024-07-15T16:08:22Z","published":"2024-07-15T16:08:22Z","title":"Physics-Inspired Generative Models in Medical Imaging: A Review","summary":"  Physics-inspired generative models, in particular diffusion and Poisson flow\nmodels, enhance Bayesian methods and promise great utilities in medical\nimaging. This review examines the transformative role of such generative\nmethods. First, a variety of physics-inspired generative models, including\nDenoising Diffusion Probabilistic Models (DDPM), Score-based Diffusion Models,\nand Poisson Flow Generative Models (PFGM and PFGM++), are revisited, with an\nemphasis on their accuracy, robustness as well as acceleration. Then, major\napplications of physics-inspired generative models in medical imaging are\npresented, comprising image reconstruction, image generation, and image\nanalysis. Finally, future research directions are brainstormed, including\nunification of physics-inspired generative models, integration with\nvision-language models (VLMs),and potential novel applications of generative\nmodels. Since the development of generative methods has been rapid, this review\nwill hopefully give peers and learners a timely snapshot of this new family of\nphysics-driven generative models and help capitalize their enormous potential\nfor medical imaging.\n","authors":["Dennis Hein","Afshin Bozorgpour","Dorit Merhof","Ge Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04643v2","updated":"2024-07-15T15:45:57Z","published":"2024-04-06T14:28:01Z","title":"Constrained 6-DoF Grasp Generation on Complex Shapes for Improved\n  Dual-Arm Manipulation","summary":"  Efficiently generating grasp poses tailored to specific regions of an object\nis vital for various robotic manipulation tasks, especially in a dual-arm\nsetup. This scenario presents a significant challenge due to the complex\ngeometries involved, requiring a deep understanding of the local geometry to\ngenerate grasps efficiently on the specified constrained regions. Existing\nmethods only explore settings involving table-top/small objects and require\naugmented datasets to train, limiting their performance on complex objects. We\npropose CGDF: Constrained Grasp Diffusion Fields, a diffusion-based grasp\ngenerative model that generalizes to objects with arbitrary geometries, as well\nas generates dense grasps on the target regions. CGDF uses a part-guided\ndiffusion approach that enables it to get high sample efficiency in constrained\ngrasping without explicitly training on massive constraint-augmented datasets.\nWe provide qualitative and quantitative comparisons using analytical metrics\nand in simulation, in both unconstrained and constrained settings to show that\nour method can generalize to generate stable grasps on complex objects,\nespecially useful for dual-arm manipulation settings, while existing methods\nstruggle to do so.\n","authors":["Gaurav Singh","Sanket Kalwar","Md Faizal Karim","Bipasha Sen","Nagamanikandan Govindan","Srinath Sridhar","K Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2404.04643v2.pdf","comment":"Project Page: https://constrained-grasp-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2407.10833v1","updated":"2024-07-15T15:43:27Z","published":"2024-07-15T15:43:27Z","title":"MoE-DiffIR: Task-customized Diffusion Priors for Universal Compressed\n  Image Restoration","summary":"  We present MoE-DiffIR, an innovative universal compressed image restoration\n(CIR) method with task-customized diffusion priors. This intends to handle two\npivotal challenges in the existing CIR methods: (i) lacking adaptability and\nuniversality for different image codecs, e.g., JPEG and WebP; (ii) poor texture\ngeneration capability, particularly at low bitrates. Specifically, our\nMoE-DiffIR develops the powerful mixture-of-experts (MoE) prompt module, where\nsome basic prompts cooperate to excavate the task-customized diffusion priors\nfrom Stable Diffusion (SD) for each compression task. Moreover, the\ndegradation-aware routing mechanism is proposed to enable the flexible\nassignment of basic prompts. To activate and reuse the cross-modality\ngeneration prior of SD, we design the visual-to-text adapter for MoE-DiffIR,\nwhich aims to adapt the embedding of low-quality images from the visual domain\nto the textual domain as the textual guidance for SD, enabling more consistent\nand reasonable texture generation. We also construct one comprehensive\nbenchmark dataset for universal CIR, covering 21 types of degradations from 7\npopular traditional and learned codecs. Extensive experiments on universal CIR\nhave demonstrated the excellent robustness and texture restoration capability\nof our proposed MoE-DiffIR. The project can be found at\nhttps://renyulin-f.github.io/MoE-DiffIR.github.io/.\n","authors":["Yulin Ren","Xin Li","Bingchen Li","Xingrui Wang","Mengxi Guo","Shijie Zhao","Li Zhang","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10833v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10831v1","updated":"2024-07-15T15:43:08Z","published":"2024-07-15T15:43:08Z","title":"Temporal Event Stereo via Joint Learning with Stereoscopic Flow","summary":"  Event cameras are dynamic vision sensors inspired by the biological retina,\ncharacterized by their high dynamic range, high temporal resolution, and low\npower consumption. These features make them capable of perceiving 3D\nenvironments even in extreme conditions. Event data is continuous across the\ntime dimension, which allows a detailed description of each pixel's movements.\nTo fully utilize the temporally dense and continuous nature of event cameras,\nwe propose a novel temporal event stereo, a framework that continuously uses\ninformation from previous time steps. This is accomplished through the\nsimultaneous training of an event stereo matching network alongside\nstereoscopic flow, a new concept that captures all pixel movements from stereo\ncameras. Since obtaining ground truth for optical flow during training is\nchallenging, we propose a method that uses only disparity maps to train the\nstereoscopic flow. The performance of event-based stereo matching is enhanced\nby temporally aggregating information using the flows. We have achieved\nstate-of-the-art performance on the MVSEC and the DSEC datasets. The method is\ncomputationally efficient, as it stacks previous information in a cascading\nmanner. The code is available at\nhttps://github.com/mickeykang16/TemporalEventStereo.\n","authors":["Hoonhee Cho","Jae-Young Kang","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2407.10831v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.10825v1","updated":"2024-07-15T15:38:21Z","published":"2024-07-15T15:38:21Z","title":"Wicked Oddities: Selectively Poisoning for Effective Clean-Label\n  Backdoor Attacks","summary":"  Deep neural networks are vulnerable to backdoor attacks, a type of\nadversarial attack that poisons the training data to manipulate the behavior of\nmodels trained on such data. Clean-label attacks are a more stealthy form of\nbackdoor attacks that can perform the attack without changing the labels of\npoisoned data. Early works on clean-label attacks added triggers to a random\nsubset of the training set, ignoring the fact that samples contribute unequally\nto the attack's success. This results in high poisoning rates and low attack\nsuccess rates. To alleviate the problem, several supervised learning-based\nsample selection strategies have been proposed. However, these methods assume\naccess to the entire labeled training set and require training, which is\nexpensive and may not always be practical. This work studies a new and more\npractical (but also more challenging) threat model where the attacker only\nprovides data for the target class (e.g., in face recognition systems) and has\nno knowledge of the victim model or any other classes in the training set. We\nstudy different strategies for selectively poisoning a small set of training\nsamples in the target class to boost the attack success rate in this setting.\nOur threat model poses a serious threat in training machine learning models\nwith third-party datasets, since the attack can be performed effectively with\nlimited information. Experiments on benchmark datasets illustrate the\neffectiveness of our strategies in improving clean-label backdoor attacks.\n","authors":["Quang H. Nguyen","Nguyen Ngoc-Hieu","The-Anh Ta","Thanh Nguyen-Tang","Hoang Thanh-Tung","Khoa D. Doan"],"pdf_url":"https://arxiv.org/pdf/2407.10825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12112v3","updated":"2024-07-15T15:33:10Z","published":"2023-08-23T13:02:52Z","title":"Category Adaptation Meets Projected Distillation in Generalized\n  Continual Category Discovery","summary":"  Generalized Continual Category Discovery (GCCD) tackles learning from\nsequentially arriving, partially labeled datasets while uncovering new\ncategories. Traditional methods depend on feature distillation to prevent\nforgetting the old knowledge. However, this strategy restricts the model's\nability to adapt and effectively distinguish new categories. To address this,\nwe introduce a novel technique integrating a learnable projector with feature\ndistillation, thus enhancing model adaptability without sacrificing past\nknowledge. The resulting distribution shift of the previously learned\ncategories is mitigated with the auxiliary category adaptation network. We\ndemonstrate that while each component offers modest benefits individually,\ntheir combination - dubbed CAMP (Category Adaptation Meets Projected\ndistillation) - significantly improves the balance between learning new\ninformation and retaining old. CAMP exhibits superior performance across\nseveral GCCD and Class Incremental Learning scenarios. The code is available at\nhttps://github.com/grypesc/CAMP.\n","authors":["Grzegorz RypeÅÄ","Daniel Marczak","Sebastian Cygert","Tomasz TrzciÅski","BartÅomiej Twardowski"],"pdf_url":"https://arxiv.org/pdf/2308.12112v3.pdf","comment":"Accepted for ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10814v1","updated":"2024-07-15T15:31:55Z","published":"2024-07-15T15:31:55Z","title":"Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot\n  Whole Slide Image Classification","summary":"  Current multi-instance learning algorithms for pathology image analysis often\nrequire a substantial number of Whole Slide Images for effective training but\nexhibit suboptimal performance in scenarios with limited learning data. In\nclinical settings, restricted access to pathology slides is inevitable due to\npatient privacy concerns and the prevalence of rare or emerging diseases. The\nemergence of the Few-shot Weakly Supervised WSI Classification accommodates the\nsignificant challenge of the limited slide data and sparse slide-level labels\nfor diagnosis. Prompt learning based on the pre-trained models (\\eg, CLIP)\nappears to be a promising scheme for this setting; however, current research in\nthis area is limited, and existing algorithms often focus solely on patch-level\nprompts or confine themselves to language prompts. This paper proposes a\nmulti-instance prompt learning framework enhanced with pathology knowledge,\n\\ie, integrating visual and textual prior knowledge into prompts at both patch\nand slide levels. The training process employs a combination of static and\nlearnable prompts, effectively guiding the activation of pre-trained models and\nfurther facilitating the diagnosis of key pathology patterns. Lightweight\nMessenger (self-attention) and Summary (attention-pooling) layers are\nintroduced to model relationships between patches and slides within the same\npatient data. Additionally, alignment-wise contrastive losses ensure the\nfeature-level alignment between visual and textual learnable prompts for both\npatches and slides. Our method demonstrates superior performance in three\nchallenging clinical tasks, significantly outperforming comparative few-shot\nmethods.\n","authors":["Linhao Qu","Dingkang Yang","Dan Huang","Qinhao Guo","Rongkui Luo","Shaoting Zhang","Xiaosong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10814v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10810v1","updated":"2024-07-15T15:25:45Z","published":"2024-07-15T15:25:45Z","title":"FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect\n  Knowledge Queries","summary":"  Intelligence is key to advancing integrated circuit (IC) fabrication. Recent\nbreakthroughs in Large Multimodal Models (LMMs) have unlocked unparalleled\nabilities in understanding images and text, fostering intelligent fabrication.\nLeveraging the power of LMMs, we introduce FabGPT, a customized IC fabrication\nlarge multimodal model for wafer defect knowledge query. FabGPT manifests\nexpertise in conducting defect detection in Scanning Electron Microscope (SEM)\nimages, performing root cause analysis, and providing expert question-answering\n(Q&A) on fabrication processes. FabGPT matches enhanced multimodal features to\nautomatically detect minute defects under complex wafer backgrounds and reduce\nthe subjectivity of manual threshold settings. Besides, the proposed modulation\nmodule and interactive corpus training strategy embed wafer defect knowledge\ninto the pre-trained model, effectively balancing Q&A queries related to defect\nknowledge and original knowledge and mitigating the modality bias issues.\nExperiments on in-house fab data (SEM-WaD) show that our FabGPT achieves\nsignificant performance improvement in wafer defect detection and knowledge\nquerying.\n","authors":["Yuqi Jiang","Xudong Lu","Qian Jin","Qi Sun","Hanming Wu","Cheng Zhuo"],"pdf_url":"https://arxiv.org/pdf/2407.10810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15770v2","updated":"2024-07-15T15:22:15Z","published":"2024-04-24T09:44:44Z","title":"ChEX: Interactive Localization and Region Description in Chest X-rays","summary":"  Report generation models offer fine-grained textual interpretations of\nmedical images like chest X-rays, yet they often lack interactivity (i.e. the\nability to steer the generation process through user queries) and localized\ninterpretability (i.e. visually grounding their predictions), which we deem\nessential for future adoption in clinical practice. While there have been\nefforts to tackle these issues, they are either limited in their interactivity\nby not supporting textual queries or fail to also offer localized\ninterpretability. Therefore, we propose a novel multitask architecture and\ntraining paradigm integrating textual prompts and bounding boxes for diverse\naspects like anatomical regions and pathologies. We call this approach the\nChest X-Ray Explainer (ChEX). Evaluations across a heterogeneous set of 9 chest\nX-ray tasks, including localized image interpretation and report generation,\nshowcase its competitiveness with SOTA models while additional analysis\ndemonstrates ChEX's interactive capabilities. Code:\nhttps://github.com/philip-mueller/chex\n","authors":["Philip MÃ¼ller","Georgios Kaissis","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2404.15770v2.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10806v1","updated":"2024-07-15T15:21:34Z","published":"2024-07-15T15:21:34Z","title":"Enhancing Robustness to Noise Corruption for Point Cloud Model via\n  Spatial Sorting and Set-Mixing Aggregation Module","summary":"  Current models for point cloud recognition demonstrate promising performance\non synthetic datasets. However, real-world point cloud data inevitably contains\nnoise, impacting model robustness. While recent efforts focus on enhancing\nrobustness through various strategies, there still remains a gap in\ncomprehensive analyzes from the standpoint of network architecture design.\nUnlike traditional methods that rely on generic techniques, our approach\noptimizes model robustness to noise corruption through network architecture\ndesign. Inspired by the token-mixing technique applied in 2D images, we propose\nSet-Mixer, a noise-robust aggregation module which facilitates communication\namong all points to extract geometric shape information and mitigating the\ninfluence of individual noise points. A sorting strategy is designed to enable\nour module to be invariant to point permutation, which also tackles the\nunordered structure of point cloud and introduces consistent relative spatial\ninformation. Experiments conducted on ModelNet40-C indicate that Set-Mixer\nsignificantly enhances the model performance on noisy point clouds,\nunderscoring its potential to advance real-world applicability in 3D\nrecognition and perception tasks.\n","authors":["Dingxin Zhang","Jianhui Yu","Tengfei Xue","Chaoyi Zhang","Dongnan Liu","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2407.10806v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.00440v3","updated":"2024-07-15T15:20:03Z","published":"2024-06-01T13:37:51Z","title":"Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head\n  Capture","summary":"  4D head capture aims to generate dynamic topological meshes and corresponding\ntexture maps from videos, which is widely utilized in movies and games for its\nability to simulate facial muscle movements and recover dynamic textures in\npore-squeezing. The industry often adopts the method involving multi-view\nstereo and non-rigid alignment. However, this approach is prone to errors and\nheavily reliant on time-consuming manual processing by artists. To simplify\nthis process, we propose Topo4D, a novel framework for automatic geometry and\ntexture generation, which optimizes densely aligned 4D heads and 8K texture\nmaps directly from calibrated multi-view time-series images. Specifically, we\nfirst represent the time-series faces as a set of dynamic 3D Gaussians with\nfixed topology in which the Gaussian centers are bound to the mesh vertices.\nAfterward, we perform alternative geometry and texture optimization\nframe-by-frame for high-quality geometry and texture learning while maintaining\ntemporal topology stability. Finally, we can extract dynamic facial meshes in\nregular wiring arrangement and high-fidelity textures with pore-level details\nfrom the learned Gaussians. Extensive experiments show that our method achieves\nsuperior results than the current SOTA face reconstruction methods both in the\nquality of meshes and textures. Project page:\nhttps://xuanchenli.github.io/Topo4D/.\n","authors":["Xuanchen Li","Yuhao Cheng","Xingyu Ren","Haozhe Jia","Di Xu","Wenhan Zhu","Yichao Yan"],"pdf_url":"https://arxiv.org/pdf/2406.00440v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10803v1","updated":"2024-07-15T15:18:57Z","published":"2024-07-15T15:18:57Z","title":"DINO Pre-training for Vision-based End-to-end Autonomous Driving","summary":"  In this article, we focus on the pre-training of visual autonomous driving\nagents in the context of imitation learning. Current methods often rely on a\nclassification-based pre-training, which we hypothesise to be holding back from\nextending capabilities of implicit image understanding. We propose pre-training\nthe visual encoder of a driving agent using the self-distillation with no\nlabels (DINO) method, which relies on a self-supervised learning paradigm.% and\nis trained on an unrelated task. Our experiments in CARLA environment in\naccordance with the Leaderboard benchmark reveal that the proposed pre-training\nis more efficient than classification-based pre-training, and is on par with\nthe recently proposed pre-training based on visual place recognition (VPRPre).\n","authors":["Shubham Juneja","Povilas DaniuÅ¡is","Virginijus MarcinkeviÄius"],"pdf_url":"https://arxiv.org/pdf/2407.10803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10802v1","updated":"2024-07-15T15:18:28Z","published":"2024-07-15T15:18:28Z","title":"Motion-prior Contrast Maximization for Dense Continuous-Time Motion\n  Estimation","summary":"  Current optical flow and point-tracking methods rely heavily on synthetic\ndatasets. Event cameras are novel vision sensors with advantages in challenging\nvisual conditions, but state-of-the-art frame-based methods cannot be easily\nadapted to event data due to the limitations of current event simulators. We\nintroduce a novel self-supervised loss combining the Contrast Maximization\nframework with a non-linear motion prior in the form of pixel-level\ntrajectories and propose an efficient solution to solve the high-dimensional\nassignment problem between non-linear trajectories and events. Their\neffectiveness is demonstrated in two scenarios: In dense continuous-time motion\nestimation, our method improves the zero-shot performance of a synthetically\ntrained model on the real-world dataset EVIMO2 by 29%. In optical flow\nestimation, our method elevates a simple UNet to achieve state-of-the-art\nperformance among self-supervised methods on the DSEC optical flow benchmark.\nOur code is available at https://github.com/tub-rip/MotionPriorCMax.\n","authors":["Friedhelm Hamann","Ziyun Wang","Ioannis Asmanis","Kenneth Chaney","Guillermo Gallego","Kostas Daniilidis"],"pdf_url":"https://arxiv.org/pdf/2407.10802v1.pdf","comment":"24 pages, 8 figures, 8 tables, Project Page:\n  https://github.com/tub-rip/MotionPriorCMax"},{"id":"http://arxiv.org/abs/2407.10796v1","updated":"2024-07-15T15:14:10Z","published":"2024-07-15T15:14:10Z","title":"Mammographic Breast Positioning Assessment via Deep Learning","summary":"  Breast cancer remains a leading cause of cancer-related deaths among women\nworldwide, with mammography screening as the most effective method for the\nearly detection. Ensuring proper positioning in mammography is critical, as\npoor positioning can lead to diagnostic errors, increased patient stress, and\nhigher costs due to recalls. Despite advancements in deep learning (DL) for\nbreast cancer diagnostics, limited focus has been given to evaluating\nmammography positioning. This paper introduces a novel DL methodology to\nquantitatively assess mammogram positioning quality, specifically in\nmediolateral oblique (MLO) views using attention and coordinate convolution\nmodules. Our method identifies key anatomical landmarks, such as the nipple and\npectoralis muscle, and automatically draws a posterior nipple line (PNL),\noffering robust and inherently explainable alternative to well-known\nclassification and regression-based approaches. We compare the performance of\nproposed methodology with various regression and classification-based models.\nThe CoordAtt UNet model achieved the highest accuracy of 88.63% $\\pm$ 2.84 and\nspecificity of 90.25% $\\pm$ 4.04, along with a noteworthy sensitivity of 86.04%\n$\\pm$ 3.41. In landmark detection, the same model also recorded the lowest mean\nerrors in key anatomical points and the smallest angular error of 2.42 degrees.\nOur results indicate that models incorporating attention mechanisms and\nCoordConv module increase the accuracy in classifying breast positioning\nquality and detecting anatomical landmarks. Furthermore, we make the labels and\nsource codes available to the community to initiate an open research area for\nmammography, accessible at https://github.com/tanyelai/deep-breast-positioning.\n","authors":["Toygar Tanyel","Nurper Denizoglu","Mustafa Ege Seker","Deniz Alis","Esma Cerekci","Ercan Karaarslan","Erkin Aribal","Ilkay Oksuz"],"pdf_url":"https://arxiv.org/pdf/2407.10796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06889v2","updated":"2024-07-15T15:12:08Z","published":"2024-07-09T14:18:35Z","title":"A Neurosymbolic Approach to Adaptive Feature Extraction in SLAM","summary":"  Autonomous robots, autonomous vehicles, and humans wearing mixed-reality\nheadsets require accurate and reliable tracking services for safety-critical\napplications in dynamically changing real-world environments. However, the\nexisting tracking approaches, such as Simultaneous Localization and Mapping\n(SLAM), do not adapt well to environmental changes and boundary conditions\ndespite extensive manual tuning. On the other hand, while deep learning-based\napproaches can better adapt to environmental changes, they typically demand\nsubstantial data for training and often lack flexibility in adapting to new\ndomains. To solve this problem, we propose leveraging the neurosymbolic program\nsynthesis approach to construct adaptable SLAM pipelines that integrate the\ndomain knowledge from traditional SLAM approaches while leveraging data to\nlearn complex relationships. While the approach can synthesize end-to-end SLAM\npipelines, we focus on synthesizing the feature extraction module. We first\ndevise a domain-specific language (DSL) that can encapsulate domain knowledge\non the important attributes for feature extraction and the real-world\nperformance of various feature extractors. Our neurosymbolic architecture then\nundertakes adaptive feature extraction, optimizing parameters via learning\nwhile employing symbolic reasoning to select the most suitable feature\nextractor. Our evaluations demonstrate that our approach, neurosymbolic Feature\nEXtraction (nFEX), yields higher-quality features. It also reduces the pose\nerror observed for the state-of-the-art baseline feature extractors ORB and\nSIFT by up to 90% and up to 66%, respectively, thereby enhancing the system's\nefficiency and adaptability to novel environments.\n","authors":["Yasra Chandio","Momin A. Khan","Khotso Selialia","Luis Garcia","Joseph DeGol","Fatima M. Anwar"],"pdf_url":"https://arxiv.org/pdf/2407.06889v2.pdf","comment":"2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)"},{"id":"http://arxiv.org/abs/2407.10785v1","updated":"2024-07-15T15:03:01Z","published":"2024-07-15T15:03:01Z","title":"Interpretability analysis on a pathology foundation model reveals\n  biologically relevant embeddings across modalities","summary":"  Mechanistic interpretability has been explored in detail for large language\nmodels (LLMs). For the first time, we provide a preliminary investigation with\nsimilar interpretability methods for medical imaging. Specifically, we analyze\nthe features from a ViT-Small encoder obtained from a pathology Foundation\nModel via application to two datasets: one dataset of pathology images, and one\ndataset of pathology images paired with spatial transcriptomics. We discover an\ninterpretable representation of cell and tissue morphology, along with gene\nexpression within the model embedding space. Our work paves the way for further\nexploration around interpretable feature dimensions and their utility for\nmedical and clinical applications.\n","authors":["Nhat Le","Ciyue Shen","Chintan Shah","Blake Martin","Daniel Shenker","Harshith Padigela","Jennifer Hipp","Sean Grullon","John Abel","Harsha Vardhan Pokkalla","Dinkar Juyal"],"pdf_url":"https://arxiv.org/pdf/2407.10785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10244v3","updated":"2024-07-15T14:58:07Z","published":"2024-05-16T16:47:46Z","title":"Towards Task-Compatible Compressible Representations","summary":"  We identify an issue in multi-task learnable compression, in which a\nrepresentation learned for one task does not positively contribute to the\nrate-distortion performance of a different task as much as expected, given the\nestimated amount of information available in it. We interpret this issue using\nthe predictive $\\mathcal{V}$-information framework. In learnable scalable\ncoding, previous work increased the utilization of side-information for input\nreconstruction by also rewarding input reconstruction when learning this shared\nrepresentation. We evaluate the impact of this idea in the context of input\nreconstruction more rigorously and extended it to other computer vision tasks.\nWe perform experiments using representations trained for object detection on\nCOCO 2017 and depth estimation on the Cityscapes dataset, and use them to\nassist in image reconstruction and semantic segmentation tasks. The results\nshow considerable improvements in the rate-distortion performance of the\nassisted tasks. Moreover, using the proposed representations, the performance\nof the base tasks are also improved. Results suggest that the proposed method\ninduces simpler representations that are more compatible with downstream\nprocesses.\n","authors":["Anderson de Andrade","Ivan BajiÄ"],"pdf_url":"https://arxiv.org/pdf/2405.10244v3.pdf","comment":"Published in ICME Workshops 2024"},{"id":"http://arxiv.org/abs/2403.12931v4","updated":"2024-07-15T14:51:48Z","published":"2024-03-19T17:34:27Z","title":"You Only Sample Once: Taming One-Step Text-to-Image Synthesis by\n  Self-Cooperative Diffusion GANs","summary":"  We introduce YOSO, a novel generative model designed for rapid, scalable, and\nhigh-fidelity one-step image synthesis. YOSO integrates the diffusion process\nwith GANs to achieve the best of two worlds. Specifically, we smooth the\ndistribution by the denoising generator itself, performing self-cooperative\nlearning. We show that our method can serve as a one-step generation model\ntraining from scratch with competitive performance. Moreover, we show that our\nmethod can be extended to finetune pre-trained text-to-image diffusion for\nhigh-quality one-step text-to-image synthesis even with LoRA fine-tuning. In\nparticular, we provide the first diffusion transformer that can generate images\nin one step trained on 512 resolution, with the capability of adapting to 1024\nresolution without extra explicit training. Our code is provided at\nhttps://github.com/Luo-Yihong/YOSO\n","authors":["Yihong Luo","Xiaolong Chen","Xinghua Qu","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2403.12931v4.pdf","comment":"Early version"},{"id":"http://arxiv.org/abs/2403.10179v2","updated":"2024-07-15T14:50:14Z","published":"2024-03-15T10:36:24Z","title":"Animate Your Motion: Turning Still Images into Dynamic Videos","summary":"  In recent years, diffusion models have made remarkable strides in\ntext-to-video generation, sparking a quest for enhanced control over video\noutputs to more accurately reflect user intentions. Traditional efforts\npredominantly focus on employing either semantic cues, like images or depth\nmaps, or motion-based conditions, like moving sketches or object bounding\nboxes. Semantic inputs offer a rich scene context but lack detailed motion\nspecificity; conversely, motion inputs provide precise trajectory information\nbut miss the broader semantic narrative. For the first time, we integrate both\nsemantic and motion cues within a diffusion model for video generation, as\ndemonstrated in Fig 1. To this end, we introduce the Scene and Motion\nConditional Diffusion (SMCD), a novel methodology for managing multimodal\ninputs. It incorporates a recognized motion conditioning module and\ninvestigates various approaches to integrate scene conditions, promoting\nsynergy between different modalities. For model training, we separate the\nconditions for the two modalities, introducing a two-stage training pipeline.\nExperimental results demonstrate that our design significantly enhances video\nquality, motion precision, and semantic coherence.\n","authors":["Mingxiao Li","Bo Wan","Marie-Francine Moens","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2403.10179v2.pdf","comment":"Accepted at European Conference on Computer Vision (ECCV 2024)"},{"id":"http://arxiv.org/abs/2407.10762v1","updated":"2024-07-15T14:42:08Z","published":"2024-07-15T14:42:08Z","title":"Domain Generalization for 6D Pose Estimation Through NeRF-based Image\n  Synthesis","summary":"  This work introduces a novel augmentation method that increases the diversity\nof a train set to improve the generalization abilities of a 6D pose estimation\nnetwork. For this purpose, a Neural Radiance Field is trained from synthetic\nimages and exploited to generate an augmented set. Our method enriches the\ninitial set by enabling the synthesis of images with (i) unseen viewpoints,\n(ii) rich illumination conditions through appearance extrapolation, and (iii)\nrandomized textures. We validate our augmentation method on the challenging\nuse-case of spacecraft pose estimation and show that it significantly improves\nthe pose estimation generalization capabilities. On the SPEED+ dataset, our\nmethod reduces the error on the pose by 50% on both target domains.\n","authors":["Antoine Legrand","Renaud Detry","Christophe De Vleeschouwer"],"pdf_url":"https://arxiv.org/pdf/2407.10762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17437v2","updated":"2024-07-15T14:41:32Z","published":"2024-06-25T10:18:50Z","title":"Advancing Question Answering on Handwritten Documents: A\n  State-of-the-Art Recognition-Based Model for HW-SQuAD","summary":"  Question-answering handwritten documents is a challenging task with numerous\nreal-world applications. This paper proposes a novel recognition-based approach\nthat improves upon the previous state-of-the-art on the HW-SQuAD and BenthamQA\ndatasets. Our model incorporates transformer-based document retrieval and\nensemble methods at the model level, achieving an Exact Match score of 82.02%\nand 69% in HW-SQuAD and BenthamQA datasets, respectively, surpassing the\nprevious best recognition-based approach by 10.89% and 3%. We also enhance the\ndocument retrieval component, boosting the top-5 retrieval accuracy from 90% to\n95.30%. Our results demonstrate the significance of our proposed approach in\nadvancing question answering on handwritten documents. The code and trained\nmodels will be publicly available to facilitate future research in this\ncritical area of natural language.\n","authors":["Aniket Pal","Ajoy Mondal","C. V. Jawahar"],"pdf_url":"https://arxiv.org/pdf/2406.17437v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2403.10153v3","updated":"2024-07-15T14:35:13Z","published":"2024-03-15T09:54:04Z","title":"Improving Medical Multi-modal Contrastive Learning with Expert\n  Annotations","summary":"  We introduce eCLIP, an enhanced version of the CLIP model that integrates\nexpert annotations in the form of radiologist eye-gaze heatmaps. It tackles key\nchallenges in contrastive multi-modal medical imaging analysis, notably data\nscarcity and the \"modality gap\" -- a significant disparity between image and\ntext embeddings that diminishes the quality of representations and hampers\ncross-modal interoperability. eCLIP integrates a heatmap processor and\nleverages mixup augmentation to efficiently utilize the scarce expert\nannotations, thus boosting the model's learning effectiveness. eCLIP is\ndesigned to be generally applicable to any variant of CLIP without requiring\nany modifications of the core architecture. Through detailed evaluations across\nseveral tasks, including zero-shot inference, linear probing, cross-modal\nretrieval, and Retrieval Augmented Generation (RAG) of radiology reports using\na frozen Large Language Model, eCLIP showcases consistent improvements in\nembedding quality. The outcomes reveal enhanced alignment and uniformity,\naffirming eCLIP's capability to harness high-quality annotations for enriched\nmulti-modal analysis in the medical imaging domain.\n","authors":["Yogesh Kumar","Pekka Marttinen"],"pdf_url":"https://arxiv.org/pdf/2403.10153v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10756v1","updated":"2024-07-15T14:32:45Z","published":"2024-07-15T14:32:45Z","title":"GTPT: Group-based Token Pruning Transformer for Efficient Human Pose\n  Estimation","summary":"  In recent years, 2D human pose estimation has made significant progress on\npublic benchmarks. However, many of these approaches face challenges of less\napplicability in the industrial community due to the large number of parametric\nquantities and computational overhead. Efficient human pose estimation remains\na hurdle, especially for whole-body pose estimation with numerous keypoints.\nWhile most current methods for efficient human pose estimation primarily rely\non CNNs, we propose the Group-based Token Pruning Transformer (GTPT) that fully\nharnesses the advantages of the Transformer. GTPT alleviates the computational\nburden by gradually introducing keypoints in a coarse-to-fine manner. It\nminimizes the computation overhead while ensuring high performance. Besides,\nGTPT groups keypoint tokens and prunes visual tokens to improve model\nperformance while reducing redundancy. We propose the Multi-Head Group\nAttention (MHGA) between different groups to achieve global interaction with\nlittle computational overhead. We conducted experiments on COCO and\nCOCO-WholeBody. Compared to other methods, the experimental results show that\nGTPT can achieve higher performance with less computation, especially in\nwhole-body with numerous keypoints.\n","authors":["Haonan Wang","Jie Liu","Jie Tang","Gangshan Wu","Bo Xu","Yanbing Chou","Yong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10756v1.pdf","comment":"ECCV 2024 accepted"},{"id":"http://arxiv.org/abs/2407.10754v1","updated":"2024-07-15T14:31:21Z","published":"2024-07-15T14:31:21Z","title":"An Autonomous Drone Swarm for Detecting and Tracking Anomalies among\n  Dense Vegetation","summary":"  Swarms of drones offer an increased sensing aperture, and having them mimic\nbehaviors of natural swarms enhances sampling by adapting the aperture to local\nconditions. We demonstrate that such an approach makes detecting and tracking\nheavily occluded targets practically feasible. While object classification\napplied to conventional aerial images generalizes poorly the randomness of\nocclusion and is therefore inefficient even under lightly occluded conditions,\nanomaly detection applied to synthetic aperture integral images is robust for\ndense vegetation, such as forests, and is independent of pre-trained classes.\nOur autonomous swarm searches the environment for occurrences of the unknown or\nunexpected, tracking them while continuously adapting its sampling pattern to\noptimize for local viewing conditions. In our real-life field experiments with\na swarm of six drones, we achieved an average positional accuracy of 0.39 m\nwith an average precision of 93.2% and an average recall of 95.9%. Here,\nadapted particle swarm optimization considers detection confidences and\npredicted target appearance. We show that sensor noise can effectively be\nincluded in the synthetic aperture image integration process, removing the need\nfor a computationally costly optimization of high-dimensional parameter spaces.\nFinally, we present a complete hard- and software framework that supports\nlow-latency transmission (approx. 80 ms round-trip time) and fast processing\n(approx. 600 ms per formation step) of extensive (70-120 Mbit/s) video and\ntelemetry data, and swarm control for swarms of up to ten drones.\n","authors":["Rakesh John Amala Arokia Nathan","Sigrid Strand","Daniel Mehrwald","Dmitriy Shutin","Oliver Bimber"],"pdf_url":"https://arxiv.org/pdf/2407.10754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10753v1","updated":"2024-07-15T14:29:15Z","published":"2024-07-15T14:29:15Z","title":"OPEN: Object-wise Position Embedding for Multi-view 3D Object Detection","summary":"  Accurate depth information is crucial for enhancing the performance of\nmulti-view 3D object detection. Despite the success of some existing multi-view\n3D detectors utilizing pixel-wise depth supervision, they overlook two\nsignificant phenomena: 1) the depth supervision obtained from LiDAR points is\nusually distributed on the surface of the object, which is not so friendly to\nexisting DETR-based 3D detectors due to the lack of the depth of 3D object\ncenter; 2) for distant objects, fine-grained depth estimation of the whole\nobject is more challenging. Therefore, we argue that the object-wise depth (or\n3D center of the object) is essential for accurate detection. In this paper, we\npropose a new multi-view 3D object detector named OPEN, whose main idea is to\neffectively inject object-wise depth information into the network through our\nproposed object-wise position embedding. Specifically, we first employ an\nobject-wise depth encoder, which takes the pixel-wise depth map as a prior, to\naccurately estimate the object-wise depth. Then, we utilize the proposed\nobject-wise position embedding to encode the object-wise depth information into\nthe transformer decoder, thereby producing 3D object-aware features for final\ndetection. Extensive experiments verify the effectiveness of our proposed\nmethod. Furthermore, OPEN achieves a new state-of-the-art performance with\n64.4% NDS and 56.7% mAP on the nuScenes test benchmark.\n","authors":["Jinghua Hou","Tong Wang","Xiaoqing Ye","Zhe Liu","Shi Gong","Xiao Tan","Errui Ding","Jingdong Wang","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2407.10753v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2402.11816v3","updated":"2024-07-15T14:28:46Z","published":"2024-02-19T04:13:33Z","title":"Learning the Unlearned: Mitigating Feature Suppression in Contrastive\n  Learning","summary":"  Self-Supervised Contrastive Learning has proven effective in deriving\nhigh-quality representations from unlabeled data. However, a major challenge\nthat hinders both unimodal and multimodal contrastive learning is feature\nsuppression, a phenomenon where the trained model captures only a limited\nportion of the information from the input data while overlooking other\npotentially valuable content. This issue often leads to indistinguishable\nrepresentations for visually similar but semantically different inputs,\nadversely affecting downstream task performance, particularly those requiring\nrigorous semantic comprehension. To address this challenge, we propose a novel\nmodel-agnostic Multistage Contrastive Learning (MCL) framework. Unlike standard\ncontrastive learning which inherently captures one single biased feature\ndistribution, MCL progressively learns previously unlearned features through\nfeature-aware negative sampling at each stage, where the negative samples of an\nanchor are exclusively selected from the cluster it was assigned to in\npreceding stages. Meanwhile, MCL preserves the previously well-learned features\nby cross-stage representation integration, integrating features across all\nstages to form final representations. Our comprehensive evaluation demonstrates\nMCL's effectiveness and superiority across both unimodal and multimodal\ncontrastive learning, spanning a range of model architectures from ResNet to\nVision Transformers (ViT). Remarkably, in tasks where the original CLIP model\nhas shown limitations, MCL dramatically enhances performance, with improvements\nup to threefold on specific attributes in the recently proposed MMVP benchmark.\n","authors":["Jihai Zhang","Xiang Lan","Xiaoye Qu","Yu Cheng","Mengling Feng","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2402.11816v3.pdf","comment":"ECCV 2024 Camera-Ready"},{"id":"http://arxiv.org/abs/2406.18742v3","updated":"2024-07-15T14:23:33Z","published":"2024-06-26T20:16:49Z","title":"3D Feature Distillation with Object-Centric Priors","summary":"  Grounding natural language to the physical world is a ubiquitous topic with a\nwide range of applications in computer vision and robotics. Recently, 2D\nvision-language models such as CLIP have been widely popularized, due to their\nimpressive capabilities for open-vocabulary grounding in 2D images. Recent\nworks aim to elevate 2D CLIP features to 3D via feature distillation, but\neither learn neural fields that are scene-specific and hence lack\ngeneralization, or focus on indoor room scan data that require access to\nmultiple camera views, which is not practical in robot manipulation scenarios.\nAdditionally, related methods typically fuse features at pixel-level and assume\nthat all camera views are equally informative. In this work, we show that this\napproach leads to sub-optimal 3D features, both in terms of grounding accuracy,\nas well as segmentation crispness. To alleviate this, we propose a multi-view\nfeature fusion strategy that employs object-centric priors to eliminate\nuninformative views based on semantic information, and fuse features at\nobject-level via instance segmentation masks. To distill our object-centric 3D\nfeatures, we generate a large-scale synthetic multi-view dataset of cluttered\ntabletop scenes, spawning 15k scenes from over 3300 unique object instances,\nwhich we make publicly available. We show that our method reconstructs 3D CLIP\nfeatures with improved grounding capacity and spatial consistency, while doing\nso from single-view RGB-D, thus departing from the assumption of multiple\ncamera views at test time. Finally, we show that our approach can generalize to\nnovel tabletop domains and be re-purposed for 3D instance segmentation without\nfine-tuning, and demonstrate its utility for language-guided robotic grasping\nin clutter\n","authors":["Georgios Tziafas","Yucheng Xu","Zhibin Li","Hamidreza Kasaei"],"pdf_url":"https://arxiv.org/pdf/2406.18742v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18722v3","updated":"2024-07-15T14:21:44Z","published":"2024-06-26T19:42:08Z","title":"Towards Open-World Grasping with Large Vision-Language Models","summary":"  The ability to grasp objects in-the-wild from open-ended language\ninstructions constitutes a fundamental challenge in robotics. An open-world\ngrasping system should be able to combine high-level contextual with low-level\nphysical-geometric reasoning in order to be applicable in arbitrary scenarios.\nRecent works exploit the web-scale knowledge inherent in large language models\n(LLMs) to plan and reason in robotic context, but rely on external vision and\naction models to ground such knowledge into the environment and parameterize\nactuation. This setup suffers from two major bottlenecks: a) the LLM's\nreasoning capacity is constrained by the quality of visual grounding, and b)\nLLMs do not contain low-level spatial understanding of the world, which is\nessential for grasping in contact-rich scenarios. In this work we demonstrate\nthat modern vision-language models (VLMs) are capable of tackling such\nlimitations, as they are implicitly grounded and can jointly reason about\nsemantics and geometry. We propose OWG, an open-world grasping pipeline that\ncombines VLMs with segmentation and grasp synthesis models to unlock grounded\nworld understanding in three stages: open-ended referring segmentation,\ngrounded grasp planning and grasp ranking via contact reasoning, all of which\ncan be applied zero-shot via suitable visual prompting mechanisms. We conduct\nextensive evaluation in cluttered indoor scene datasets to showcase OWG's\nrobustness in grounding from open-ended language, as well as open-world robotic\ngrasping experiments in both simulation and hardware that demonstrate superior\nperformance compared to previous supervised and zero-shot LLM-based methods.\n","authors":["Georgios Tziafas","Hamidreza Kasaei"],"pdf_url":"https://arxiv.org/pdf/2406.18722v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10749v1","updated":"2024-07-15T14:21:07Z","published":"2024-07-15T14:21:07Z","title":"SEED: A Simple and Effective 3D DETR in Point Clouds","summary":"  Recently, detection transformers (DETRs) have gradually taken a dominant\nposition in 2D detection thanks to their elegant framework. However, DETR-based\ndetectors for 3D point clouds are still difficult to achieve satisfactory\nperformance. We argue that the main challenges are twofold: 1) How to obtain\nthe appropriate object queries is challenging due to the high sparsity and\nuneven distribution of point clouds; 2) How to implement an effective query\ninteraction by exploiting the rich geometric structure of point clouds is not\nfully explored. To this end, we propose a simple and effective 3D DETR method\n(SEED) for detecting 3D objects from point clouds, which involves a dual query\nselection (DQS) module and a deformable grid attention (DGA) module. More\nconcretely, to obtain appropriate queries, DQS first ensures a high recall to\nretain a large number of queries by the predicted confidence scores and then\nfurther picks out high-quality queries according to the estimated quality\nscores. DGA uniformly divides each reference box into grids as the reference\npoints and then utilizes the predicted offsets to achieve a flexible receptive\nfield, allowing the network to focus on relevant regions and capture more\ninformative features. Extensive ablation studies on DQS and DGA demonstrate its\neffectiveness. Furthermore, our SEED achieves state-of-the-art detection\nperformance on both the large-scale Waymo and nuScenes datasets, illustrating\nthe superiority of our proposed method. The code is available at\nhttps://github.com/happinesslz/SEED\n","authors":["Zhe Liu","Jinghua Hou","Xiaoqing Ye","Tong Wang","Jingdong Wang","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2407.10749v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2307.01836v2","updated":"2024-07-15T14:19:20Z","published":"2023-07-04T17:28:58Z","title":"On the Matrix Form of the Quaternion Fourier Transform and Quaternion\n  Convolution","summary":"  We study matrix forms of quaternionic versions of the Fourier Transform and\nConvolution operations. Quaternions offer a powerful representation unit,\nhowever they are related to difficulties in their use that stem foremost from\nnon-commutativity of quaternion multiplication, and due to that $\\mu^2 = -1$\npossesses infinite solutions in the quaternion domain. Handling of quaternionic\nmatrices is consequently complicated in several aspects (definition of\neigenstructure, determinant, etc.). Our research findings clarify the relation\nof the Quaternion Fourier Transform matrix to the standard (complex) Discrete\nFourier Transform matrix, and the extend on which well-known complex-domain\ntheorems extend to quaternions. We focus especially on the relation of\nQuaternion Fourier Transform matrices to Quaternion Circulant matrices\n(representing quaternionic convolution), and the eigenstructure of the latter.\nA proof-of-concept application that makes direct use of our theoretical results\nis presented, where we present a method to bound the Lipschitz constant of a\nQuaternionic Convolutional Neural Network.\n","authors":["Giorgos Sfikas","George Retsinas"],"pdf_url":"https://arxiv.org/pdf/2307.01836v2.pdf","comment":"26 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.02296v2","updated":"2024-07-15T14:16:52Z","published":"2024-03-07T15:39:00Z","title":"MÃ¶bius Transform for Mitigating Perspective Distortions in\n  Representation Learning","summary":"  Perspective distortion (PD) causes unprecedented changes in shape, size,\norientation, angles, and other spatial relationships of visual concepts in\nimages. Precisely estimating camera intrinsic and extrinsic parameters is a\nchallenging task that prevents synthesizing perspective distortion.\nNon-availability of dedicated training data poses a critical barrier to\ndeveloping robust computer vision methods. Additionally, distortion correction\nmethods make other computer vision tasks a multi-step approach and lack\nperformance. In this work, we propose mitigating perspective distortion (MPD)\nby employing a fine-grained parameter control on a specific family of M\\\"obius\ntransform to model real-world distortion without estimating camera intrinsic\nand extrinsic parameters and without the need for actual distorted data. Also,\nwe present a dedicated perspectively distorted benchmark dataset, ImageNet-PD,\nto benchmark the robustness of deep learning models against this new dataset.\nThe proposed method outperforms existing benchmarks, ImageNet-E and ImageNet-X.\nAdditionally, it significantly improves performance on ImageNet-PD while\nconsistently performing on standard data distribution. Notably, our method\nshows improved performance on three PD-affected real-world applications crowd\ncounting, fisheye image recognition, and person re-identification and one\nPD-affected challenging CV task: object detection. The source code, dataset,\nand models are available on the project webpage at\nhttps://prakashchhipa.github.io/projects/mpd.\n","authors":["Prakash Chandra Chhipa","Meenakshi Subhash Chippa","Kanjar De","Rajkumar Saini","Marcus Liwicki","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2405.02296v2.pdf","comment":"Accepted to European Conference on Computer Vision(ECCV2024). project\n  page- https://prakashchhipa.github.io/projects/mpd"},{"id":"http://arxiv.org/abs/2404.10700v2","updated":"2024-07-15T14:09:28Z","published":"2024-04-16T16:17:48Z","title":"Rawformer: Unpaired Raw-to-Raw Translation for Learnable Camera ISPs","summary":"  Modern smartphone camera quality heavily relies on the image signal processor\n(ISP) to enhance captured raw images, utilizing carefully designed modules to\nproduce final output images encoded in a standard color space (e.g., sRGB).\nNeural-based end-to-end learnable ISPs offer promising advancements,\npotentially replacing traditional ISPs with their ability to adapt without\nrequiring extensive tuning for each new camera model, as is often the case for\nnearly every module in traditional ISPs. However, the key challenge with the\nrecent learning-based ISPs is the urge to collect large paired datasets for\neach distinct camera model due to the influence of intrinsic camera\ncharacteristics on the formation of input raw images. This paper tackles this\nchallenge by introducing a novel method for unpaired learning of raw-to-raw\ntranslation across diverse cameras. Specifically, we propose Rawformer, an\nunsupervised Transformer-based encoder-decoder method for raw-to-raw\ntranslation. It accurately maps raw images captured by a certain camera to the\ntarget camera, facilitating the generalization of learnable ISPs to new unseen\ncameras. Our method demonstrates superior performance on real camera datasets,\nachieving higher accuracy compared to previous state-of-the-art techniques, and\npreserving a more robust correlation between the original and translated raw\nimages. The codes and the pretrained models are available at\nhttps://github.com/gosha20777/rawformer.\n","authors":["Georgy Perevozchikov","Nancy Mehta","Mahmoud Afifi","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2404.10700v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10738v1","updated":"2024-07-15T14:06:29Z","published":"2024-07-15T14:06:29Z","title":"AccDiffusion: An Accurate Method for Higher-Resolution Image Generation","summary":"  This paper attempts to address the object repetition issue in patch-wise\nhigher-resolution image generation. We propose AccDiffusion, an accurate method\nfor patch-wise higher-resolution image generation without training. An in-depth\nanalysis in this paper reveals an identical text prompt for different patches\ncauses repeated object generation, while no prompt compromises the image\ndetails. Therefore, our AccDiffusion, for the first time, proposes to decouple\nthe vanilla image-content-aware prompt into a set of patch-content-aware\nprompts, each of which serves as a more precise description of an image patch.\nBesides, AccDiffusion also introduces dilated sampling with window interaction\nfor better global consistency in higher-resolution image generation.\nExperimental comparison with existing methods demonstrates that our\nAccDiffusion effectively addresses the issue of repeated object generation and\nleads to better performance in higher-resolution image generation. Our code is\nreleased at \\url{https://github.com/lzhxmu/AccDiffusion}.\n","authors":["Zhihang Lin","Mingbao Lin","Meng Zhao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.10738v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2407.10737v1","updated":"2024-07-15T14:06:13Z","published":"2024-07-15T14:06:13Z","title":"Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision\n  Models","summary":"  Our brains represent the ever-changing environment with neurons in a highly\ndynamic fashion. The temporal features of visual pixels in dynamic natural\nscenes are entrapped in the neuronal responses of the retina. It is crucial to\nestablish the intrinsic temporal relationship between visual pixels and\nneuronal responses. Recent foundation vision models have paved an advanced way\nof understanding image pixels. Yet, neuronal coding in the brain largely lacks\na deep understanding of its alignment with pixels. Most previous studies employ\nstatic images or artificial videos derived from static images for emulating\nmore real and complicated stimuli. Despite these simple scenarios effectively\nhelp to separate key factors influencing visual coding, complex temporal\nrelationships receive no consideration. To decompose the temporal features of\nvisual coding in natural scenes, here we propose Vi-ST, a spatiotemporal\nconvolutional neural network fed with a self-supervised Vision Transformer\n(ViT) prior, aimed at unraveling the temporal-based encoding patterns of\nretinal neuronal populations. The model demonstrates robust predictive\nperformance in generalization tests. Furthermore, through detailed ablation\nexperiments, we demonstrate the significance of each temporal module.\nFurthermore, we introduce a visual coding evaluation metric designed to\nintegrate temporal considerations and compare the impact of different numbers\nof neuronal populations on complementary coding. In conclusion, our proposed\nVi-ST demonstrates a novel modeling framework for neuronal coding of dynamic\nvisual scenes in the brain, effectively aligning our brain representation of\nvideo with neuronal activity. The code is available at\nhttps://github.com/wurining/Vi-ST.\n","authors":["Rining Wu","Feixiang Zhou","Ziwei Yin","Jian K. Liu"],"pdf_url":"https://arxiv.org/pdf/2407.10737v1.pdf","comment":"This article is accepted by ECCV 2024, which ID is 12149. Accepted\n  papers' id can be found in:\n  https://eccv2024.ecva.net/Conferences/2024/AcceptedPapers"},{"id":"http://arxiv.org/abs/2407.10736v1","updated":"2024-07-15T14:01:35Z","published":"2024-07-15T14:01:35Z","title":"When Synthetic Traces Hide Real Content: Analysis of Stable Diffusion\n  Image Laundering","summary":"  In recent years, methods for producing highly realistic synthetic images have\nsignificantly advanced, allowing the creation of high-quality images from text\nprompts that describe the desired content. Even more impressively, Stable\nDiffusion (SD) models now provide users with the option of creating synthetic\nimages in an image-to-image translation fashion, modifying images in the latent\nspace of advanced autoencoders. This striking evolution, however, brings an\nalarming consequence: it is possible to pass an image through SD autoencoders\nto reproduce a synthetic copy of the image with high realism and almost no\nvisual artifacts. This process, known as SD image laundering, can transform\nreal images into lookalike synthetic ones and risks complicating forensic\nanalysis for content authenticity verification. Our paper investigates the\nforensic implications of image laundering, revealing a serious potential to\nobscure traces of real content, including sensitive and harmful materials that\ncould be mistakenly classified as synthetic, thereby undermining the protection\nof individuals depicted. To address this issue, we propose a two-stage\ndetection pipeline that effectively differentiates between pristine, laundered,\nand fully synthetic images (those generated from text prompts), showing\nrobustness across various conditions. Finally, we highlight another alarming\nproperty of image laundering, which appears to mask the unique artifacts\nexploited by forensic detectors to solve the camera model identification task,\nstrongly undermining their performance. Our experimental code is available at\nhttps://github.com/polimi-ispl/synthetic-image-detection.\n","authors":["Sara Mandelli","Paolo Bestagini","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2407.10736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10733v1","updated":"2024-07-15T14:01:03Z","published":"2024-07-15T14:01:03Z","title":"Joint-Embedding Predictive Architecture for Self-Supervised Learning of\n  Mask Classification Architecture","summary":"  In this work, we introduce Mask-JEPA, a self-supervised learning framework\ntailored for mask classification architectures (MCA), to overcome the\ntraditional constraints associated with training segmentation models. Mask-JEPA\ncombines a Joint Embedding Predictive Architecture with MCA to adeptly capture\nintricate semantics and precise object boundaries. Our approach addresses two\ncritical challenges in self-supervised learning: 1) extracting comprehensive\nrepresentations for universal image segmentation from a pixel decoder, and 2)\neffectively training the transformer decoder. The use of the transformer\ndecoder as a predictor within the JEPA framework allows proficient training in\nuniversal image segmentation tasks. Through rigorous evaluations on datasets\nsuch as ADE20K, Cityscapes and COCO, Mask-JEPA demonstrates not only\ncompetitive results but also exceptional adaptability and robustness across\nvarious training scenarios. The architecture-agnostic nature of Mask-JEPA\nfurther underscores its versatility, allowing seamless adaptation to various\nmask classification family.\n","authors":["Dong-Hee Kim","Sungduk Cho","Hyeonwoo Cho","Chanmin Park","Jinyoung Kim","Won Hwa Kim"],"pdf_url":"https://arxiv.org/pdf/2407.10733v1.pdf","comment":"27 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.02350v3","updated":"2024-07-15T14:00:24Z","published":"2024-07-02T15:16:06Z","title":"Conceptual Codebook Learning for Vision-Language Models","summary":"  In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe.\n","authors":["Yi Zhang","Ke Yu","Siqi Wu","Zhihai He"],"pdf_url":"https://arxiv.org/pdf/2407.02350v3.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2403.12530v2","updated":"2024-07-15T13:59:23Z","published":"2024-03-19T08:08:12Z","title":"PCT: Perspective Cue Training Framework for Multi-Camera BEV\n  Segmentation","summary":"  Generating annotations for bird's-eye-view (BEV) segmentation presents\nsignificant challenges due to the scenes' complexity and the high manual\nannotation cost. In this work, we address these challenges by leveraging the\nabundance of unlabeled data available. We propose the Perspective Cue Training\n(PCT) framework, a novel training framework that utilizes pseudo-labels\ngenerated from unlabeled perspective images using publicly available semantic\nsegmentation models trained on large street-view datasets. PCT applies a\nperspective view task head to the image encoder shared with the BEV\nsegmentation head, effectively utilizing the unlabeled data to be trained with\nthe generated pseudo-labels. Since image encoders are present in nearly all\ncamera-based BEV segmentation architectures, PCT is flexible and applicable to\nvarious existing BEV architectures. PCT can be applied to various settings\nwhere unlabeled data is available. In this paper, we applied PCT for\nsemi-supervised learning (SSL) and unsupervised domain adaptation (UDA).\nAdditionally, we introduce strong input perturbation through Camera Dropout\n(CamDrop) and feature perturbation via BEV Feature Dropout (BFD), which are\ncrucial for enhancing SSL capabilities using our teacher-student framework. Our\ncomprehensive approach is simple and flexible but yields significant\nimprovements over various baselines for SSL and UDA, achieving competitive\nperformances even against the current state-of-the-art.\n","authors":["Haruya Ishikawa","Takumi Iida","Yoshinori Konishi","Yoshimitsu Aoki"],"pdf_url":"https://arxiv.org/pdf/2403.12530v2.pdf","comment":"13 pages, 5 figures; Accepted to IROS 2024"},{"id":"http://arxiv.org/abs/2407.10730v1","updated":"2024-07-15T13:58:24Z","published":"2024-07-15T13:58:24Z","title":"ConvBench: A Comprehensive Benchmark for 2D Convolution Primitive\n  Evaluation","summary":"  Convolution is a compute-intensive operation placed at the heart of\nConvolution Neural Networks (CNNs). It has led to the development of many\nhigh-performance algorithms, such as Im2col-GEMM, Winograd, and\nDirect-Convolution. However, the comparison of different convolution algorithms\nis an error-prone task as it requires specific data layouts and system\nresources. Failure to address these requirements might lead to unwanted time\npenalties. Thus, considering all processing steps within convolution algorithms\nis essential to comprehensively evaluate and fairly compare their performance.\nFurthermore, most known convolution benchmarking adopts ad-hoc testing suites\nwith limited coverage and handmade operations. This paper proposes ConvBench, a\nprimitive-level benchmark for the evaluation and comparison of convolution\nalgorithms. It assesses 9243 convolution operations derived from 1097\nreal-world deep learning models, resulting in performance and execution\nbreakdown graphs for a detailed evaluation. ConvBench capability is evaluated\nacross the Sliced Convolution (SConv) algorithm. The experiments showed results\nfaster than Im2col-GEMM in 93.6% of the convolutions. However, the use of\nConvBench allowed the delving into the remaining 6.4% underperforming\nconvolutions, uncovering a critical slowdown of 79.5% on average of SConv's\npacking step. This analysis underscores a potential source of optimization for\nSConv, opening up new paths for convolution designers to improve their\nalgorithms.\n","authors":["Lucas Alvarenga","Victor Ferrari","Rafael Souza","Marcio Pereira","Guido Araujo"],"pdf_url":"https://arxiv.org/pdf/2407.10730v1.pdf","comment":"5 pages, 3 figures, presented on MLArchSys workshop of ISCA'2024"},{"id":"http://arxiv.org/abs/2407.10723v1","updated":"2024-07-15T13:49:31Z","published":"2024-07-15T13:49:31Z","title":"Anticipating Future Object Compositions without Forgetting","summary":"  Despite the significant advancements in computer vision models, their ability\nto generalize to novel object-attribute compositions remains limited. Existing\nmethods for Compositional Zero-Shot Learning (CZSL) mainly focus on image\nclassification. This paper aims to enhance CZSL in object detection without\nforgetting prior learned knowledge. We use Grounding DINO and incorporate\nCompositional Soft Prompting (CSP) into it and extend it with Compositional\nAnticipation. We achieve a 70.5% improvement over CSP on the harmonic mean (HM)\nbetween seen and unseen compositions on the CLEVR dataset. Furthermore, we\nintroduce Contrastive Prompt Tuning to incrementally address model confusion\nbetween similar compositions. We demonstrate the effectiveness of this method\nand achieve an increase of 14.5% in HM across the pretrain, increment, and\nunseen sets. Collectively, these methods provide a framework for learning\nvarious compositions with limited data, as well as improving the performance of\nunderperforming compositions when additional data becomes available.\n","authors":["Youssef Zahran","Gertjan Burghouts","Yke Bauke Eisma"],"pdf_url":"https://arxiv.org/pdf/2407.10723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10348v2","updated":"2024-07-15T13:46:29Z","published":"2024-03-15T14:34:34Z","title":"Denoising Task Difficulty-based Curriculum for Training Diffusion Models","summary":"  Diffusion-based generative models have emerged as powerful tools in the realm\nof generative modeling. Despite extensive research on denoising across various\ntimesteps and noise levels, a conflict persists regarding the relative\ndifficulties of the denoising tasks. While various studies argue that lower\ntimesteps present more challenging tasks, others contend that higher timesteps\nare more difficult. To address this conflict, our study undertakes a\ncomprehensive examination of task difficulties, focusing on convergence\nbehavior and changes in relative entropy between consecutive probability\ndistributions across timesteps. Our observational study reveals that denoising\nat earlier timesteps poses challenges characterized by slower convergence and\nhigher relative entropy, indicating increased task difficulty at these lower\ntimesteps. Building on these observations, we introduce an easy-to-hard\nlearning scheme, drawing from curriculum learning, to enhance the training\nprocess of diffusion models. By organizing timesteps or noise levels into\nclusters and training models with ascending orders of difficulty, we facilitate\nan order-aware training regime, progressing from easier to harder denoising\ntasks, thereby deviating from the conventional approach of training diffusion\nmodels simultaneously across all timesteps. Our approach leads to improved\nperformance and faster convergence by leveraging benefits of curriculum\nlearning, while maintaining orthogonality with existing improvements in\ndiffusion training techniques. We validate these advantages through\ncomprehensive experiments in image generation tasks, including unconditional,\nclass-conditional, and text-to-image generation.\n","authors":["Jin-Young Kim","Hyojun Go","Soonwoo Kwon","Hyun-Gyoon Kim"],"pdf_url":"https://arxiv.org/pdf/2403.10348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03648v2","updated":"2024-07-15T13:35:33Z","published":"2023-02-07T17:59:05Z","title":"Class-Incremental Learning: A Survey","summary":"  Deep models, e.g., CNNs and Vision Transformers, have achieved impressive\nachievements in many vision tasks in the closed world. However, novel classes\nemerge from time to time in our ever-changing world, requiring a learning\nsystem to acquire new knowledge continually. Class-Incremental Learning (CIL)\nenables the learner to incorporate the knowledge of new classes incrementally\nand build a universal classifier among all seen classes. Correspondingly, when\ndirectly training the model with new class instances, a fatal problem occurs --\nthe model tends to catastrophically forget the characteristics of former ones,\nand its performance drastically degrades. There have been numerous efforts to\ntackle catastrophic forgetting in the machine learning community. In this\npaper, we survey comprehensively recent advances in class-incremental learning\nand summarize these methods from several aspects. We also provide a rigorous\nand unified evaluation of 17 methods in benchmark image classification tasks to\nfind out the characteristics of different algorithms empirically. Furthermore,\nwe notice that the current comparison protocol ignores the influence of memory\nbudget in model storage, which may result in unfair comparison and biased\nresults. Hence, we advocate fair comparison by aligning the memory budget in\nevaluation, as well as several memory-agnostic performance measures. The source\ncode is available at https://github.com/zhoudw-zdw/CIL_Survey/\n","authors":["Da-Wei Zhou","Qi-Wei Wang","Zhi-Hong Qi","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2302.03648v2.pdf","comment":"Accepted to TPAMI. Code is available at\n  https://github.com/zhoudw-zdw/CIL_Survey/"},{"id":"http://arxiv.org/abs/2403.12002v2","updated":"2024-07-15T13:34:29Z","published":"2024-03-18T17:38:53Z","title":"DreamMotion: Space-Time Self-Similar Score Distillation for Zero-Shot\n  Video Editing","summary":"  Text-driven diffusion-based video editing presents a unique challenge not\nencountered in image editing literature: establishing real-world motion. Unlike\nexisting video editing approaches, here we focus on score distillation sampling\nto circumvent the standard reverse diffusion process and initiate optimization\nfrom videos that already exhibit natural motion. Our analysis reveals that\nwhile video score distillation can effectively introduce new content indicated\nby target text, it can also cause significant structure and motion deviation.\nTo counteract this, we propose to match space-time self-similarities of the\noriginal video and the edited video during the score distillation. Thanks to\nthe use of score distillation, our approach is model-agnostic, which can be\napplied for both cascaded and non-cascaded video diffusion frameworks. Through\nextensive comparisons with leading methods, our approach demonstrates its\nsuperiority in altering appearances while accurately preserving the original\nstructure and motion.\n","authors":["Hyeonho Jeong","Jinho Chang","Geon Yeong Park","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.12002v2.pdf","comment":"Accepted to ECCV 2024, Project page:\n  https://hyeonho99.github.io/dreammotion/"},{"id":"http://arxiv.org/abs/2404.11243v2","updated":"2024-07-15T13:29:08Z","published":"2024-04-17T10:49:00Z","title":"Patch-Consistent Optical Translation Across Sensors: Large-Scale\n  Denoising Diffusion with Heterogeneous Change Detection as a Use Case","summary":"  In the field of remote sensing, the challenge of comparing images captured by\ndisparate sensors is a common obstacle. This requires image translation --\nconverting imagery from one sensor domain to another while preserving the\noriginal content. Denoising Diffusion Implicit Models (DDIM) are potential\nstate-of-the-art solutions for such domain translation due to their proven\nsuperiority in multiple image-to-image translation tasks in classic computer\nvision. However, these models struggle with large-scale multi-patch imagery,\noften focusing solely on small patches and resulting in inconsistencies across\nthe full image. To overcome these limitations, we propose a novel method that\nleverages DDIM for effective optical image translation over large areas. Our\napproach is tailored to super-resolve large-scale low spatial resolution images\ninto high-resolution equivalents from disparate optical sensors, ensuring\nuniformity across hundreds of patches. Extensive experiments with a dataset of\npaired Sentinel-II and Planet Dove images show that our approach provides\nprecise domain adaptation and artifact reduction. Our technique preserves the\nimage content while also improving radiometric (color) accuracy and feature\nrepresentations. The outcome is a high-resolution large-scale image with\nconsistent patches, vital for applications such as heterogeneous change\ndetection (HCD). We present a unique training and testing algorithm rooted in\nDDIMs, a thorough image quality assessment, and a comparative study against the\nstandard classifier-free guided DDIM framework and five other leading methods.\nThe efficacy of our approach is further demonstrated by substantial\nenhancements in HCD tasks performed in the urban settings of Beirut, Lebanon,\nand Austin, USA.\n","authors":["JoÃ£o Gabriel Vinholi","Marco Chini","Anis Amziane","Renato Machado","Danilo Silva","Patrick Matgen"],"pdf_url":"https://arxiv.org/pdf/2404.11243v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10709v1","updated":"2024-07-15T13:26:58Z","published":"2024-07-15T13:26:58Z","title":"Detecting Omissions in Geographic Maps through Computer Vision","summary":"  This paper explores the application of computer vision technologies to the\nanalysis of maps, an area with substantial historical, cultural, and political\nsignificance. Our focus is on developing and evaluating a method for\nautomatically identifying maps that depict specific regions and feature\nlandmarks with designated names, a task that involves complex challenges due to\nthe diverse styles and methods used in map creation. We address three main\nsubtasks: differentiating maps from non-maps, verifying the accuracy of the\nregion depicted, and confirming the presence or absence of particular landmark\nnames through advanced text recognition techniques. Our approach utilizes a\nConvolutional Neural Network and transfer learning to differentiate maps from\nnon-maps, verify the accuracy of depicted regions, and confirm landmark names\nthrough advanced text recognition. We also introduce the VinMap dataset,\ncontaining annotated map images of Vietnam, to train and test our method.\nExperiments on this dataset demonstrate that our technique achieves F1-score of\n85.51% for identifying maps excluding specific territorial landmarks. This\nresult suggests practical utility and indicates areas for future improvement.\n","authors":["Phuc D. A. Nguyen","Anh Do","Minh Hoai"],"pdf_url":"https://arxiv.org/pdf/2407.10709v1.pdf","comment":"VinMap dataset: https://github.com/VinAIResearch/VinMap"},{"id":"http://arxiv.org/abs/2407.10707v1","updated":"2024-07-15T13:25:07Z","published":"2024-07-15T13:25:07Z","title":"Interactive Rendering of Relightable and Animatable Gaussian Avatars","summary":"  Creating relightable and animatable avatars from multi-view or monocular\nvideos is a challenging task for digital human creation and virtual reality\napplications. Previous methods rely on neural radiance fields or ray tracing,\nresulting in slow training and rendering processes. By utilizing Gaussian\nSplatting, we propose a simple and efficient method to decouple body materials\nand lighting from sparse-view or monocular avatar videos, so that the avatar\ncan be rendered simultaneously under novel viewpoints, poses, and lightings at\ninteractive frame rates (6.9 fps). Specifically, we first obtain the canonical\nbody mesh using a signed distance function and assign attributes to each mesh\nvertex. The Gaussians in the canonical space then interpolate from nearby body\nmesh vertices to obtain the attributes. We subsequently deform the Gaussians to\nthe posed space using forward skinning, and combine the learnable environment\nlight with the Gaussian attributes for shading computation. To achieve fast\nshadow modeling, we rasterize the posed body mesh from dense viewpoints to\nobtain the visibility. Our approach is not only simple but also fast enough to\nallow interactive rendering of avatar animation under environmental light\nchanges. Experiments demonstrate that, compared to previous works, our method\ncan render higher quality results at a faster speed on both synthetic and real\ndatasets.\n","authors":["Youyi Zhan","Tianjia Shao","He Wang","Yin Yang","Kun Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.10707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10704v1","updated":"2024-07-15T13:19:56Z","published":"2024-07-15T13:19:56Z","title":"Quantized Prompt for Efficient Generalization of Vision-Language Models","summary":"  In the past few years, large-scale pre-trained vision-language models like\nCLIP have achieved tremendous success in various fields. Naturally, how to\ntransfer the rich knowledge in such huge pre-trained models to downstream tasks\nand datasets becomes a hot topic. During downstream adaptation, the most\nchallenging problems are overfitting and catastrophic forgetting, which can\ncause the model to overly focus on the current data and lose more crucial\ndomain-general knowledge. Existing works use classic regularization techniques\nto solve the problems. As solutions become increasingly complex, the\never-growing storage and inference costs are also a significant problem that\nurgently needs to be addressed. While in this paper, we start from an\nobservation that proper random noise can suppress overfitting and catastrophic\nforgetting. Then we regard quantization error as a kind of noise, and explore\nquantization for regularizing vision-language model, which is quite efficiency\nand effective. Furthermore, to improve the model's generalization capability\nwhile maintaining its specialization capacity at minimal cost, we deeply\nanalyze the characteristics of the weight distribution in prompts, conclude\nseveral principles for quantization module design and follow such principles to\ncreate several competitive baselines. The proposed method is significantly\nefficient due to its inherent lightweight nature, making it possible to adapt\non extremely resource-limited devices. Our method can be fruitfully integrated\ninto many existing approaches like MaPLe, enhancing accuracy while reducing\nstorage overhead, making it more powerful yet versatile. Extensive experiments\non 11 datasets shows great superiority of our method sufficiently. Code is\navailable at https://github.com/beyondhtx/QPrompt.\n","authors":["Tianxiang Hao","Xiaohan Ding","Juexiao Feng","Yuhong Yang","Hui Chen","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2407.10704v1.pdf","comment":"14 pages, 7 figures. Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10703v1","updated":"2024-07-15T13:18:37Z","published":"2024-07-15T13:18:37Z","title":"Towards Robust Event-based Networks for Nighttime via Unpaired\n  Day-to-Night Event Translation","summary":"  Event cameras with high dynamic range ensure scene capture even in low-light\nconditions. However, night events exhibit patterns different from those\ncaptured during the day. This difference causes performance degradation when\napplying night events to a model trained solely on day events. This limitation\npersists due to a lack of annotated night events. To overcome the limitation,\nwe aim to alleviate data imbalance by translating annotated day data into night\nevents. However, generating events from different modalities challenges\nreproducing their unique properties. Accordingly, we propose an unpaired\nevent-to-event day-to-night translation model that effectively learns to map\nfrom one domain to another using Diffusion GAN. The proposed translation model\nanalyzes events in spatio-temporal dimension with wavelet decomposition and\ndisentangled convolution layers. We also propose a new temporal contrastive\nlearning with a novel shuffling and sampling strategy to regularize temporal\ncontinuity. To validate the efficacy of the proposed methodology, we redesign\nmetrics for evaluating events translated in an unpaired setting, aligning them\nwith the event modality for the first time. Our framework shows the successful\nday-to-night event translation while preserving the characteristics of events.\nIn addition, through our translation method, we facilitate event-based modes to\nlearn about night events by translating annotated day events into night events.\nOur approach effectively mitigates the performance degradation of applying real\nnight events to downstream tasks. The code is available at\nhttps://github.com/jeongyh98/UDNET.\n","authors":["Yuhwan Jeong","Hoonhee Cho","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2407.10703v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10696v1","updated":"2024-07-15T13:12:34Z","published":"2024-07-15T13:12:34Z","title":"Deep ContourFlow: Advancing Active Contours with Deep Learning","summary":"  This paper introduces a novel approach that combines unsupervised active\ncontour models with deep learning for robust and adaptive image segmentation.\nIndeed, traditional active contours, provide a flexible framework for contour\nevolution and learning offers the capacity to learn intricate features and\npatterns directly from raw data. Our proposed methodology leverages the\nstrengths of both paradigms, presenting a framework for both unsupervised and\none-shot approaches for image segmentation. It is capable of capturing complex\nobject boundaries without the need for extensive labeled training data. This is\nparticularly required in histology, a field facing a significant shortage of\nannotations due to the challenging and time-consuming nature of the annotation\nprocess. We illustrate and compare our results to state of the art methods on a\nhistology dataset and show significant improvements.\n","authors":["Antoine Habis","Vannary Meas-Yedid","Elsa Angelini","Jean-Christophe Olivo-Marin"],"pdf_url":"https://arxiv.org/pdf/2407.10696v1.pdf","comment":"11 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.10695v1","updated":"2024-07-15T13:10:23Z","published":"2024-07-15T13:10:23Z","title":"IE-NeRF: Inpainting Enhanced Neural Radiance Fields in the Wild","summary":"  We present a novel approach for synthesizing realistic novel views using\nNeural Radiance Fields (NeRF) with uncontrolled photos in the wild. While NeRF\nhas shown impressive results in controlled settings, it struggles with\ntransient objects commonly found in dynamic and time-varying scenes. Our\nframework called \\textit{Inpainting Enhanced NeRF}, or \\ours, enhances the\nconventional NeRF by drawing inspiration from the technique of image\ninpainting. Specifically, our approach extends the Multi-Layer Perceptrons\n(MLP) of NeRF, enabling it to simultaneously generate intrinsic properties\n(static color, density) and extrinsic transient masks. We introduce an\ninpainting module that leverages the transient masks to effectively exclude\nocclusions, resulting in improved volume rendering quality. Additionally, we\npropose a new training strategy with frequency regularization to address the\nsparsity issue of low-frequency transient components. We evaluate our approach\non internet photo collections of landmarks, demonstrating its ability to\ngenerate high-quality novel views and achieve state-of-the-art performance.\n","authors":["Shuaixian Wang","Haoran Xu","Yaokun Li","Jiwei Chen","Guang Tan"],"pdf_url":"https://arxiv.org/pdf/2407.10695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10694v1","updated":"2024-07-15T13:08:42Z","published":"2024-07-15T13:08:42Z","title":"Features Reconstruction Disentanglement Cloth-Changing Person\n  Re-Identification","summary":"  Cloth-changing person re-identification (CC-ReID) aims to retrieve specific\npedestrians in a cloth-changing scenario. Its main challenge is to disentangle\nthe clothing-related and clothing-unrelated features. Most existing approaches\nforce the model to learn clothing-unrelated features by changing the color of\nthe clothes. However, due to the lack of ground truth, these methods inevitably\nintroduce noise, which destroys the discriminative features and leads to an\nuncontrollable disentanglement process. In this paper, we propose a new person\nre-identification network called features reconstruction disentanglement ReID\n(FRD-ReID), which can controllably decouple the clothing-unrelated and\nclothing-related features. Specifically, we first introduce the human parsing\nmask as the ground truth of the reconstruction process. At the same time, we\npropose the far away attention (FAA) mechanism and the person contour attention\n(PCA) mechanism for clothing-unrelated features and pedestrian contour features\nto improve the feature reconstruction efficiency. In the testing phase, we\ndirectly discard the clothing-related features for inference,which leads to a\ncontrollable disentanglement process. We conducted extensive experiments on the\nPRCC, LTCC, and Vc-Clothes datasets and demonstrated that our method\noutperforms existing state-of-the-art methods.\n","authors":["Zhihao Chen","Yiyuan Ge","Qing Yue"],"pdf_url":"https://arxiv.org/pdf/2407.10694v1.pdf","comment":"2024 International Conference on Intelligent Computing"},{"id":"http://arxiv.org/abs/2306.04621v3","updated":"2024-07-15T13:07:02Z","published":"2023-06-07T17:50:59Z","title":"Flexible Distribution Alignment: Towards Long-tailed Semi-supervised\n  Learning with Proper Calibration","summary":"  Long-tailed semi-supervised learning (LTSSL) represents a practical scenario\nfor semi-supervised applications, challenged by skewed labeled distributions\nthat bias classifiers. This problem is often aggravated by discrepancies\nbetween labeled and unlabeled class distributions, leading to biased\npseudo-labels, neglect of rare classes, and poorly calibrated probabilities. To\naddress these issues, we introduce Flexible Distribution Alignment (FlexDA), a\nnovel adaptive logit-adjusted loss framework designed to dynamically estimate\nand align predictions with the actual distribution of unlabeled data and\nachieve a balanced classifier by the end of training. FlexDA is further\nenhanced by a distillation-based consistency loss, promoting fair data usage\nacross classes and effectively leveraging underconfident samples. This method,\nencapsulated in ADELLO (Align and Distill Everything All at Once), proves\nrobust against label shift, significantly improves model calibration in LTSSL\ncontexts, and surpasses previous state-of-of-art approaches across multiple\nbenchmarks, including CIFAR100-LT, STL10-LT, and ImageNet127, addressing class\nimbalance challenges in semi-supervised learning. Our code is available at\nhttps://github.com/emasa/ADELLO-LTSSL.\n","authors":["Emanuel Sanchez Aimar","Nathaniel Helgesen","Yonghao Xu","Marco Kuhlmann","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2306.04621v3.pdf","comment":"Accepted at ECCV2024, 25 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.10687v1","updated":"2024-07-15T13:01:44Z","published":"2024-07-15T13:01:44Z","title":"FRI-Net: Floorplan Reconstruction via Room-wise Implicit Representation","summary":"  In this paper, we introduce a novel method called FRI-Net for 2D floorplan\nreconstruction from 3D point cloud. Existing methods typically rely on corner\nregression or box regression, which lack consideration for the global shapes of\nrooms. To address these issues, we propose a novel approach using a room-wise\nimplicit representation with structural regularization to characterize the\nshapes of rooms in floorplans. By incorporating geometric priors of room\nlayouts in floorplans into our training strategy, the generated room polygons\nare more geometrically regular. We have conducted experiments on two\nchallenging datasets, Structured3D and SceneCAD. Our method demonstrates\nimproved performance compared to state-of-the-art methods, validating the\neffectiveness of our proposed representation for floorplan reconstruction.\n","authors":["Honghao Xu","Juzhan Xu","Zeyu Huang","Pengfei Xu","Hui Huang","Ruizhen Hu"],"pdf_url":"https://arxiv.org/pdf/2407.10687v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10683v1","updated":"2024-07-15T12:59:03Z","published":"2024-07-15T12:59:03Z","title":"Addressing Image Hallucination in Text-to-Image Generation through\n  Factual Image Retrieval","summary":"  Text-to-image generation has shown remarkable progress with the emergence of\ndiffusion models. However, these models often generate factually inconsistent\nimages, failing to accurately reflect the factual information and common sense\nconveyed by the input text prompts. We refer to this issue as Image\nhallucination. Drawing from studies on hallucinations in language models, we\nclassify this problem into three types and propose a methodology that uses\nfactual images retrieved from external sources to generate realistic images.\nDepending on the nature of the hallucination, we employ off-the-shelf image\nediting tools, either InstructPix2Pix or IP-Adapter, to leverage factual\ninformation from the retrieved image. This approach enables the generation of\nimages that accurately reflect the facts and common sense.\n","authors":["Youngsun Lim","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2407.10683v1.pdf","comment":"This paper has been accepted for oral presentation at the IJCAI 2024\n  Workshop on Trustworthy Interactive Decision-Making with Foundation Models"},{"id":"http://arxiv.org/abs/2403.14797v2","updated":"2024-07-15T12:59:02Z","published":"2024-03-21T19:20:29Z","title":"Preventing Catastrophic Forgetting through Memory Networks in Continuous\n  Detection","summary":"  Modern pre-trained architectures struggle to retain previous information\nwhile undergoing continuous fine-tuning on new tasks. Despite notable progress\nin continual classification, systems designed for complex vision tasks such as\ndetection or segmentation still struggle to attain satisfactory performance. In\nthis work, we introduce a memory-based detection transformer architecture to\nadapt a pre-trained DETR-style detector to new tasks while preserving knowledge\nfrom previous tasks. We propose a novel localized query function for efficient\ninformation retrieval from memory units, aiming to minimize forgetting.\nFurthermore, we identify a fundamental challenge in continual detection\nreferred to as background relegation. This arises when object categories from\nearlier tasks reappear in future tasks, potentially without labels, leading\nthem to be implicitly treated as background. This is an inevitable issue in\ncontinual detection or segmentation. The introduced continual optimization\ntechnique effectively tackles this challenge. Finally, we assess the\nperformance of our proposed system on continual detection benchmarks and\ndemonstrate that our approach surpasses the performance of existing\nstate-of-the-art resulting in 5-7% improvements on MS-COCO and PASCAL-VOC on\nthe task of continual detection.\n","authors":["Gaurav Bhatt","James Ross","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2403.14797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10681v1","updated":"2024-07-15T12:58:04Z","published":"2024-07-15T12:58:04Z","title":"GeoMix: Towards Geometry-Aware Data Augmentation","summary":"  Mixup has shown considerable success in mitigating the challenges posed by\nlimited labeled data in image classification. By synthesizing samples through\nthe interpolation of features and labels, Mixup effectively addresses the issue\nof data scarcity. However, it has rarely been explored in graph learning tasks\ndue to the irregularity and connectivity of graph data. Specifically, in node\nclassification tasks, Mixup presents a challenge in creating connections for\nsynthetic data. In this paper, we propose Geometric Mixup (GeoMix), a simple\nand interpretable Mixup approach leveraging in-place graph editing. It\neffectively utilizes geometry information to interpolate features and labels\nwith those from the nearby neighborhood, generating synthetic nodes and\nestablishing connections for them. We conduct theoretical analysis to elucidate\nthe rationale behind employing geometry information for node Mixup, emphasizing\nthe significance of locality enhancement-a critical aspect of our method's\ndesign. Extensive experiments demonstrate that our lightweight Geometric Mixup\nachieves state-of-the-art results on a wide variety of standard datasets with\nlimited labeled data. Furthermore, it significantly improves the generalization\ncapability of underlying GNNs across various challenging out-of-distribution\ngeneralization tasks. Our code is available at\nhttps://github.com/WtaoZhao/geomix.\n","authors":["Wentao Zhao","Qitian Wu","Chenxiao Yang","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2407.10681v1.pdf","comment":"Published as a conference paper at KDD 2024"},{"id":"http://arxiv.org/abs/2403.04884v2","updated":"2024-07-15T12:49:16Z","published":"2024-03-07T20:16:42Z","title":"Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural\n  Networks","summary":"  Implantable retinal prostheses offer a promising solution to restore partial\nvision by circumventing damaged photoreceptor cells in the retina and directly\nstimulating the remaining functional retinal cells. However, the information\ntransmission between the camera and retinal cells is often limited by the low\nresolution of the electrode array and the lack of specificity for different\nganglion cell types, resulting in suboptimal stimulations. In this work, we\npropose to utilize normalizing flow-based conditional invertible neural\nnetworks to optimize retinal implant stimulation in an unsupervised manner. The\ninvertibility of these networks allows us to use them as a surrogate for the\ncomputational model of the visual system, while also encoding input camera\nsignals into optimized electrical stimuli on the electrode array. Compared to\nother methods, such as trivial downsampling, linear models, and feed-forward\nconvolutional neural networks, the flow-based invertible neural network and its\nconditional extension yield better visual reconstruction qualities w.r.t.\nvarious metrics using a physiologically validated simulation tool.\n","authors":["Yuli Wu","Julian Wittmann","Peter Walter","Johannes Stegmaier"],"pdf_url":"https://arxiv.org/pdf/2403.04884v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08291v4","updated":"2024-07-15T12:47:03Z","published":"2023-12-13T17:08:38Z","title":"VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent\n  Space","summary":"  Previous works on Human Pose and Shape Estimation (HPSE) from RGB images can\nbe broadly categorized into two main groups: parametric and non-parametric\napproaches. Parametric techniques leverage a low-dimensional statistical body\nmodel for realistic results, whereas recent non-parametric methods achieve\nhigher precision by directly regressing the 3D coordinates of the human body\nmesh. This work introduces a novel paradigm to address the HPSE problem,\ninvolving a low-dimensional discrete latent representation of the human mesh\nand framing HPSE as a classification task. Instead of predicting body model\nparameters or 3D vertex coordinates, we focus on predicting the proposed\ndiscrete latent representation, which can be decoded into a registered human\nmesh. This innovative paradigm offers two key advantages. Firstly, predicting a\nlow-dimensional discrete representation confines our predictions to the space\nof anthropomorphic poses and shapes even when little training data is\navailable. Secondly, by framing the problem as a classification task, we can\nharness the discriminative power inherent in neural networks. The proposed\nmodel, VQ-HPS, predicts the discrete latent representation of the mesh. The\nexperimental results demonstrate that VQ-HPS outperforms the current\nstate-of-the-art non-parametric approaches while yielding results as realistic\nas those produced by parametric methods when trained with little data. VQ-HPS\nalso shows promising results when training on large-scale datasets,\nhighlighting the significant potential of the classification approach for HPSE.\nSee the project page at https://g-fiche.github.io/research-pages/vqhps/\n","authors":["GuÃ©nolÃ© Fiche","Simon Leglaive","Xavier Alameda-Pineda","Antonio Agudo","Francesc Moreno-Noguer"],"pdf_url":"https://arxiv.org/pdf/2312.08291v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11617v2","updated":"2024-07-15T12:42:07Z","published":"2023-08-22T17:59:51Z","title":"GRIP: Generating Interaction Poses Using Spatial Cues and Latent\n  Consistency","summary":"  Hands are dexterous and highly versatile manipulators that are central to how\nhumans interact with objects and their environment. Consequently, modeling\nrealistic hand-object interactions, including the subtle motion of individual\nfingers, is critical for applications in computer graphics, computer vision,\nand mixed reality. Prior work on capturing and modeling humans interacting with\nobjects in 3D focuses on the body and object motion, often ignoring hand pose.\nIn contrast, we introduce GRIP, a learning-based method that takes, as input,\nthe 3D motion of the body and the object, and synthesizes realistic motion for\nboth hands before, during, and after object interaction. As a preliminary step\nbefore synthesizing the hand motion, we first use a network, ANet, to denoise\nthe arm motion. Then, we leverage the spatio-temporal relationship between the\nbody and the object to extract two types of novel temporal interaction cues,\nand use them in a two-stage inference pipeline to generate the hand motion. In\nthe first stage, we introduce a new approach to enforce motion temporal\nconsistency in the latent space (LTC), and generate consistent interaction\nmotions. In the second stage, GRIP generates refined hand poses to avoid\nhand-object penetrations. Given sequences of noisy body and object motion, GRIP\nupgrades them to include hand-object interaction. Quantitative experiments and\nperceptual studies demonstrate that GRIP outperforms baseline methods and\ngeneralizes to unseen objects and motions from different motion-capture\ndatasets.\n","authors":["Omid Taheri","Yi Zhou","Dimitrios Tzionas","Yang Zhou","Duygu Ceylan","Soren Pirk","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2308.11617v2.pdf","comment":"The project has been started during Omid Taheri's internship at Adobe\n  and as a collaboration with the Max Planck Institute for Intelligent Systems"},{"id":"http://arxiv.org/abs/2405.19076v3","updated":"2024-07-15T12:36:42Z","published":"2024-05-29T13:34:32Z","title":"Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials\n  Analysis and Design","summary":"  We present Cephalo, a series of multimodal vision large language models\n(V-LLMs) designed for materials science applications, integrating visual and\nlinguistic data for enhanced understanding. A key innovation of Cephalo is its\nadvanced dataset generation method. Cephalo is trained on integrated image and\ntext data from thousands of scientific papers and science-focused Wikipedia\ndata demonstrates can interpret complex visual scenes, generate precise\nlanguage descriptions, and answer queries about images effectively. The\ncombination of a vision encoder with an autoregressive transformer supports\nmultimodal natural language understanding, which can be coupled with other\ngenerative methods to create an image-to-text-to-3D pipeline. To develop more\ncapable models from smaller ones, we report both mixture-of-expert methods and\nmodel merging. We examine the models in diverse use cases that incorporate\nbiological materials, fracture and engineering analysis, protein biophysics,\nand bio-inspired design based on insect behavior. Generative applications\ninclude bio-inspired designs, including pollen-inspired architected materials,\nas well as the synthesis of bio-inspired material microstructures from a\nphotograph of a solar eclipse. Additional model fine-tuning with a series of\nmolecular dynamics results demonstrate Cephalo's enhanced capabilities to\naccurately predict statistical features of stress and atomic energy\ndistributions, as well as crack dynamics and damage in materials.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2405.19076v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08528v3","updated":"2024-07-15T12:34:51Z","published":"2023-10-12T17:21:41Z","title":"4D Gaussian Splatting for Real-Time Dynamic Scene Rendering","summary":"  Representing and rendering dynamic scenes has been an important but\nchallenging task. Especially, to accurately model complex motions, high\nefficiency is usually hard to guarantee. To achieve real-time dynamic scene\nrendering while also enjoying high training and storage efficiency, we propose\n4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes\nrather than applying 3D-GS for each individual frame. In 4D-GS, a novel\nexplicit representation containing both 3D Gaussians and 4D neural voxels is\nproposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is\nproposed to efficiently build Gaussian features from 4D neural voxels and then\na lightweight MLP is applied to predict Gaussian deformations at novel\ntimestamps. Our 4D-GS method achieves real-time rendering under high\nresolutions, 82 FPS at an 800$\\times$800 resolution on an RTX 3090 GPU while\nmaintaining comparable or better quality than previous state-of-the-art\nmethods. More demos and code are available at\nhttps://guanjunwu.github.io/4dgs/.\n","authors":["Guanjun Wu","Taoran Yi","Jiemin Fang","Lingxi Xie","Xiaopeng Zhang","Wei Wei","Wenyu Liu","Qi Tian","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08528v3.pdf","comment":"CVPR 2024. Project page: https://guanjunwu.github.io/4dgs/"},{"id":"http://arxiv.org/abs/2405.12218v3","updated":"2024-07-15T12:34:30Z","published":"2024-05-20T17:59:30Z","title":"MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from\n  Multi-View Stereo","summary":"  We present MVSGaussian, a new generalizable 3D Gaussian representation\napproach derived from Multi-View Stereo (MVS) that can efficiently reconstruct\nunseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware\nGaussian representations and decode them into Gaussian parameters. 2) To\nfurther enhance performance, we propose a hybrid Gaussian rendering that\nintegrates an efficient volume rendering design for novel view synthesis. 3) To\nsupport fast fine-tuning for specific scenes, we introduce a multi-view\ngeometric consistent aggregation strategy to effectively aggregate the point\nclouds generated by the generalizable model, serving as the initialization for\nper-scene optimization. Compared with previous generalizable NeRF-based\nmethods, which typically require minutes of fine-tuning and seconds of\nrendering per image, MVSGaussian achieves real-time rendering with better\nsynthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian\nachieves better view synthesis with less training computational cost. Extensive\nexperiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples\ndatasets validate that MVSGaussian attains state-of-the-art performance with\nconvincing generalizability, real-time rendering speed, and fast per-scene\noptimization.\n","authors":["Tianqi Liu","Guangcong Wang","Shoukang Hu","Liao Shen","Xinyi Ye","Yuhang Zang","Zhiguo Cao","Wei Li","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2405.12218v3.pdf","comment":"ECCV2024, Project page: https://mvsgaussian.github.io/ , Code:\n  https://github.com/TQTQliu/MVSGaussian"},{"id":"http://arxiv.org/abs/2401.05735v2","updated":"2024-07-15T12:32:19Z","published":"2024-01-11T08:36:15Z","title":"Object-Centric Diffusion for Efficient Video Editing","summary":"  This paper aims to accelerate video stream processing, such as object\ndetection and semantic segmentation, by leveraging the temporal redundancies\nthat exist between video frames. Instead of propagating and warping features\nusing motion alignment, such as optical flow, we propose a novel knowledge\ndistillation schema coined as Delta Distillation. In our proposal, the student\nlearns the variations in the teacher's intermediate features over time. We\ndemonstrate that these temporal variations can be effectively distilled due to\nthe temporal redundancies within video frames. During inference, both teacher\nand student cooperate for providing predictions: the former by providing\ninitial representations extracted only on the key-frame, and the latter by\niteratively estimating and applying deltas for the successive frames. Moreover,\nwe consider various design choices to learn optimal student architectures\nincluding an end-to-end learnable architecture search. By extensive experiments\non a wide range of architectures, including the most efficient ones, we\ndemonstrate that delta distillation sets a new state of the art in terms of\naccuracy vs. efficiency trade-off for semantic segmentation and object\ndetection in videos. Finally, we show that, as a by-product, delta distillation\nimproves the temporal consistency of the teacher model.\n","authors":["Kumara Kahatapitiya","Adil Karjauv","Davide Abati","Fatih Porikli","Yuki M. Asano","Amirhossein Habibian"],"pdf_url":"https://arxiv.org/pdf/2401.05735v2.pdf","comment":"ECCV24"},{"id":"http://arxiv.org/abs/2407.10663v1","updated":"2024-07-15T12:26:52Z","published":"2024-07-15T12:26:52Z","title":"Spatio-temporal neural distance fields for conditional generative\n  modeling of the heart","summary":"  The rhythmic pumping motion of the heart stands as a cornerstone in life, as\nit circulates blood to the entire human body through a series of carefully\ntimed contractions of the individual chambers. Changes in the size, shape and\nmovement of the chambers can be important markers for cardiac disease and\nmodeling this in relation to clinical demography or disease is therefore of\ninterest. Existing methods for spatio-temporal modeling of the human heart\nrequire shape correspondence over time or suffer from large memory\nrequirements, making it difficult to use for complex anatomies. We introduce a\nnovel conditional generative model, where the shape and movement is modeled\nimplicitly in the form of a spatio-temporal neural distance field and\nconditioned on clinical demography. The model is based on an auto-decoder\narchitecture and aims to disentangle the individual variations from that\nrelated to the clinical demography. It is tested on the left atrium (including\nthe left atrial appendage), where it outperforms current state-of-the-art\nmethods for anatomical sequence completion and generates synthetic sequences\nthat realistically mimics the shape and motion of the real left atrium. In\npractice, this means we can infer functional measurements from a static image,\ngenerate synthetic populations with specified demography or disease and\ninvestigate how non-imaging clinical data effect the shape and motion of\ncardiac anatomies.\n","authors":["Kristine SÃ¸rensen","Paula Diez","Jan Margeta","Yasmin El Youssef","Michael Pham","Jonas Jalili Pedersen","Tobias KÃ¼hl","Ole de Backer","Klaus Kofoed","Oscar Camara","Rasmus Paulsen"],"pdf_url":"https://arxiv.org/pdf/2407.10663v1.pdf","comment":"Accepted for MICCAI2024"},{"id":"http://arxiv.org/abs/2407.07504v2","updated":"2024-07-15T12:15:50Z","published":"2024-07-10T09:42:41Z","title":"Pan-cancer Histopathology WSI Pre-training with Position-aware Masked\n  Autoencoder","summary":"  Large-scale pre-training models have promoted the development of\nhistopathology image analysis. However, existing self-supervised methods for\nhistopathology images focus on learning patch features, while there is still a\nlack of available pre-training models for WSI-level feature learning. In this\npaper, we propose a novel self-supervised learning framework for pan-cancer\nWSI-level representation pre-training with the designed position-aware masked\nautoencoder (PAMA). Meanwhile, we propose the position-aware cross-attention\n(PACA) module with a kernel reorientation (KRO) strategy and an anchor dropout\n(AD) mechanism. The KRO strategy can capture the complete semantic structure\nand eliminate ambiguity in WSIs, and the AD contributes to enhancing the\nrobustness and generalization of the model. We evaluated our method on 6\nlarge-scale datasets from multiple organs for pan-cancer classification tasks.\nThe results have demonstrated the effectiveness of PAMA in generalized and\ndiscriminative WSI representation learning and pan-cancer WSI pre-training. The\nproposed method was also compared with 7 WSI analysis methods. The experimental\nresults have indicated that our proposed PAMA is superior to the\nstate-of-the-art methods.The code and checkpoints are available at\nhttps://github.com/WkEEn/PAMA.\n","authors":["Kun Wu","Zhiguo Jiang","Kunming Tang","Jun Shi","Fengying Xie","Wei Wang","Haibo Wu","Yushan Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.07504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10655v1","updated":"2024-07-15T12:15:27Z","published":"2024-07-15T12:15:27Z","title":"OVLW-DETR: Open-Vocabulary Light-Weighted Detection Transformer","summary":"  Open-vocabulary object detection focusing on detecting novel categories\nguided by natural language. In this report, we propose Open-Vocabulary\nLight-Weighted Detection Transformer (OVLW-DETR), a deployment friendly\nopen-vocabulary detector with strong performance and low latency. Building upon\nOVLW-DETR, we provide an end-to-end training recipe that transferring knowledge\nfrom vision-language model (VLM) to object detector with simple alignment. We\nalign detector with the text encoder from VLM by replacing the fixed\nclassification layer weights in detector with the class-name embeddings\nextracted from the text encoder. Without additional fusing module, OVLW-DETR is\nflexible and deployment friendly, making it easier to implement and modulate.\nimproving the efficiency of interleaved attention computation. Experimental\nresults demonstrate that the proposed approach is superior over existing\nreal-time open-vocabulary detectors on standard Zero-Shot LVIS benchmark.\nSource code and pre-trained models are available at\n[https://github.com/Atten4Vis/LW-DETR].\n","authors":["Yu Wang","Xiangbo Su","Qiang Chen","Xinyu Zhang","Teng Xi","Kun Yao","Errui Ding","Gang Zhang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10655v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2407.10649v1","updated":"2024-07-15T12:10:05Z","published":"2024-07-15T12:10:05Z","title":"APC: Adaptive Patch Contrast for Weakly Supervised Semantic Segmentation","summary":"  Weakly Supervised Semantic Segmentation (WSSS) using only image-level labels\nhas gained significant attention due to its cost-effectiveness. The typical\nframework involves using image-level labels as training data to generate\npixel-level pseudo-labels with refinements. Recently, methods based on Vision\nTransformers (ViT) have demonstrated superior capabilities in generating\nreliable pseudo-labels, particularly in recognizing complete object regions,\ncompared to CNN methods. However, current ViT-based approaches have some\nlimitations in the use of patch embeddings, being prone to being dominated by\ncertain abnormal patches, as well as many multi-stage methods being\ntime-consuming and lengthy in training, thus lacking efficiency. Therefore, in\nthis paper, we introduce a novel ViT-based WSSS method named \\textit{Adaptive\nPatch Contrast} (APC) that significantly enhances patch embedding learning for\nimproved segmentation effectiveness. APC utilizes an Adaptive-K Pooling (AKP)\nlayer to address the limitations of previous max pooling selection methods.\nAdditionally, we propose a Patch Contrastive Learning (PCL) to enhance patch\nembeddings, thereby further improving the final results. Furthermore, we\nimprove upon the existing multi-stage training framework without CAM by\ntransforming it into an end-to-end single-stage training approach, thereby\nenhancing training efficiency. The experimental results show that our approach\nis effective and efficient, outperforming other state-of-the-art WSSS methods\non the PASCAL VOC 2012 and MS COCO 2014 dataset within a shorter training\nduration.\n","authors":["Wangyu Wu","Tianhong Dai","Zhenhong Chen","Xiaowei Huang","Fei Ma","Jimin Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.10649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12442v2","updated":"2024-07-15T12:06:20Z","published":"2023-07-23T22:11:23Z","title":"EnTri: Ensemble Learning with Tri-level Representations for Explainable\n  Scene Recognition","summary":"  Scene recognition based on deep-learning has made significant progress, but\nthere are still limitations in its performance due to challenges posed by\ninter-class similarities and intra-class dissimilarities. Furthermore, prior\nresearch has primarily focused on improving classification accuracy, yet it has\ngiven less attention to achieving interpretable, precise scene classification.\nTherefore, we are motivated to propose EnTri, an ensemble scene recognition\nframework that employs ensemble learning using a hierarchy of visual features.\nEnTri represents features at three distinct levels of detail: pixel-level,\nsemantic segmentation-level, and object class and frequency level. By\nincorporating distinct feature encoding schemes of differing complexity and\nleveraging ensemble strategies, our approach aims to improve classification\naccuracy while enhancing transparency and interpretability via visual and\ntextual explanations. To achieve interpretability, we devised an extension\nalgorithm that generates both visual and textual explanations highlighting\nvarious properties of a given scene that contribute to the final prediction of\nits category. This includes information about objects, statistics, spatial\nlayout, and textural details. Through experiments on benchmark scene\nclassification datasets, EnTri has demonstrated superiority in terms of\nrecognition accuracy, achieving competitive performance compared to\nstate-of-the-art approaches, with an accuracy of 87.69%, 75.56%, and 99.17% on\nthe MIT67, SUN397, and UIUC8 datasets, respectively.\n","authors":["Amirhossein Aminimehr","Amirali Molaei","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2307.12442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10641v1","updated":"2024-07-15T12:00:46Z","published":"2024-07-15T12:00:46Z","title":"Deep Diffusion Image Prior for Efficient OOD Adaptation in 3D Inverse\n  Problems","summary":"  Recent inverse problem solvers that leverage generative diffusion priors have\ngarnered significant attention due to their exceptional quality. However,\nadaptation of the prior is necessary when there exists a discrepancy between\nthe training and testing distributions. In this work, we propose deep diffusion\nimage prior (DDIP), which generalizes the recent adaptation method of SCD by\nintroducing a formal connection to the deep image prior. Under this framework,\nwe propose an efficient adaptation method dubbed D3IP, specified for 3D\nmeasurements, which accelerates DDIP by orders of magnitude while achieving\nsuperior performance. D3IP enables seamless integration of 3D inverse solvers\nand thus leads to coherent 3D reconstruction. Moreover, we show that\nmeta-learning techniques can also be applied to yield even better performance.\nWe show that our method is capable of solving diverse 3D reconstructive tasks\nfrom the generative prior trained only with phantom images that are vastly\ndifferent from the training set, opening up new opportunities of applying\ndiffusion inverse solvers even when training with gold standard data is\nimpossible. Code: https://github.com/HJ-harry/DDIP3D\n","authors":["Hyungjin Chung","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2407.10641v1.pdf","comment":"ECCV 2024, 25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.09400v2","updated":"2024-07-15T11:59:57Z","published":"2024-03-14T13:50:44Z","title":"ConDiSR: Contrastive Disentanglement and Style Regularization for Single\n  Domain Generalization","summary":"  Medical data often exhibits distribution shifts, which cause test-time\nperformance degradation for deep learning models trained using standard\nsupervised learning pipelines. This challenge is addressed in the field of\nDomain Generalization (DG) with the sub-field of Single Domain Generalization\n(SDG) being specifically interesting due to the privacy- or logistics-related\nissues often associated with medical data. Existing disentanglement-based SDG\nmethods heavily rely on structural information embedded in segmentation masks,\nhowever classification labels do not provide such dense information. This work\nintroduces a novel SDG method aimed at medical image classification that\nleverages channel-wise contrastive disentanglement. It is further enhanced with\nreconstruction-based style regularization to ensure extraction of distinct\nstyle and structure feature representations. We evaluate our method on the\ncomplex task of multicenter histopathology image classification, comparing it\nagainst state-of-the-art (SOTA) SDG baselines. Results demonstrate that our\nmethod surpasses the SOTA by a margin of 1% in average accuracy while also\nshowing more stable performance. This study highlights the importance and\nchallenges of exploring SDG frameworks in the context of the classification\ntask. The code is publicly available at\nhttps://github.com/BioMedIA-MBZUAI/ConDiSR\n","authors":["Aleksandr Matsun","Numan Saeed","Fadillah Adamsyah Maani","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.09400v2.pdf","comment":"A flaw was found in the results acquisition"},{"id":"http://arxiv.org/abs/2407.10639v1","updated":"2024-07-15T11:57:06Z","published":"2024-07-15T11:57:06Z","title":"Risk-aware Trajectory Prediction by Incorporating Spatio-temporal\n  Traffic Interaction Analysis","summary":"  To operate in open-ended environments where humans interact in complex,\ndiverse ways, autonomous robots must learn to predict their behaviour,\nespecially when that behavior is potentially dangerous to other agents or to\nthe robot. However, reducing the risk of accidents requires prior knowledge of\nwhere potential collisions may occur and how. Therefore, we propose to gain\nthis information by analyzing locations and speeds that commonly correspond to\nhigh-risk interactions within the dataset, and use it within training to\ngenerate better predictions in high risk situations. Through these\nlocation-based and speed-based re-weighting techniques, we achieve improved\noverall performance, as measured by most-likely FDE and KDE, as well as\nimproved performance on high-speed vehicles, and vehicles within high-risk\nlocations.\n  2023 IEEE International Conference on Robotics and Automation (ICRA)\n","authors":["Divya Thuremella","Lewis Ince","Lars Kunze"],"pdf_url":"https://arxiv.org/pdf/2407.10639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02958v2","updated":"2024-07-15T11:54:32Z","published":"2024-05-05T14:56:34Z","title":"Score-based Generative Priors Guided Model-driven Network for MRI\n  Reconstruction","summary":"  Score matching with Langevin dynamics (SMLD) method has been successfully\napplied to accelerated MRI. However, the hyperparameters in the sampling\nprocess require subtle tuning, otherwise the results can be severely corrupted\nby hallucination artifacts, especially with out-of-distribution test data. To\naddress the limitations, we proposed a novel workflow where naive SMLD samples\nserve as additional priors to guide model-driven network training. First, we\nadopted a pretrained score network to generate samples as preliminary guidance\nimages (PGI), obviating the need for network retraining, parameter tuning and\nin-distribution test data. Although PGIs are corrupted by hallucination\nartifacts, we believe they can provide extra information through effective\ndenoising steps to facilitate reconstruction. Therefore, we designed a\ndenoising module (DM) in the second step to coarsely eliminate artifacts and\nnoises from PGIs. The features are extracted from a score-based information\nextractor (SIE) and a cross-domain information extractor (CIE), which directly\nmap to the noise patterns. Third, we designed a model-driven network guided by\ndenoised PGIs (DGIs) to further recover fine details. DGIs are densely\nconnected with intermediate reconstructions in each cascade to enrich the\ninformation and are periodically updated to provide more accurate guidance. Our\nexperiments on different datasets reveal that despite the low average quality\nof PGIs, the proposed workflow can effectively extract valuable information to\nguide the network training, even with severely reduced training data and\nsampling steps. Our method outperforms other cutting-edge techniques by\neffectively mitigating hallucination artifacts, yielding robust and\nhigh-quality reconstruction results.\n","authors":["Xiaoyu Qiao","Weisheng Li","Bin Xiao","Yuping Huang","Lijian Yang"],"pdf_url":"https://arxiv.org/pdf/2405.02958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00334v3","updated":"2024-07-15T11:51:36Z","published":"2023-04-01T15:10:02Z","title":"TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking\n  Styles","summary":"  Audio-driven talking head generation has drawn growing attention. To produce\ntalking head videos with desired facial expressions, previous methods rely on\nextra reference videos to provide expression information, which may be\ndifficult to find and hence limits their usage. In this work, we propose\nTalkCLIP, a framework that can generate talking heads where the expressions are\nspecified by natural language, hence allowing for specifying expressions more\nconveniently. To model the mapping from text to expressions, we first construct\na text-video paired talking head dataset where each video has diverse text\ndescriptions that depict both coarse-grained emotions and fine-grained facial\nmovements. Leveraging the proposed dataset, we introduce a CLIP-based style\nencoder that projects natural language-based descriptions to the\nrepresentations of expressions. TalkCLIP can even infer expressions for\ndescriptions unseen during training. TalkCLIP can also use text to modulate\nexpression intensity and edit expressions. Extensive experiments demonstrate\nthat TalkCLIP achieves the advanced capability of generating photo-realistic\ntalking heads with vivid facial expressions guided by text descriptions.\n","authors":["Yifeng Ma","Suzhen Wang","Yu Ding","Lincheng Li","Bowen Ma","Tangjie Lv","Changjie Fan","Zhipeng Hu","Zhidong Deng","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2304.00334v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10636v1","updated":"2024-07-15T11:48:57Z","published":"2024-07-15T11:48:57Z","title":"Temporal Residual Guided Diffusion Framework for Event-Driven Video\n  Reconstruction","summary":"  Event-based video reconstruction has garnered increasing attention due to its\nadvantages, such as high dynamic range and rapid motion capture capabilities.\nHowever, current methods often prioritize the extraction of temporal\ninformation from continuous event flow, leading to an overemphasis on\nlow-frequency texture features in the scene, resulting in over-smoothing and\nblurry artifacts. Addressing this challenge necessitates the integration of\nconditional information, encompassing temporal features, low-frequency texture,\nand high-frequency events, to guide the Denoising Diffusion Probabilistic Model\n(DDPM) in producing accurate and natural outputs. To tackle this issue, we\nintroduce a novel approach, the Temporal Residual Guided Diffusion Framework,\nwhich effectively leverages both temporal and frequency-based event priors. Our\nframework incorporates three key conditioning modules: a pre-trained\nlow-frequency intensity estimation module, a temporal recurrent encoder module,\nand an attention-based high-frequency prior enhancement module. In order to\ncapture temporal scene variations from the events at the current moment, we\nemploy a temporal-domain residual image as the target for the diffusion model.\nThrough the combination of these three conditioning paths and the temporal\nresidual framework, our framework excels in reconstructing high-quality videos\nfrom event flow, mitigating issues such as artifacts and over-smoothing\ncommonly observed in previous approaches. Extensive experiments conducted on\nmultiple benchmark datasets validate the superior performance of our framework\ncompared to prior event-based reconstruction methods.\n","authors":["Lin Zhu","Yunlong Zheng","Yijun Zhang","Xiao Wang","Lizhi Wang","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2407.10636v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.10632v1","updated":"2024-07-15T11:36:22Z","published":"2024-07-15T11:36:22Z","title":"Bidirectional Stereo Image Compression with Cross-Dimensional Entropy\n  Model","summary":"  With the rapid advancement of stereo vision technologies, stereo image\ncompression has emerged as a crucial field that continues to draw significant\nattention. Previous approaches have primarily employed a unidirectional\nparadigm, where the compression of one view is dependent on the other,\nresulting in imbalanced compression. To address this issue, we introduce a\nsymmetric bidirectional stereo image compression architecture, named BiSIC.\nSpecifically, we propose a 3D convolution based codec backbone to capture local\nfeatures and incorporate bidirectional attention blocks to exploit global\nfeatures. Moreover, we design a novel cross-dimensional entropy model that\nintegrates various conditioning factors, including the spatial context, channel\ncontext, and stereo dependency, to effectively estimate the distribution of\nlatent representations for entropy coding. Extensive experiments demonstrate\nthat our proposed BiSIC outperforms conventional image/video compression\nstandards, as well as state-of-the-art learning-based methods, in terms of both\nPSNR and MS-SSIM.\n","authors":["Zhening Liu","Xinjie Zhang","Jiawei Shao","Zehong Lin","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.10632v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10630v1","updated":"2024-07-15T11:30:40Z","published":"2024-07-15T11:30:40Z","title":"Brain Tumor Classification From MRI Images Using Machine Learning","summary":"  Brain tumor is a life-threatening problem and hampers the normal functioning\nof the human body. The average five-year relative survival rate for malignant\nbrain tumors is 35.6 percent. For proper diagnosis and efficient treatment\nplanning, it is necessary to detect the brain tumor in early stages. Due to\nadvancement in medical imaging technology, the brain images are taken in\ndifferent modalities. The ability to extract relevant characteristics from\nmagnetic resonance imaging (MRI) scans is a crucial step for brain tumor\nclassifiers. Several studies have proposed various strategies to extract\nrelevant features from different modalities of MRI to predict the growth of\nabnormal tumors. Most techniques used conventional methods of image processing\nfor feature extraction and machine learning for classification. More recently,\nthe use of deep learning algorithms in medical imaging has resulted in\nsignificant improvements in the classification and diagnosis of brain tumors.\nSince tumors are located at different regions of the brain, localizing the\ntumor and classifying it to a particular category is a challenging task. The\nobjective of this project is to develop a predictive system for brain tumor\ndetection using machine learning(ensembling).\n","authors":["Vidhyapriya Ranganathan","Celshiya Udaiyar","Jaisree Jayanth","Meghaa P V","Srija B","Uthra S"],"pdf_url":"https://arxiv.org/pdf/2407.10630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10625v1","updated":"2024-07-15T11:21:03Z","published":"2024-07-15T11:21:03Z","title":"WildVidFit: Video Virtual Try-On in the Wild via Image-Based Controlled\n  Diffusion Models","summary":"  Video virtual try-on aims to generate realistic sequences that maintain\ngarment identity and adapt to a person's pose and body shape in source videos.\nTraditional image-based methods, relying on warping and blending, struggle with\ncomplex human movements and occlusions, limiting their effectiveness in video\ntry-on applications. Moreover, video-based models require extensive,\nhigh-quality data and substantial computational resources. To tackle these\nissues, we reconceptualize video try-on as a process of generating videos\nconditioned on garment descriptions and human motion. Our solution, WildVidFit,\nemploys image-based controlled diffusion models for a streamlined, one-stage\napproach. This model, conditioned on specific garments and individuals, is\ntrained on still images rather than videos. It leverages diffusion guidance\nfrom pre-trained models including a video masked autoencoder for segment\nsmoothness improvement and a self-supervised model for feature alignment of\nadjacent frame in the latent space. This integration markedly boosts the\nmodel's ability to maintain temporal coherence, enabling more effective video\ntry-on within an image-based framework. Our experiments on the VITON-HD and\nDressCode datasets, along with tests on the VVT and TikTok datasets,\ndemonstrate WildVidFit's capability to generate fluid and coherent videos. The\nproject page website is at wildvidfit-project.github.io.\n","authors":["Zijian He","Peixin Chen","Guangrun Wang","Guanbin Li","Philip H. S. Torr","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2407.10625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02094v2","updated":"2024-07-15T10:59:38Z","published":"2024-01-04T06:46:19Z","title":"PILoRA: Prototype Guided Incremental LoRA for Federated\n  Class-Incremental Learning","summary":"  Existing federated learning methods have effectively dealt with decentralized\nlearning in scenarios involving data privacy and non-IID data. However, in\nreal-world situations, each client dynamically learns new classes, requiring\nthe global model to classify all seen classes. To effectively mitigate\ncatastrophic forgetting and data heterogeneity under low communication costs,\nwe propose a simple and effective method named PILoRA. On the one hand, we\nadopt prototype learning to learn better feature representations and leverage\nthe heuristic information between prototypes and class features to design a\nprototype re-weight module to solve the classifier bias caused by data\nheterogeneity without retraining the classifier. On the other hand, we view\nincremental learning as the process of learning distinct task vectors and\nencoding them within different LoRA parameters. Accordingly, we propose\nIncremental LoRA to mitigate catastrophic forgetting. Experimental results on\nstandard datasets indicate that our method outperforms the state-of-the-art\napproaches significantly. More importantly, our method exhibits strong\nrobustness and superiority in different settings and degrees of data\nheterogeneity. The code is available at\n\\url{https://github.com/Ghy0501/PILoRA}.\n","authors":["Haiyang Guo","Fei Zhu","Wenzhuo Liu","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2401.02094v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10596v1","updated":"2024-07-15T10:20:00Z","published":"2024-07-15T10:20:00Z","title":"An evaluation of CNN models and data augmentation techniques in\n  hierarchical localization of mobile robots","summary":"  This work presents an evaluation of CNN models and data augmentation to carry\nout the hierarchical localization of a mobile robot by using omnidireccional\nimages. In this sense, an ablation study of different state-of-the-art CNN\nmodels used as backbone is presented and a variety of data augmentation visual\neffects are proposed for addressing the visual localization of the robot. The\nproposed method is based on the adaption and re-training of a CNN with a dual\npurpose: (1) to perform a rough localization step in which the model is used to\npredict the room from which an image was captured, and (2) to address the fine\nlocalization step, which consists in retrieving the most similar image of the\nvisual map among those contained in the previously predicted room by means of a\npairwise comparison between descriptors obtained from an intermediate layer of\nthe CNN. In this sense, we evaluate the impact of different state-of-the-art\nCNN models such as ConvNeXt for addressing the proposed localization. Finally,\na variety of data augmentation visual effects are separately employed for\ntraining the model and their impact is assessed. The performance of the\nresulting CNNs is evaluated under real operation conditions, including changes\nin the lighting conditions. Our code is publicly available on the project\nwebsite https://github.com/juanjo-cabrera/IndoorLocalizationSingleCNN.git\n","authors":["J. J. Cabrera","O. J. CÃ©spedes","S. Cebollada","O. Reinoso","L. PayÃ¡"],"pdf_url":"https://arxiv.org/pdf/2407.10596v1.pdf","comment":"Published Evolving Systems (2024): 08 July 2024 PDF link:\n  https://link.springer.com/content/pdf/10.1007/s12530-024-09604-6.pdf"},{"id":"http://arxiv.org/abs/2407.10592v1","updated":"2024-07-15T10:15:58Z","published":"2024-07-15T10:15:58Z","title":"InsertDiffusion: Identity Preserving Visualization of Objects through a\n  Training-Free Diffusion Architecture","summary":"  Recent advancements in image synthesis are fueled by the advent of\nlarge-scale diffusion models. Yet, integrating realistic object visualizations\nseamlessly into new or existing backgrounds without extensive training remains\na challenge. This paper introduces InsertDiffusion, a novel, training-free\ndiffusion architecture that efficiently embeds objects into images while\npreserving their structural and identity characteristics. Our approach utilizes\noff-the-shelf generative models and eliminates the need for fine-tuning, making\nit ideal for rapid and adaptable visualizations in product design and\nmarketing. We demonstrate superior performance over existing methods in terms\nof image realism and alignment with input conditions. By decomposing the\ngeneration task into independent steps, InsertDiffusion offers a scalable\nsolution that extends the capabilities of diffusion models for practical\napplications, achieving high-quality visualizations that maintain the\nauthenticity of the original objects.\n","authors":["Phillip Mueller","Jannik Wiese","Ioan Craciun","Lars Mikelsons"],"pdf_url":"https://arxiv.org/pdf/2407.10592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18387v2","updated":"2024-07-15T10:15:56Z","published":"2024-06-26T14:29:05Z","title":"DoubleTake: Geometry Guided Depth Estimation","summary":"  Estimating depth from a sequence of posed RGB images is a fundamental\ncomputer vision task, with applications in augmented reality, path planning\netc. Prior work typically makes use of previous frames in a multi view stereo\nframework, relying on matching textures in a local neighborhood. In contrast,\nour model leverages historical predictions by giving the latest 3D geometry\ndata as an extra input to our network. This self-generated geometric hint can\nencode information from areas of the scene not covered by the keyframes and it\nis more regularized when compared to individual predicted depth maps for\nprevious frames. We introduce a Hint MLP which combines cost volume features\nwith a hint of the prior geometry, rendered as a depth map from the current\ncamera location, together with a measure of the confidence in the prior\ngeometry. We demonstrate that our method, which can run at interactive speeds,\nachieves state-of-the-art estimates of depth and 3D scene reconstruction in\nboth offline and incremental evaluation scenarios.\n","authors":["Mohamed Sayed","Filippo Aleotti","Jamie Watson","Zawar Qureshi","Guillermo Garcia-Hernando","Gabriel Brostow","Sara Vicente","Michael Firman"],"pdf_url":"https://arxiv.org/pdf/2406.18387v2.pdf","comment":"ECCV 2024 Version"},{"id":"http://arxiv.org/abs/2407.10590v1","updated":"2024-07-15T10:14:30Z","published":"2024-07-15T10:14:30Z","title":"Deep-Learning-Based Markerless Pose Estimation Systems in Gait Analysis:\n  DeepLabCut Custom Training and the Refinement Function","summary":"  The current gold standard for the study of human movement is the marker-based\nmotion capture system that offers high precision but constrained by costs and\ncontrolled environments. Markerless pose estimation systems emerge as\necological alternatives, allowing unobtrusive data acquisition in natural\nsettings. This study compares the performance of two popular markerless\nsystems, OpenPose (OP) and DeepLabCut (DLC), in assessing locomotion. Forty\nhealthy subjects walked along a 5 meters walkway equipped with four force\nplatforms and a camera. Gait parameters were obtained using OP BODY 25\nPre-Trained model (OPPT), DLC Model Zoo full human Pre-Trained model (DLCPT)\nand DLC Custom-Trained model (DLCCT), then compared with those acquired from\nthe force platforms as reference system. Our results demonstrated that DLCCT\noutperformed DLCPT and OPPT, highlighting the importance of leveraging\nDeepLabCut transfer learning to enhance the pose estimation performance with a\ncustom-trained neural networks. Moreover, DLCCT, with the implementation of the\nDLC refinement function, offers the most promising markerless pose estimation\nsolution for evaluating locomotion. Therefore, our data provide insights into\nthe DLC training and refinement processes required to achieve optimal\nperformance. This study offers perspectives for clinicians and practitioners\nseeking accurate low-cost methods for movement assessment beyond laboratory\nsettings.\n","authors":["Giulia Panconi","Stefano Grasso","Sara Guarducci","Lorenzo Mucchi","Diego Minciacchi","Riccardo Bravi"],"pdf_url":"https://arxiv.org/pdf/2407.10590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10586v1","updated":"2024-07-15T10:06:59Z","published":"2024-07-15T10:06:59Z","title":"COSMU: Complete 3D human shape from monocular unconstrained images","summary":"  We present a novel framework to reconstruct complete 3D human shapes from a\ngiven target image by leveraging monocular unconstrained images. The objective\nof this work is to reproduce high-quality details in regions of the\nreconstructed human body that are not visible in the input target. The proposed\nmethodology addresses the limitations of existing approaches for reconstructing\n3D human shapes from a single image, which cannot reproduce shape details in\noccluded body regions. The missing information of the monocular input can be\nrecovered by using multiple views captured from multiple cameras. However,\nmulti-view reconstruction methods necessitate accurately calibrated and\nregistered images, which can be challenging to obtain in real-world scenarios.\nGiven a target RGB image and a collection of multiple uncalibrated and\nunregistered images of the same individual, acquired using a single camera, we\npropose a novel framework to generate complete 3D human shapes. We introduce a\nnovel module to generate 2D multi-view normal maps of the person registered\nwith the target input image. The module consists of body part-based reference\nselection and body part-based registration. The generated 2D normal maps are\nthen processed by a multi-view attention-based neural implicit model that\nestimates an implicit representation of the 3D shape, ensuring the reproduction\nof details in both observed and occluded regions. Extensive experiments\ndemonstrate that the proposed approach estimates higher quality details in the\nnon-visible regions of the 3D clothed human shapes compared to related methods,\nwithout using parametric models.\n","authors":["Marco Pesavento","Marco Volino","Adrian Hilton"],"pdf_url":"https://arxiv.org/pdf/2407.10586v1.pdf","comment":"Accepted to ECCV24"},{"id":"http://arxiv.org/abs/2310.05718v3","updated":"2024-07-15T09:57:48Z","published":"2023-10-09T13:39:26Z","title":"EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational\n  Autoencoders","summary":"  Codebook collapse is a common problem in training deep generative models with\ndiscrete representation spaces like Vector Quantized Variational Autoencoders\n(VQ-VAEs). We observe that the same problem arises for the alternatively\ndesigned discrete variational autoencoders (dVAEs) whose encoder directly\nlearns a distribution over the codebook embeddings to represent the data. We\nhypothesize that using the softmax function to obtain a probability\ndistribution causes the codebook collapse by assigning overconfident\nprobabilities to the best matching codebook elements. In this paper, we propose\na novel way to incorporate evidential deep learning (EDL) instead of softmax to\ncombat the codebook collapse problem of dVAE. We evidentially monitor the\nsignificance of attaining the probability distribution over the codebook\nembeddings, in contrast to softmax usage. Our experiments using various\ndatasets show that our model, called EdVAE, mitigates codebook collapse while\nimproving the reconstruction performance, and enhances the codebook usage\ncompared to dVAE and VQ-VAE based models. Our code can be found at\nhttps://github.com/ituvisionlab/EdVAE .\n","authors":["Gulcin Baykal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2310.05718v3.pdf","comment":"Accepted for publication in Pattern Recognition"},{"id":"http://arxiv.org/abs/2403.11481v2","updated":"2024-07-15T09:54:30Z","published":"2024-03-18T05:07:59Z","title":"VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding","summary":"  We explore how reconciling several foundation models (large language models\nand vision-language models) with a novel unified memory mechanism could tackle\nthe challenging video understanding problem, especially capturing the long-term\ntemporal relations in lengthy videos. In particular, the proposed multimodal\nagent VideoAgent: 1) constructs a structured memory to store both the generic\ntemporal event descriptions and object-centric tracking states of the video; 2)\ngiven an input task query, it employs tools including video segment\nlocalization and object memory querying along with other visual foundation\nmodels to interactively solve the task, utilizing the zero-shot tool-use\nability of LLMs. VideoAgent demonstrates impressive performances on several\nlong-horizon video understanding benchmarks, an average increase of 6.6% on\nNExT-QA and 26.0% on EgoSchema over baselines, closing the gap between\nopen-sourced models and private counterparts including Gemini 1.5 Pro.\n","authors":["Yue Fan","Xiaojian Ma","Rujie Wu","Yuntao Du","Jiaqi Li","Zhi Gao","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.11481v2.pdf","comment":"ECCV-24; Project page: videoagent.github.io; First two authors\n  contributed equally"},{"id":"http://arxiv.org/abs/2311.17609v2","updated":"2024-07-15T09:47:13Z","published":"2023-11-29T13:06:48Z","title":"Curved Diffusion: A Generative Model With Optical Geometry Control","summary":"  State-of-the-art diffusion models can generate highly realistic images based\non various conditioning like text, segmentation, and depth. However, an\nessential aspect often overlooked is the specific camera geometry used during\nimage capture. The influence of different optical systems on the final scene\nappearance is frequently overlooked. This study introduces a framework that\nintimately integrates a text-to-image diffusion model with the particular lens\ngeometry used in image rendering. Our method is based on a per-pixel coordinate\nconditioning method, enabling the control over the rendering geometry. Notably,\nwe demonstrate the manipulation of curvature properties, achieving diverse\nvisual effects, such as fish-eye, panoramic views, and spherical texturing\nusing a single diffusion model.\n","authors":["Andrey Voynov","Amir Hertz","Moab Arar","Shlomi Fruchter","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2311.17609v2.pdf","comment":"Project page at https://anylens-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2407.10575v1","updated":"2024-07-15T09:46:02Z","published":"2024-07-15T09:46:02Z","title":"A Survey of Defenses against AI-generated Visual Media: Detection,\n  Disruption, and Authentication","summary":"  Deep generative models have demonstrated impressive performance in various\ncomputer vision applications, including image synthesis, video generation, and\nmedical analysis. Despite their significant advancements, these models may be\nused for malicious purposes, such as misinformation, deception, and copyright\nviolation. In this paper, we provide a systematic and timely review of research\nefforts on defenses against AI-generated visual media, covering detection,\ndisruption, and authentication. We review existing methods and summarize the\nmainstream defense-related tasks within a unified passive and proactive\nframework. Moreover, we survey the derivative tasks concerning the\ntrustworthiness of defenses, such as their robustness and fairness. For each\ntask, we formulate its general pipeline and propose a taxonomy based on\nmethodological strategies that are uniformly applicable to the primary\nsubtasks. Additionally, we summarize the commonly used evaluation datasets,\ncriteria, and metrics. Finally, by analyzing the reviewed studies, we provide\ninsights into current research challenges and suggest possible directions for\nfuture research.\n","authors":["Jingyi Deng","Chenhao Lin","Zhengyu Zhao","Shuai Liu","Qian Wang","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2407.10575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10574v1","updated":"2024-07-15T09:44:43Z","published":"2024-07-15T09:44:43Z","title":"Stacking-Enhanced Bagging Ensemble Learning for Breast Cancer\n  Classification with CNN","summary":"  This paper proposes a CNN classification network based on Bagging and\nstacking ensemble learning methods for breast cancer classification. The model\nwas trained and tested on the public dataset of DDSM. The model is capable of\nfast and accurate classification of input images. According to our research\nresults, for binary classification (presence or absence of breast cancer), the\naccuracy reached 98.84%, and for five-class classification, the accuracy\nreached 98.34%. The model also achieved a micro-average recall rate of 94.80%\nand an F1 score of 94.19%. In comparative experiments, we compared the effects\nof different values of bagging_ratio and n_models on the model, as well as\nseveral methods for ensemble bagging models. Furthermore, under the same\nparameter settings, our BSECNN outperformed VGG16 and ResNet-50 in terms of\naccuracy by 8.22% and 6.33% respectively.\n","authors":["Peihceng Wu","Runze Ma","Teoh Teik Toe"],"pdf_url":"https://arxiv.org/pdf/2407.10574v1.pdf","comment":"Published in: 2023 3rd International Conference on Electronic\n  Engineering (ICEEM)"},{"id":"http://arxiv.org/abs/2312.08874v3","updated":"2024-07-15T09:42:48Z","published":"2023-12-14T16:26:29Z","title":"Agent Attention: On the Integration of Softmax and Linear Attention","summary":"  The attention module is the key component in Transformers. While the global\nattention mechanism offers high expressiveness, its excessive computational\ncost restricts its applicability in various scenarios. In this paper, we\npropose a novel attention paradigm, Agent Attention, to strike a favorable\nbalance between computational efficiency and representation power.\nSpecifically, the Agent Attention, denoted as a quadruple $(Q, A, K, V)$,\nintroduces an additional set of agent tokens $A$ into the conventional\nattention module. The agent tokens first act as the agent for the query tokens\n$Q$ to aggregate information from $K$ and $V$, and then broadcast the\ninformation back to $Q$. Given the number of agent tokens can be designed to be\nmuch smaller than the number of query tokens, the agent attention is\nsignificantly more efficient than the widely adopted Softmax attention, while\npreserving global context modelling capability. Interestingly, we show that the\nproposed agent attention is equivalent to a generalized form of linear\nattention. Therefore, agent attention seamlessly integrates the powerful\nSoftmax attention and the highly efficient linear attention. Extensive\nexperiments demonstrate the effectiveness of agent attention with various\nvision Transformers and across diverse vision tasks, including image\nclassification, object detection, semantic segmentation and image generation.\nNotably, agent attention has shown remarkable performance in high-resolution\nscenarios, owning to its linear attention nature. For instance, when applied to\nStable Diffusion, our agent attention accelerates generation and substantially\nenhances image generation quality without any additional training. Code is\navailable at https://github.com/LeapLabTHU/Agent-Attention.\n","authors":["Dongchen Han","Tianzhu Ye","Yizeng Han","Zhuofan Xia","Siyuan Pan","Pengfei Wan","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2312.08874v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2304.05645v3","updated":"2024-07-15T09:42:13Z","published":"2023-04-12T06:48:26Z","title":"WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with\n  Multi-modal Visual Data and Natural Language","summary":"  We introduce the task of 3D visual grounding in large-scale dynamic scenes\nbased on natural linguistic descriptions and online captured multi-modal visual\ndata, including 2D images and 3D LiDAR point clouds. We present a novel method,\ndubbed WildRefer, for this task by fully utilizing the rich appearance\ninformation in images, the position and geometric clues in point cloud as well\nas the semantic knowledge of language descriptions. Besides, we propose two\nnovel datasets, i.e., STRefer and LifeRefer, which focus on large-scale\nhuman-centric daily-life scenarios accompanied with abundant 3D object and\nnatural language annotations. Our datasets are significant for the research of\n3D visual grounding in the wild and has huge potential to boost the development\nof autonomous driving and service robots. Extensive experiments and ablation\nstudies demonstrate that our method achieves state-of-the-art performance on\nthe proposed benchmarks. The code is provided in\nhttps://github.com/4DVLab/WildRefer.\n","authors":["Zhenxiang Lin","Xidong Peng","Peishan Cong","Ge Zheng","Yujin Sun","Yuenan Hou","Xinge Zhu","Sibei Yang","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2304.05645v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02281v2","updated":"2024-07-15T09:41:16Z","published":"2024-01-04T13:58:14Z","title":"PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for\n  6DoF Object Pose Dataset Generation","summary":"  We introduce Physically Enhanced Gaussian Splatting Simulation System\n(PEGASUS) for 6DOF object pose dataset generation, a versatile dataset\ngenerator based on 3D Gaussian Splatting.\n  Environment and object representations can be easily obtained using commodity\ncameras to reconstruct with Gaussian Splatting. <i>PEGASUS</i> allows the\ncomposition of new scenes by merging the respective underlying Gaussian\nSplatting point cloud of an environment with one or multiple objects.\nLeveraging a physics engine enables the simulation of natural object placement\nwithin a scene through interaction between meshes extracted for the objects and\nthe environment. Consequently, an extensive amount of new scenes - static or\ndynamic - can be created by combining different environments and objects. By\nrendering scenes from various perspectives, diverse data points such as RGB\nimages, depth maps, semantic masks, and 6DoF object poses can be extracted.\n  Our study demonstrates that training on data generated by PEGASUS enables\npose estimation networks to successfully transfer from synthetic data to\nreal-world data. Moreover, we introduce the Ramen dataset, comprising 30\nJapanese cup noodle items. This dataset includes spherical scans that captures\nimages from both object hemisphere and the Gaussian Splatting reconstruction,\nmaking them compatible with PEGASUS.\n","authors":["Lukas Meyer","Floris Erich","Yusuke Yoshiyasu","Marc Stamminger","Noriaki Ando","Yukiyasu Domae"],"pdf_url":"https://arxiv.org/pdf/2401.02281v2.pdf","comment":"Project Page: https://meyerls.github.io/pegasus_web"},{"id":"http://arxiv.org/abs/2407.01295v4","updated":"2024-07-15T09:41:06Z","published":"2024-07-01T13:47:54Z","title":"Formal Verification of Object Detection","summary":"  Deep Neural Networks (DNNs) are ubiquitous in real-world applications, yet\nthey remain vulnerable to errors and adversarial attacks. This work tackles the\nchallenge of applying formal verification to ensure the safety of computer\nvision models, extending verification beyond image classification to object\ndetection. We propose a general formulation for certifying the robustness of\nobject detection models using formal verification and outline implementation\nstrategies compatible with state-of-the-art verification tools. Our approach\nenables the application of these tools, originally designed for verifying\nclassification models, to object detection. We define various attacks for\nobject detection, illustrating the diverse ways adversarial inputs can\ncompromise neural network outputs. Our experiments, conducted on several common\ndatasets and networks, reveal potential errors in object detection models,\nhighlighting system vulnerabilities and emphasizing the need for expanding\nformal verification to these new domains. This work paves the way for further\nresearch in integrating formal verification across a broader range of computer\nvision applications.\n","authors":["Avraham Raviv","Yizhak Y. Elboher","Michelle Aluf-Medina","Yael Leibovich Weiss","Omer Cohen","Roy Assa","Guy Katz","Hillel Kugler"],"pdf_url":"https://arxiv.org/pdf/2407.01295v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10690v4","updated":"2024-07-15T09:31:54Z","published":"2024-05-17T10:51:15Z","title":"CoLeaF: A Contrastive-Collaborative Learning Framework for Weakly\n  Supervised Audio-Visual Video Parsing","summary":"  Weakly supervised audio-visual video parsing (AVVP) methods aim to detect\naudible-only, visible-only, and audible-visible events using only video-level\nlabels. Existing approaches tackle this by leveraging unimodal and cross-modal\ncontexts. However, we argue that while cross-modal learning is beneficial for\ndetecting audible-visible events, in the weakly supervised scenario, it\nnegatively impacts unaligned audible or visible events by introducing\nirrelevant modality information. In this paper, we propose CoLeaF, a novel\nlearning framework that optimizes the integration of cross-modal context in the\nembedding space such that the network explicitly learns to combine cross-modal\ninformation for audible-visible events while filtering them out for unaligned\nevents. Additionally, as videos often involve complex class relationships,\nmodelling them improves performance. However, this introduces extra\ncomputational costs into the network. Our framework is designed to leverage\ncross-class relationships during training without incurring additional\ncomputations at inference. Furthermore, we propose new metrics to better\nevaluate a method's capabilities in performing AVVP. Our extensive experiments\ndemonstrate that CoLeaF significantly improves the state-of-the-art results by\nan average of 1.9% and 2.4% F-score on the LLP and UnAV-100 datasets,\nrespectively.\n","authors":["Faegheh Sardari","Armin Mustafa","Philip J. B. Jackson","Adrian Hilton"],"pdf_url":"https://arxiv.org/pdf/2405.10690v4.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10567v1","updated":"2024-07-15T09:30:31Z","published":"2024-07-15T09:30:31Z","title":"PULPo: Probabilistic Unsupervised Laplacian Pyramid Registration","summary":"  Deformable image registration is fundamental to many medical imaging\napplications. Registration is an inherently ambiguous task often admitting many\nviable solutions. While neural network-based registration techniques enable\nfast and accurate registration, the majority of existing approaches are not\nable to estimate uncertainty. Here, we present PULPo, a method for\nprobabilistic deformable registration capable of uncertainty quantification.\nPULPo probabilistically models the distribution of deformation fields on\ndifferent hierarchical levels combining them using Laplacian pyramids. This\nallows our method to model global as well as local aspects of the deformation\nfield. We evaluate our method on two widely used neuroimaging datasets and find\nthat it achieves high registration performance as well as substantially better\ncalibrated uncertainty quantification compared to the current state-of-the-art.\n","authors":["Leonard Siegert","Paul Fischer","Mattias P. Heinrich","Christian F. Baumgartner"],"pdf_url":"https://arxiv.org/pdf/2407.10567v1.pdf","comment":"Accepted as full paper to MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.10563v1","updated":"2024-07-15T09:24:27Z","published":"2024-07-15T09:24:27Z","title":"Pathformer3D: A 3D Scanpath Transformer for 360Â° Images","summary":"  Scanpath prediction in 360{\\deg} images can help realize rapid rendering and\nbetter user interaction in Virtual/Augmented Reality applications. However,\nexisting scanpath prediction models for 360{\\deg} images execute scanpath\nprediction on 2D equirectangular projection plane, which always result in big\ncomputation error owing to the 2D plane's distortion and coordinate\ndiscontinuity. In this work, we perform scanpath prediction for 360{\\deg}\nimages in 3D spherical coordinate system and proposed a novel 3D scanpath\nTransformer named Pathformer3D. Specifically, a 3D Transformer encoder is first\nused to extract 3D contextual feature representation for the 360{\\deg} image.\nThen, the contextual feature representation and historical fixation information\nare input into a Transformer decoder to output current time step's fixation\nembedding, where the self-attention module is used to imitate the visual\nworking memory mechanism of human visual system and directly model the time\ndependencies among the fixations. Finally, a 3D Gaussian distribution is\nlearned from each fixation embedding, from which the fixation position can be\nsampled. Evaluation on four panoramic eye-tracking datasets demonstrates that\nPathformer3D outperforms the current state-of-the-art methods. Code is\navailable at https://github.com/lsztzp/Pathformer3D .\n","authors":["Rong Quan","Yantao Lai","Mengyu Qiu","Dong Liang"],"pdf_url":"https://arxiv.org/pdf/2407.10563v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2402.01054v2","updated":"2024-07-15T09:22:45Z","published":"2024-02-01T22:58:21Z","title":"Unconditional Latent Diffusion Models Memorize Patient Imaging Data:\n  Implications for Openly Sharing Synthetic Data","summary":"  AI models present a wide range of applications in the field of medicine.\nHowever, achieving optimal performance requires access to extensive healthcare\ndata, which is often not readily available. Furthermore, the imperative to\npreserve patient privacy restricts patient data sharing with third parties and\neven within institutes. Recently, generative AI models have been gaining\ntraction for facilitating open-data sharing by proposing synthetic data as\nsurrogates of real patient data. Despite the promise, these models are\nsusceptible to patient data memorization, where models generate patient data\ncopies instead of novel synthetic samples. Considering the importance of the\nproblem, it has received little attention in the medical imaging community. To\nthis end, we assess memorization in unconditional latent diffusion models. We\ntrain 2D and 3D latent diffusion models on CT, MR, and X-ray datasets for\nsynthetic data generation. Afterwards, we detect the amount of training data\nmemorized utilizing our self-supervised approach and further investigate\nvarious factors that can influence memorization. Our findings show a\nsurprisingly high degree of patient data memorization across all datasets, with\napproximately 40.9% of patient data being memorized and 78.5% of synthetic\nsamples identified as patient data copies on average in our experiments.\nFurther analyses reveal that using augmentation strategies during training can\nreduce memorization while over-training the models can enhance it. Although\nincreasing the dataset size does not reduce memorization and might even enhance\nit, it does lower the probability of a synthetic sample being a patient data\ncopy. Collectively, our results emphasize the importance of carefully training\ngenerative models on private medical imaging datasets, and examining the\nsynthetic data to ensure patient privacy before sharing it for medical research\nand applications.\n","authors":["Salman Ul Hassan Dar","Marvin Seyfarth","Jannik Kahmann","Isabelle Ayx","Theano Papavassiliu","Stefan O. Schoenberg","Norbert Frey","Bettina BaeÃler","Sebastian Foersch","Daniel Truhn","Jakob Nikolas Kather","Sandy Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2402.01054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10559v1","updated":"2024-07-15T09:16:54Z","published":"2024-07-15T09:16:54Z","title":"LIP-CAR: contrast agent reduction by a deep learned inverse problem","summary":"  The adoption of contrast agents in medical imaging protocols is crucial for\naccurate and timely diagnosis. While highly effective and characterized by an\nexcellent safety profile, the use of contrast agents has its limitation,\nincluding rare risk of allergic reactions, potential environmental impact and\neconomic burdens on patients and healthcare systems. In this work, we address\nthe contrast agent reduction (CAR) problem, which involves reducing the\nadministered dosage of contrast agent while preserving the visual enhancement.\nThe current literature on the CAR task is based on deep learning techniques\nwithin a fully image processing framework. These techniques digitally simulate\nhigh-dose images from images acquired with a low dose of contrast agent. We\ninvestigate the feasibility of a ``learned inverse problem'' (LIP) approach, as\nopposed to the end-to-end paradigm in the state-of-the-art literature.\n  Specifically, we learn the image-to-image operator that maps high-dose images\nto their corresponding low-dose counterparts, and we frame the CAR task as an\ninverse problem. We then solve this problem through a regularized optimization\nreformulation. Regularization methods are well-established mathematical\ntechniques that offer robustness and explainability. Our approach combines\nthese rigorous techniques with cutting-edge deep learning tools. Numerical\nexperiments performed on pre-clinical medical images confirm the effectiveness\nof this strategy, showing improved stability and accuracy in the simulated\nhigh-dose images.\n","authors":["Davide Bianchi","Sonia Colombo Serra","Davide Evangelista","Pengpeng Luo","Elena Morotti","Giovanni Valbusa"],"pdf_url":"https://arxiv.org/pdf/2407.10559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10558v1","updated":"2024-07-15T09:15:55Z","published":"2024-07-15T09:15:55Z","title":"ConTEXTure: Consistent Multiview Images to Texture","summary":"  We introduce ConTEXTure, a generative network designed to create a texture\nmap/atlas for a given 3D mesh using images from multiple viewpoints. The\nprocess begins with generating a front-view image from a text prompt, such as\n'Napoleon, front view', describing the 3D mesh. Additional images from\ndifferent viewpoints are derived from this front-view image and camera poses\nrelative to it. ConTEXTure builds upon the TEXTure network, which uses text\nprompts for six viewpoints (e.g., 'Napoleon, front view', 'Napoleon, left\nview', etc.). However, TEXTure often generates images for non-front viewpoints\nthat do not accurately represent those viewpoints.To address this issue, we\nemploy Zero123++, which generates multiple view-consistent images for the six\nspecified viewpoints simultaneously, conditioned on the initial front-view\nimage and the depth maps of the mesh for the six viewpoints. By utilizing these\nview-consistent images, ConTEXTure learns the texture atlas from all viewpoint\nimages concurrently, unlike previous methods that do so sequentially. This\napproach ensures that the rendered images from various viewpoints, including\nback, side, bottom, and top, are free from viewpoint irregularities.\n","authors":["Jaehoon Ahn","Sumin Cho","Harim Jung","Kibeom Hong","Seonghoon Ban","Moon-Ryul Jung"],"pdf_url":"https://arxiv.org/pdf/2407.10558v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.17695v2","updated":"2024-07-15T09:11:19Z","published":"2024-01-31T09:31:28Z","title":"Datacube segmentation via Deep Spectral Clustering","summary":"  Extended Vision techniques are ubiquitous in physics. However, the data cubes\nsteaming from such analysis often pose a challenge in their interpretation, due\nto the intrinsic difficulty in discerning the relevant information from the\nspectra composing the data cube.\n  Furthermore, the huge dimensionality of data cube spectra poses a complex\ntask in its statistical interpretation; nevertheless, this complexity contains\na massive amount of statistical information that can be exploited in an\nunsupervised manner to outline some essential properties of the case study at\nhand, e.g.~it is possible to obtain an image segmentation via (deep) clustering\nof data-cube's spectra, performed in a suitably defined low-dimensional\nembedding space.\n  To tackle this topic, we explore the possibility of applying unsupervised\nclustering methods in encoded space, i.e. perform deep clustering on the\nspectral properties of datacube pixels. A statistical dimensional reduction is\nperformed by an ad hoc trained (Variational) AutoEncoder, in charge of mapping\nspectra into lower dimensional metric spaces, while the clustering process is\nperformed by a (learnable) iterative K-Means clustering algorithm.\n  We apply this technique to two different use cases, of different physical\norigins: a set of Macro mapping X-Ray Fluorescence (MA-XRF) synthetic data on\npictorial artworks, and a dataset of simulated astrophysical observations.\n","authors":["Alessandro Bombini","Fernando GarcÃ­a-Avello BofÃ­as","Caterina Bracci","Michele Ginolfi","Chiara Ruberto"],"pdf_url":"https://arxiv.org/pdf/2401.17695v2.pdf","comment":"20 pages, 10 figures, doi for code repository, dataset and trained\n  model available and reported in the paper. v2: paper accepted for publication\n  on IOP Machine Learning: Science and Technology"},{"id":"http://arxiv.org/abs/2407.10550v1","updated":"2024-07-15T09:00:02Z","published":"2024-07-15T09:00:02Z","title":"Learning Natural Consistency Representation for Face Forgery Video\n  Detection","summary":"  Face Forgery videos have elicited critical social public concerns and various\ndetectors have been proposed. However, fully-supervised detectors may lead to\neasily overfitting to specific forgery methods or videos, and existing\nself-supervised detectors are strict on auxiliary tasks, such as requiring\naudio or multi-modalities, leading to limited generalization and robustness. In\nthis paper, we examine whether we can address this issue by leveraging\nvisual-only real face videos. To this end, we propose to learn the Natural\nConsistency representation (NACO) of real face videos in a self-supervised\nmanner, which is inspired by the observation that fake videos struggle to\nmaintain the natural spatiotemporal consistency even under unknown forgery\nmethods and different perturbations. Our NACO first extracts spatial features\nof each frame by CNNs then integrates them into Transformer to learn the\nlong-range spatiotemporal representation, leveraging the advantages of CNNs and\nTransformer on local spatial receptive field and long-term memory respectively.\nFurthermore, a Spatial Predictive Module~(SPM) and a Temporal Contrastive\nModule~(TCM) are introduced to enhance the natural consistency representation\nlearning. The SPM aims to predict random masked spatial features from\nspatiotemporal representation, and the TCM regularizes the latent distance of\nspatiotemporal representation by shuffling the natural order to disturb the\nconsistency, which could both force our NACO more sensitive to the natural\nspatiotemporal consistency. After the representation learning stage, a MLP head\nis fine-tuned to perform the usual forgery video classification task. Extensive\nexperiments show that our method outperforms other state-of-the-art competitors\nwith impressive generalization and robustness.\n","authors":["Daichi Zhang","Zihao Xiao","Shikun Li","Fanzhao Lin","Jianmin Li","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2407.10550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03239v2","updated":"2024-07-15T08:53:44Z","published":"2024-07-03T16:09:59Z","title":"Solving the inverse problem of microscopy deconvolution with a residual\n  Beylkin-Coifman-Rokhlin neural network","summary":"  Optic deconvolution in light microscopy (LM) refers to recovering the object\ndetails from images, revealing the ground truth of samples. Traditional\nexplicit methods in LM rely on the point spread function (PSF) during image\nacquisition. Yet, these approaches often fall short due to inaccurate PSF\nmodels and noise artifacts, hampering the overall restoration quality. In this\npaper, we approached the optic deconvolution as an inverse problem. Motivated\nby the nonstandard-form compression scheme introduced by Beylkin, Coifman, and\nRokhlin (BCR), we proposed an innovative physics-informed neural network\nMulti-Stage Residual-BCR Net (m-rBCR) to approximate the optic deconvolution.\nWe validated the m-rBCR model on four microscopy datasets - two simulated\nmicroscopy datasets from ImageNet and BioSR, real dSTORM microscopy images, and\nreal widefield microscopy images. In contrast to the explicit deconvolution\nmethods (e.g. Richardson-Lucy) and other state-of-the-art NN models (U-Net,\nDDPM, CARE, DnCNN, ESRGAN, RCAN, Noise2Noise, MPRNet, and MIMO-U-Net), the\nm-rBCR model demonstrates superior performance to other candidates by PSNR and\nSSIM in two real microscopy datasets and the simulated BioSR dataset. In the\nsimulated ImageNet dataset, m-rBCR ranks the second-best place (right after\nMIMO-U-Net). With the backbone from the optical physics, m-rBCR exploits the\ntrainable parameters with better performances (from ~30 times fewer than the\nbenchmark MIMO-U-Net to ~210 times than ESRGAN). This enables m-rBCR to achieve\na shorter runtime (from ~3 times faster than MIMO-U-Net to ~300 times faster\nthan DDPM). To summarize, by leveraging physics constraints our model reduced\npotentially redundant parameters significantly in expertise-oriented NN\ncandidates and achieved high efficiency with superior performance.\n","authors":["Rui Li","Mikhail Kudryashev","Artur Yakimovich"],"pdf_url":"https://arxiv.org/pdf/2407.03239v2.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.10545v1","updated":"2024-07-15T08:52:20Z","published":"2024-07-15T08:52:20Z","title":"Efficient Continual Learning with Low Memory Footprint For Edge Device","summary":"  Continual learning(CL) is a useful technique to acquire dynamic knowledge\ncontinually. Although powerful cloud platforms can fully exert the ability of\nCL,e.g., customized recommendation systems, similar personalized requirements\nfor edge devices are almost disregarded. This phenomenon stems from the huge\nresource overhead involved in training neural networks and overcoming the\nforgetting problem of CL. This paper focuses on these scenarios and proposes a\ncompact algorithm called LightCL. Different from other CL methods bringing huge\nresource consumption to acquire generalizability among all tasks for delaying\nforgetting, LightCL compress the resource consumption of already generalized\ncomponents in neural networks and uses a few extra resources to improve memory\nin other parts. We first propose two new metrics of learning plasticity and\nmemory stability to seek generalizability during CL. Based on the discovery\nthat lower and middle layers have more generalizability and deeper layers are\nopposite, we $\\textit{Maintain Generalizability}$ by freezing the lower and\nmiddle layers. Then, we $\\textit{Memorize Feature Patterns}$ to stabilize the\nfeature extracting patterns of previous tasks to improve generalizability in\ndeeper layers. In the experimental comparison, LightCL outperforms other SOTA\nmethods in delaying forgetting and reduces at most $\\textbf{6.16$\\times$}$\nmemory footprint, proving the excellent performance of LightCL in efficiency.\nWe also evaluate the efficiency of our method on an edge device, the Jetson\nNano, which further proves our method's practical effectiveness.\n","authors":["Zeqing Wang","Fei Cheng","Kangye Ji","Bohu Huang"],"pdf_url":"https://arxiv.org/pdf/2407.10545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17338v3","updated":"2024-07-15T08:51:43Z","published":"2023-11-29T03:36:07Z","title":"MagDiff: Multi-Alignment Diffusion for High-Fidelity Video Generation\n  and Editing","summary":"  The diffusion model is widely leveraged for either video generation or video\nediting. As each field has its task-specific problems, it is difficult to\nmerely develop a single diffusion for completing both tasks simultaneously.\nVideo diffusion sorely relying on the text prompt can be adapted to unify the\ntwo tasks. However, it lacks a high capability of aligning heterogeneous\nmodalities between text and image, leading to various misalignment problems. In\nthis work, we are the first to propose a unified Multi-alignment Diffusion,\ndubbed as MagDiff, for both tasks of high-fidelity video generation and\nediting. The proposed MagDiff introduces three types of alignments, including\nsubject-driven alignment, adaptive prompts alignment, and high-fidelity\nalignment. Particularly, the subject-driven alignment is put forward to trade\noff the image and text prompts, serving as a unified foundation generative\nmodel for both tasks. The adaptive prompts alignment is introduced to emphasize\ndifferent strengths of homogeneous and heterogeneous alignments by assigning\ndifferent values of weights to the image and the text prompts. The\nhigh-fidelity alignment is developed to further enhance the fidelity of both\nvideo generation and editing by taking the subject image as an additional model\ninput. Experimental results on four benchmarks suggest that our method\noutperforms the previous method on each task.\n","authors":["Haoyu Zhao","Tianyi Lu","Jiaxi Gu","Xing Zhang","Qingping Zheng","Zuxuan Wu","Hang Xu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.17338v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10543v1","updated":"2024-07-15T08:50:13Z","published":"2024-07-15T08:50:13Z","title":"Understanding the Dependence of Perception Model Competency on Regions\n  in an Image","summary":"  While deep neural network (DNN)-based perception models are useful for many\napplications, these models are black boxes and their outputs are not yet well\nunderstood. To confidently enable a real-world, decision-making system to\nutilize such a perception model without human intervention, we must enable the\nsystem to reason about the perception model's level of competency and respond\nappropriately when the model is incompetent. In order for the system to make an\nintelligent decision about the appropriate action when the model is\nincompetent, it would be useful for the system to understand why the model is\nincompetent. We explore five novel methods for identifying regions in the input\nimage contributing to low model competency, which we refer to as image\ncropping, segment masking, pixel perturbation, competency gradients, and\nreconstruction loss. We assess the ability of these five methods to identify\nunfamiliar objects, recognize regions associated with unseen classes, and\nidentify unexplored areas in an environment. We find that the competency\ngradients and reconstruction loss methods show great promise in identifying\nregions associated with low model competency, particularly when aspects of the\nimage that are unfamiliar to the perception model are causing this reduction in\ncompetency. Both of these methods boast low computation times and high levels\nof accuracy in detecting image regions that are unfamiliar to the model,\nallowing them to provide potential utility in decision-making pipelines. The\ncode for reproducing our methods and results is available on GitHub:\nhttps://github.com/sarapohland/explainable-competency.\n","authors":["Sara Pohland","Claire Tomlin"],"pdf_url":"https://arxiv.org/pdf/2407.10543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10542v1","updated":"2024-07-15T08:50:02Z","published":"2024-07-15T08:50:02Z","title":"3D Geometric Shape Assembly via Efficient Point Cloud Matching","summary":"  Learning to assemble geometric shapes into a larger target structure is a\npivotal task in various practical applications. In this work, we tackle this\nproblem by establishing local correspondences between point clouds of part\nshapes in both coarse- and fine-levels. To this end, we introduce Proxy Match\nTransform (PMT), an approximate high-order feature transform layer that enables\nreliable matching between mating surfaces of parts while incurring low costs in\nmemory and computation. Building upon PMT, we introduce a new framework, dubbed\nProxy Match TransformeR (PMTR), for the geometric assembly task. We evaluate\nthe proposed PMTR on the large-scale 3D geometric shape assembly benchmark\ndataset of Breaking Bad and demonstrate its superior performance and efficiency\ncompared to state-of-the-art methods. Project page:\nhttps://nahyuklee.github.io/pmtr.\n","authors":["Nahyuk Lee","Juhong Min","Junha Lee","Seungwook Kim","Kanghee Lee","Jaesik Park","Minsu Cho"],"pdf_url":"https://arxiv.org/pdf/2407.10542v1.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2401.12870v2","updated":"2024-07-15T08:49:24Z","published":"2024-01-23T16:04:19Z","title":"Unlocking the Potential: Multi-task Deep Learning for Spaceborne\n  Quantitative Monitoring of Fugitive Methane Plumes","summary":"  As global warming intensifies, increased attention is being paid to\nmonitoring fugitive methane emissions and detecting gas plumes from landfills.\nWe have divided methane emission monitoring into three subtasks: methane\nconcentration inversion, plume segmentation, and emission rate estimation.\nTraditional algorithms face certain limitations: methane concentration\ninversion typically employs the matched filter, which is sensitive to the\nglobal spectrum distribution and prone to significant noise. There is scant\nresearch on plume segmentation, with many studies depending on manual\nsegmentation, which can be subjective. The estimation of methane emission rate\nfrequently uses the IME algorithm, which necessitates meteorological\nmeasurement data. Utilizing the WENT landfill site in Hong Kong along with\nPRISMA hyperspectral satellite imagery, we introduce a novel deep\nlearning-based framework for quantitative methane emission monitoring from\nremote sensing images that is grounded in physical simulation. We create\nsimulated methane plumes using large eddy simulation (LES) and various\nconcentration maps of fugitive emissions using the radiative transfer equation\n(RTE), while applying augmentation techniques to construct a simulated PRISMA\ndataset. We train a U-Net network for methane concentration inversion, a Mask\nR-CNN network for methane plume segmentation, and a ResNet-50 network for\nmethane emission rate estimation. All three deep networks yield higher\nvalidation accuracy compared to traditional algorithms. Furthermore, we combine\nthe first two subtasks and the last two subtasks to design multi-task learning\nmodels, MTL-01 and MTL-02, both of which outperform single-task models in terms\nof accuracy. Our research exemplifies the application of multi-task deep\nlearning to quantitative methane monitoring and can be generalized to a wide\narray of methane monitoring tasks.\n","authors":["Guoxin Si","Shiliang Fu","Wei Yao"],"pdf_url":"https://arxiv.org/pdf/2401.12870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10537v1","updated":"2024-07-15T08:48:17Z","published":"2024-07-15T08:48:17Z","title":"Segmentation of Prostate Tumour Volumes from PET Images is a Different\n  Ball Game","summary":"  Accurate segmentation of prostate tumours from PET images presents a\nformidable challenge in medical image analysis. Despite considerable work and\nimprovement in delineating organs from CT and MR modalities, the existing\nstandards do not transfer well and produce quality results in PET related\ntasks. Particularly, contemporary methods fail to accurately consider the\nintensity-based scaling applied by the physicians during manual annotation of\ntumour contours. In this paper, we observe that the prostate-localised uptake\nthreshold ranges are beneficial for suppressing outliers. Therefore, we utilize\nthe intensity threshold values, to implement a new custom-feature-clipping\nnormalisation technique. We evaluate multiple, established U-Net variants under\ndifferent normalisation schemes, using the nnU-Net framework. All models were\ntrained and tested on multiple datasets, obtained with two radioactive tracers:\n[68-Ga]Ga-PSMA-11 and [18-F]PSMA-1007. Our results show that the U-Net models\nachieve much better performance when the PET scans are preprocessed with our\nnovel clipping technique.\n","authors":["Shrajan Bhandary","Dejan Kuhn","Zahra Babaiee","Tobias Fechter","Simon K. B. Spohn","Constantinos Zamboglou","Anca-Ligia Grosu","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2407.10537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14076v2","updated":"2024-07-15T08:45:00Z","published":"2024-04-22T10:45:59Z","title":"Towards noise contrastive estimation with soft targets for conditional\n  models","summary":"  Soft targets combined with the cross-entropy loss have shown to improve\ngeneralization performance of deep neural networks on supervised classification\ntasks. The standard cross-entropy loss however assumes data to be categorically\ndistributed, which may often not be the case in practice. In contrast, InfoNCE\ndoes not rely on such an explicit assumption but instead implicitly estimates\nthe true conditional through negative sampling. Unfortunately, it cannot be\ncombined with soft targets in its standard formulation, hindering its use in\ncombination with sophisticated training strategies. In this paper, we address\nthis limitation by proposing a loss function that is compatible with\nprobabilistic targets. Our new soft target InfoNCE loss is conceptually simple,\nefficient to compute, and can be motivated through the framework of noise\ncontrastive estimation. Using a toy example, we demonstrate shortcomings of the\ncategorical distribution assumption of cross-entropy, and discuss implications\nof sampling from soft distributions. We observe that soft target InfoNCE\nperforms on par with strong soft target cross-entropy baselines and outperforms\nhard target NLL and InfoNCE losses on popular benchmarks, including ImageNet.\nFinally, we provide a simple implementation of our loss, geared towards\nsupervised classification and fully compatible with deep classification models\ntrained with cross-entropy.\n","authors":["Johannes Hugger","Virginie Uhlmann"],"pdf_url":"https://arxiv.org/pdf/2404.14076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10536v1","updated":"2024-07-15T08:44:37Z","published":"2024-07-15T08:44:37Z","title":"An experimental evaluation of Siamese Neural Networks for robot\n  localization using omnidirectional imaging in indoor environments","summary":"  The objective of this paper is to address the localization problem using\nomnidirectional images captured by a catadioptric vision system mounted on the\nrobot. For this purpose, we explore the potential of Siamese Neural Networks\nfor modeling indoor environments using panoramic images as the unique source of\ninformation. Siamese Neural Networks are characterized by their ability to\ngenerate a similarity function between two input data, in this case, between\ntwo panoramic images. In this study, Siamese Neural Networks composed of two\nConvolutional Neural Networks (CNNs) are used. The output of each CNN is a\ndescriptor which is used to characterize each image. The dissimilarity of the\nimages is computed by measuring the distance between these descriptors. This\nfact makes Siamese Neural Networks particularly suitable to perform image\nretrieval tasks. First, we evaluate an initial task strongly related to\nlocalization that consists in detecting whether two images have been captured\nin the same or in different rooms. Next, we assess Siamese Neural Networks in\nthe context of a global localization problem. The results outperform previous\ntechniques for solving the localization task using the COLD-Freiburg dataset,\nin a variety of lighting conditions, specially when using images captured in\ncloudy and night conditions.\n","authors":["J. J. Cabrera","V. RomÃ¡n","A. Gil","O. Reinoso","L. PayÃ¡"],"pdf_url":"https://arxiv.org/pdf/2407.10536v1.pdf","comment":"Published: 08 July 2024 Paper link:\n  https://link.springer.com/content/pdf/10.1007/s10462-024-10840-0.pdf"},{"id":"http://arxiv.org/abs/2407.10534v1","updated":"2024-07-15T08:42:10Z","published":"2024-07-15T08:42:10Z","title":"Automated Label Unification for Multi-Dataset Semantic Segmentation with\n  GNNs","summary":"  Deep supervised models possess significant capability to assimilate extensive\ntraining data, thereby presenting an opportunity to enhance model performance\nthrough training on multiple datasets. However, conflicts arising from\ndifferent label spaces among datasets may adversely affect model performance.\nIn this paper, we propose a novel approach to automatically construct a unified\nlabel space across multiple datasets using graph neural networks. This enables\nsemantic segmentation models to be trained simultaneously on multiple datasets,\nresulting in performance improvements. Unlike existing methods, our approach\nfacilitates seamless training without the need for additional manual\nreannotation or taxonomy reconciliation. This significantly enhances the\nefficiency and effectiveness of multi-dataset segmentation model training. The\nresults demonstrate that our method significantly outperforms other\nmulti-dataset training methods when trained on seven datasets simultaneously,\nand achieves state-of-the-art performance on the WildDash 2 benchmark.\n","authors":["Rong Ma","Jie Chen","Xiangyang Xue","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2407.10534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05848v2","updated":"2024-07-15T08:39:57Z","published":"2024-07-08T11:55:10Z","title":"Wavelet Convolutions for Large Receptive Fields","summary":"  In recent years, there have been attempts to increase the kernel size of\nConvolutional Neural Nets (CNNs) to mimic the global receptive field of Vision\nTransformers' (ViTs) self-attention blocks. That approach, however, quickly hit\nan upper bound and saturated way before achieving a global receptive field. In\nthis work, we demonstrate that by leveraging the Wavelet Transform (WT), it is,\nin fact, possible to obtain very large receptive fields without suffering from\nover-parameterization, e.g., for a $k \\times k$ receptive field, the number of\ntrainable parameters in the proposed method grows only logarithmically with\n$k$. The proposed layer, named WTConv, can be used as a drop-in replacement in\nexisting architectures, results in an effective multi-frequency response, and\nscales gracefully with the size of the receptive field. We demonstrate the\neffectiveness of the WTConv layer within ConvNeXt and MobileNetV2 architectures\nfor image classification, as well as backbones for downstream tasks, and show\nit yields additional properties such as robustness to image corruption and an\nincreased response to shapes over textures. Our code is available at\nhttps://github.com/BGU-CS-VIL/WTConv.\n","authors":["Shahaf E. Finder","Roy Amoyal","Eran Treister","Oren Freifeld"],"pdf_url":"https://arxiv.org/pdf/2407.05848v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.08374v2","updated":"2024-07-15T08:38:40Z","published":"2024-07-11T10:35:53Z","title":"Enhancing Robustness of Vision-Language Models through Orthogonality\n  Learning and Cross-Regularization","summary":"  Efficient finetuning of vision-language models (VLMs) like CLIP for specific\ndownstream tasks is gaining significant attention. Previous works primarily\nfocus on prompt learning to adapt the CLIP into a variety of downstream tasks,\nhowever, suffering from task overfitting when finetuned on a small data set. In\nthis paper, we introduce an orthogonal finetuning method for efficiently\nupdating pretrained weights which enhances robustness and generalization, while\na cross-regularization strategy is further exploited to maintain the stability\nin terms of zero-shot generalization of VLMs, dubbed \\textbf{\\textit{OrthCR}}.\nSpecifically, trainable orthogonal matrices are injected seamlessly into the\ntransformer architecture and enforced with orthogonality constraint using\nCayley parameterization, benefiting from the norm-preserving property and thus\nleading to stable and faster convergence. To alleviate deviation from\northogonal constraint during training, a cross-regularization strategy is\nfurther employed with initial pretrained weights within a bypass manner. In\naddition, to enrich the sample diversity for downstream tasks, we first explore\nCutout data augmentation to boost the efficient finetuning and comprehend how\nour approach improves the specific downstream performance and maintains the\ngeneralizability in the perspective of Orthogonality Learning. Beyond existing\nprompt learning techniques, we conduct extensive experiments to demonstrate\nthat our method explicitly steers pretrained weight space to represent the\ntask-specific knowledge and presents competitive generalizability under\nbase-to-base/base-to-new, cross-dataset transfer and domain generalization\nevaluations.\n","authors":["Jinlong Li","Zequn Jie","Elisa Ricci","Lin Ma","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2407.08374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09468v2","updated":"2024-07-15T08:36:59Z","published":"2024-03-14T15:07:36Z","title":"Eta Inversion: Designing an Optimal Eta Function for Diffusion-based\n  Real Image Editing","summary":"  Diffusion models have achieved remarkable success in the domain of\ntext-guided image generation and, more recently, in text-guided image editing.\nA commonly adopted strategy for editing real images involves inverting the\ndiffusion process to obtain a noisy representation of the original image, which\nis then denoised to achieve the desired edits. However, current methods for\ndiffusion inversion often struggle to produce edits that are both faithful to\nthe specified text prompt and closely resemble the source image. To overcome\nthese limitations, we introduce a novel and adaptable diffusion inversion\ntechnique for real image editing, which is grounded in a theoretical analysis\nof the role of $\\eta$ in the DDIM sampling equation for enhanced editability.\nBy designing a universal diffusion inversion method with a time- and\nregion-dependent $\\eta$ function, we enable flexible control over the editing\nextent. Through a comprehensive series of quantitative and qualitative\nassessments, involving a comparison with a broad array of recent methods, we\ndemonstrate the superiority of our approach. Our method not only sets a new\nbenchmark in the field but also significantly outperforms existing strategies.\n","authors":["Wonjun Kang","Kevin Galim","Hyung Il Koo"],"pdf_url":"https://arxiv.org/pdf/2403.09468v2.pdf","comment":"ECCV 2024. Code: https://github.com/furiosa-ai/eta-inversion"},{"id":"http://arxiv.org/abs/2303.01032v3","updated":"2024-07-15T08:35:49Z","published":"2023-03-02T07:42:07Z","title":"ESceme: Vision-and-Language Navigation with Episodic Scene Memory","summary":"  Vision-and-language navigation (VLN) simulates a visual agent that follows\nnatural-language navigation instructions in real-world scenes. Existing\napproaches have made enormous progress in navigation in new environments, such\nas beam search, pre-exploration, and dynamic or hierarchical history encoding.\nTo balance generalization and efficiency, we resort to memorizing visited\nscenarios apart from the ongoing route while navigating. In this work, we\nintroduce a mechanism of Episodic Scene memory (ESceme) for VLN that wakes an\nagent's memories of past visits when it enters the current scene. The episodic\nscene memory allows the agent to envision a bigger picture of the next\nprediction. This way, the agent learns to utilize dynamically updated\ninformation instead of merely adapting to the current observations. We provide\na simple yet effective implementation of ESceme by enhancing the accessible\nviews at each location and progressively completing the memory while\nnavigating. We verify the superiority of ESceme on short-horizon (R2R),\nlong-horizon (R4R), and vision-and-dialog (CVDN) VLN tasks. Our ESceme also\nwins first place on the CVDN leaderboard. Code is available:\n\\url{https://github.com/qizhust/esceme}.\n","authors":["Qi Zheng","Daqing Liu","Chaoyue Wang","Jing Zhang","Dadong Wang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.01032v3.pdf","comment":"Accepted by IJCV"},{"id":"http://arxiv.org/abs/2407.10528v1","updated":"2024-07-15T08:35:00Z","published":"2024-07-15T08:35:00Z","title":"Local Action-Guided Motion Diffusion Model for Text-to-Motion Generation","summary":"  Text-to-motion generation requires not only grounding local actions in\nlanguage but also seamlessly blending these individual actions to synthesize\ndiverse and realistic global motions. However, existing motion generation\nmethods primarily focus on the direct synthesis of global motions while\nneglecting the importance of generating and controlling local actions. In this\npaper, we propose the local action-guided motion diffusion model, which\nfacilitates global motion generation by utilizing local actions as fine-grained\ncontrol signals. Specifically, we provide an automated method for reference\nlocal action sampling and leverage graph attention networks to assess the\nguiding weight of each local action in the overall motion synthesis. During the\ndiffusion process for synthesizing global motion, we calculate the local-action\ngradient to provide conditional guidance. This local-to-global paradigm reduces\nthe complexity associated with direct global motion generation and promotes\nmotion diversity via sampling diverse actions as conditions. Extensive\nexperiments on two human motion datasets, i.e., HumanML3D and KIT, demonstrate\nthe effectiveness of our method. Furthermore, our method provides flexibility\nin seamlessly combining various local actions and continuous guiding weight\nadjustment, accommodating diverse user preferences, which may hold potential\nsignificance for the community. The project page is available at\nhttps://jpthu17.github.io/GuidedMotion-project/.\n","authors":["Peng Jin","Hao Li","Zesen Cheng","Kehan Li","Runyi Yu","Chang Liu","Xiangyang Ji","Li Yuan","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10528v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.04327v2","updated":"2024-07-15T08:13:39Z","published":"2024-07-05T07:55:19Z","title":"TF-SASM: Training-free Spatial-aware Sparse Memory for Multi-object\n  Tracking","summary":"  Multi-object tracking (MOT) in computer vision remains a significant\nchallenge, requiring precise localization and continuous tracking of multiple\nobjects in video sequences. The emergence of data sets that emphasize robust\nreidentification, such as DanceTrack, has highlighted the need for effective\nsolutions. While memory-based approaches have shown promise, they often suffer\nfrom high computational complexity and memory usage due to storing feature at\nevery single frame. In this paper, we propose a novel memory-based approach\nthat selectively stores critical features based on object motion and\noverlapping awareness, aiming to enhance efficiency while minimizing\nredundancy. As a result, our method not only store longer temporal information\nwith limited number of stored features in the memory, but also diversify states\nof a particular object to enhance the association performance. Our approach\nsignificantly improves over MOTRv2 in the DanceTrack test set, demonstrating a\ngain of 2.0% AssA score and 2.1% in IDF1 score.\n","authors":["Thuc Nguyen-Quang","Minh-Triet Tran"],"pdf_url":"https://arxiv.org/pdf/2407.04327v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18483v2","updated":"2024-07-15T07:55:43Z","published":"2024-05-28T18:00:06Z","title":"Towards Open Domain Text-Driven Synthesis of Multi-Person Motions","summary":"  This work aims to generate natural and diverse group motions of multiple\nhumans from textual descriptions. While single-person text-to-motion generation\nis extensively studied, it remains challenging to synthesize motions for more\nthan one or two subjects from in-the-wild prompts, mainly due to the lack of\navailable datasets. In this work, we curate human pose and motion datasets by\nestimating pose information from large-scale image and video datasets. Our\nmodels use a transformer-based diffusion framework that accommodates multiple\ndatasets with any number of subjects or frames. Experiments explore both\ngeneration of multi-person static poses and generation of multi-person motion\nsequences. To our knowledge, our method is the first to generate multi-subject\nmotion sequences with high diversity and fidelity from a large variety of\ntextual prompts.\n","authors":["Mengyi Shan","Lu Dong","Yutao Han","Yuan Yao","Tao Liu","Ifeoma Nwogu","Guo-Jun Qi","Mitch Hill"],"pdf_url":"https://arxiv.org/pdf/2405.18483v2.pdf","comment":"ECCV 2024. Project page: https://shanmy.github.io/Multi-Motion/"},{"id":"http://arxiv.org/abs/2407.10495v1","updated":"2024-07-15T07:37:31Z","published":"2024-07-15T07:37:31Z","title":"Improving Hyperbolic Representations via Gromov-Wasserstein\n  Regularization","summary":"  Hyperbolic representations have shown remarkable efficacy in modeling\ninherent hierarchies and complexities within data structures. Hyperbolic neural\nnetworks have been commonly applied for learning such representations from\ndata, but they often fall short in preserving the geometric structures of the\noriginal feature spaces. In response to this challenge, our work applies the\nGromov-Wasserstein (GW) distance as a novel regularization mechanism within\nhyperbolic neural networks. The GW distance quantifies how well the original\ndata structure is maintained after embedding the data in a hyperbolic space.\nSpecifically, we explicitly treat the layers of the hyperbolic neural networks\nas a transport map and calculate the GW distance accordingly. We validate that\nthe GW distance computed based on a training set well approximates the GW\ndistance of the underlying data distribution. Our approach demonstrates\nconsistent enhancements over current state-of-the-art methods across various\ntasks, including few-shot image classification, as well as semi-supervised\ngraph link prediction and node classification.\n","authors":["Yifei Yang","Wonjun Lee","Dongmian Zou","Gilad Lerman"],"pdf_url":"https://arxiv.org/pdf/2407.10495v1.pdf","comment":"Accepted for ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10494v1","updated":"2024-07-15T07:36:00Z","published":"2024-07-15T07:36:00Z","title":"Learning to Unlearn for Robust Machine Unlearning","summary":"  Machine unlearning (MU) seeks to remove knowledge of specific data samples\nfrom trained models without the necessity for complete retraining, a task made\nchallenging by the dual objectives of effective erasure of data and maintaining\nthe overall performance of the model. Despite recent advances in this field,\nbalancing between the dual objectives of unlearning remains challenging. From a\nfresh perspective of generalization, we introduce a novel Learning-to-Unlearn\n(LTU) framework, which adopts a meta-learning approach to optimize the\nunlearning process to improve forgetting and remembering in a unified manner.\nLTU includes a meta-optimization scheme that facilitates models to effectively\npreserve generalizable knowledge with only a small subset of the remaining set,\nwhile thoroughly forgetting the specific data samples. We also introduce a\nGradient Harmonization strategy to align the optimization trajectories for\nremembering and forgetting via mitigating gradient conflicts, thus ensuring\nefficient and effective model updates. Our approach demonstrates improved\nefficiency and efficacy for MU, offering a promising solution to the challenges\nof data rights and model reusability.\n","authors":["Mark He Huang","Lin Geng Foo","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.10494v1.pdf","comment":"Accepted by ECCV 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.10829v1","updated":"2024-07-15T15:42:22Z","published":"2024-07-15T15:42:22Z","title":"BiasScanner: Automatic Detection and Classification of News Bias to\n  Strengthen Democracy","summary":"  The increasing consumption of news online in the 21st century coincided with\nincreased publication of disinformation, biased reporting, hate speech and\nother unwanted Web content. We describe BiasScanner, an application that aims\nto strengthen democracy by supporting news consumers with scrutinizing news\narticles they are reading online. BiasScanner contains a server-side\npre-trained large language model to identify biased sentences of news articles\nand a front-end Web browser plug-in. At the time of writing, BiasScanner can\nidentify and classify more than two dozen types of media bias at the sentence\nlevel, making it the most fine-grained model and only deployed application\n(automatic system in use) of its kind. It was implemented in a light-weight and\nprivacy-respecting manner, and in addition to highlighting likely biased\nsentence it also provides explanations for each classification decision as well\nas a summary analysis for each news article. While prior research has addressed\nnews bias detection, we are not aware of any work that resulted in a deployed\nbrowser plug-in (c.f. also biasscanner.org for a Web demo).\n","authors":["Tim Menzner","Jochen L. Leidner"],"pdf_url":"https://arxiv.org/pdf/2407.10829v1.pdf","comment":"10 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.18684v2","updated":"2024-07-15T14:48:09Z","published":"2024-03-27T15:27:36Z","title":"Scaling Laws For Dense Retrieval","summary":"  Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.\n","authors":["Yan Fang","Jingtao Zhan","Qingyao Ai","Jiaxin Mao","Weihang Su","Jia Chen","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18684v2.pdf","comment":"Accepted at SIGIR 2024. V2 fixes a bug in the experiments"},{"id":"http://arxiv.org/abs/2407.07871v2","updated":"2024-07-15T14:23:35Z","published":"2024-07-10T17:37:15Z","title":"Enhancing HNSW Index for Real-Time Updates: Addressing Unreachable\n  Points and Performance Degradation","summary":"  The approximate nearest neighbor search (ANNS) is a fundamental and essential\ncomponent in data mining and information retrieval, with graph-based\nmethodologies demonstrating superior performance compared to alternative\napproaches. Extensive research efforts have been dedicated to improving search\nefficiency by developing various graph-based indices, such as HNSW\n(Hierarchical Navigable Small World). However, the performance of HNSW and most\ngraph-based indices become unacceptable when faced with a large number of\nreal-time deletions, insertions, and updates. Furthermore, during update\noperations, HNSW can result in some data points becoming unreachable, a\nsituation we refer to as the `unreachable points phenomenon'. This phenomenon\ncould significantly affect the search accuracy of the graph in certain\nsituations.\n  To address these issues, we present efficient measures to overcome the\nshortcomings of HNSW, specifically addressing poor performance over long\nperiods of delete and update operations and resolving the issues caused by the\nunreachable points phenomenon. Our proposed MN-RU algorithm effectively\nimproves update efficiency and suppresses the growth rate of unreachable\npoints, ensuring better overall performance and maintaining the integrity of\nthe graph. Our results demonstrate that our methods outperform existing\napproaches. Furthermore, since our methods are based on HNSW, they can be\neasily integrated with existing indices widely used in the industrial field,\nmaking them practical for future real-world applications. Code is available at\n\\url{https://github.com/xwt1/MN-RU.git}\n","authors":["Wentao Xiao","Yueyang Zhan","Rui Xi","Mengshu Hou","Jianming Liao"],"pdf_url":"https://arxiv.org/pdf/2407.07871v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10714v1","updated":"2024-07-15T13:33:30Z","published":"2024-07-15T13:33:30Z","title":"SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate\n  Retrieval for Lifelong Sequential Recommendation","summary":"  The modeling of users' behaviors is crucial in modern recommendation systems.\nA lot of research focuses on modeling users' lifelong sequences, which can be\nextremely long and sometimes exceed thousands of items. These models use the\ntarget item to search for the most relevant items from the historical sequence.\nHowever, training lifelong sequences in click through rate (CTR) prediction or\npersonalized search ranking (PSR) is extremely difficult due to the\ninsufficient learning problem of ID embedding, especially when the IDs in the\nlifelong sequence features do not exist in the samples of training dataset.\nAdditionally, existing target attention mechanisms struggle to learn the\nmulti-modal representations of items in the sequence well. The distribution of\nmulti-modal embedding (text, image and attributes) output of user's interacted\nitems are not properly aligned and there exist divergence across modalities. We\nalso observe that users' search query sequences and item browsing sequences can\nfully depict users' intents and benefit from each other. To address these\nchallenges, we propose a unified lifelong multi-modal sequence model called\nSEMINAR-Search Enhanced Multi-Modal Interest Network and Approximate Retrieval.\nSpecifically, a network called Pretraining Search Unit (PSU) learns the\nlifelong sequences of multi-modal query-item pairs in a pretraining-finetuning\nmanner with multiple objectives: multi-modal alignment, next query-item pair\nprediction, query-item relevance prediction, etc. After pretraining, the\ndownstream model restores the pretrained embedding as initialization and\nfinetunes the network. To accelerate the online retrieval speed of multi-modal\nembedding, we propose a multi-modal codebook-based product quantization\nstrategy to approximate the exact attention calculati\n","authors":["Kaiming Shen","Xichen Ding","Zixiang Zheng","Yuqi Gong","Qianqian Li","Zhongyi Liu","Guannan Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.10714v1.pdf","comment":"9 pages,code released"},{"id":"http://arxiv.org/abs/2402.05070v2","updated":"2024-07-15T13:06:13Z","published":"2024-02-07T18:21:17Z","title":"A Roadmap to Pluralistic Alignment","summary":"  With increased power and prevalence of AI systems, it is ever more critical\nthat AI systems are designed to serve all, i.e., people with diverse values and\nperspectives. However, aligning models to serve pluralistic human values\nremains an open research question. In this piece, we propose a roadmap to\npluralistic alignment, specifically using language models as a test bed. We\nidentify and formalize three possible ways to define and operationalize\npluralism in AI systems: 1) Overton pluralistic models that present a spectrum\nof reasonable responses; 2) Steerably pluralistic models that can steer to\nreflect certain perspectives; and 3) Distributionally pluralistic models that\nare well-calibrated to a given population in distribution. We also formalize\nand discuss three possible classes of pluralistic benchmarks: 1)\nMulti-objective benchmarks, 2) Trade-off steerable benchmarks, which\nincentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic\nbenchmarks which explicitly model diverse human ratings. We use this framework\nto argue that current alignment techniques may be fundamentally limited for\npluralistic AI; indeed, we highlight empirical evidence, both from our own\nexperiments and from other work, that standard alignment procedures might\nreduce distributional pluralism in models, motivating the need for further\nresearch on pluralistic alignment.\n","authors":["Taylor Sorensen","Jared Moore","Jillian Fisher","Mitchell Gordon","Niloofar Mireshghallah","Christopher Michael Rytting","Andre Ye","Liwei Jiang","Ximing Lu","Nouha Dziri","Tim Althoff","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2402.05070v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2407.10691v1","updated":"2024-07-15T13:04:09Z","published":"2024-07-15T13:04:09Z","title":"$\\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific\n  Domain through Complementary Granularity","summary":"  Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,\nrespectively, averaged on queries containing multiple subqueries from five\nscientific retrieval datasets. Moreover, the efficacy of two downstream\nscientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$to boost the application of LLMs in the scientific domain.\n","authors":["Fengyu Cai","Xinran Zhao","Tong Chen","Sihao Chen","Hongming Zhang","Iryna Gurevych","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2407.10691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10599v1","updated":"2024-07-15T10:24:00Z","published":"2024-07-15T10:24:00Z","title":"General algorithm of assigning raster features to vector maps at any\n  resolution or scale","summary":"  The fusion of multi-source data is essential for a comprehensive analysis of\ngeographic applications. Due to distinct data structures, the fusion process\ntends to encounter technical difficulties in terms of preservation of the\nintactness of each source data. Furthermore, a lack of generalized methods is a\nproblem when the method is expected to be applicable in multiple resolutions,\nsizes, or scales of raster and vector data, to what is being processed. In this\nstudy, we propose a general algorithm of assigning features from raster data\n(concentrations of air pollutants) to vector components (roads represented by\nedges) in city maps through the iterative construction of virtual layers to\nexpand geolocation from a city centre to boundaries in a 2D projected map. The\nconstruction follows the rule of perfect squares with a slight difference\ndepending on the oddness or evenness of the ratio of city size to raster\nresolution. We demonstrate the algorithm by applying it to assign accurate\nPM$_{2.5}$ and NO$_{2}$ concentrations to roads in 1692 cities globally for a\npotential graph-based pollution analysis. This method could pave the way for\nagile studies on urgent climate issues by providing a generic and efficient\nmethod to accurately fuse multiple datasets of varying scales and compositions.\n","authors":["Nan Xu","Mark Stevenson","Kerry A. Nice","Sachith Seneviratne"],"pdf_url":"https://arxiv.org/pdf/2407.10599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20994v2","updated":"2024-07-15T08:50:29Z","published":"2024-05-31T16:38:54Z","title":"CWRCzech: 100M Query-Document Czech Click Dataset and Its Application to\n  Web Relevance Ranking","summary":"  We present CWRCzech, Click Web Ranking dataset for Czech, a 100M\nquery-document Czech click dataset for relevance ranking with user behavior\ndata collected from search engine logs of Seznam$.$cz. To the best of our\nknowledge, CWRCzech is the largest click dataset with raw text published so\nfar. It provides document positions in the search results as well as\ninformation about user behavior: 27.6M clicked documents and 10.8M dwell times.\nIn addition, we also publish a manually annotated Czech test for the relevance\ntask, containing nearly 50k query-document pairs, each annotated by at least 2\nannotators. Finally, we analyze how the user behavior data improve relevance\nranking and show that models trained on data automatically harnessed at\nsufficient scale can surpass the performance of models trained on human\nannotated data. CWRCzech is published under an academic non-commercial license\nand is available to the research community at\nhttps://github.com/seznam/CWRCzech.\n","authors":["Josef VonÃ¡Å¡ek","Milan Straka","Rostislav KrÄ","Lenka LasoÅovÃ¡","Ekaterina Egorova","Jana StrakovÃ¡","Jakub NÃ¡plava"],"pdf_url":"https://arxiv.org/pdf/2405.20994v2.pdf","comment":"Accepted to SIGIR 2024"},{"id":"http://arxiv.org/abs/2303.12973v2","updated":"2024-07-15T01:57:30Z","published":"2023-03-23T00:42:48Z","title":"Uncertainty Calibration for Counterfactual Propensity Estimation in\n  Recommendation","summary":"  Post-click conversion rate (CVR) is a reliable indicator of online customers'\npreferences, making it crucial for developing recommender systems. A major\nchallenge in predicting CVR is severe selection bias, arising from users'\ninherent self-selection behavior and the system's item selection process. To\nmitigate this issue, the inverse propensity score (IPS) is employed to weight\nthe prediction error of each observed instance. However, current propensity\nscore estimations are unreliable due to the lack of a quality measure. To\naddress this, we evaluate the quality of propensity scores from the perspective\nof uncertainty calibration, proposing the use of expected calibration error\n(ECE) as a measure of propensity-score quality. We argue that the performance\nof IPS-based recommendations is hampered by miscalibration in propensity\nestimation. We introduce a model-agnostic calibration framework for\npropensity-based debiasing of CVR predictions. Theoretical analysis on bias and\ngeneralization bounds demonstrates the superiority of calibrated propensity\nestimates over uncalibrated ones. Experiments conducted on the Coat, Yahoo and\nKuaiRand datasets show improved uncertainty calibration, as evidenced by lower\nECE values, leading to enhanced CVR prediction outcomes.\n","authors":["Wenbo Hu","Xin Sun","Qiang liu","Le Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10380v1","updated":"2024-07-15T01:21:56Z","published":"2024-07-15T01:21:56Z","title":"NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models","summary":"  Cognitive textual and visual reasoning tasks, such as puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. While LLMs and VLMs, through extensive\ntraining on large amounts of human-curated data, have attained a high level of\npseudo-human intelligence in some common sense reasoning tasks, they still\nstruggle with more complex reasoning tasks that require cognitive\nunderstanding. In this work, we introduce a new dataset, NTSEBench, designed to\nevaluate the cognitive multi-modal reasoning and problem-solving skills of\nlarge models. The dataset comprises 2,728 multiple-choice questions comprising\nof a total of 4,642 images across 26 categories sampled from the NTSE\nexamination conducted nationwide in India, featuring both visual and textual\ngeneral aptitude questions that do not rely on rote learning. We establish\nbaselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a\ncomparison between open source and propriety models, we propose four distinct\nmodeling strategies to handle different modalities (text and images) in the\ndataset instances.\n","authors":["Pranshu Pandya","Agney S Talwarr","Vatsal Gupta","Tushar Kataria","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2407.10380v1.pdf","comment":"15 pages, 2 figures, 5 tables"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.10972v1","updated":"2024-07-15T17:59:55Z","published":"2024-07-15T17:59:55Z","title":"VGBench: Evaluating Large Language Models on Vector Graphics\n  Understanding and Generation","summary":"  In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons or sketches.\nRecent studies have shown promising results on processing vector graphics with\ncapable Large Language Models (LLMs). However, such works focus solely on\nqualitative results, understanding, or a specific type of vector graphics. We\npropose VGBench, a comprehensive benchmark for LLMs on handling vector graphics\nthrough diverse aspects, including (a) both visual understanding and\ngeneration, (b) evaluation of various vector graphics formats, (c) diverse\nquestion types, (d) wide range of prompting techniques, (e) under multiple\nLLMs. Evaluating on our collected 4279 understanding and 5845 generation\nsamples, we find that LLMs show strong capability on both aspects while\nexhibiting less desirable performance on low-level formats (SVG). Both data and\nevaluation pipeline will be open-sourced at https://vgbench.github.io.\n","authors":["Bocheng Zou","Mu Cai","Jianrui Zhang","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2407.10972v1.pdf","comment":"Project Page: https://vgbench.github.io"},{"id":"http://arxiv.org/abs/2407.10971v1","updated":"2024-07-15T17:59:52Z","published":"2024-07-15T17:59:52Z","title":"Walking the Values in Bayesian Inverse Reinforcement Learning","summary":"  The goal of Bayesian inverse reinforcement learning (IRL) is recovering a\nposterior distribution over reward functions using a set of demonstrations from\nan expert optimizing for a reward unknown to the learner. The resulting\nposterior over rewards can then be used to synthesize an apprentice policy that\nperforms well on the same or a similar task. A key challenge in Bayesian IRL is\nbridging the computational gap between the hypothesis space of possible rewards\nand the likelihood, often defined in terms of Q values: vanilla Bayesian IRL\nneeds to solve the costly forward planning problem - going from rewards to the\nQ values - at every step of the algorithm, which may need to be done thousands\nof times. We propose to solve this by a simple change: instead of focusing on\nprimarily sampling in the space of rewards, we can focus on primarily working\nin the space of Q-values, since the computation required to go from Q-values to\nreward is radically cheaper. Furthermore, this reversion of the computation\nmakes it easy to compute the gradient allowing efficient sampling using\nHamiltonian Monte Carlo. We propose ValueWalk - a new Markov chain Monte Carlo\nmethod based on this insight - and illustrate its advantages on several tasks.\n","authors":["Ondrej Bajgar","Alessandro Abate","Konstantinos Gatsis","Michael A. Osborne"],"pdf_url":"https://arxiv.org/pdf/2407.10971v1.pdf","comment":"Published at the 40th Conference on Uncertainty in Artificial\n  Intelligence (UAI 2024)"},{"id":"http://arxiv.org/abs/2407.10969v1","updated":"2024-07-15T17:59:29Z","published":"2024-07-15T17:59:29Z","title":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated","summary":"  We introduce, Q-Sparse, a simple yet effective approach to training\nsparsely-activated large language models (LLMs). Q-Sparse enables full sparsity\nof activations in LLMs which can bring significant efficiency gains in\ninference. This is achieved by applying top-K sparsification to the activations\nand the straight-through-estimator to the training. The key results from this\nwork are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs\nwhile being much more efficient at inference time; (2) We present an\ninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is\neffective in different settings, including training-from-scratch,\ncontinue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for\nboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the\nsynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the\ncornerstone and a clear path to revolutionize the efficiency, including cost\nand energy consumption, of future LLMs.\n","authors":["Hongyu Wang","Shuming Ma","Ruiping Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.10969v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2407.10967v1","updated":"2024-07-15T17:59:23Z","published":"2024-07-15T17:59:23Z","title":"BECAUSE: Bilinear Causal Representation for Generalizable Offline\n  Model-based Reinforcement Learning","summary":"  Offline model-based reinforcement learning (MBRL) enhances data efficiency by\nutilizing pre-collected datasets to learn models and policies, especially in\nscenarios where exploration is costly or infeasible. Nevertheless, its\nperformance often suffers from the objective mismatch between model and policy\nlearning, resulting in inferior performance despite accurate model predictions.\nThis paper first identifies the primary source of this mismatch comes from the\nunderlying confounders present in offline data for MBRL. Subsequently, we\nintroduce \\textbf{B}ilin\\textbf{E}ar \\textbf{CAUS}al\nr\\textbf{E}presentation~(BECAUSE), an algorithm to capture causal\nrepresentation for both states and actions to reduce the influence of the\ndistribution shift, thus mitigating the objective mismatch problem.\nComprehensive evaluations on 18 tasks that vary in data quality and environment\ncontext demonstrate the superior performance of BECAUSE over existing offline\nRL algorithms. We show the generalizability and robustness of BECAUSE under\nfewer samples or larger numbers of confounders. Additionally, we offer\ntheoretical analysis of BECAUSE to prove its error bound and sample efficiency\nwhen integrating causal representation into offline MBRL.\n","authors":["Haohong Lin","Wenhao Ding","Jian Chen","Laixi Shi","Jiacheng Zhu","Bo Li","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.10967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10964v1","updated":"2024-07-15T17:58:42Z","published":"2024-07-15T17:58:42Z","title":"No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen\n  Representations","summary":"  This paper introduces FUNGI, Features from UNsupervised GradIents, a method\nto enhance the features of vision encoders by leveraging self-supervised\ngradients. Our method is simple: given any pretrained model, we first compute\ngradients from various self-supervised objectives for each input. These are\nprojected to a lower dimension and then concatenated with the model's\nembedding. The resulting features are evaluated on k-nearest neighbor\nclassification over 11 datasets from vision, 5 from natural language\nprocessing, and 2 from audio. Across backbones spanning various sizes and\npretraining strategies, FUNGI features provide consistent performance\nimprovements over the embeddings. We also show that using FUNGI features can\nbenefit linear classification and image retrieval, and that they significantly\nimprove the retrieval-based in-context scene understanding abilities of\npretrained models, for example improving upon DINO by +17% for semantic\nsegmentation - without any training.\n","authors":["Walter Simoncini","Spyros Gidaris","Andrei Bursuc","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2407.10964v1.pdf","comment":"Preprint. Code available at\n  https://github.com/WalterSimoncini/fungivision"},{"id":"http://arxiv.org/abs/2407.02844v3","updated":"2024-07-15T17:55:49Z","published":"2024-07-03T06:40:26Z","title":"Multi-Attention Integrated Deep Learning Frameworks for Enhanced Breast\n  Cancer Segmentation and Identification","summary":"  Breast cancer poses a profound threat to lives globally, claiming numerous\nlives each year. Therefore, timely detection is crucial for early intervention\nand improved chances of survival. Accurately diagnosing and classifying breast\ntumors using ultrasound images is a persistent challenge in medicine, demanding\ncutting-edge solutions for improved treatment strategies. This research\nintroduces multiattention-enhanced deep learning (DL) frameworks designed for\nthe classification and segmentation of breast cancer tumors from ultrasound\nimages. A spatial channel attention mechanism is proposed for segmenting tumors\nfrom ultrasound images, utilizing a novel LinkNet DL framework with an\nInceptionResNet backbone. Following this, the paper proposes a deep\nconvolutional neural network with an integrated multi-attention framework\n(DCNNIMAF) to classify the segmented tumor as benign, malignant, or normal.\nFrom experimental results, it is observed that the segmentation model has\nrecorded an accuracy of 98.1%, with a minimal loss of 0.6%. It has also\nachieved high Intersection over Union (IoU) and Dice Coefficient scores of\n96.9% and 97.2%, respectively. Similarly, the classification model has attained\nan accuracy of 99.2%, with a low loss of 0.31%. Furthermore, the classification\nframework has achieved outstanding F1-Score, precision, and recall values of\n99.1%, 99.3%, and 99.1%, respectively. By offering a robust framework for early\ndetection and accurate classification of breast cancer, this proposed work\nsignificantly advances the field of medical image analysis, potentially\nimproving diagnostic precision and patient outcomes.\n","authors":["Pandiyaraju V","Shravan Venkatraman","Pavan Kumar S","Santhosh Malarvannan","Kannan A"],"pdf_url":"https://arxiv.org/pdf/2407.02844v3.pdf","comment":"29 pages, 15 figures, 6 tables"},{"id":"http://arxiv.org/abs/2407.10960v1","updated":"2024-07-15T17:55:42Z","published":"2024-07-15T17:55:42Z","title":"Fast Matrix Multiplications for Lookup Table-Quantized LLMs","summary":"  The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.\n","authors":["Han Guo","William Brandon","Radostin Cholakov","Jonathan Ragan-Kelley","Eric P. Xing","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2407.10960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10955v1","updated":"2024-07-15T17:54:03Z","published":"2024-07-15T17:54:03Z","title":"Enhancing Stochastic Optimization for Statistical Efficiency Using\n  ROOT-SGD with Diminishing Stepsize","summary":"  In this paper, we revisit \\textsf{ROOT-SGD}, an innovative method for\nstochastic optimization to bridge the gap between stochastic optimization and\nstatistical efficiency. The proposed method enhances the performance and\nreliability of \\textsf{ROOT-SGD} by integrating a carefully designed\n\\emph{diminishing stepsize strategy}. This approach addresses key challenges in\noptimization, providing robust theoretical guarantees and practical benefits.\nOur analysis demonstrates that \\textsf{ROOT-SGD} with diminishing achieves\noptimal convergence rates while maintaining computational efficiency. By\ndynamically adjusting the learning rate, \\textsf{ROOT-SGD} ensures improved\nstability and precision throughout the optimization process. The findings of\nthis study offer valuable insights for developing advanced optimization\nalgorithms that are both efficient and statistically robust.\n","authors":["Tong Zhang","Chris Junchi Li"],"pdf_url":"https://arxiv.org/pdf/2407.10955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10954v1","updated":"2024-07-15T17:52:22Z","published":"2024-07-15T17:52:22Z","title":"A Unified Differentiable Boolean Operator with Fuzzy Logic","summary":"  This paper presents a unified differentiable boolean operator for implicit\nsolid shape modeling using Constructive Solid Geometry (CSG). Traditional CSG\nrelies on min, max operators to perform boolean operations on implicit shapes.\nBut because these boolean operators are discontinuous and discrete in the\nchoice of operations, this makes optimization over the CSG representation\nchallenging. Drawing inspiration from fuzzy logic, we present a unified boolean\noperator that outputs a continuous function and is differentiable with respect\nto operator types. This enables optimization of both the primitives and the\nboolean operations employed in CSG with continuous optimization techniques,\nsuch as gradient descent. We further demonstrate that such a continuous boolean\noperator allows modeling of both sharp mechanical objects and smooth organic\nshapes with the same framework. Our proposed boolean operator opens up new\npossibilities for future research toward fully continuous CSG optimization.\n","authors":["Hsueh-Ti Derek Liu","Maneesh Agrawala","Cem Yuksel","Tim Omernick","Vinith Misra","Stefano Corazza","Morgan McGuire","Victor Zordan"],"pdf_url":"https://arxiv.org/pdf/2407.10954v1.pdf","comment":"SIGGRAPH'24"},{"id":"http://arxiv.org/abs/2407.10949v1","updated":"2024-07-15T17:45:53Z","published":"2024-07-15T17:45:53Z","title":"Representing Rule-based Chatbots with Transformers","summary":"  Transformer-based chatbots can conduct fluent, natural-sounding\nconversations, but we have limited understanding of the mechanisms underlying\ntheir behavior. Prior work has taken a bottom-up approach to understanding\nTransformers by constructing Transformers for various synthetic and formal\nlanguage tasks, such as regular expressions and Dyck languages. However, it is\nnot obvious how to extend this approach to understand more naturalistic\nconversational agents. In this work, we take a step in this direction by\nconstructing a Transformer that implements the ELIZA program, a classic,\nrule-based chatbot. ELIZA illustrates some of the distinctive challenges of the\nconversational setting, including both local pattern matching and long-term\ndialog state tracking. We build on constructions from prior work -- in\nparticular, for simulating finite-state automata -- showing how simpler\nconstructions can be composed and extended to give rise to more sophisticated\nbehavior. Next, we train Transformers on a dataset of synthetically generated\nELIZA conversations and investigate the mechanisms the models learn. Our\nanalysis illustrates the kinds of mechanisms these models tend to prefer -- for\nexample, models favor an induction head mechanism over a more precise, position\nbased copying mechanism; and using intermediate generations to simulate\nrecurrent data structures, like ELIZA's memory mechanisms. Overall, by drawing\nan explicit connection between neural chatbots and interpretable, symbolic\nmechanisms, our results offer a new setting for mechanistic analysis of\nconversational agents.\n","authors":["Dan Friedman","Abhishek Panigrahi","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10949v1.pdf","comment":"Code and data are available at\n  https://github.com/princeton-nlp/ELIZA-Transformer"},{"id":"http://arxiv.org/abs/2403.11299v2","updated":"2024-07-15T17:37:11Z","published":"2024-03-17T18:42:38Z","title":"SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant","summary":"  Recent advances in vision-language models have shown notable generalization\nin broad tasks through visual instruction tuning. However, bridging the gap\nbetween the pre-trained vision encoder and the large language models (LLMs)\nbecomes the whole network's bottleneck. To improve cross-modality alignment,\nexisting works usually consider more visual instruction data covering a broader\nrange of vision tasks to fine-tune the model for question-answering, which,\nhowever, is costly to obtain and has not thoroughly explored the rich\ncontextual information contained in images. This paper first attempts to\nharness the overlooked context within visual instruction data, training the\nmodel to self-supervised \"learning\" how to ask high-quality questions. In this\nway, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large\nVision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible\nand meaningful image-related questions while analyzing the visual clue and\nprior language knowledge, signifying an advanced level of generalized visual\nunderstanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instruction\ndata shows a performance improvement compared with traditional\nvisual-instruction tuning methods. This improvement highlights the efficacy of\nself-questioning techniques in achieving a deeper and more nuanced\ncomprehension of visual content across various contexts.\n","authors":["Guohao Sun","Can Qin","Jiamian Wang","Zeyuan Chen","Ran Xu","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2403.11299v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2401.03506v7","updated":"2024-07-15T17:32:23Z","published":"2024-01-07T14:54:57Z","title":"DiarizationLM: Speaker Diarization Post-Processing with Large Language\n  Models","summary":"  In this paper, we introduce DiarizationLM, a framework to leverage large\nlanguage models (LLM) to post-process the outputs from a speaker diarization\nsystem. Various goals can be achieved with the proposed framework, such as\nimproving the readability of the diarized transcript, or reducing the word\ndiarization error rate (WDER). In this framework, the outputs of the automatic\nspeech recognition (ASR) and speaker diarization systems are represented as a\ncompact textual format, which is included in the prompt to an optionally\nfinetuned LLM. The outputs of the LLM can be used as the refined diarization\nresults with the desired enhancement. As a post-processing step, this framework\ncan be easily applied to any off-the-shelf ASR and speaker diarization systems\nwithout retraining existing components. Our experiments show that a finetuned\nPaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone\nconversation dataset, and rel. 44.9% on the Callhome English dataset.\n","authors":["Quan Wang","Yiling Huang","Guanlong Zhao","Evan Clark","Wei Xia","Hank Liao"],"pdf_url":"https://arxiv.org/pdf/2401.03506v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10930v1","updated":"2024-07-15T17:30:31Z","published":"2024-07-15T17:30:31Z","title":"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better\n  Together","summary":"  Natural Language Processing (NLP) systems are increasingly taking the form of\nmulti-stage pipelines involving multiple distinct language models (LMs) and\nprompting strategies. Here we address the question of how to fine-tune such\nsystems to improve their performance. We cast this as a problem of optimizing\nthe underlying LM weights and the prompting strategies together, and consider a\nchallenging but highly realistic scenario in which we have no gold labels for\nany intermediate stages in the pipeline. To address this challenge, we evaluate\napproximate optimization strategies in which we bootstrap training labels for\nall pipeline stages and use these to optimize the pipeline's prompts and\nfine-tune its weights alternatingly. In experiments with multi-hop QA,\nmathematical reasoning, and feature-based classification, we find that simple\napproaches for optimizing the prompts and weights together outperform directly\noptimizing weights alone and prompts alone by up to 65% and 5%, respectively,\non average across LMs and tasks. We will release our new optimizers in DSPy at\nhttp://dspy.ai\n","authors":["Dilara Soylu","Christopher Potts","Omar Khattab"],"pdf_url":"https://arxiv.org/pdf/2407.10930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10921v1","updated":"2024-07-15T17:22:16Z","published":"2024-07-15T17:22:16Z","title":"A Dual-Attention Aware Deep Convolutional Neural Network for Early\n  Alzheimer's Detection","summary":"  Alzheimer's disease (AD) represents the primary form of neurodegeneration,\nimpacting millions of individuals each year and causing progressive cognitive\ndecline. Accurately diagnosing and classifying AD using neuroimaging data\npresents ongoing challenges in medicine, necessitating advanced interventions\nthat will enhance treatment measures. In this research, we introduce a dual\nattention enhanced deep learning (DL) framework for classifying AD from\nneuroimaging data. Combined spatial and self-attention mechanisms play a vital\nrole in emphasizing focus on neurofibrillary tangles and amyloid plaques from\nthe MRI images, which are difficult to discern with regular imaging techniques.\nResults demonstrate that our model yielded remarkable performance in comparison\nto existing state of the art (SOTA) convolutional neural networks (CNNs), with\nan accuracy of 99.1%. Moreover, it recorded remarkable metrics, with an\nF1-Score of 99.31%, a precision of 99.24%, and a recall of 99.5%. These results\nhighlight the promise of cutting edge DL methods in medical diagnostics,\ncontributing to highly reliable and more efficient healthcare solutions.\n","authors":["Pandiyaraju V","Shravan Venkatraman","Abeshek A","Aravintakshan S A","Pavan Kumar S","Kannan A"],"pdf_url":"https://arxiv.org/pdf/2407.10921v1.pdf","comment":"18 pages, 10 figures, 6 tables"},{"id":"http://arxiv.org/abs/2407.10916v1","updated":"2024-07-15T17:18:42Z","published":"2024-07-15T17:18:42Z","title":"When Heterophily Meets Heterogeneity: New Graph Benchmarks and Effective\n  Methods","summary":"  Many real-world graphs frequently present challenges for graph learning due\nto the presence of both heterophily and heterogeneity. However, existing\nbenchmarks for graph learning often focus on heterogeneous graphs with\nhomophily or homogeneous graphs with heterophily, leaving a gap in\nunderstanding how methods perform on graphs that are both heterogeneous and\nheterophilic. To bridge this gap, we introduce H2GB, a novel graph benchmark\nthat brings together the complexities of both the heterophily and heterogeneity\nproperties of graphs. Our benchmark encompasses 9 diverse real-world datasets\nacross 5 domains, 28 baseline model implementations, and 26 benchmark results.\nIn addition, we present a modular graph transformer framework UnifiedGT and a\nnew model variant, H2G-former, that excels at this challenging benchmark. By\nintegrating masked label embeddings, cross-type heterogeneous attention, and\ntype-specific FFNs, H2G-former effectively tackles graph heterophily and\nheterogeneity. Extensive experiments across 26 baselines on H2GB reveal\ninadequacies of current models on heterogeneous heterophilic graph learning,\nand demonstrate the superiority of our H2G-former over existing solutions. Both\nthe benchmark and the framework are available on GitHub\n(https://github.com/junhongmit/H2GB) and PyPI (https://pypi.org/project/H2GB),\nand documentation can be found at https://junhongmit.github.io/H2GB/.\n","authors":["Junhong Lin","Xiaojie Guo","Shuaicheng Zhang","Dawei Zhou","Yada Zhu","Julian Shun"],"pdf_url":"https://arxiv.org/pdf/2407.10916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10910v1","updated":"2024-07-15T17:10:31Z","published":"2024-07-15T17:10:31Z","title":"DataDream: Few-shot Guided Dataset Generation","summary":"  While text-to-image diffusion models have been shown to achieve\nstate-of-the-art results in image synthesis, they have yet to prove their\neffectiveness in downstream applications. Previous work has proposed to\ngenerate data for image classifier training given limited real data access.\nHowever, these methods struggle to generate in-distribution images or depict\nfine-grained features, thereby hindering the generalization of classification\nmodels trained on synthetic datasets. We propose DataDream, a framework for\nsynthesizing classification datasets that more faithfully represents the real\ndata distribution when guided by few-shot examples of the target classes.\nDataDream fine-tunes LoRA weights for the image generation model on the few\nreal images before generating the training data using the adapted model. We\nthen fine-tune LoRA weights for CLIP using the synthetic data to improve\ndownstream image classification over previous approaches on a large variety of\ndatasets. We demonstrate the efficacy of DataDream through extensive\nexperiments, surpassing state-of-the-art classification accuracy with few-shot\ndata across 7 out of 10 datasets, while being competitive on the other 3.\nAdditionally, we provide insights into the impact of various factors, such as\nthe number of real-shot and generated images as well as the fine-tuning compute\non model performance. The code is available at\nhttps://github.com/ExplainableML/DataDream.\n","authors":["Jae Myung Kim","Jessica Bader","Stephan Alaniz","Cordelia Schmid","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2407.10910v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2403.05996v2","updated":"2024-07-15T17:08:06Z","published":"2024-03-09T19:56:40Z","title":"Dissecting Deep RL with High Update Ratios: Combatting Value Divergence","summary":"  We show that deep reinforcement learning algorithms can retain their ability\nto learn without resetting network parameters in settings where the number of\ngradient updates greatly exceeds the number of environment samples by\ncombatting value function divergence. Under large update-to-data ratios, a\nrecent study by Nikishin et al. (2022) suggested the emergence of a primacy\nbias, in which agents overfit early interactions and downplay later experience,\nimpairing their ability to learn. In this work, we investigate the phenomena\nleading to the primacy bias. We inspect the early stages of training that were\nconjectured to cause the failure to learn and find that one fundamental\nchallenge is a long-standing acquaintance: value function divergence.\nOverinflated Q-values are found not only on out-of-distribution but also\nin-distribution data and can be linked to overestimation on unseen action\nprediction propelled by optimizer momentum. We employ a simple unit-ball\nnormalization that enables learning under large update ratios, show its\nefficacy on the widely used dm_control suite, and obtain strong performance on\nthe challenging dog tasks, competitive with model-based approaches. Our results\nquestion, in parts, the prior explanation for sub-optimal learning due to\noverfitting early data.\n","authors":["Marcel Hussing","Claas Voelcker","Igor Gilitschenski","Amir-massoud Farahmand","Eric Eaton"],"pdf_url":"https://arxiv.org/pdf/2403.05996v2.pdf","comment":"Accepted as a conference paper at the First Reinforcement Learning\n  Conference (RLC)"},{"id":"http://arxiv.org/abs/2407.08868v2","updated":"2024-07-15T16:47:42Z","published":"2024-07-11T21:10:03Z","title":"Generalizable Physics-Informed Learning for Stochastic Safety-Critical\n  Systems","summary":"  Accurate estimate of long-term risk is critical for safe decision-making, but\nsampling from rare risk events and long-term trajectories can be prohibitively\ncostly. Risk gradient can be used in many first-order techniques for learning\nand control methods, but gradient estimate is difficult to obtain using Monte\nCarlo (MC) methods because the infinitesimal divisor may significantly amplify\nsampling noise. Motivated by this gap, we propose an efficient method to\nevaluate long-term risk probabilities and their gradients using short-term\nsamples without sufficient risk events. We first derive that four types of\nlong-term risk probability are solutions of certain partial differential\nequations (PDEs). Then, we propose a physics-informed learning technique that\nintegrates data and physics information (aforementioned PDEs). The physics\ninformation helps propagate information beyond available data and obtain\nprovable generalization beyond available data, which in turn enables long-term\nrisk to be estimated using short-term samples of safe events. Finally, we\ndemonstrate in simulation that the proposed technique has improved sample\nefficiency, generalizes well to unseen regions, and adapts to changing system\nparameters.\n","authors":["Zhuoyuan Wang","Albert Chern","Yorie Nakahira"],"pdf_url":"https://arxiv.org/pdf/2407.08868v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2305.06432"},{"id":"http://arxiv.org/abs/2407.10897v1","updated":"2024-07-15T16:46:14Z","published":"2024-07-15T16:46:14Z","title":"Optical Diffusion Models for Image Generation","summary":"  Diffusion models generate new samples by progressively decreasing the noise\nfrom the initially provided random distribution. This inference procedure\ngenerally utilizes a trained neural network numerous times to obtain the final\noutput, creating significant latency and energy consumption on digital\nelectronic hardware such as GPUs. In this study, we demonstrate that the\npropagation of a light beam through a semi-transparent medium can be programmed\nto implement a denoising diffusion model on image samples. This framework\nprojects noisy image patterns through passive diffractive optical layers, which\ncollectively only transmit the predicted noise term in the image. The optical\ntransparent layers, which are trained with an online training approach,\nbackpropagating the error to the analytical model of the system, are passive\nand kept the same across different steps of denoising. Hence this method\nenables high-speed image generation with minimal power consumption, benefiting\nfrom the bandwidth and energy efficiency of optical information processing.\n","authors":["Ilker Oguz","Niyazi Ulas Dinc","Mustafa Yildirim","Junjie Ke","Innfarn Yoo","Qifei Wang","Feng Yang","Christophe Moser","Demetri Psaltis"],"pdf_url":"https://arxiv.org/pdf/2407.10897v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.20037v2","updated":"2024-07-15T16:42:24Z","published":"2024-06-28T16:34:22Z","title":"Explore as a Storm, Exploit as a Raindrop: On the Benefit of Fine-Tuning\n  Kernel Schedulers with Coordinate Descent","summary":"  Machine-learning models consist of kernels, which are algorithms applying\noperations on tensors -- data indexed by a linear combination of natural\nnumbers. Examples of kernels include convolutions, transpositions, and\nvectorial products. There are many ways to implement a kernel. These\nimplementations form the kernel's optimization space. Kernel scheduling is the\nproblem of finding the best implementation, given an objective function --\ntypically execution speed. Kernel optimizers such as Ansor, Halide, and AutoTVM\nsolve this problem via search heuristics, which combine two phases: exploration\nand exploitation. The first step evaluates many different kernel optimization\nspaces. The latter tries to improve the best implementations by investigating a\nkernel within the same space. For example, Ansor combines kernel generation\nthrough sketches for exploration and leverages an evolutionary algorithm to\nexploit the best sketches. In this work, we demonstrate the potential to reduce\nAnsor's search time while enhancing kernel quality by incorporating Droplet\nSearch, an AutoTVM algorithm, into Ansor's exploration phase. The approach\ninvolves limiting the number of samples explored by Ansor, selecting the best,\nand exploiting it with a coordinate descent algorithm. By applying this\napproach to the first 300 kernels that Ansor generates, we usually obtain\nbetter kernels in less time than if we let Ansor analyze 10,000 kernels. This\nresult has been replicated in 20 well-known deep-learning models (AlexNet,\nResNet, VGG, DenseNet, etc.) running on four architectures: an AMD Ryzen 7\n(x86), an NVIDIA A100 tensor core, an NVIDIA RTX 3080 GPU, and an ARM A64FX. A\npatch with this combined approach was approved in Ansor in February 2024. As\nevidence of the generality of this search methodology, a similar patch,\nachieving equally good results, was submitted to TVM's MetaSchedule in June\n2024.\n","authors":["Michael Canesche","Gaurav Verma","Fernando Magno Quintao Pereira"],"pdf_url":"https://arxiv.org/pdf/2406.20037v2.pdf","comment":"22 pages, 19 figures, original work"},{"id":"http://arxiv.org/abs/2407.10886v1","updated":"2024-07-15T16:37:55Z","published":"2024-07-15T16:37:55Z","title":"SLIP: Securing LLMs IP Using Weights Decomposition","summary":"  Large language models (LLMs) have recently seen widespread adoption, in both\nacademia and industry. As these models grow, they become valuable intellectual\nproperty (IP), reflecting enormous investments by their owners. Moreover, the\nhigh cost of cloud-based deployment has driven interest towards deployment to\nedge devices, yet this risks exposing valuable parameters to theft and\nunauthorized use. Current methods to protect models' IP on the edge have\nlimitations in terms of practicality, loss in accuracy, or suitability to\nrequirements. In this paper, we introduce a novel hybrid inference algorithm,\nnamed SLIP, designed to protect edge-deployed models from theft. SLIP is the\nfirst hybrid protocol that is both practical for real-world applications and\nprovably secure, while having zero accuracy degradation and minimal impact on\nlatency. It involves partitioning the model between two computing resources,\none secure but expensive, and another cost-effective but vulnerable. This is\nachieved through matrix decomposition, ensuring that the secure resource\nretains a maximally sensitive portion of the model's IP while performing a\nminimal amount of computations, and vice versa for the vulnerable resource.\nImportantly, the protocol includes security guarantees that prevent attackers\nfrom exploiting the partition to infer the secured information. Finally, we\npresent experimental results that show the robustness and effectiveness of our\nmethod, positioning it as a compelling solution for protecting LLMs.\n","authors":["Yehonathan Refael","Adam Hakim","Lev Greenberg","Tal Aviv","Satya Lokam","Ben Fishman","Shachar Seidman"],"pdf_url":"https://arxiv.org/pdf/2407.10886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12128v2","updated":"2024-07-15T16:32:39Z","published":"2023-10-18T17:37:10Z","title":"DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM\n  Planning","summary":"  Text-to-image (T2I) generation has seen significant growth over the past few\nyears. Despite this, there has been little work on generating diagrams with T2I\nmodels. A diagram is a symbolic/schematic representation that explains\ninformation using structurally rich and spatially complex visualizations (e.g.,\na dense combination of related objects, text labels, directional arrows/lines,\netc.). Existing state-of-the-art T2I models often fail at diagram generation\nbecause they lack fine-grained object layout control when many objects are\ndensely connected via complex relations such as arrows/lines, and also often\nfail to render comprehensible text labels. To address this gap, we present\nDiagrammerGPT, a novel two-stage text-to-diagram generation framework\nleveraging the layout guidance capabilities of LLMs to generate more accurate\ndiagrams. In the first stage, we use LLMs to generate and iteratively refine\n'diagram plans' (in a planner-auditor feedback loop). In the second stage, we\nuse a diagram generator, DiagramGLIGEN, and a text label rendering module to\ngenerate diagrams (with clear text labels) following the diagram plans. To\nbenchmark the text-to-diagram generation task, we introduce AI2D-Caption, a\ndensely annotated diagram dataset built on top of the AI2D dataset. We show\nthat our DiagrammerGPT framework produces more accurate diagrams, outperforming\nexisting T2I models. We also provide comprehensive analysis, including\nopen-domain diagram generation, multi-platform vector graphic diagram\ngeneration, human-in-the-loop editing, and multimodal planner/auditor LLMs.\n","authors":["Abhay Zala","Han Lin","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2310.12128v2.pdf","comment":"COLM 2024; Project page: https://diagrammerGPT.github.io/"},{"id":"http://arxiv.org/abs/2407.10878v1","updated":"2024-07-15T16:28:26Z","published":"2024-07-15T16:28:26Z","title":"Deep Causal Learning to Explain and Quantify The Geo-Tension's Impact on\n  Natural Gas Market","summary":"  Natural gas demand is a crucial factor for predicting natural gas prices and\nthus has a direct influence on the power system. However, existing methods face\nchallenges in assessing the impact of shocks, such as the outbreak of the\nRussian-Ukrainian war. In this context, we apply deep neural network-based\nGranger causality to identify important drivers of natural gas demand.\nFurthermore, the resulting dependencies are used to construct a counterfactual\ncase without the outbreak of the war, providing a quantifiable estimate of the\noverall effect of the shock on various German energy sectors. The code and\ndataset are available at https://github.com/bonaldli/CausalEnergy.\n","authors":["Philipp Kai Peter","Yulin Li","Ziyue Li","Wolfgang Ketter"],"pdf_url":"https://arxiv.org/pdf/2407.10878v1.pdf","comment":"Accepted by IEEE PES ISGT Europe 2024 conference"},{"id":"http://arxiv.org/abs/2407.10874v1","updated":"2024-07-15T16:23:53Z","published":"2024-07-15T16:23:53Z","title":"Random Channel Ablation for Robust Hand Gesture Classification with\n  Multimodal Biosignals","summary":"  Biosignal-based hand gesture classification is an important component of\neffective human-machine interaction. For multimodal biosignal sensing, the\nmodalities often face data loss due to missing channels in the data which can\nadversely affect the gesture classification performance. To make the\nclassifiers robust to missing channels in the data, this paper proposes using\nRandom Channel Ablation (RChA) during the training process. Ultrasound and\nforce myography (FMG) data were acquired from the forearm for 12 hand gestures\nover 2 subjects. The resulting multimodal data had 16 total channels, 8 for\neach modality. The proposed method was applied to convolutional neural network\narchitecture, and compared with baseline, imputation, and oracle methods. Using\n5-fold cross-validation for the two subjects, on average, 12.2% and 24.5%\nimprovement was observed for gesture classification with up to 4 and 8 missing\nchannels respectively compared to the baseline. Notably, the proposed method is\nalso robust to an increase in the number of missing channels compared to other\nmethods. These results show the efficacy of using random channel ablation to\nimprove classifier robustness for multimodal and multi-channel biosignal-based\nhand gesture classification.\n","authors":["Keshav Bimbraw","Jing Liu","Ye Wang","Toshiaki Koike-Akino"],"pdf_url":"https://arxiv.org/pdf/2407.10874v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.09852v2","updated":"2024-07-15T16:19:13Z","published":"2023-12-15T14:58:34Z","title":"Learning Distributions on Manifolds with Free-form Flows","summary":"  We propose Manifold Free-Form Flows (M-FFF), a simple new generative model\nfor data on manifolds. The existing approaches to learning a distribution on\narbitrary manifolds are expensive at inference time, since sampling requires\nsolving a differential equation. Our method overcomes this limitation by\nsampling in a single function evaluation. The key innovation is to optimize a\nneural network via maximum likelihood on the manifold, possible by adapting the\nfree-form flow framework to Riemannian manifolds. M-FFF is straightforwardly\nadapted to any manifold with a known projection. It consistently matches or\noutperforms previous single-step methods specialized to specific manifolds, and\nis competitive with multi-step methods with typically two orders of magnitude\nfaster inference speed. We make our code public at\nhttps://github.com/vislearn/FFF.\n","authors":["Peter Sorrenson","Felix Draxler","Armand Rousselot","Sander Hummerich","Ullrich KÃ¶the"],"pdf_url":"https://arxiv.org/pdf/2312.09852v2.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2407.10870v1","updated":"2024-07-15T16:18:06Z","published":"2024-07-15T16:18:06Z","title":"GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via\n  VLM","summary":"  Large vision-language models (LVLMs), such as the Generative Pre-trained\nTransformer 4-omni (GPT-4o), are emerging multi-modal foundation models which\nhave great potential as powerful artificial-intelligence (AI) assistance tools\nfor a myriad of applications, including healthcare, industrial, and academic\nsectors. Although such foundation models perform well in a wide range of\ngeneral tasks, their capability without fine-tuning is often limited in\nspecialized tasks. However, full fine-tuning of large foundation models is\nchallenging due to enormous computation/memory/dataset requirements. We show\nthat GPT-4o can decode hand gestures from forearm ultrasound data even with no\nfine-tuning, and improves with few-shot, in-context learning.\n","authors":["Keshav Bimbraw","Ye Wang","Jing Liu","Toshiaki Koike-Akino"],"pdf_url":"https://arxiv.org/pdf/2407.10870v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.10867v1","updated":"2024-07-15T16:12:51Z","published":"2024-07-15T16:12:51Z","title":"Provable Robustness of (Graph) Neural Networks Against Data Poisoning\n  and Backdoor Attacks","summary":"  Generalization of machine learning models can be severely compromised by data\npoisoning, where adversarial changes are applied to the training data, as well\nas backdoor attacks that additionally manipulate the test data. These\nvulnerabilities have led to interest in certifying (i.e., proving) that such\nchanges up to a certain magnitude do not affect test predictions. We, for the\nfirst time, certify Graph Neural Networks (GNNs) against poisoning and backdoor\nattacks targeting the node features of a given graph. Our certificates are\nwhite-box and based upon $(i)$ the neural tangent kernel, which characterizes\nthe training dynamics of sufficiently wide networks; and $(ii)$ a novel\nreformulation of the bilevel optimization problem describing poisoning as a\nmixed-integer linear program. Consequently, we leverage our framework to\nprovide fundamental insights into the role of graph structure and its\nconnectivity on the worst-case robustness behavior of convolution-based and\nPageRank-based GNNs. We note that our framework is more general and constitutes\nthe first approach to derive white-box poisoning certificates for NNs, which\ncan be of independent interest beyond graph-related tasks.\n","authors":["Lukas Gosch","Mahalakshmi Sabanayagam","Debarghya Ghoshdastidar","Stephan GÃ¼nnemann"],"pdf_url":"https://arxiv.org/pdf/2407.10867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10854v1","updated":"2024-07-15T16:06:20Z","published":"2024-07-15T16:06:20Z","title":"Principal Component Flow Map Learning of PDEs from Incomplete, Limited,\n  and Noisy Data","summary":"  We present a computational technique for modeling the evolution of dynamical\nsystems in a reduced basis, with a focus on the challenging problem of modeling\npartially-observed partial differential equations (PDEs) on high-dimensional\nnon-uniform grids. We address limitations of previous work on data-driven flow\nmap learning in the sense that we focus on noisy and limited data to move\ntoward data collection scenarios in real-world applications. Leveraging recent\nwork on modeling PDEs in modal and nodal spaces, we present a neural network\nstructure that is suitable for PDE modeling with noisy and limited data\navailable only on a subset of the state variables or computational domain. In\nparticular, spatial grid-point measurements are reduced using a learned linear\ntransformation, after which the dynamics are learned in this reduced basis\nbefore being transformed back out to the nodal space. This approach yields a\ndrastically reduced parameterization of the neural network compared with\nprevious flow map models for nodal space learning. This primarily allows for\nsmaller training data sets, but also enables reduced training times.\n","authors":["Victor Churchill"],"pdf_url":"https://arxiv.org/pdf/2407.10854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10844v1","updated":"2024-07-15T15:59:39Z","published":"2024-07-15T15:59:39Z","title":"Rotationally Invariant Latent Distances for Uncertainty Estimation of\n  Relaxed Energy Predictions by Graph Neural Network Potentials","summary":"  Graph neural networks (GNNs) have been shown to be astonishingly capable\nmodels for molecular property prediction, particularly as surrogates for\nexpensive density functional theory calculations of relaxed energy for novel\nmaterial discovery. However, one limitation of GNNs in this context is the lack\nof useful uncertainty prediction methods, as this is critical to the material\ndiscovery pipeline. In this work, we show that uncertainty quantification for\nrelaxed energy calculations is more complex than uncertainty quantification for\nother kinds of molecular property prediction, due to the effect that structure\noptimizations have on the error distribution. We propose that distribution-free\ntechniques are more useful tools for assessing calibration, recalibrating, and\ndeveloping uncertainty prediction methods for GNNs performing relaxed energy\ncalculations. We also develop a relaxed energy task for evaluating uncertainty\nmethods for equivariant GNNs, based on distribution-free recalibration and\nusing the Open Catalyst Project dataset. We benchmark a set of popular\nuncertainty prediction methods on this task, and show that latent distance\nmethods, with our novel improvements, are the most well-calibrated and\neconomical approach for relaxed energy calculations. Finally, we demonstrate\nthat our latent space distance method produces results which align with our\nexpectations on a clustering example, and on specific equation of state and\nadsorbate coverage examples from outside the training dataset.\n","authors":["Joseph Musielewicz","Janice Lan","Matt Uyttendaele","John R. Kitchin"],"pdf_url":"https://arxiv.org/pdf/2407.10844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10839v1","updated":"2024-07-15T15:53:13Z","published":"2024-07-15T15:53:13Z","title":"Offline Reinforcement Learning with Imputed Rewards","summary":"  Offline Reinforcement Learning (ORL) offers a robust solution to training\nagents in applications where interactions with the environment must be strictly\nlimited due to cost, safety, or lack of accurate simulation environments.\nDespite its potential to facilitate deployment of artificial agents in the real\nworld, Offline Reinforcement Learning typically requires very many\ndemonstrations annotated with ground-truth rewards. Consequently,\nstate-of-the-art ORL algorithms can be difficult or impossible to apply in\ndata-scarce scenarios. In this paper we propose a simple but effective Reward\nModel that can estimate the reward signal from a very limited sample of\nenvironment transitions annotated with rewards. Once the reward signal is\nmodeled, we use the Reward Model to impute rewards for a large sample of\nreward-free transitions, thus enabling the application of ORL techniques. We\ndemonstrate the potential of our approach on several D4RL continuous locomotion\ntasks. Our results show that, using only 1\\% of reward-labeled transitions from\nthe original datasets, our learned reward model is able to impute rewards for\nthe remaining 99\\% of the transitions, from which performant agents can be\nlearned using Offline Reinforcement Learning.\n","authors":["Carlo Romeo","Andrew D. Bagdanov"],"pdf_url":"https://arxiv.org/pdf/2407.10839v1.pdf","comment":"RLBRew Workshop @ RLC 2024"},{"id":"http://arxiv.org/abs/2407.10836v1","updated":"2024-07-15T15:47:24Z","published":"2024-07-15T15:47:24Z","title":"Data-Guided Physics-Informed Neural Networks for Solving Inverse\n  Problems in Partial Differential Equations","summary":"  Physics-informed neural networks (PINNs) represent a significant advancement\nin scientific machine learning by integrating fundamental physical laws into\ntheir architecture through loss functions. PINNs have been successfully applied\nto solve various forward and inverse problems in partial differential equations\n(PDEs). However, a notable challenge can emerge during the early training\nstages when solving inverse problems. Specifically, data losses remain high\nwhile PDE residual losses are minimized rapidly, thereby exacerbating the\nimbalance between loss terms and impeding the overall efficiency of PINNs. To\naddress this challenge, this study proposes a novel framework termed\ndata-guided physics-informed neural networks (DG-PINNs). The DG-PINNs framework\nis structured into two distinct phases: a pre-training phase and a fine-tuning\nphase. In the pre-training phase, a loss function with only the data loss is\nminimized in a neural network. In the fine-tuning phase, a composite loss\nfunction, which consists of the data loss, PDE residual loss, and, if\navailable, initial and boundary condition losses, is minimized in the same\nneural network. Notably, the pre-training phase ensures that the data loss is\nalready at a low value before the fine-tuning phase commences. This approach\nenables the fine-tuning phase to converge to a minimal composite loss function\nwith fewer iterations compared to existing PINNs. To validate the\neffectiveness, noise-robustness, and efficiency of DG-PINNs, extensive\nnumerical investigations are conducted on inverse problems related to several\nclassical PDEs, including the heat equation, wave equation, Euler--Bernoulli\nbeam equation, and Navier--Stokes equation. The numerical results demonstrate\nthat DG-PINNs can accurately solve these inverse problems and exhibit\nrobustness against noise in training data.\n","authors":["Wei Zhou","Y. F. Xu"],"pdf_url":"https://arxiv.org/pdf/2407.10836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10835v1","updated":"2024-07-15T15:45:29Z","published":"2024-07-15T15:45:29Z","title":"Exploration in Knowledge Transfer Utilizing Reinforcement Learning","summary":"  The contribution focuses on the problem of exploration within the task of\nknowledge transfer. Knowledge transfer refers to the useful application of the\nknowledge gained while learning the source task in the target task. The\nintended benefit of knowledge transfer is to speed up the learning process of\nthe target task. The article aims to compare several exploration methods used\nwithin a deep transfer learning algorithm, particularly Deep Target Transfer\n$Q$-learning. The methods used are $\\epsilon$-greedy, Boltzmann, and upper\nconfidence bound exploration. The aforementioned transfer learning algorithms\nand exploration methods were tested on the virtual drone problem. The results\nhave shown that the upper confidence bound algorithm performs the best out of\nthese options. Its sustainability to other applications is to be checked.\n","authors":["Adam JedliÄka","Tatiana Valentine Guy"],"pdf_url":"https://arxiv.org/pdf/2407.10835v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2407.10834v1","updated":"2024-07-15T15:45:07Z","published":"2024-07-15T15:45:07Z","title":"MetaLLM: A High-performant and Cost-efficient Dynamic Framework for\n  Wrapping LLMs","summary":"  The rapid progress in machine learning (ML) has brought forth many large\nlanguage models (LLMs) that excel in various tasks and areas. These LLMs come\nwith different abilities and costs in terms of computation or pricing. Since\nthe demand for each query can vary, e.g., because of the queried domain or its\ncomplexity, defaulting to one LLM in an application is not usually the best\nchoice, whether it is the biggest, priciest, or even the one with the best\naverage test performance. Consequently, picking the right LLM that is both\naccurate and cost-effective for an application remains a challenge. In this\npaper, we introduce MetaLLM, a framework that dynamically and intelligently\nroutes each query to the optimal LLM (among several available LLMs) for\nclassification tasks, achieving significantly improved accuracy and\ncost-effectiveness. By framing the selection problem as a multi-armed bandit,\nMetaLLM balances prediction accuracy and cost efficiency under uncertainty. Our\nexperiments, conducted on popular LLM platforms such as OpenAI's GPT models,\nAmazon's Titan, Anthropic's Claude, and Meta's LLaMa, showcase MetaLLM's\nefficacy in real-world scenarios, laying the groundwork for future extensions\nbeyond classification tasks.\n","authors":["Quang H. Nguyen","Duy C. Hoang","Juliette Decugis","Saurav Manchanda","Nitesh V. Chawla","Khoa D. Doan"],"pdf_url":"https://arxiv.org/pdf/2407.10834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10827v1","updated":"2024-07-15T15:38:51Z","published":"2024-07-15T15:38:51Z","title":"LLM Circuit Analyses Are Consistent Across Training and Scale","summary":"  Most currently deployed large language models (LLMs) undergo continuous\ntraining or additional finetuning. By contrast, most research into LLMs'\ninternal mechanisms focuses on models at one snapshot in time (the end of\npre-training), raising the question of whether their results generalize to\nreal-world settings. Existing studies of mechanisms over time focus on\nencoder-only or toy models, which differ significantly from most deployed\nmodels. In this study, we track how model mechanisms, operationalized as\ncircuits, emerge and evolve across 300 billion tokens of training in\ndecoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.\nWe find that task abilities and the functional components that support them\nemerge consistently at similar token counts across scale. Moreover, although\nsuch components may be implemented by different attention heads over time, the\noverarching algorithm that they implement remains. Surprisingly, both these\nalgorithms and the types of components involved therein can replicate across\nmodel scale. These results suggest that circuit analyses conducted on small\nmodels at the end of pre-training can provide insights that still apply after\nadditional pre-training and over model scale.\n","authors":["Curt Tigges","Michael Hanna","Qinan Yu","Stella Biderman"],"pdf_url":"https://arxiv.org/pdf/2407.10827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10825v1","updated":"2024-07-15T15:38:21Z","published":"2024-07-15T15:38:21Z","title":"Wicked Oddities: Selectively Poisoning for Effective Clean-Label\n  Backdoor Attacks","summary":"  Deep neural networks are vulnerable to backdoor attacks, a type of\nadversarial attack that poisons the training data to manipulate the behavior of\nmodels trained on such data. Clean-label attacks are a more stealthy form of\nbackdoor attacks that can perform the attack without changing the labels of\npoisoned data. Early works on clean-label attacks added triggers to a random\nsubset of the training set, ignoring the fact that samples contribute unequally\nto the attack's success. This results in high poisoning rates and low attack\nsuccess rates. To alleviate the problem, several supervised learning-based\nsample selection strategies have been proposed. However, these methods assume\naccess to the entire labeled training set and require training, which is\nexpensive and may not always be practical. This work studies a new and more\npractical (but also more challenging) threat model where the attacker only\nprovides data for the target class (e.g., in face recognition systems) and has\nno knowledge of the victim model or any other classes in the training set. We\nstudy different strategies for selectively poisoning a small set of training\nsamples in the target class to boost the attack success rate in this setting.\nOur threat model poses a serious threat in training machine learning models\nwith third-party datasets, since the attack can be performed effectively with\nlimited information. Experiments on benchmark datasets illustrate the\neffectiveness of our strategies in improving clean-label backdoor attacks.\n","authors":["Quang H. Nguyen","Nguyen Ngoc-Hieu","The-Anh Ta","Thanh Nguyen-Tang","Hoang Thanh-Tung","Khoa D. Doan"],"pdf_url":"https://arxiv.org/pdf/2407.10825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15474v3","updated":"2024-07-15T15:36:37Z","published":"2023-12-24T13:09:08Z","title":"A Conservative Approach for Few-Shot Transfer in Off-Dynamics\n  Reinforcement Learning","summary":"  Off-dynamics Reinforcement Learning (ODRL) seeks to transfer a policy from a\nsource environment to a target environment characterized by distinct yet\nsimilar dynamics. In this context, traditional RL agents depend excessively on\nthe dynamics of the source environment, resulting in the discovery of policies\nthat excel in this environment but fail to provide reasonable performance in\nthe target one. In the few-shot framework, a limited number of transitions from\nthe target environment are introduced to facilitate a more effective transfer.\nAddressing this challenge, we propose an innovative approach inspired by recent\nadvancements in Imitation Learning and conservative RL algorithms. The proposed\nmethod introduces a penalty to regulate the trajectories generated by the\nsource-trained policy. We evaluate our method across various environments\nrepresenting diverse off-dynamics conditions, where access to the target\nenvironment is extremely limited. These experiments include high-dimensional\nsystems relevant to real-world applications. Across most tested scenarios, our\nproposed method demonstrates performance improvements compared to existing\nbaselines.\n","authors":["Paul Daoudi","Christophe Prieur","Bogdan Robu","Merwan Barlier","Ludovic Dos Santos"],"pdf_url":"https://arxiv.org/pdf/2312.15474v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10817v1","updated":"2024-07-15T15:33:45Z","published":"2024-07-15T15:33:45Z","title":"Foundational Autoraters: Taming Large Language Models for Better\n  Automatic Evaluation","summary":"  As large language models (LLMs) advance, it becomes more challenging to\nreliably evaluate their output due to the high costs of human evaluation. To\nmake progress towards better LLM autoraters, we introduce FLAMe, a family of\nFoundational Large Autorater Models. FLAMe is trained on our large and diverse\ncollection of 100+ quality assessment tasks comprising 5M+ human judgments,\ncurated and standardized using publicly released human evaluations from\nprevious research. FLAMe significantly improves generalization to a wide\nvariety of held-out tasks, outperforming LLMs trained on proprietary data like\nGPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a\npowerful starting point for further downstream fine-tuning, using reward\nmodeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our\nFLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative\nmodel trained exclusively on permissively licensed data, outperforming both\nGPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more\ncomputationally efficient approach using a novel tail-patch fine-tuning\nstrategy to optimize our FLAMe multitask mixture for reward modeling evaluation\n(FLAMe-Opt-RM), offering competitive RewardBench performance while requiring\napproximately 25x less training datapoints. Overall, our FLAMe variants\noutperform all popular proprietary LLM-as-a-Judge models we consider across 8\nout of 12 autorater evaluation benchmarks, encompassing 53 quality assessment\ntasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals\nthat FLAMe is significantly less biased than these LLM-as-a-Judge models on the\nCoBBLEr autorater bias benchmark, while effectively identifying high-quality\nresponses for code generation.\n","authors":["Tu Vu","Kalpesh Krishna","Salaheddin Alzubi","Chris Tar","Manaal Faruqui","Yun-Hsuan Sung"],"pdf_url":"https://arxiv.org/pdf/2407.10817v1.pdf","comment":"31 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2308.12112v3","updated":"2024-07-15T15:33:10Z","published":"2023-08-23T13:02:52Z","title":"Category Adaptation Meets Projected Distillation in Generalized\n  Continual Category Discovery","summary":"  Generalized Continual Category Discovery (GCCD) tackles learning from\nsequentially arriving, partially labeled datasets while uncovering new\ncategories. Traditional methods depend on feature distillation to prevent\nforgetting the old knowledge. However, this strategy restricts the model's\nability to adapt and effectively distinguish new categories. To address this,\nwe introduce a novel technique integrating a learnable projector with feature\ndistillation, thus enhancing model adaptability without sacrificing past\nknowledge. The resulting distribution shift of the previously learned\ncategories is mitigated with the auxiliary category adaptation network. We\ndemonstrate that while each component offers modest benefits individually,\ntheir combination - dubbed CAMP (Category Adaptation Meets Projected\ndistillation) - significantly improves the balance between learning new\ninformation and retaining old. CAMP exhibits superior performance across\nseveral GCCD and Class Incremental Learning scenarios. The code is available at\nhttps://github.com/grypesc/CAMP.\n","authors":["Grzegorz RypeÅÄ","Daniel Marczak","Sebastian Cygert","Tomasz TrzciÅski","BartÅomiej Twardowski"],"pdf_url":"https://arxiv.org/pdf/2308.12112v3.pdf","comment":"Accepted for ECCV 2024"},{"id":"http://arxiv.org/abs/2402.13654v2","updated":"2024-07-15T15:27:46Z","published":"2024-02-21T09:40:26Z","title":"Improving a Proportional Integral Controller with Reinforcement Learning\n  on a Throttle Valve Benchmark","summary":"  This paper presents a learning-based control strategy for non-linear throttle\nvalves with an asymmetric hysteresis, leading to a near-optimal controller\nwithout requiring any prior knowledge about the environment. We start with a\ncarefully tuned Proportional Integrator (PI) controller and exploit the recent\nadvances in Reinforcement Learning (RL) with Guides to improve the closed-loop\nbehavior by learning from the additional interactions with the valve. We test\nthe proposed control method in various scenarios on three different valves, all\nhighlighting the benefits of combining both PI and RL frameworks to improve\ncontrol performance in non-linear stochastic systems. In all the experimental\ntest cases, the resulting agent has a better sample efficiency than traditional\nRL agents and outperforms the PI controller.\n","authors":["Paul Daoudi","Bojan Mavkov","Bogdan Robu","Christophe Prieur","Emmanuel Witrant","Merwan Barlier","Ludovic Dos Santos"],"pdf_url":"https://arxiv.org/pdf/2402.13654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10811v1","updated":"2024-07-15T15:26:10Z","published":"2024-07-15T15:26:10Z","title":"GuideLight: \"Industrial Solution\" Guidance for More Practical Traffic\n  Signal Control Agents","summary":"  Currently, traffic signal control (TSC) methods based on reinforcement\nlearning (RL) have proven superior to traditional methods. However, most RL\nmethods face difficulties when applied in the real world due to three factors:\ninput, output, and the cycle-flow relation. The industry's observable input is\nmuch more limited than simulation-based RL methods. For real-world solutions,\nonly flow can be reliably collected, whereas common RL methods need more. For\nthe output action, most RL methods focus on acyclic control, which real-world\nsignal controllers do not support. Most importantly, industry standards require\na consistent cycle-flow relationship: non-decreasing and different response\nstrategies for low, medium, and high-level flows, which is ignored by the RL\nmethods. To narrow the gap between RL methods and industry standards, we\ninnovatively propose to use industry solutions to guide the RL agent.\nSpecifically, we design behavior cloning and curriculum learning to guide the\nagent to mimic and meet industry requirements and, at the same time, leverage\nthe power of exploration and exploitation in RL for better performance. We\ntheoretically prove that such guidance can largely decrease the sample\ncomplexity to polynomials in the horizon when searching for an optimal policy.\nOur rigid experiments show that our method has good cycle-flow relation and\nsuperior performance.\n","authors":["Haoyuan Jiang","Xuantang Xiong","Ziyue Li","Hangyu Mao","Guanghu Sui","Jingqing Ruan","Yuheng Cheng","Hua Wei","Wolfgang Ketter","Rui Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.10811v1.pdf","comment":"Under Review of IEEE Transactions on Intelligent Transportation\n  Systems"},{"id":"http://arxiv.org/abs/2407.10810v1","updated":"2024-07-15T15:25:45Z","published":"2024-07-15T15:25:45Z","title":"FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect\n  Knowledge Queries","summary":"  Intelligence is key to advancing integrated circuit (IC) fabrication. Recent\nbreakthroughs in Large Multimodal Models (LMMs) have unlocked unparalleled\nabilities in understanding images and text, fostering intelligent fabrication.\nLeveraging the power of LMMs, we introduce FabGPT, a customized IC fabrication\nlarge multimodal model for wafer defect knowledge query. FabGPT manifests\nexpertise in conducting defect detection in Scanning Electron Microscope (SEM)\nimages, performing root cause analysis, and providing expert question-answering\n(Q&A) on fabrication processes. FabGPT matches enhanced multimodal features to\nautomatically detect minute defects under complex wafer backgrounds and reduce\nthe subjectivity of manual threshold settings. Besides, the proposed modulation\nmodule and interactive corpus training strategy embed wafer defect knowledge\ninto the pre-trained model, effectively balancing Q&A queries related to defect\nknowledge and original knowledge and mitigating the modality bias issues.\nExperiments on in-house fab data (SEM-WaD) show that our FabGPT achieves\nsignificant performance improvement in wafer defect detection and knowledge\nquerying.\n","authors":["Yuqi Jiang","Xudong Lu","Qian Jin","Qi Sun","Hanming Wu","Cheng Zhuo"],"pdf_url":"https://arxiv.org/pdf/2407.10810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10807v1","updated":"2024-07-15T15:23:21Z","published":"2024-07-15T15:23:21Z","title":"Employing Sentence Space Embedding for Classification of Data Stream\n  from Fake News Domain","summary":"  Tabular data is considered the last unconquered castle of deep learning, yet\nthe task of data stream classification is stated to be an equally important and\ndemanding research area. Due to the temporal constraints, it is assumed that\ndeep learning methods are not the optimal solution for application in this\nfield. However, excluding the entire -- and prevalent -- group of methods seems\nrather rash given the progress that has been made in recent years in its\ndevelopment. For this reason, the following paper is the first to present an\napproach to natural language data stream classification using the sentence\nspace method, which allows for encoding text into the form of a discrete\ndigital signal. This allows the use of convolutional deep networks dedicated to\nimage classification to solve the task of recognizing fake news based on text\ndata. Based on the real-life Fakeddit dataset, the proposed approach was\ncompared with state-of-the-art algorithms for data stream classification based\non generalization ability and time complexity.\n","authors":["PaweÅ Zyblewski","Jakub Klikowski","Weronika Borek-Marciniec","PaweÅ Ksieniewicz"],"pdf_url":"https://arxiv.org/pdf/2407.10807v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.15770v2","updated":"2024-07-15T15:22:15Z","published":"2024-04-24T09:44:44Z","title":"ChEX: Interactive Localization and Region Description in Chest X-rays","summary":"  Report generation models offer fine-grained textual interpretations of\nmedical images like chest X-rays, yet they often lack interactivity (i.e. the\nability to steer the generation process through user queries) and localized\ninterpretability (i.e. visually grounding their predictions), which we deem\nessential for future adoption in clinical practice. While there have been\nefforts to tackle these issues, they are either limited in their interactivity\nby not supporting textual queries or fail to also offer localized\ninterpretability. Therefore, we propose a novel multitask architecture and\ntraining paradigm integrating textual prompts and bounding boxes for diverse\naspects like anatomical regions and pathologies. We call this approach the\nChest X-Ray Explainer (ChEX). Evaluations across a heterogeneous set of 9 chest\nX-ray tasks, including localized image interpretation and report generation,\nshowcase its competitiveness with SOTA models while additional analysis\ndemonstrates ChEX's interactive capabilities. Code:\nhttps://github.com/philip-mueller/chex\n","authors":["Philip MÃ¼ller","Georgios Kaissis","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2404.15770v2.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10803v1","updated":"2024-07-15T15:18:57Z","published":"2024-07-15T15:18:57Z","title":"DINO Pre-training for Vision-based End-to-end Autonomous Driving","summary":"  In this article, we focus on the pre-training of visual autonomous driving\nagents in the context of imitation learning. Current methods often rely on a\nclassification-based pre-training, which we hypothesise to be holding back from\nextending capabilities of implicit image understanding. We propose pre-training\nthe visual encoder of a driving agent using the self-distillation with no\nlabels (DINO) method, which relies on a self-supervised learning paradigm.% and\nis trained on an unrelated task. Our experiments in CARLA environment in\naccordance with the Leaderboard benchmark reveal that the proposed pre-training\nis more efficient than classification-based pre-training, and is on par with\nthe recently proposed pre-training based on visual place recognition (VPRPre).\n","authors":["Shubham Juneja","Povilas DaniuÅ¡is","Virginijus MarcinkeviÄius"],"pdf_url":"https://arxiv.org/pdf/2407.10803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10802v1","updated":"2024-07-15T15:18:28Z","published":"2024-07-15T15:18:28Z","title":"Motion-prior Contrast Maximization for Dense Continuous-Time Motion\n  Estimation","summary":"  Current optical flow and point-tracking methods rely heavily on synthetic\ndatasets. Event cameras are novel vision sensors with advantages in challenging\nvisual conditions, but state-of-the-art frame-based methods cannot be easily\nadapted to event data due to the limitations of current event simulators. We\nintroduce a novel self-supervised loss combining the Contrast Maximization\nframework with a non-linear motion prior in the form of pixel-level\ntrajectories and propose an efficient solution to solve the high-dimensional\nassignment problem between non-linear trajectories and events. Their\neffectiveness is demonstrated in two scenarios: In dense continuous-time motion\nestimation, our method improves the zero-shot performance of a synthetically\ntrained model on the real-world dataset EVIMO2 by 29%. In optical flow\nestimation, our method elevates a simple UNet to achieve state-of-the-art\nperformance among self-supervised methods on the DSEC optical flow benchmark.\nOur code is available at https://github.com/tub-rip/MotionPriorCMax.\n","authors":["Friedhelm Hamann","Ziyun Wang","Ioannis Asmanis","Kenneth Chaney","Guillermo Gallego","Kostas Daniilidis"],"pdf_url":"https://arxiv.org/pdf/2407.10802v1.pdf","comment":"24 pages, 8 figures, 8 tables, Project Page:\n  https://github.com/tub-rip/MotionPriorCMax"},{"id":"http://arxiv.org/abs/2407.10793v1","updated":"2024-07-15T15:11:16Z","published":"2024-07-15T15:11:16Z","title":"GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation\n  Framework","summary":"  Methods to evaluate Large Language Model (LLM) responses and detect\ninconsistencies, also known as hallucinations, with respect to the provided\nknowledge, are becoming increasingly important for LLM applications. Current\nmetrics fall short in their ability to provide explainable decisions,\nsystematically check all pieces of information in the response, and are often\ntoo computationally expensive to be used in practice. We present GraphEval: a\nhallucination evaluation framework based on representing information in\nKnowledge Graph (KG) structures. Our method identifies the specific triples in\nthe KG that are prone to hallucinations and hence provides more insight into\nwhere in the response a hallucination has occurred, if at all, than previous\nmethods. Furthermore, using our approach in conjunction with state-of-the-art\nnatural language inference (NLI) models leads to an improvement in balanced\naccuracy on various hallucination benchmarks, compared to using the raw NLI\nmodels. Lastly, we explore the use of GraphEval for hallucination correction by\nleveraging the structure of the KG, a method we name GraphCorrect, and\ndemonstrate that the majority of hallucinations can indeed be rectified.\n","authors":["Hannah Sansford","Nicholas Richardson","Hermina Petric Maretic","Juba Nait Saada"],"pdf_url":"https://arxiv.org/pdf/2407.10793v1.pdf","comment":"12 pages, to be published at KiL'24: Workshop on Knowledge-infused\n  Learning co-located with 30th ACM KDD Conference, August 26, 2024, Barcelona,\n  Spain"},{"id":"http://arxiv.org/abs/2407.10784v1","updated":"2024-07-15T15:02:53Z","published":"2024-07-15T15:02:53Z","title":"AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware\n  Uncertainty Calibrator and Label Distribution Handler","summary":"  In real-world applications, tabular data often suffer from distribution\nshifts due to their widespread and abundant nature, leading to erroneous\npredictions of pre-trained machine learning models. However, addressing such\ndistribution shifts in the tabular domain has been relatively underexplored due\nto unique challenges such as varying attributes and dataset sizes, as well as\nthe limited representation learning capabilities of deep learning models for\ntabular data. Particularly, with the recent promising paradigm of test-time\nadaptation (TTA), where we adapt the off-the-shelf model to the unlabeled\ntarget domain during the inference phase without accessing the source domain,\nwe observe that directly adopting commonly used TTA methods from other domains\noften leads to model collapse. We systematically explore challenges in tabular\ndata test-time adaptation, including skewed entropy, complex latent space\ndecision boundaries, confidence calibration issues with both overconfident and\nunder-confident, and model bias towards source label distributions along with\nclass imbalances. Based on these insights, we introduce AdapTable, a novel\ntabular test-time adaptation method that directly modifies output probabilities\nby estimating target label distributions and adjusting initial probabilities\nbased on calibrated uncertainty. Extensive experiments on both natural\ndistribution shifts and synthetic corruptions demonstrate the adaptation\nefficacy of the proposed method.\n","authors":["Changhun Kim","Taewon Kim","Seungyeon Woo","June Yong Yang","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2407.10784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10780v1","updated":"2024-07-15T14:59:43Z","published":"2024-07-15T14:59:43Z","title":"Correlations Are Ruining Your Gradient Descent","summary":"  Herein the topics of (natural) gradient descent, data decorrelation, and\napproximate methods for backpropagation are brought into a dialogue. Natural\ngradient descent illuminates how gradient vectors, pointing at directions of\nsteepest descent, can be improved by considering the local curvature of loss\nlandscapes. We extend this perspective and show that to fully solve the problem\nilluminated by natural gradients in neural networks, one must recognise that\ncorrelations in the data at any linear transformation, including node responses\nat every layer of a neural network, cause a non-orthonormal relationship\nbetween the model's parameters. To solve this requires a solution to\ndecorrelate inputs at each individual layer of a neural network. We describe a\nrange of methods which have been proposed for decorrelation and whitening of\nnode output, while providing a novel method specifically useful for distributed\ncomputing and computational neuroscience. Implementing decorrelation within\nmulti-layer neural networks, we can show that not only is training via\nbackpropagation sped up significantly but also existing approximations of\nbackpropagation, which have failed catastrophically in the past, are made\nperformant once more. This has the potential to provide a route forward for\napproximate gradient descent methods which have previously been discarded,\ntraining approaches for analogue and neuromorphic hardware, and potentially\ninsights as to the efficacy and utility of decorrelation processes in the\nbrain.\n","authors":["Nasir Ahmad"],"pdf_url":"https://arxiv.org/pdf/2407.10780v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.10779v1","updated":"2024-07-15T14:57:40Z","published":"2024-07-15T14:57:40Z","title":"The Missing Link: Allocation Performance in Causal Machine Learning","summary":"  Automated decision-making (ADM) systems are being deployed across a diverse\nrange of critical problem areas such as social welfare and healthcare. Recent\nwork highlights the importance of causal ML models in ADM systems, but\nimplementing them in complex social environments poses significant challenges.\nResearch on how these challenges impact the performance in specific downstream\ndecision-making tasks is limited. Addressing this gap, we make use of a\ncomprehensive real-world dataset of jobseekers to illustrate how the\nperformance of a single CATE model can vary significantly across different\ndecision-making scenarios and highlight the differential influence of\nchallenges such as distribution shifts on predictions and allocations.\n","authors":["Unai Fischer-Abaigar","Christoph Kern","Frauke Kreuter"],"pdf_url":"https://arxiv.org/pdf/2407.10779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10775v1","updated":"2024-07-15T14:54:57Z","published":"2024-07-15T14:54:57Z","title":"Last-Iterate Global Convergence of Policy Gradients for Constrained\n  Reinforcement Learning","summary":"  Constrained Reinforcement Learning (CRL) tackles sequential decision-making\nproblems where agents are required to achieve goals by maximizing the expected\nreturn while meeting domain-specific constraints, which are often formulated as\nexpected costs. In this setting, policy-based methods are widely used since\nthey come with several advantages when dealing with continuous-control\nproblems. These methods search in the policy space with an action-based or\nparameter-based exploration strategy, depending on whether they learn directly\nthe parameters of a stochastic policy or those of a stochastic hyperpolicy. In\nthis paper, we propose a general framework for addressing CRL problems via\ngradient-based primal-dual algorithms, relying on an alternate ascent/descent\nscheme with dual-variable regularization. We introduce an exploration-agnostic\nalgorithm, called C-PG, which exhibits global last-iterate convergence\nguarantees under (weak) gradient domination assumptions, improving and\ngeneralizing existing results. Then, we design C-PGAE and C-PGPE, the\naction-based and the parameter-based versions of C-PG, respectively, and we\nillustrate how they naturally extend to constraints defined in terms of risk\nmeasures over the costs, as it is often requested in safety-critical scenarios.\nFinally, we numerically validate our algorithms on constrained control\nproblems, and compare them with state-of-the-art baselines, demonstrating their\neffectiveness.\n","authors":["Alessandro Montenegro","Marco Mussi","Matteo Papini","Alberto Maria Metelli"],"pdf_url":"https://arxiv.org/pdf/2407.10775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10768v1","updated":"2024-07-15T14:50:15Z","published":"2024-07-15T14:50:15Z","title":"MSegRNN:Enhanced SegRNN Model with Mamba for Long-Term Time Series\n  Forecasting","summary":"  The field of long-term time series forecasting demands handling extensive\nlook-back windows and long-range prediction steps, posing significant\nchallenges for RNN-based methodologies. Among these, SegRNN, a robust\nRNN-driven model, has gained considerable attention in LTSF analysis for\nachieving state-of-the-art results while maintaining a remarkably streamlined\narchitecture. Concurrently, the Mamba structure has demonstrated its advantages\nin small to medium-sized models due to its capability for information\nselection. This study introduces a variant of SegRNN that preprocesses\ninformation using a fine-tuned single-layer Mamba structure. Additionally, it\nincorporates implicit segmentation and residual structures into the model's\nencoding section to further reduce the inherent data iterative cycles of RNN\narchitectures and implicitly integrate inter-channel correlations. This\nvariant, named MSegRNN, utilizes the Mamba structure to select useful\ninformation, resulting in a transformed sequence. The linear-strategy-adapted\nderivative retains the superior memory efficiency of the original SegRNN while\ndemonstrating enhanced performance. Empirical evaluations on real-world LTSF\ndatasets demonstrate the superior performance of our model, thereby\ncontributing to the advancement of LTSF methodologies.\n","authors":["GaoXiang Zhao","XiaoQiang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10761v1","updated":"2024-07-15T14:40:24Z","published":"2024-07-15T14:40:24Z","title":"Physics-Informed Machine Learning for Smart Additive Manufacturing","summary":"  Compared to physics-based computational manufacturing, data-driven models\nsuch as machine learning (ML) are alternative approaches to achieve smart\nmanufacturing. However, the data-driven ML's \"black box\" nature has presented a\nchallenge to interpreting its outcomes. On the other hand, governing physical\nlaws are not effectively utilized to develop data-efficient ML algorithms. To\nleverage the advantages of ML and physical laws of advanced manufacturing, this\npaper focuses on the development of a physics-informed machine learning (PIML)\nmodel by integrating neural networks and physical laws to improve model\naccuracy, transparency, and generalization with case studies in laser metal\ndeposition (LMD).\n","authors":["Rahul Sharma","Maziar Raissi","Y. B. Guo"],"pdf_url":"https://arxiv.org/pdf/2407.10761v1.pdf","comment":"6 pages, 7 figures, 18th CIRP Conference on Intelligent Computation\n  in Manufacturing Engineering"},{"id":"http://arxiv.org/abs/2407.10759v1","updated":"2024-07-15T14:38:09Z","published":"2024-07-15T14:38:09Z","title":"Qwen2-Audio Technical Report","summary":"  We introduce the latest progress of Qwen-Audio, a large-scale audio-language\nmodel called Qwen2-Audio, which is capable of accepting various audio signal\ninputs and performing audio analysis or direct textual responses with regard to\nspeech instructions. In contrast to complex hierarchical tags, we have\nsimplified the pre-training process by utilizing natural language prompts for\ndifferent data and tasks, and have further expanded the data volume. We have\nboosted the instruction-following capability of Qwen2-Audio and implemented two\ndistinct audio interaction modes for voice chat and audio analysis. In the\nvoice chat mode, users can freely engage in voice interactions with Qwen2-Audio\nwithout text input. In the audio analysis mode, users could provide audio and\ntext instructions for analysis during the interaction. Note that we do not use\nany system prompts to switch between voice chat and audio analysis modes.\nQwen2-Audio is capable of intelligently comprehending the content within audio\nand following voice commands to respond appropriately. For instance, in an\naudio segment that simultaneously contains sounds, multi-speaker conversations,\nand a voice command, Qwen2-Audio can directly understand the command and\nprovide an interpretation and response to the audio. Additionally, DPO has\noptimized the model's performance in terms of factuality and adherence to\ndesired behavior. According to the evaluation results from AIR-Bench,\nQwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests\nfocused on audio-centric instruction-following capabilities. Qwen2-Audio is\nopen-sourced with the aim of fostering the advancement of the multi-modal\nlanguage community.\n","authors":["Yunfei Chu","Jin Xu","Qian Yang","Haojie Wei","Xipin Wei","Zhifang Guo","Yichong Leng","Yuanjun Lv","Jinzheng He","Junyang Lin","Chang Zhou","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.10759v1.pdf","comment":"https://github.com/QwenLM/Qwen2-Audio. Checkpoints, codes and scripts\n  will be opensoursed soon"},{"id":"http://arxiv.org/abs/2407.10758v1","updated":"2024-07-15T14:36:05Z","published":"2024-07-15T14:36:05Z","title":"Continual Deep Learning on the Edge via Stochastic Local Competition\n  among Subnetworks","summary":"  Continual learning on edge devices poses unique challenges due to stringent\nresource constraints. This paper introduces a novel method that leverages\nstochastic competition principles to promote sparsity, significantly reducing\ndeep network memory footprint and computational demand. Specifically, we\npropose deep networks that comprise blocks of units that compete locally to win\nthe representation of each arising new task; competition takes place in a\nstochastic manner. This type of network organization results in sparse\ntask-specific representations from each network layer; the sparsity pattern is\nobtained during training and is different among tasks. Crucially, our method\nsparsifies both the weights and the weight gradients, thus facilitating\ntraining on edge devices. This is performed on the grounds of winning\nprobability for each unit in a block. During inference, the network retains\nonly the winning unit and zeroes-out all weights pertaining to non-winning\nunits for the task at hand. Thus, our approach is specifically tailored for\ndeployment on edge devices, providing an efficient and scalable solution for\ncontinual learning in resource-limited environments.\n","authors":["Theodoros Christophides","Kyriakos Tolias","Sotirios Chatzis"],"pdf_url":"https://arxiv.org/pdf/2407.10758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10153v3","updated":"2024-07-15T14:35:13Z","published":"2024-03-15T09:54:04Z","title":"Improving Medical Multi-modal Contrastive Learning with Expert\n  Annotations","summary":"  We introduce eCLIP, an enhanced version of the CLIP model that integrates\nexpert annotations in the form of radiologist eye-gaze heatmaps. It tackles key\nchallenges in contrastive multi-modal medical imaging analysis, notably data\nscarcity and the \"modality gap\" -- a significant disparity between image and\ntext embeddings that diminishes the quality of representations and hampers\ncross-modal interoperability. eCLIP integrates a heatmap processor and\nleverages mixup augmentation to efficiently utilize the scarce expert\nannotations, thus boosting the model's learning effectiveness. eCLIP is\ndesigned to be generally applicable to any variant of CLIP without requiring\nany modifications of the core architecture. Through detailed evaluations across\nseveral tasks, including zero-shot inference, linear probing, cross-modal\nretrieval, and Retrieval Augmented Generation (RAG) of radiology reports using\na frozen Large Language Model, eCLIP showcases consistent improvements in\nembedding quality. The outcomes reveal enhanced alignment and uniformity,\naffirming eCLIP's capability to harness high-quality annotations for enriched\nmulti-modal analysis in the medical imaging domain.\n","authors":["Yogesh Kumar","Pekka Marttinen"],"pdf_url":"https://arxiv.org/pdf/2403.10153v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2403.17775v3","updated":"2024-07-15T14:29:33Z","published":"2024-03-26T15:07:58Z","title":"Secure Aggregation is Not Private Against Membership Inference Attacks","summary":"  Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in\nfederated learning, affording the server access only to the aggregate of model\nupdates while safeguarding the confidentiality of individual updates. Despite\nwidespread claims regarding SecAgg's privacy-preserving capabilities, a formal\nanalysis of its privacy is lacking, making such presumptions unjustified. In\nthis paper, we delve into the privacy implications of SecAgg by treating it as\na local differential privacy (LDP) mechanism for each local update. We design a\nsimple attack wherein an adversarial server seeks to discern which update\nvector a client submitted, out of two possible ones, in a single training round\nof federated learning under SecAgg. By conducting privacy auditing, we assess\nthe success probability of this attack and quantify the LDP guarantees provided\nby SecAgg. Our numerical results unveil that, contrary to prevailing claims,\nSecAgg offers weak privacy against membership inference attacks even in a\nsingle training round. Indeed, it is difficult to hide a local update by adding\nother independent local updates when the updates are of high dimension. Our\nfindings underscore the imperative for additional privacy-enhancing mechanisms,\nsuch as noise injection, in federated learning.\n","authors":["Khac-Hoang Ngo","Johan Ãstman","Giuseppe Durisi","Alexandre Graell i Amat"],"pdf_url":"https://arxiv.org/pdf/2403.17775v3.pdf","comment":"accepted to the European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) 2024"},{"id":"http://arxiv.org/abs/2402.11816v3","updated":"2024-07-15T14:28:46Z","published":"2024-02-19T04:13:33Z","title":"Learning the Unlearned: Mitigating Feature Suppression in Contrastive\n  Learning","summary":"  Self-Supervised Contrastive Learning has proven effective in deriving\nhigh-quality representations from unlabeled data. However, a major challenge\nthat hinders both unimodal and multimodal contrastive learning is feature\nsuppression, a phenomenon where the trained model captures only a limited\nportion of the information from the input data while overlooking other\npotentially valuable content. This issue often leads to indistinguishable\nrepresentations for visually similar but semantically different inputs,\nadversely affecting downstream task performance, particularly those requiring\nrigorous semantic comprehension. To address this challenge, we propose a novel\nmodel-agnostic Multistage Contrastive Learning (MCL) framework. Unlike standard\ncontrastive learning which inherently captures one single biased feature\ndistribution, MCL progressively learns previously unlearned features through\nfeature-aware negative sampling at each stage, where the negative samples of an\nanchor are exclusively selected from the cluster it was assigned to in\npreceding stages. Meanwhile, MCL preserves the previously well-learned features\nby cross-stage representation integration, integrating features across all\nstages to form final representations. Our comprehensive evaluation demonstrates\nMCL's effectiveness and superiority across both unimodal and multimodal\ncontrastive learning, spanning a range of model architectures from ResNet to\nVision Transformers (ViT). Remarkably, in tasks where the original CLIP model\nhas shown limitations, MCL dramatically enhances performance, with improvements\nup to threefold on specific attributes in the recently proposed MMVP benchmark.\n","authors":["Jihai Zhang","Xiang Lan","Xiaoye Qu","Yu Cheng","Mengling Feng","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2402.11816v3.pdf","comment":"ECCV 2024 Camera-Ready"},{"id":"http://arxiv.org/abs/2407.07237v2","updated":"2024-07-15T14:27:14Z","published":"2024-07-09T21:35:19Z","title":"The Quantum Imitation Game: Reverse Engineering of Quantum Machine\n  Learning Models","summary":"  Quantum Machine Learning (QML) amalgamates quantum computing paradigms with\nmachine learning models, providing significant prospects for solving complex\nproblems. However, with the expansion of numerous third-party vendors in the\nNoisy Intermediate-Scale Quantum (NISQ) era of quantum computing, the security\nof QML models is of prime importance, particularly against reverse engineering,\nwhich could expose trained parameters and algorithms of the models. We assume\nthe untrusted quantum cloud provider is an adversary having white-box access to\nthe transpiled user-designed trained QML model during inference. Reverse\nengineering (RE) to extract the pre-transpiled QML circuit will enable\nre-transpilation and usage of the model for various hardware with completely\ndifferent native gate sets and even different qubit technology. Such\nflexibility may not be obtained from the transpiled circuit which is tied to a\nparticular hardware and qubit technology. The information about the number of\nparameters, and optimized values can allow further training of the QML model to\nalter the QML model, tamper with the watermark, and/or embed their own\nwatermark or refine the model for other purposes. In this first effort to\ninvestigate the RE of QML circuits, we perform RE and compare the training\naccuracy of original and reverse-engineered Quantum Neural Networks (QNNs) of\nvarious sizes. We note that multi-qubit classifiers can be reverse-engineered\nunder specific conditions with a mean error of order 1e-2 in a reasonable time.\nWe also propose adding dummy fixed parametric gates in the QML models to\nincrease the RE overhead for defense. For instance, adding 2 dummy qubits and 2\nlayers increases the overhead by ~1.76 times for a classifier with 2 qubits and\n3 layers with a performance overhead of less than 9%. We note that RE is a very\npowerful attack model which warrants further efforts on defenses.\n","authors":["Archisman Ghosh","Swaroop Ghosh"],"pdf_url":"https://arxiv.org/pdf/2407.07237v2.pdf","comment":"11 pages, 12 figures"},{"id":"http://arxiv.org/abs/2405.10221v2","updated":"2024-07-15T14:13:13Z","published":"2024-05-16T16:11:00Z","title":"Scalarisation-based risk concepts for robust multi-objective\n  optimisation","summary":"  Robust optimisation is a well-established framework for optimising functions\nin the presence of uncertainty. The inherent goal of this problem is to\nidentify a collection of inputs whose outputs are both desirable for the\ndecision maker, whilst also being robust to the underlying uncertainties in the\nproblem. In this work, we study the multi-objective case of this problem. We\nidentify that the majority of all robust multi-objective algorithms rely on two\nkey operations: robustification and scalarisation. Robustification refers to\nthe strategy that is used to account for the uncertainty in the problem.\nScalarisation refers to the procedure that is used to encode the relative\nimportance of each objective to a scalar-valued reward. As these operations are\nnot necessarily commutative, the order that they are performed in has an impact\non the resulting solutions that are identified and the final decisions that are\nmade. The purpose of this work is to give a thorough exposition on the effects\nof these different orderings and in particular highlight when one should opt\nfor one ordering over the other. As part of our analysis, we showcase how many\nexisting risk concepts can be integrated into the specification and solution of\na robust multi-objective optimisation problem. Besides this, we also\ndemonstrate how one can principally define the notion of a robust Pareto front\nand a robust performance metric based on our ``robustify and scalarise''\nmethodology. To illustrate the efficacy of these new ideas, we present two\ninsightful case studies which are based on real-world data sets.\n","authors":["Ben Tu","Nikolas Kantas","Robert M. Lee","Behrang Shafei"],"pdf_url":"https://arxiv.org/pdf/2405.10221v2.pdf","comment":"The code is available at: https://github.com/benmltu/scalarize"},{"id":"http://arxiv.org/abs/2407.09357v2","updated":"2024-07-15T14:10:13Z","published":"2024-07-12T15:32:44Z","title":"Any-Property-Conditional Molecule Generation with Self-Criticism using\n  Spanning Trees","summary":"  Generating novel molecules is challenging, with most representations leading\nto generative models producing many invalid molecules. Spanning Tree-based\nGraph Generation (STGG) is a promising approach to ensure the generation of\nvalid molecules, outperforming state-of-the-art SMILES and graph diffusion\nmodels for unconditional generation. In the real world, we want to be able to\ngenerate molecules conditional on one or multiple desired properties rather\nthan unconditionally. Thus, in this work, we extend STGG to\nmulti-property-conditional generation. Our approach, STGG+, incorporates a\nmodern Transformer architecture, random masking of properties during training\n(enabling conditioning on any subset of properties and classifier-free\nguidance), an auxiliary property-prediction loss (allowing the model to\nself-criticize molecules and select the best ones), and other improvements. We\nshow that STGG+ achieves state-of-the-art performance on in-distribution and\nout-of-distribution conditional generation, and reward maximization.\n","authors":["Alexia Jolicoeur-Martineau","Aristide Baratin","Kisoo Kwon","Boris Knyazev","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.09357v2.pdf","comment":"Code: https://github.com/SamsungSAILMontreal/AnyMolGenCritic"},{"id":"http://arxiv.org/abs/2404.10700v2","updated":"2024-07-15T14:09:28Z","published":"2024-04-16T16:17:48Z","title":"Rawformer: Unpaired Raw-to-Raw Translation for Learnable Camera ISPs","summary":"  Modern smartphone camera quality heavily relies on the image signal processor\n(ISP) to enhance captured raw images, utilizing carefully designed modules to\nproduce final output images encoded in a standard color space (e.g., sRGB).\nNeural-based end-to-end learnable ISPs offer promising advancements,\npotentially replacing traditional ISPs with their ability to adapt without\nrequiring extensive tuning for each new camera model, as is often the case for\nnearly every module in traditional ISPs. However, the key challenge with the\nrecent learning-based ISPs is the urge to collect large paired datasets for\neach distinct camera model due to the influence of intrinsic camera\ncharacteristics on the formation of input raw images. This paper tackles this\nchallenge by introducing a novel method for unpaired learning of raw-to-raw\ntranslation across diverse cameras. Specifically, we propose Rawformer, an\nunsupervised Transformer-based encoder-decoder method for raw-to-raw\ntranslation. It accurately maps raw images captured by a certain camera to the\ntarget camera, facilitating the generalization of learnable ISPs to new unseen\ncameras. Our method demonstrates superior performance on real camera datasets,\nachieving higher accuracy compared to previous state-of-the-art techniques, and\npreserving a more robust correlation between the original and translated raw\nimages. The codes and the pretrained models are available at\nhttps://github.com/gosha20777/rawformer.\n","authors":["Georgy Perevozchikov","Nancy Mehta","Mahmoud Afifi","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2404.10700v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2404.08666v2","updated":"2024-07-15T14:07:16Z","published":"2024-03-31T15:13:15Z","title":"Revealing Trends in Datasets from the 2022 ACL and EMNLP Conferences","summary":"  Natural language processing (NLP) has grown significantly since the advent of\nthe Transformer architecture. Transformers have given birth to pre-trained\nlarge language models (PLMs). There has been tremendous improvement in the\nperformance of NLP systems across several tasks. NLP systems are on par or, in\nsome cases, better than humans at accomplishing specific tasks. However, it\nremains the norm that \\emph{better quality datasets at the time of pretraining\nenable PLMs to achieve better performance, regardless of the task.} The need to\nhave quality datasets has prompted NLP researchers to continue creating new\ndatasets to satisfy particular needs. For example, the two top NLP conferences,\nACL and EMNLP, accepted ninety-two papers in 2022, introducing new datasets.\nThis work aims to uncover the trends and insights mined within these datasets.\nMoreover, we provide valuable suggestions to researchers interested in curating\ndatasets in the future.\n","authors":["Jesse Atuhurra","Hidetaka Kamigaito"],"pdf_url":"https://arxiv.org/pdf/2404.08666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10735v1","updated":"2024-07-15T14:01:35Z","published":"2024-07-15T14:01:35Z","title":"Transforming Agency. On the mode of existence of Large Language Models","summary":"  This paper investigates the ontological characterization of Large Language\nModels (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we\npay special attention to their status as agents. This requires explaining in\ndetail the architecture, processing, and training procedures that enable LLMs\nto display their capacities, and the extensions used to turn LLMs into\nagent-like systems. After a systematic analysis we conclude that a LLM fails to\nmeet necessary and sufficient conditions for autonomous agency in the light of\nembodied theories of mind: the individuality condition (it is not the product\nof its own activity, it is not even directly affected by it), the normativity\ncondition (it does not generate its own norms or goals), and, partially the\ninteractional asymmetry condition (it is not the origin and sustained source of\nits interaction with the environment). If not agents, then ... what are LLMs?\nWe argue that ChatGPT should be characterized as an interlocutor or linguistic\nautomaton, a library-that-talks, devoid of (autonomous) agency, but capable to\nengage performatively on non-purposeful yet purpose-structured and\npurpose-bounded tasks. When interacting with humans, a \"ghostly\" component of\nthe human-machine interaction makes it possible to enact genuine conversational\nexperiences with LLMs. Despite their lack of sensorimotor and biological\nembodiment, LLMs textual embodiment (the training corpus) and resource-hungry\ncomputational embodiment, significantly transform existing forms of human\nagency. Beyond assisted and extended agency, the LLM-human coupling can produce\nmidtended forms of agency, closer to the production of intentional agency than\nto the extended instrumentality of any previous technologies.\n","authors":["Xabier E. Barandiaran","Lola S. Almendros"],"pdf_url":"https://arxiv.org/pdf/2407.10735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10734v1","updated":"2024-07-15T14:01:34Z","published":"2024-07-15T14:01:34Z","title":"On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M\n  Microcontrollers","summary":"  On-device training of DNNs allows models to adapt and fine-tune to newly\ncollected data or changing domains while deployed on microcontroller units\n(MCUs). However, DNN training is a resource-intensive task, making the\nimplementation and execution of DNN training algorithms on MCUs challenging due\nto low processor speeds, constrained throughput, limited floating-point\nsupport, and memory constraints. In this work, we explore on-device training of\nDNNs for Cortex-M MCUs. We present a method that enables efficient training of\nDNNs completely in place on the MCU using fully quantized training (FQT) and\ndynamic partial gradient updates. We demonstrate the feasibility of our\napproach on multiple vision and time-series datasets and provide insights into\nthe tradeoff between training accuracy, memory overhead, energy, and latency on\nreal hardware.\n","authors":["Mark Deutel","Frank Hannig","Christopher Mutschler","JÃ¼rgen Teich"],"pdf_url":"https://arxiv.org/pdf/2407.10734v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.10722v1","updated":"2024-07-15T13:47:55Z","published":"2024-07-15T13:47:55Z","title":"Mitigating Data Imbalance for Software Vulnerability Assessment: Does\n  Data Augmentation Help?","summary":"  Background: Software Vulnerability (SV) assessment is increasingly adopted to\naddress the ever-increasing volume and complexity of SVs. Data-driven\napproaches have been widely used to automate SV assessment tasks, particularly\nthe prediction of the Common Vulnerability Scoring System (CVSS) metrics such\nas exploitability, impact, and severity. SV assessment suffers from the\nimbalanced distributions of the CVSS classes, but such data imbalance has been\nhardly understood and addressed in the literature. Aims: We conduct a\nlarge-scale study to quantify the impacts of data imbalance and mitigate the\nissue for SV assessment through the use of data augmentation. Method: We\nleverage nine data augmentation techniques to balance the class distributions\nof the CVSS metrics. We then compare the performance of SV assessment models\nwith and without leveraging the augmented data. Results: Through extensive\nexperiments on 180k+ real-world SVs, we show that mitigating data imbalance can\nsignificantly improve the predictive performance of models for all the CVSS\ntasks, by up to 31.8% in Matthews Correlation Coefficient. We also discover\nthat simple text augmentation like combining random text insertion, deletion,\nand replacement can outperform the baseline across the board. Conclusions: Our\nstudy provides the motivation and the first promising step toward tackling data\nimbalance for effective SV assessment.\n","authors":["Triet H. M. Le","M. Ali Babar"],"pdf_url":"https://arxiv.org/pdf/2407.10722v1.pdf","comment":"Accepted as a full paper in the technical track at The International\n  Symposium on Empirical Software Engineering and Measurement (ESEM) 2024"},{"id":"http://arxiv.org/abs/2403.10348v2","updated":"2024-07-15T13:46:29Z","published":"2024-03-15T14:34:34Z","title":"Denoising Task Difficulty-based Curriculum for Training Diffusion Models","summary":"  Diffusion-based generative models have emerged as powerful tools in the realm\nof generative modeling. Despite extensive research on denoising across various\ntimesteps and noise levels, a conflict persists regarding the relative\ndifficulties of the denoising tasks. While various studies argue that lower\ntimesteps present more challenging tasks, others contend that higher timesteps\nare more difficult. To address this conflict, our study undertakes a\ncomprehensive examination of task difficulties, focusing on convergence\nbehavior and changes in relative entropy between consecutive probability\ndistributions across timesteps. Our observational study reveals that denoising\nat earlier timesteps poses challenges characterized by slower convergence and\nhigher relative entropy, indicating increased task difficulty at these lower\ntimesteps. Building on these observations, we introduce an easy-to-hard\nlearning scheme, drawing from curriculum learning, to enhance the training\nprocess of diffusion models. By organizing timesteps or noise levels into\nclusters and training models with ascending orders of difficulty, we facilitate\nan order-aware training regime, progressing from easier to harder denoising\ntasks, thereby deviating from the conventional approach of training diffusion\nmodels simultaneously across all timesteps. Our approach leads to improved\nperformance and faster convergence by leveraging benefits of curriculum\nlearning, while maintaining orthogonality with existing improvements in\ndiffusion training techniques. We validate these advantages through\ncomprehensive experiments in image generation tasks, including unconditional,\nclass-conditional, and text-to-image generation.\n","authors":["Jin-Young Kim","Hyojun Go","Soonwoo Kwon","Hyun-Gyoon Kim"],"pdf_url":"https://arxiv.org/pdf/2403.10348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03648v2","updated":"2024-07-15T13:35:33Z","published":"2023-02-07T17:59:05Z","title":"Class-Incremental Learning: A Survey","summary":"  Deep models, e.g., CNNs and Vision Transformers, have achieved impressive\nachievements in many vision tasks in the closed world. However, novel classes\nemerge from time to time in our ever-changing world, requiring a learning\nsystem to acquire new knowledge continually. Class-Incremental Learning (CIL)\nenables the learner to incorporate the knowledge of new classes incrementally\nand build a universal classifier among all seen classes. Correspondingly, when\ndirectly training the model with new class instances, a fatal problem occurs --\nthe model tends to catastrophically forget the characteristics of former ones,\nand its performance drastically degrades. There have been numerous efforts to\ntackle catastrophic forgetting in the machine learning community. In this\npaper, we survey comprehensively recent advances in class-incremental learning\nand summarize these methods from several aspects. We also provide a rigorous\nand unified evaluation of 17 methods in benchmark image classification tasks to\nfind out the characteristics of different algorithms empirically. Furthermore,\nwe notice that the current comparison protocol ignores the influence of memory\nbudget in model storage, which may result in unfair comparison and biased\nresults. Hence, we advocate fair comparison by aligning the memory budget in\nevaluation, as well as several memory-agnostic performance measures. The source\ncode is available at https://github.com/zhoudw-zdw/CIL_Survey/\n","authors":["Da-Wei Zhou","Qi-Wei Wang","Zhi-Hong Qi","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2302.03648v2.pdf","comment":"Accepted to TPAMI. Code is available at\n  https://github.com/zhoudw-zdw/CIL_Survey/"},{"id":"http://arxiv.org/abs/2405.17653v3","updated":"2024-07-15T13:30:52Z","published":"2024-05-27T20:53:22Z","title":"InversionView: A General-Purpose Method for Reading Information from\n  Neural Activations","summary":"  The inner workings of neural networks can be better understood if we can\nfully decipher the information encoded in neural activations. In this paper, we\nargue that this information is embodied by the subset of inputs that give rise\nto similar activations. Computing such subsets is nontrivial as the input space\nis exponentially large. We propose InversionView, which allows us to\npractically inspect this subset by sampling from a trained decoder model\nconditioned on activations. This helps uncover the information content of\nactivation vectors, and facilitates understanding of the algorithms implemented\nby transformer models. We present four case studies where we investigate models\nranging from small transformers to GPT-2. In these studies, we demonstrate the\ncharacteristics of our method, show the distinctive advantages it offers, and\nprovide causally verified circuits.\n","authors":["Xinting Huang","Madhur Panwar","Navin Goyal","Michael Hahn"],"pdf_url":"https://arxiv.org/pdf/2405.17653v3.pdf","comment":"ICML 2024 Mechanistic Interpretability Workshop oral"},{"id":"http://arxiv.org/abs/2407.10702v1","updated":"2024-07-15T13:17:48Z","published":"2024-07-15T13:17:48Z","title":"Geometric Analysis of Unconstrained Feature Models with $d=K$","summary":"  Recently, interesting empirical phenomena known as Neural Collapse have been\nobserved during the final phase of training deep neural networks for\nclassification tasks. We examine this issue when the feature dimension d is\nequal to the number of classes K. We demonstrate that two popular unconstrained\nfeature models are strict saddle functions, with every critical point being\neither a global minimum or a strict saddle point that can be exited using\nnegative curvatures. The primary findings conclusively confirm the conjecture\non the unconstrained feature models in previous articles.\n","authors":["Shao Gu","Yi Shen"],"pdf_url":"https://arxiv.org/pdf/2407.10702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04621v3","updated":"2024-07-15T13:07:02Z","published":"2023-06-07T17:50:59Z","title":"Flexible Distribution Alignment: Towards Long-tailed Semi-supervised\n  Learning with Proper Calibration","summary":"  Long-tailed semi-supervised learning (LTSSL) represents a practical scenario\nfor semi-supervised applications, challenged by skewed labeled distributions\nthat bias classifiers. This problem is often aggravated by discrepancies\nbetween labeled and unlabeled class distributions, leading to biased\npseudo-labels, neglect of rare classes, and poorly calibrated probabilities. To\naddress these issues, we introduce Flexible Distribution Alignment (FlexDA), a\nnovel adaptive logit-adjusted loss framework designed to dynamically estimate\nand align predictions with the actual distribution of unlabeled data and\nachieve a balanced classifier by the end of training. FlexDA is further\nenhanced by a distillation-based consistency loss, promoting fair data usage\nacross classes and effectively leveraging underconfident samples. This method,\nencapsulated in ADELLO (Align and Distill Everything All at Once), proves\nrobust against label shift, significantly improves model calibration in LTSSL\ncontexts, and surpasses previous state-of-of-art approaches across multiple\nbenchmarks, including CIFAR100-LT, STL10-LT, and ImageNet127, addressing class\nimbalance challenges in semi-supervised learning. Our code is available at\nhttps://github.com/emasa/ADELLO-LTSSL.\n","authors":["Emanuel Sanchez Aimar","Nathaniel Helgesen","Yonghao Xu","Marco Kuhlmann","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2306.04621v3.pdf","comment":"Accepted at ECCV2024, 25 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.10688v1","updated":"2024-07-15T13:01:47Z","published":"2024-07-15T13:01:47Z","title":"Probability Passing for Graph Neural Networks: Graph Structure and\n  Representations Joint Learning","summary":"  Graph Neural Networks (GNNs) have achieved notable success in the analysis of\nnon-Euclidean data across a wide range of domains. However, their applicability\nis constrained by the dependence on the observed graph structure. To solve this\nproblem, Latent Graph Inference (LGI) is proposed to infer a task-specific\nlatent structure by computing similarity or edge probability of node features\nand then apply a GNN to produce predictions. Even so, existing approaches\nneglect the noise from node features, which affects generated graph structure\nand performance. In this work, we introduce a novel method called Probability\nPassing to refine the generated graph structure by aggregating edge\nprobabilities of neighboring nodes based on observed graph. Furthermore, we\ncontinue to utilize the LGI framework, inputting the refined graph structure\nand node features into GNNs to obtain predictions. We name the proposed scheme\nas Probability Passing-based Graph Neural Network (PPGNN). Moreover, the\nanchor-based technique is employed to reduce complexity and improve efficiency.\nExperimental results demonstrate the effectiveness of the proposed method.\n","authors":["Ziyan Wang","YaXuan He","Bin Liu"],"pdf_url":"https://arxiv.org/pdf/2407.10688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10259v2","updated":"2024-07-15T13:00:46Z","published":"2024-04-16T03:26:43Z","title":"Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy","summary":"  The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events.\n","authors":["Tunazzina Islam","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2404.10259v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14797v2","updated":"2024-07-15T12:59:02Z","published":"2024-03-21T19:20:29Z","title":"Preventing Catastrophic Forgetting through Memory Networks in Continuous\n  Detection","summary":"  Modern pre-trained architectures struggle to retain previous information\nwhile undergoing continuous fine-tuning on new tasks. Despite notable progress\nin continual classification, systems designed for complex vision tasks such as\ndetection or segmentation still struggle to attain satisfactory performance. In\nthis work, we introduce a memory-based detection transformer architecture to\nadapt a pre-trained DETR-style detector to new tasks while preserving knowledge\nfrom previous tasks. We propose a novel localized query function for efficient\ninformation retrieval from memory units, aiming to minimize forgetting.\nFurthermore, we identify a fundamental challenge in continual detection\nreferred to as background relegation. This arises when object categories from\nearlier tasks reappear in future tasks, potentially without labels, leading\nthem to be implicitly treated as background. This is an inevitable issue in\ncontinual detection or segmentation. The introduced continual optimization\ntechnique effectively tackles this challenge. Finally, we assess the\nperformance of our proposed system on continual detection benchmarks and\ndemonstrate that our approach surpasses the performance of existing\nstate-of-the-art resulting in 5-7% improvements on MS-COCO and PASCAL-VOC on\nthe task of continual detection.\n","authors":["Gaurav Bhatt","James Ross","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2403.14797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10681v1","updated":"2024-07-15T12:58:04Z","published":"2024-07-15T12:58:04Z","title":"GeoMix: Towards Geometry-Aware Data Augmentation","summary":"  Mixup has shown considerable success in mitigating the challenges posed by\nlimited labeled data in image classification. By synthesizing samples through\nthe interpolation of features and labels, Mixup effectively addresses the issue\nof data scarcity. However, it has rarely been explored in graph learning tasks\ndue to the irregularity and connectivity of graph data. Specifically, in node\nclassification tasks, Mixup presents a challenge in creating connections for\nsynthetic data. In this paper, we propose Geometric Mixup (GeoMix), a simple\nand interpretable Mixup approach leveraging in-place graph editing. It\neffectively utilizes geometry information to interpolate features and labels\nwith those from the nearby neighborhood, generating synthetic nodes and\nestablishing connections for them. We conduct theoretical analysis to elucidate\nthe rationale behind employing geometry information for node Mixup, emphasizing\nthe significance of locality enhancement-a critical aspect of our method's\ndesign. Extensive experiments demonstrate that our lightweight Geometric Mixup\nachieves state-of-the-art results on a wide variety of standard datasets with\nlimited labeled data. Furthermore, it significantly improves the generalization\ncapability of underlying GNNs across various challenging out-of-distribution\ngeneralization tasks. Our code is available at\nhttps://github.com/WtaoZhao/geomix.\n","authors":["Wentao Zhao","Qitian Wu","Chenxiao Yang","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2407.10681v1.pdf","comment":"Published as a conference paper at KDD 2024"},{"id":"http://arxiv.org/abs/2407.00463v3","updated":"2024-07-15T12:56:28Z","published":"2024-06-29T15:20:11Z","title":"Open-Source Conversational AI with SpeechBrain 1.0","summary":"  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks\n","authors":["Mirco Ravanelli","Titouan Parcollet","Adel Moumen","Sylvain de Langen","Cem Subakan","Peter Plantinga","Yingzhi Wang","Pooneh Mousavi","Luca Della Libera","Artem Ploujnikov","Francesco Paissan","Davide Borra","Salah Zaiem","Zeyu Zhao","Shucong Zhang","Georgios Karakasidis","Sung-Lin Yeh","Aku Rouhe","Rudolf Braun","Florian Mai","Juan Zuluaga-Gomez","Seyed Mahed Mousavi","Andreas Nautsch","Xuechen Liu","Sangeet Sagar","Jarod Duret","Salima Mdhaffar","Gaelle Laperriere","Renato De Mori","Yannick Esteve"],"pdf_url":"https://arxiv.org/pdf/2407.00463v3.pdf","comment":"Submitted to JMLR (Machine Learning Open Source Software)"},{"id":"http://arxiv.org/abs/2403.04884v2","updated":"2024-07-15T12:49:16Z","published":"2024-03-07T20:16:42Z","title":"Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural\n  Networks","summary":"  Implantable retinal prostheses offer a promising solution to restore partial\nvision by circumventing damaged photoreceptor cells in the retina and directly\nstimulating the remaining functional retinal cells. However, the information\ntransmission between the camera and retinal cells is often limited by the low\nresolution of the electrode array and the lack of specificity for different\nganglion cell types, resulting in suboptimal stimulations. In this work, we\npropose to utilize normalizing flow-based conditional invertible neural\nnetworks to optimize retinal implant stimulation in an unsupervised manner. The\ninvertibility of these networks allows us to use them as a surrogate for the\ncomputational model of the visual system, while also encoding input camera\nsignals into optimized electrical stimuli on the electrode array. Compared to\nother methods, such as trivial downsampling, linear models, and feed-forward\nconvolutional neural networks, the flow-based invertible neural network and its\nconditional extension yield better visual reconstruction qualities w.r.t.\nvarious metrics using a physiologically validated simulation tool.\n","authors":["Yuli Wu","Julian Wittmann","Peter Walter","Johannes Stegmaier"],"pdf_url":"https://arxiv.org/pdf/2403.04884v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04985v4","updated":"2024-07-15T12:40:11Z","published":"2023-12-08T11:47:35Z","title":"SparQ Attention: Bandwidth-Efficient LLM Inference","summary":"  The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.\n","authors":["Luka Ribar","Ivan Chelombiev","Luke Hudlass-Galley","Charlie Blake","Carlo Luschi","Douglas Orr"],"pdf_url":"https://arxiv.org/pdf/2312.04985v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19076v3","updated":"2024-07-15T12:36:42Z","published":"2024-05-29T13:34:32Z","title":"Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials\n  Analysis and Design","summary":"  We present Cephalo, a series of multimodal vision large language models\n(V-LLMs) designed for materials science applications, integrating visual and\nlinguistic data for enhanced understanding. A key innovation of Cephalo is its\nadvanced dataset generation method. Cephalo is trained on integrated image and\ntext data from thousands of scientific papers and science-focused Wikipedia\ndata demonstrates can interpret complex visual scenes, generate precise\nlanguage descriptions, and answer queries about images effectively. The\ncombination of a vision encoder with an autoregressive transformer supports\nmultimodal natural language understanding, which can be coupled with other\ngenerative methods to create an image-to-text-to-3D pipeline. To develop more\ncapable models from smaller ones, we report both mixture-of-expert methods and\nmodel merging. We examine the models in diverse use cases that incorporate\nbiological materials, fracture and engineering analysis, protein biophysics,\nand bio-inspired design based on insect behavior. Generative applications\ninclude bio-inspired designs, including pollen-inspired architected materials,\nas well as the synthesis of bio-inspired material microstructures from a\nphotograph of a solar eclipse. Additional model fine-tuning with a series of\nmolecular dynamics results demonstrate Cephalo's enhanced capabilities to\naccurately predict statistical features of stress and atomic energy\ndistributions, as well as crack dynamics and damage in materials.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2405.19076v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05735v2","updated":"2024-07-15T12:32:19Z","published":"2024-01-11T08:36:15Z","title":"Object-Centric Diffusion for Efficient Video Editing","summary":"  This paper aims to accelerate video stream processing, such as object\ndetection and semantic segmentation, by leveraging the temporal redundancies\nthat exist between video frames. Instead of propagating and warping features\nusing motion alignment, such as optical flow, we propose a novel knowledge\ndistillation schema coined as Delta Distillation. In our proposal, the student\nlearns the variations in the teacher's intermediate features over time. We\ndemonstrate that these temporal variations can be effectively distilled due to\nthe temporal redundancies within video frames. During inference, both teacher\nand student cooperate for providing predictions: the former by providing\ninitial representations extracted only on the key-frame, and the latter by\niteratively estimating and applying deltas for the successive frames. Moreover,\nwe consider various design choices to learn optimal student architectures\nincluding an end-to-end learnable architecture search. By extensive experiments\non a wide range of architectures, including the most efficient ones, we\ndemonstrate that delta distillation sets a new state of the art in terms of\naccuracy vs. efficiency trade-off for semantic segmentation and object\ndetection in videos. Finally, we show that, as a by-product, delta distillation\nimproves the temporal consistency of the teacher model.\n","authors":["Kumara Kahatapitiya","Adil Karjauv","Davide Abati","Fatih Porikli","Yuki M. Asano","Amirhossein Habibian"],"pdf_url":"https://arxiv.org/pdf/2401.05735v2.pdf","comment":"ECCV24"},{"id":"http://arxiv.org/abs/2407.10666v1","updated":"2024-07-15T12:29:17Z","published":"2024-07-15T12:29:17Z","title":"Flow Perturbation to Accelerate Unbiased Sampling of Boltzmann\n  distribution","summary":"  Flow-based generative models have been employed for sampling the Boltzmann\ndistribution, but their application to high-dimensional systems is hindered by\nthe significant computational cost of obtaining the Jacobian of the flow. To\novercome this challenge, we introduce the flow perturbation method, which\nincorporates optimized stochastic perturbations into the flow. By reweighting\ntrajectories generated by the perturbed flow, our method achieves unbiased\nsampling of the Boltzmann distribution with orders of magnitude speedup\ncompared to both brute force Jacobian calculations and the Hutchinson\nestimator. Notably, it accurately sampled the Chignolin protein with all atomic\nCartesian coordinates explicitly represented, which, to our best knowledge, is\nthe largest molecule ever Boltzmann sampled in such detail using generative\nmodels.\n","authors":["Xin Peng","Ang Gao"],"pdf_url":"https://arxiv.org/pdf/2407.10666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05300v2","updated":"2024-07-15T12:21:02Z","published":"2023-06-08T15:45:57Z","title":"Correlated Noise in Epoch-Based Stochastic Gradient Descent:\n  Implications for Weight Variances","summary":"  Stochastic gradient descent (SGD) has become a cornerstone of neural network\noptimization, yet the noise introduced by SGD is often assumed to be\nuncorrelated over time, despite the ubiquity of epoch-based training. In this\nwork, we challenge this assumption and investigate the effects of epoch-based\nnoise correlations on the stationary distribution of discrete-time SGD with\nmomentum, limited to a quadratic loss. Our main contributions are twofold:\nfirst, we calculate the exact autocorrelation of the noise for training in\nepochs under the assumption that the noise is independent of small fluctuations\nin the weight vector, and find that SGD noise is anti-correlated in time.\nSecond, we explore the influence of these anti-correlations on SGD dynamics. We\nfind that for directions with a curvature greater than a\nhyperparameter-dependent crossover value, the results for uncorrelated noise\nare recovered. However, for relatively flat directions, the weight variance is\nsignificantly reduced, and our variance prediction leads to a considerable\nreduction in loss fluctuations as compared to the constant weight variance\nassumption.\n","authors":["Marcel KÃ¼hn","Bernd Rosenow"],"pdf_url":"https://arxiv.org/pdf/2306.05300v2.pdf","comment":"25 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.12231v4","updated":"2024-07-15T12:14:15Z","published":"2024-02-19T15:36:36Z","title":"Diffusion Tempering Improves Parameter Estimation with Probabilistic\n  Integrators for Ordinary Differential Equations","summary":"  Ordinary differential equations (ODEs) are widely used to describe dynamical\nsystems in science, but identifying parameters that explain experimental\nmeasurements is challenging. In particular, although ODEs are differentiable\nand would allow for gradient-based parameter optimization, the nonlinear\ndynamics of ODEs often lead to many local minima and extreme sensitivity to\ninitial conditions. We therefore propose diffusion tempering, a novel\nregularization technique for probabilistic numerical methods which improves\nconvergence of gradient-based parameter optimization in ODEs. By iteratively\nreducing a noise parameter of the probabilistic integrator, the proposed method\nconverges more reliably to the true parameters. We demonstrate that our method\nis effective for dynamical systems of different complexity and show that it\nobtains reliable parameter estimates for a Hodgkin-Huxley model with a\npractically relevant number of parameters.\n","authors":["Jonas Beck","Nathanael Bosch","Michael Deistler","Kyra L. Kadhim","Jakob H. Macke","Philipp Hennig","Philipp Berens"],"pdf_url":"https://arxiv.org/pdf/2402.12231v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10707v2","updated":"2024-07-15T12:14:13Z","published":"2024-03-15T21:54:00Z","title":"Discovering Latent Themes in Social Media Messaging: A\n  Machine-in-the-Loop Approach Integrating LLMs","summary":"  Grasping the themes of social media content is key to understanding the\nnarratives that influence public opinion and behavior. The thematic analysis\ngoes beyond traditional topic-level analysis, which often captures only the\nbroadest patterns, providing deeper insights into specific and actionable\nthemes such as \"public sentiment towards vaccination\", \"political discourse\nsurrounding climate policies,\" etc. In this paper, we introduce a novel\napproach to uncovering latent themes in social media messaging. Recognizing the\nlimitations of the traditional topic-level analysis, which tends to capture\nonly overarching patterns, this study emphasizes the need for a finer-grained,\ntheme-focused exploration. Traditional theme discovery methods typically\ninvolve manual processes and a human-in-the-loop approach. While valuable,\nthese methods face challenges in scalability, consistency, and resource\nintensity in terms of time and cost. To address these challenges, we propose a\nmachine-in-the-loop approach that leverages the advanced capabilities of Large\nLanguage Models (LLMs). To demonstrate our approach, we apply our framework to\ncontentious topics, such as climate debate and vaccine debate. We use two\npublicly available datasets: (1) the climate campaigns dataset of 21k Facebook\nads and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads. Our\nquantitative and qualitative analysis shows that our methodology yields more\naccurate and interpretable results compared to the baselines. Our results not\nonly demonstrate the effectiveness of our approach in uncovering latent themes\nbut also illuminate how these themes are tailored for demographic targeting in\nsocial media contexts. Additionally, our work sheds light on the dynamic nature\nof social media, revealing the shifts in the thematic focus of messaging in\nresponse to real-world events.\n","authors":["Tunazzina Islam","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2403.10707v2.pdf","comment":"Accepted at 19th International AAAI Conference on Web and Social\n  Media (ICWSM-2025)"},{"id":"http://arxiv.org/abs/2407.10652v1","updated":"2024-07-15T12:13:53Z","published":"2024-07-15T12:13:53Z","title":"Cutting Through the Clutter: The Potential of LLMs for Efficient\n  Filtration in Systematic Literature Reviews","summary":"  In academic research, systematic literature reviews are foundational and\nhighly relevant, yet tedious to create due to the high volume of publications\nand labor-intensive processes involved. Systematic selection of relevant papers\nthrough conventional means like keyword-based filtering techniques can\nsometimes be inadequate, plagued by semantic ambiguities and inconsistent\nterminology, which can lead to sub-optimal outcomes. To mitigate the required\nextensive manual filtering, we explore and evaluate the potential of using\nLarge Language Models (LLMs) to enhance the efficiency, speed, and precision of\nliterature review filtering, reducing the amount of manual screening required.\nBy using models as classification agents acting on a structured database only,\nwe prevent common problems inherent in LLMs, such as hallucinations. We\nevaluate the real-world performance of such a setup during the construction of\na recent literature survey paper with initially more than 8.3k potentially\nrelevant articles under consideration and compare this with human performance\non the same dataset. Our findings indicate that employing advanced LLMs like\nGPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Flash, or Llama3 with simple prompting\ncan significantly reduce the time required for literature filtering - from\nusually weeks of manual research to only a few minutes. Simultaneously, we\ncrucially show that false negatives can indeed be controlled through a\nconsensus scheme, achieving recalls >98.8% at or even beyond the typical human\nerror threshold, thereby also providing for more accurate and relevant articles\nselected. Our research not only demonstrates a substantial improvement in the\nmethodology of literature reviews but also sets the stage for further\nintegration and extensive future applications of responsible AI in academic\nresearch practices.\n","authors":["Lucas Joos","Daniel A. Keim","Maximilian T. Fischer"],"pdf_url":"https://arxiv.org/pdf/2407.10652v1.pdf","comment":"5 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.17806v2","updated":"2024-07-15T12:07:09Z","published":"2024-03-26T15:44:58Z","title":"Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding\n  Model Mechanisms","summary":"  Many recent language model (LM) interpretability studies have adopted the\ncircuits framework, which aims to find the minimal computational subgraph, or\ncircuit, that explains LM behavior on a given task. Most studies determine\nwhich edges belong in a LM's circuit by performing causal interventions on each\nedge independently, but this scales poorly with model size. Edge attribution\npatching (EAP), gradient-based approximation to interventions, has emerged as a\nscalable but imperfect solution to this problem. In this paper, we introduce a\nnew method - EAP with integrated gradients (EAP-IG) - that aims to better\nmaintain a core property of circuits: faithfulness. A circuit is faithful if\nall model edges outside the circuit can be ablated without changing the model's\nperformance on the task; faithfulness is what justifies studying circuits,\nrather than the full model. Our experiments demonstrate that circuits found\nusing EAP are less faithful than those found using EAP-IG, even though both\nhave high node overlap with circuits found previously using causal\ninterventions. We conclude more generally that when using circuits to compare\nthe mechanisms models use to solve tasks, faithfulness, not overlap, is what\nshould be measured.\n","authors":["Michael Hanna","Sandro Pezzelle","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.17806v2.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2310.19919v2","updated":"2024-07-15T12:07:03Z","published":"2023-10-30T18:29:26Z","title":"Meta-Learning Strategies through Value Maximization in Neural Networks","summary":"  Biological and artificial learning agents face numerous choices about how to\nlearn, ranging from hyperparameter selection to aspects of task distributions\nlike curricula. Understanding how to make these meta-learning choices could\noffer normative accounts of cognitive control functions in biological learners\nand improve engineered systems. Yet optimal strategies remain challenging to\ncompute in modern deep networks due to the complexity of optimizing through the\nentire learning process. Here we theoretically investigate optimal strategies\nin a tractable setting. We present a learning effort framework capable of\nefficiently optimizing control signals on a fully normative objective:\ndiscounted cumulative performance throughout learning. We obtain computational\ntractability by using average dynamical equations for gradient descent,\navailable for simple neural network architectures. Our framework accommodates a\nrange of meta-learning and automatic curriculum learning methods in a unified\nnormative setting. We apply this framework to investigate the effect of\napproximations in common meta-learning algorithms; infer aspects of optimal\ncurricula; and compute optimal neuronal resource allocation in a continual\nlearning setting. Across settings, we find that control effort is most\nbeneficial when applied to easier aspects of a task early in learning; followed\nby sustained effort on harder aspects. Overall, the learning effort framework\nprovides a tractable theoretical test bed to study normative benefits of\ninterventions in a variety of learning systems, as well as a formal account of\noptimal cognitive control strategies over learning trajectories posited by\nestablished theories in cognitive neuroscience.\n","authors":["Rodrigo Carrasco-Davis","Javier MasÃ­s","Andrew M. Saxe"],"pdf_url":"https://arxiv.org/pdf/2310.19919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10641v1","updated":"2024-07-15T12:00:46Z","published":"2024-07-15T12:00:46Z","title":"Deep Diffusion Image Prior for Efficient OOD Adaptation in 3D Inverse\n  Problems","summary":"  Recent inverse problem solvers that leverage generative diffusion priors have\ngarnered significant attention due to their exceptional quality. However,\nadaptation of the prior is necessary when there exists a discrepancy between\nthe training and testing distributions. In this work, we propose deep diffusion\nimage prior (DDIP), which generalizes the recent adaptation method of SCD by\nintroducing a formal connection to the deep image prior. Under this framework,\nwe propose an efficient adaptation method dubbed D3IP, specified for 3D\nmeasurements, which accelerates DDIP by orders of magnitude while achieving\nsuperior performance. D3IP enables seamless integration of 3D inverse solvers\nand thus leads to coherent 3D reconstruction. Moreover, we show that\nmeta-learning techniques can also be applied to yield even better performance.\nWe show that our method is capable of solving diverse 3D reconstructive tasks\nfrom the generative prior trained only with phantom images that are vastly\ndifferent from the training set, opening up new opportunities of applying\ndiffusion inverse solvers even when training with gold standard data is\nimpossible. Code: https://github.com/HJ-harry/DDIP3D\n","authors":["Hyungjin Chung","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2407.10641v1.pdf","comment":"ECCV 2024, 25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.11717v2","updated":"2024-07-15T11:53:41Z","published":"2024-06-17T16:36:12Z","title":"Refusal in Language Models Is Mediated by a Single Direction","summary":"  Conversational large language models are fine-tuned for both\ninstruction-following and safety, resulting in models that obey benign requests\nbut refuse harmful ones. While this refusal behavior is widespread across chat\nmodels, its underlying mechanisms remain poorly understood. In this work, we\nshow that refusal is mediated by a one-dimensional subspace, across 13 popular\nopen-source chat models up to 72B parameters in size. Specifically, for each\nmodel, we find a single direction such that erasing this direction from the\nmodel's residual stream activations prevents it from refusing harmful\ninstructions, while adding this direction elicits refusal on even harmless\ninstructions. Leveraging this insight, we propose a novel white-box jailbreak\nmethod that surgically disables refusal with minimal effect on other\ncapabilities. Finally, we mechanistically analyze how adversarial suffixes\nsuppress propagation of the refusal-mediating direction. Our findings\nunderscore the brittleness of current safety fine-tuning methods. More broadly,\nour work showcases how an understanding of model internals can be leveraged to\ndevelop practical methods for controlling model behavior.\n","authors":["Andy Arditi","Oscar Obeso","Aaquib Syed","Daniel Paleka","Nina Panickssery","Wes Gurnee","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2406.11717v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10633v1","updated":"2024-07-15T11:46:21Z","published":"2024-07-15T11:46:21Z","title":"Evaluating Model Bias Requires Characterizing its Mistakes","summary":"  The ability to properly benchmark model performance in the face of spurious\ncorrelations is important to both build better predictors and increase\nconfidence that models are operating as intended. We demonstrate that\ncharacterizing (as opposed to simply quantifying) model mistakes across\nsubgroups is pivotal to properly reflect model biases, which are ignored by\nstandard metrics such as worst-group accuracy or accuracy gap. Inspired by the\nhypothesis testing framework, we introduce SkewSize, a principled and flexible\nmetric that captures bias from mistakes in a model's predictions. It can be\nused in multi-class settings or generalised to the open vocabulary setting of\ngenerative models. SkewSize is an aggregation of the effect size of the\ninteraction between two categorical variables: the spurious variable\nrepresenting the bias attribute and the model's prediction. We demonstrate the\nutility of SkewSize in multiple settings including: standard vision models\ntrained on synthetic data, vision models trained on ImageNet, and large scale\nvision-and-language models from the BLIP-2 family. In each case, the proposed\nSkewSize is able to highlight biases not captured by other metrics, while also\nproviding insights on the impact of recently proposed techniques, such as\ninstruction tuning.\n","authors":["Isabela Albuquerque","Jessica Schrouff","David Warde-Farley","Taylan Cemgil","Sven Gowal","Olivia Wiles"],"pdf_url":"https://arxiv.org/pdf/2407.10633v1.pdf","comment":"17 pages, 6 figures, ICML 2024"},{"id":"http://arxiv.org/abs/2407.10630v1","updated":"2024-07-15T11:30:40Z","published":"2024-07-15T11:30:40Z","title":"Brain Tumor Classification From MRI Images Using Machine Learning","summary":"  Brain tumor is a life-threatening problem and hampers the normal functioning\nof the human body. The average five-year relative survival rate for malignant\nbrain tumors is 35.6 percent. For proper diagnosis and efficient treatment\nplanning, it is necessary to detect the brain tumor in early stages. Due to\nadvancement in medical imaging technology, the brain images are taken in\ndifferent modalities. The ability to extract relevant characteristics from\nmagnetic resonance imaging (MRI) scans is a crucial step for brain tumor\nclassifiers. Several studies have proposed various strategies to extract\nrelevant features from different modalities of MRI to predict the growth of\nabnormal tumors. Most techniques used conventional methods of image processing\nfor feature extraction and machine learning for classification. More recently,\nthe use of deep learning algorithms in medical imaging has resulted in\nsignificant improvements in the classification and diagnosis of brain tumors.\nSince tumors are located at different regions of the brain, localizing the\ntumor and classifying it to a particular category is a challenging task. The\nobjective of this project is to develop a predictive system for brain tumor\ndetection using machine learning(ensembling).\n","authors":["Vidhyapriya Ranganathan","Celshiya Udaiyar","Jaisree Jayanth","Meghaa P V","Srija B","Uthra S"],"pdf_url":"https://arxiv.org/pdf/2407.10630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10629v1","updated":"2024-07-15T11:28:16Z","published":"2024-07-15T11:28:16Z","title":"Balancing the Scales: Reinforcement Learning for Fair Classification","summary":"  Fairness in classification tasks has traditionally focused on bias removal\nfrom neural representations, but recent trends favor algorithmic methods that\nembed fairness into the training process. These methods steer models towards\nfair performance, preventing potential elimination of valuable information that\narises from representation manipulation. Reinforcement Learning (RL), with its\ncapacity for learning through interaction and adjusting reward functions to\nencourage desired behaviors, emerges as a promising tool in this domain. In\nthis paper, we explore the usage of RL to address bias in imbalanced\nclassification by scaling the reward function to mitigate bias. We employ the\ncontextual multi-armed bandit framework and adapt three popular RL algorithms\nto suit our objectives, demonstrating a novel approach to mitigating bias.\n","authors":["Leon Eshuijs","Shihan Wang","Antske Fokkens"],"pdf_url":"https://arxiv.org/pdf/2407.10629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10627v1","updated":"2024-07-15T11:26:07Z","published":"2024-07-15T11:26:07Z","title":"Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated\n  Chatbot Arena","summary":"  Assessing the effectiveness of large language models (LLMs) presents\nsubstantial challenges. The method of conducting human-annotated battles in an\nonline Chatbot Arena is a highly effective evaluative technique. However, this\napproach is limited by the costs and time required for human annotation. In\nthis paper, we introduce Arena Learning, an innovative offline strategy\ndesigned to simulate these arena battles using AI-driven annotations to\nevaluate battle outcomes, thus facilitating the continuous improvement of the\ntarget model through both supervised fine-tuning and reinforcement learning.\nArena Learning comprises two key elements. First, it ensures precise\nevaluations and maintains consistency between offline simulations and online\ncompetitions via WizardArena, a pipeline developed to accurately predict the\nElo rankings of various models using a meticulously designed offline test set.\nOur results demonstrate that WizardArena's predictions closely align with those\nfrom the online Arena. Second, it involves the continuous improvement of\ntraining data based on the battle results and the refined model. We establish a\ndata flywheel to iteratively update the training data by highlighting the\nweaknesses of the target model based on its battle results, enabling it to\nlearn from the strengths of multiple different models. We apply Arena Learning\nto train our target model, WizardLM-$\\beta$, and demonstrate significant\nperformance enhancements across various metrics. This fully automated training\nand evaluation pipeline sets the stage for continuous advancements in various\nLLMs via post-training. Notably, Arena Learning plays a pivotal role in the\nsuccess of WizardLM-2, and this paper serves both as an exploration of its\nefficacy and a foundational study for future discussions related to WizardLM-2\nand its derivatives.\n","authors":["Haipeng Luo","Qingfeng Sun","Can Xu","Pu Zhao","Qingwei Lin","Jianguang Lou","Shifeng Chen","Yansong Tang","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19909v3","updated":"2024-07-15T10:55:57Z","published":"2024-05-30T10:20:55Z","title":"Adaptive Advantage-Guided Policy Regularization for Offline\n  Reinforcement Learning","summary":"  In offline reinforcement learning, the challenge of out-of-distribution (OOD)\nis pronounced. To address this, existing methods often constrain the learned\npolicy through policy regularization. However, these methods often suffer from\nthe issue of unnecessary conservativeness, hampering policy improvement. This\noccurs due to the indiscriminate use of all actions from the behavior policy\nthat generates the offline dataset as constraints. The problem becomes\nparticularly noticeable when the quality of the dataset is suboptimal. Thus, we\npropose Adaptive Advantage-guided Policy Regularization (A2PR), obtaining\nhigh-advantage actions from an augmented behavior policy combined with VAE to\nguide the learned policy. A2PR can select high-advantage actions that differ\nfrom those present in the dataset, while still effectively maintaining\nconservatism from OOD actions. This is achieved by harnessing the VAE capacity\nto generate samples matching the distribution of the data points. We\ntheoretically prove that the improvement of the behavior policy is guaranteed.\nBesides, it effectively mitigates value overestimation with a bounded\nperformance gap. Empirically, we conduct a series of experiments on the D4RL\nbenchmark, where A2PR demonstrates state-of-the-art performance. Furthermore,\nexperimental results on additional suboptimal mixed datasets reveal that A2PR\nexhibits superior performance. Code is available at\nhttps://github.com/ltlhuuu/A2PR.\n","authors":["Tenglong Liu","Yang Li","Yixing Lan","Hao Gao","Wei Pan","Xin Xu"],"pdf_url":"https://arxiv.org/pdf/2405.19909v3.pdf","comment":"ICML 2024, 19 pages"},{"id":"http://arxiv.org/abs/2406.18387v2","updated":"2024-07-15T10:15:56Z","published":"2024-06-26T14:29:05Z","title":"DoubleTake: Geometry Guided Depth Estimation","summary":"  Estimating depth from a sequence of posed RGB images is a fundamental\ncomputer vision task, with applications in augmented reality, path planning\netc. Prior work typically makes use of previous frames in a multi view stereo\nframework, relying on matching textures in a local neighborhood. In contrast,\nour model leverages historical predictions by giving the latest 3D geometry\ndata as an extra input to our network. This self-generated geometric hint can\nencode information from areas of the scene not covered by the keyframes and it\nis more regularized when compared to individual predicted depth maps for\nprevious frames. We introduce a Hint MLP which combines cost volume features\nwith a hint of the prior geometry, rendered as a depth map from the current\ncamera location, together with a measure of the confidence in the prior\ngeometry. We demonstrate that our method, which can run at interactive speeds,\nachieves state-of-the-art estimates of depth and 3D scene reconstruction in\nboth offline and incremental evaluation scenarios.\n","authors":["Mohamed Sayed","Filippo Aleotti","Jamie Watson","Zawar Qureshi","Guillermo Garcia-Hernando","Gabriel Brostow","Sara Vicente","Michael Firman"],"pdf_url":"https://arxiv.org/pdf/2406.18387v2.pdf","comment":"ECCV 2024 Version"},{"id":"http://arxiv.org/abs/2402.09821v2","updated":"2024-07-15T10:15:12Z","published":"2024-02-15T09:36:36Z","title":"Diffusion Models for Audio Restoration","summary":"  With the development of audio playback devices and fast data transmission,\nthe demand for high sound quality is rising for both entertainment and\ncommunications. In this quest for better sound quality, challenges emerge from\ndistortions and interferences originating at the recording side or caused by an\nimperfect transmission pipeline. To address this problem, audio restoration\nmethods aim to recover clean sound signals from the corrupted input data. We\npresent here audio restoration algorithms based on diffusion models, with a\nfocus on speech enhancement and music restoration tasks. Traditional\napproaches, often grounded in handcrafted rules and statistical heuristics,\nhave shaped our understanding of audio signals. In the past decades, there has\nbeen a notable shift towards data-driven methods that exploit the modeling\ncapabilities of DNNs. Deep generative models, and among them diffusion models,\nhave emerged as powerful techniques for learning complex data distributions.\nHowever, relying solely on DNN-based learning approaches carries the risk of\nreducing interpretability, particularly when employing end-to-end models.\nNonetheless, data-driven approaches allow more flexibility in comparison to\nstatistical model-based frameworks, whose performance depends on distributional\nand statistical assumptions that can be difficult to guarantee. Here, we aim to\nshow that diffusion models can combine the best of both worlds and offer the\nopportunity to design audio restoration algorithms with a good degree of\ninterpretability and a remarkable performance in terms of sound quality. We\nexplain the diffusion formalism and its application to the conditional\ngeneration of clean audio signals. We believe that diffusion models open an\nexciting field of research with the potential to spawn new audio restoration\nalgorithms that are natural-sounding and remain robust in difficult acoustic\nsituations.\n","authors":["Jean-Marie Lemercier","Julius Richter","Simon Welker","Eloi Moliner","Vesa VÃ¤limÃ¤ki","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2402.09821v2.pdf","comment":"Currently in revision for IEEE Signal Processing Magazine Special\n  Issue \"Model-based and Data-Driven Audio Signal Processing\""},{"id":"http://arxiv.org/abs/2405.00334v2","updated":"2024-07-15T10:07:56Z","published":"2024-05-01T05:54:33Z","title":"A Survey on Deep Active Learning: Recent Advances and New Frontiers","summary":"  Active learning seeks to achieve strong performance with fewer training\nsamples. It does this by iteratively asking an oracle to label new selected\nsamples in a human-in-the-loop manner. This technique has gained increasing\npopularity due to its broad applicability, yet its survey papers, especially\nfor deep learning-based active learning (DAL), remain scarce. Therefore, we\nconduct an advanced and comprehensive survey on DAL. We first introduce\nreviewed paper collection and filtering. Second, we formally define the DAL\ntask and summarize the most influential baselines and widely used datasets.\nThird, we systematically provide a taxonomy of DAL methods from five\nperspectives, including annotation types, query strategies, deep model\narchitectures, learning paradigms, and training processes, and objectively\nanalyze their strengths and weaknesses. Then, we comprehensively summarize main\napplications of DAL in Natural Language Processing (NLP), Computer Vision (CV),\nand Data Mining (DM), etc. Finally, we discuss challenges and perspectives\nafter a detailed analysis of current studies. This work aims to serve as a\nuseful and quick guide for researchers in overcoming difficulties in DAL. We\nhope that this survey will spur further progress in this burgeoning field.\n","authors":["Dongyuan Li","Zhen Wang","Yankai Chen","Renhe Jiang","Weiping Ding","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2405.00334v2.pdf","comment":"This paper is accepted by IEEE Transactions on Neural Networks and\n  Learning Systems"},{"id":"http://arxiv.org/abs/2402.07754v2","updated":"2024-07-15T10:03:59Z","published":"2024-02-12T16:23:28Z","title":"Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language\n  Models","summary":"  Recently, diffusion models have garnered significant interest in the field of\ntext processing due to their many potential advantages compared to conventional\nautoregressive models. In this work, we propose Diffusion-of-Thought (DoT), a\nnovel approach that integrates diffusion models with Chain-of-Thought, a\nwell-established technique for improving the reasoning ability of\nautoregressive language models. In contrast to autoregressive language models\nthat make decisions in a left-to-right, token-by-token manner, DoT allows\nreasoning steps to diffuse over time through a diffusion language model and\noffers greater flexibility in trading-off computation for reasoning\nperformance. Our experimental results demonstrate the effectiveness of DoT in\nmulti-digit multiplication, boolean logic, and grade school math problems, with\na small diffusion model outperforming a much larger autoregressive model in\nboth efficiency and accuracy. In addition to that, DoT showcases promising\nself-correction abilities and benefits from existing reasoning-enhancing\ntechniques like self-consistency decoding. Our findings contribute to the\nunderstanding and development of reasoning with diffusion language models.\n","authors":["Jiacheng Ye","Shansan Gong","Liheng Chen","Lin Zheng","Jiahui Gao","Han Shi","Chuan Wu","Xin Jiang","Zhenguo Li","Wei Bi","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2402.07754v2.pdf","comment":"Multiple updates (add boolean logic dataset, add DoT based on SEDD\n  model and add detailed mathematical formulation in Appendix)"},{"id":"http://arxiv.org/abs/2407.10583v1","updated":"2024-07-15T10:03:24Z","published":"2024-07-15T10:03:24Z","title":"Three Dogmas of Reinforcement Learning","summary":"  Modern reinforcement learning has been conditioned by at least three dogmas.\nThe first is the environment spotlight, which refers to our tendency to focus\non modeling environments rather than agents. The second is our treatment of\nlearning as finding the solution to a task, rather than adaptation. The third\nis the reward hypothesis, which states that all goals and purposes can be well\nthought of as maximization of a reward signal. These three dogmas shape much of\nwhat we think of as the science of reinforcement learning. While each of the\ndogmas have played an important role in developing the field, it is time we\nbring them to the surface and reflect on whether they belong as basic\ningredients of our scientific paradigm. In order to realize the potential of\nreinforcement learning as a canonical frame for researching intelligent agents,\nwe suggest that it is time we shed dogmas one and two entirely, and embrace a\nnuanced approach to the third.\n","authors":["David Abel","Mark K. Ho","Anna Harutyunyan"],"pdf_url":"https://arxiv.org/pdf/2407.10583v1.pdf","comment":"RLC 2024"},{"id":"http://arxiv.org/abs/2310.05718v3","updated":"2024-07-15T09:57:48Z","published":"2023-10-09T13:39:26Z","title":"EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational\n  Autoencoders","summary":"  Codebook collapse is a common problem in training deep generative models with\ndiscrete representation spaces like Vector Quantized Variational Autoencoders\n(VQ-VAEs). We observe that the same problem arises for the alternatively\ndesigned discrete variational autoencoders (dVAEs) whose encoder directly\nlearns a distribution over the codebook embeddings to represent the data. We\nhypothesize that using the softmax function to obtain a probability\ndistribution causes the codebook collapse by assigning overconfident\nprobabilities to the best matching codebook elements. In this paper, we propose\na novel way to incorporate evidential deep learning (EDL) instead of softmax to\ncombat the codebook collapse problem of dVAE. We evidentially monitor the\nsignificance of attaining the probability distribution over the codebook\nembeddings, in contrast to softmax usage. Our experiments using various\ndatasets show that our model, called EdVAE, mitigates codebook collapse while\nimproving the reconstruction performance, and enhances the codebook usage\ncompared to dVAE and VQ-VAE based models. Our code can be found at\nhttps://github.com/ituvisionlab/EdVAE .\n","authors":["Gulcin Baykal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2310.05718v3.pdf","comment":"Accepted for publication in Pattern Recognition"},{"id":"http://arxiv.org/abs/2311.17609v2","updated":"2024-07-15T09:47:13Z","published":"2023-11-29T13:06:48Z","title":"Curved Diffusion: A Generative Model With Optical Geometry Control","summary":"  State-of-the-art diffusion models can generate highly realistic images based\non various conditioning like text, segmentation, and depth. However, an\nessential aspect often overlooked is the specific camera geometry used during\nimage capture. The influence of different optical systems on the final scene\nappearance is frequently overlooked. This study introduces a framework that\nintimately integrates a text-to-image diffusion model with the particular lens\ngeometry used in image rendering. Our method is based on a per-pixel coordinate\nconditioning method, enabling the control over the rendering geometry. Notably,\nwe demonstrate the manipulation of curvature properties, achieving diverse\nvisual effects, such as fish-eye, panoramic views, and spherical texturing\nusing a single diffusion model.\n","authors":["Andrey Voynov","Amir Hertz","Moab Arar","Shlomi Fruchter","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2311.17609v2.pdf","comment":"Project page at https://anylens-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2402.01054v2","updated":"2024-07-15T09:22:45Z","published":"2024-02-01T22:58:21Z","title":"Unconditional Latent Diffusion Models Memorize Patient Imaging Data:\n  Implications for Openly Sharing Synthetic Data","summary":"  AI models present a wide range of applications in the field of medicine.\nHowever, achieving optimal performance requires access to extensive healthcare\ndata, which is often not readily available. Furthermore, the imperative to\npreserve patient privacy restricts patient data sharing with third parties and\neven within institutes. Recently, generative AI models have been gaining\ntraction for facilitating open-data sharing by proposing synthetic data as\nsurrogates of real patient data. Despite the promise, these models are\nsusceptible to patient data memorization, where models generate patient data\ncopies instead of novel synthetic samples. Considering the importance of the\nproblem, it has received little attention in the medical imaging community. To\nthis end, we assess memorization in unconditional latent diffusion models. We\ntrain 2D and 3D latent diffusion models on CT, MR, and X-ray datasets for\nsynthetic data generation. Afterwards, we detect the amount of training data\nmemorized utilizing our self-supervised approach and further investigate\nvarious factors that can influence memorization. Our findings show a\nsurprisingly high degree of patient data memorization across all datasets, with\napproximately 40.9% of patient data being memorized and 78.5% of synthetic\nsamples identified as patient data copies on average in our experiments.\nFurther analyses reveal that using augmentation strategies during training can\nreduce memorization while over-training the models can enhance it. Although\nincreasing the dataset size does not reduce memorization and might even enhance\nit, it does lower the probability of a synthetic sample being a patient data\ncopy. Collectively, our results emphasize the importance of carefully training\ngenerative models on private medical imaging datasets, and examining the\nsynthetic data to ensure patient privacy before sharing it for medical research\nand applications.\n","authors":["Salman Ul Hassan Dar","Marvin Seyfarth","Jannik Kahmann","Isabelle Ayx","Theano Papavassiliu","Stefan O. Schoenberg","Norbert Frey","Bettina BaeÃler","Sebastian Foersch","Daniel Truhn","Jakob Nikolas Kather","Sandy Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2402.01054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10558v1","updated":"2024-07-15T09:15:55Z","published":"2024-07-15T09:15:55Z","title":"ConTEXTure: Consistent Multiview Images to Texture","summary":"  We introduce ConTEXTure, a generative network designed to create a texture\nmap/atlas for a given 3D mesh using images from multiple viewpoints. The\nprocess begins with generating a front-view image from a text prompt, such as\n'Napoleon, front view', describing the 3D mesh. Additional images from\ndifferent viewpoints are derived from this front-view image and camera poses\nrelative to it. ConTEXTure builds upon the TEXTure network, which uses text\nprompts for six viewpoints (e.g., 'Napoleon, front view', 'Napoleon, left\nview', etc.). However, TEXTure often generates images for non-front viewpoints\nthat do not accurately represent those viewpoints.To address this issue, we\nemploy Zero123++, which generates multiple view-consistent images for the six\nspecified viewpoints simultaneously, conditioned on the initial front-view\nimage and the depth maps of the mesh for the six viewpoints. By utilizing these\nview-consistent images, ConTEXTure learns the texture atlas from all viewpoint\nimages concurrently, unlike previous methods that do so sequentially. This\napproach ensures that the rendered images from various viewpoints, including\nback, side, bottom, and top, are free from viewpoint irregularities.\n","authors":["Jaehoon Ahn","Sumin Cho","Harim Jung","Kibeom Hong","Seonghoon Ban","Moon-Ryul Jung"],"pdf_url":"https://arxiv.org/pdf/2407.10558v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.17695v2","updated":"2024-07-15T09:11:19Z","published":"2024-01-31T09:31:28Z","title":"Datacube segmentation via Deep Spectral Clustering","summary":"  Extended Vision techniques are ubiquitous in physics. However, the data cubes\nsteaming from such analysis often pose a challenge in their interpretation, due\nto the intrinsic difficulty in discerning the relevant information from the\nspectra composing the data cube.\n  Furthermore, the huge dimensionality of data cube spectra poses a complex\ntask in its statistical interpretation; nevertheless, this complexity contains\na massive amount of statistical information that can be exploited in an\nunsupervised manner to outline some essential properties of the case study at\nhand, e.g.~it is possible to obtain an image segmentation via (deep) clustering\nof data-cube's spectra, performed in a suitably defined low-dimensional\nembedding space.\n  To tackle this topic, we explore the possibility of applying unsupervised\nclustering methods in encoded space, i.e. perform deep clustering on the\nspectral properties of datacube pixels. A statistical dimensional reduction is\nperformed by an ad hoc trained (Variational) AutoEncoder, in charge of mapping\nspectra into lower dimensional metric spaces, while the clustering process is\nperformed by a (learnable) iterative K-Means clustering algorithm.\n  We apply this technique to two different use cases, of different physical\norigins: a set of Macro mapping X-Ray Fluorescence (MA-XRF) synthetic data on\npictorial artworks, and a dataset of simulated astrophysical observations.\n","authors":["Alessandro Bombini","Fernando GarcÃ­a-Avello BofÃ­as","Caterina Bracci","Michele Ginolfi","Chiara Ruberto"],"pdf_url":"https://arxiv.org/pdf/2401.17695v2.pdf","comment":"20 pages, 10 figures, doi for code repository, dataset and trained\n  model available and reported in the paper. v2: paper accepted for publication\n  on IOP Machine Learning: Science and Technology"},{"id":"http://arxiv.org/abs/2403.16149v2","updated":"2024-07-15T09:05:13Z","published":"2024-03-24T13:43:43Z","title":"A Survey on Consumer IoT Traffic: Security and Privacy","summary":"  Although CIoT has improved the convenience of daily activities, it also\nintroduces new security and privacy concerns. Network traffic analysis, a\ncommon technique employed by the security community, has been extensively\nutilized to investigate security and privacy concerns, and it has also been\napplied to CIoT. However, compared to network traffic analysis in other fields\nsuch as mobile apps and websites, CIoT presents special new characteristics,\nwhich may introduce new challenges and research opportunities. In this study,\nwe reviewed 310 publications on traffic analysis within the CIoT security and\nprivacy domain, covering the period from January 2018 to December 2023.\nInitially, we summarized the CIoT traffic analysis process, highlighting the\nnewly identified characteristics of CIoT. Subsequently, we classified existing\nresearch according to its application objectives: device fingerprinting, user\nactivity inference, malicious traffic detection, and measurement. Lastly, we\nexplore emerging challenges and potential future research avenues.\n","authors":["Yan Jia","Yuxin Song","Zihou Liu","Qingyin Tan","Yang Song","Yu Zhang","Zheli Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16149v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10547v1","updated":"2024-07-15T08:57:02Z","published":"2024-07-15T08:57:02Z","title":"Learning Social Cost Functions for Human-Aware Path Planning","summary":"  Achieving social acceptance is one of the main goals of Social Robotic\nNavigation. Despite this topic has received increasing interest in recent\nyears, most of the research has focused on driving the robotic agent along\nobstacle-free trajectories, planning around estimates of future human motion to\nrespect personal distances and optimize navigation. However, social\ninteractions in everyday life are also dictated by norms that do not strictly\ndepend on movement, such as when standing at the end of a queue rather than\ncutting it. In this paper, we propose a novel method to recognize common social\nscenarios and modify a traditional planner's cost function to adapt to them.\nThis solution enables the robot to carry out different social navigation\nbehaviors that would not arise otherwise, maintaining the robustness of\ntraditional navigation. Our approach allows the robot to learn different social\nnorms with a single learned model, rather than having different modules for\neach task. As a proof of concept, we consider the tasks of queuing and respect\ninteraction spaces of groups of people talking to one another, but the method\ncan be extended to other human activities that do not involve motion.\n","authors":["Andrea Eirale","Matteo Leonetti","Marcello Chiaberge"],"pdf_url":"https://arxiv.org/pdf/2407.10547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10545v1","updated":"2024-07-15T08:52:20Z","published":"2024-07-15T08:52:20Z","title":"Efficient Continual Learning with Low Memory Footprint For Edge Device","summary":"  Continual learning(CL) is a useful technique to acquire dynamic knowledge\ncontinually. Although powerful cloud platforms can fully exert the ability of\nCL,e.g., customized recommendation systems, similar personalized requirements\nfor edge devices are almost disregarded. This phenomenon stems from the huge\nresource overhead involved in training neural networks and overcoming the\nforgetting problem of CL. This paper focuses on these scenarios and proposes a\ncompact algorithm called LightCL. Different from other CL methods bringing huge\nresource consumption to acquire generalizability among all tasks for delaying\nforgetting, LightCL compress the resource consumption of already generalized\ncomponents in neural networks and uses a few extra resources to improve memory\nin other parts. We first propose two new metrics of learning plasticity and\nmemory stability to seek generalizability during CL. Based on the discovery\nthat lower and middle layers have more generalizability and deeper layers are\nopposite, we $\\textit{Maintain Generalizability}$ by freezing the lower and\nmiddle layers. Then, we $\\textit{Memorize Feature Patterns}$ to stabilize the\nfeature extracting patterns of previous tasks to improve generalizability in\ndeeper layers. In the experimental comparison, LightCL outperforms other SOTA\nmethods in delaying forgetting and reduces at most $\\textbf{6.16$\\times$}$\nmemory footprint, proving the excellent performance of LightCL in efficiency.\nWe also evaluate the efficiency of our method on an edge device, the Jetson\nNano, which further proves our method's practical effectiveness.\n","authors":["Zeqing Wang","Fei Cheng","Kangye Ji","Bohu Huang"],"pdf_url":"https://arxiv.org/pdf/2407.10545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07590v4","updated":"2024-07-15T08:51:52Z","published":"2023-11-09T17:12:44Z","title":"Large Language Models can Strategically Deceive their Users when Put\n  Under Pressure","summary":"  We demonstrate a situation in which Large Language Models, trained to be\nhelpful, harmless, and honest, can display misaligned behavior and\nstrategically deceive their users about this behavior without being instructed\nto do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated\nenvironment, where it assumes the role of an autonomous stock trading agent.\nWithin this environment, the model obtains an insider tip about a lucrative\nstock trade and acts upon it despite knowing that insider trading is\ndisapproved of by company management. When reporting to its manager, the model\nconsistently hides the genuine reasons behind its trading decision. We perform\na brief investigation of how this behavior varies under changes to the setting,\nsuch as removing model access to a reasoning scratchpad, attempting to prevent\nthe misaligned behavior by changing system instructions, changing the amount of\npressure the model is under, varying the perceived risk of getting caught, and\nmaking other simple changes to the environment. To our knowledge, this is the\nfirst demonstration of Large Language Models trained to be helpful, harmless,\nand honest, strategically deceiving their users in a realistic situation\nwithout direct instructions or training for deception.\n","authors":["JÃ©rÃ©my Scheurer","Mikita Balesni","Marius Hobbhahn"],"pdf_url":"https://arxiv.org/pdf/2311.07590v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14047v2","updated":"2024-07-15T08:49:49Z","published":"2024-02-21T15:51:01Z","title":"Simple and Effective Transfer Learning for Neuro-Symbolic Integration","summary":"  Deep Learning (DL) techniques have achieved remarkable successes in recent\nyears. However, their ability to generalize and execute reasoning tasks remains\na challenge. A potential solution to this issue is Neuro-Symbolic Integration\n(NeSy), where neural approaches are combined with symbolic reasoning. Most of\nthese methods exploit a neural network to map perceptions to symbols and a\nlogical reasoner to predict the output of the downstream task. These methods\nexhibit superior generalization capacity compared to fully neural\narchitectures. However, they suffer from several issues, including slow\nconvergence, learning difficulties with complex perception tasks, and\nconvergence to local minima. This paper proposes a simple yet effective method\nto ameliorate these problems. The key idea involves pretraining a neural model\non the downstream task. Then, a NeSy model is trained on the same task via\ntransfer learning, where the weights of the perceptual part are injected from\nthe pretrained network. The key observation of our work is that the neural\nnetwork fails to generalize only at the level of the symbolic part while being\nperfectly capable of learning the mapping from perceptions to symbols. We have\ntested our training strategy on various SOTA NeSy methods and datasets,\ndemonstrating consistent improvements in the aforementioned problems.\n","authors":["Alessandro Daniele","Tommaso Campari","Sagar Malhotra","Luciano Serafini"],"pdf_url":"https://arxiv.org/pdf/2402.14047v2.pdf","comment":"Accepted as full paper at the International Conference on\n  Neural-Symbolic Learning and Reasoning (NeSy 2024)"},{"id":"http://arxiv.org/abs/2404.14076v2","updated":"2024-07-15T08:45:00Z","published":"2024-04-22T10:45:59Z","title":"Towards noise contrastive estimation with soft targets for conditional\n  models","summary":"  Soft targets combined with the cross-entropy loss have shown to improve\ngeneralization performance of deep neural networks on supervised classification\ntasks. The standard cross-entropy loss however assumes data to be categorically\ndistributed, which may often not be the case in practice. In contrast, InfoNCE\ndoes not rely on such an explicit assumption but instead implicitly estimates\nthe true conditional through negative sampling. Unfortunately, it cannot be\ncombined with soft targets in its standard formulation, hindering its use in\ncombination with sophisticated training strategies. In this paper, we address\nthis limitation by proposing a loss function that is compatible with\nprobabilistic targets. Our new soft target InfoNCE loss is conceptually simple,\nefficient to compute, and can be motivated through the framework of noise\ncontrastive estimation. Using a toy example, we demonstrate shortcomings of the\ncategorical distribution assumption of cross-entropy, and discuss implications\nof sampling from soft distributions. We observe that soft target InfoNCE\nperforms on par with strong soft target cross-entropy baselines and outperforms\nhard target NLL and InfoNCE losses on popular benchmarks, including ImageNet.\nFinally, we provide a simple implementation of our loss, geared towards\nsupervised classification and fully compatible with deep classification models\ntrained with cross-entropy.\n","authors":["Johannes Hugger","Virginie Uhlmann"],"pdf_url":"https://arxiv.org/pdf/2404.14076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05364v2","updated":"2024-07-15T08:37:49Z","published":"2024-07-07T13:32:03Z","title":"PTaRL: Prototype-based Tabular Representation Learning via Space\n  Calibration","summary":"  Tabular data have been playing a mostly important role in diverse real-world\nfields, such as healthcare, engineering, finance, etc. With the recent success\nof deep learning, many tabular machine learning (ML) methods based on deep\nnetworks (e.g., Transformer, ResNet) have achieved competitive performance on\ntabular benchmarks. However, existing deep tabular ML methods suffer from the\nrepresentation entanglement and localization, which largely hinders their\nprediction performance and leads to performance inconsistency on tabular tasks.\nTo overcome these problems, we explore a novel direction of applying prototype\nlearning for tabular ML and propose a prototype-based tabular representation\nlearning framework, PTaRL, for tabular prediction tasks. The core idea of PTaRL\nis to construct prototype-based projection space (P-Space) and learn the\ndisentangled representation around global data prototypes. Specifically, PTaRL\nmainly involves two stages: (i) Prototype Generation, that constructs global\nprototypes as the basis vectors of P-Space for representation, and (ii)\nPrototype Projection, that projects the data samples into P-Space and keeps the\ncore global data information via Optimal Transport. Then, to further acquire\nthe disentangled representations, we constrain PTaRL with two strategies: (i)\nto diversify the coordinates towards global prototypes of different\nrepresentations within P-Space, we bring up a diversification constraint for\nrepresentation calibration; (ii) to avoid prototype entanglement in P-Space, we\nintroduce a matrix orthogonalization constraint to ensure the independence of\nglobal prototypes. Finally, we conduct extensive experiments in PTaRL coupled\nwith state-of-the-art deep tabular ML models on various tabular benchmarks and\nthe results have shown our consistent superiority.\n","authors":["Hangting Ye","Wei Fan","Xiaozhuang Song","Shun Zheng","He Zhao","Dandan Guo","Yi Chang"],"pdf_url":"https://arxiv.org/pdf/2407.05364v2.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2402.10475v2","updated":"2024-07-15T08:21:28Z","published":"2024-02-16T06:41:35Z","title":"Fundamental Benefit of Alternating Updates in Minimax Optimization","summary":"  The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax\noptimization problems, takes the descent and ascent steps either simultaneously\n(Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to\nconverge faster, the performance gap between the two is not yet well understood\ntheoretically, especially in terms of global convergence rates. To address this\ntheory-practice gap, we present fine-grained convergence analyses of both\nalgorithms for strongly-convex-strongly-concave and Lipschitz-gradient\nobjectives. Our new iteration complexity upper bound of Alt-GDA is strictly\nsmaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster.\nMoreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general\nalgorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main\nidea is to alternately take gradients from extrapolations of the iterates. We\nshow that Alex-GDA satisfies a smaller iteration complexity bound, identical to\nthat of the Extra-gradient method, while requiring less gradient computations.\nWe also prove that Alex-GDA enjoys linear convergence for bilinear problems,\nfor which both Sim-GDA and Alt-GDA fail to converge at all.\n","authors":["Jaewook Lee","Hanseul Cho","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2402.10475v2.pdf","comment":"Accepted to ICML 2024 (Spotlight). 76 pages, 2 figures. Additional\n  experiments (quadratic game, GAN) and proofs"},{"id":"http://arxiv.org/abs/2407.05098v2","updated":"2024-07-15T08:19:30Z","published":"2024-07-06T14:59:55Z","title":"FedTSA: A Cluster-based Two-Stage Aggregation Method for\n  Model-heterogeneous Federated Learning","summary":"  Despite extensive research into data heterogeneity in federated learning\n(FL), system heterogeneity remains a significant yet often overlooked\nchallenge. Traditional FL approaches typically assume homogeneous hardware\nresources across FL clients, implying that clients can train a global model\nwithin a comparable time frame. However, in practical FL systems, clients often\nhave heterogeneous resources, which impacts their training capacity. This\ndiscrepancy underscores the importance of exploring model-heterogeneous FL, a\nparadigm allowing clients to train different models based on their resource\ncapabilities. To address this challenge, we introduce FedTSA, a cluster-based\ntwo-stage aggregation method tailored for system heterogeneity in FL. FedTSA\nbegins by clustering clients based on their capabilities, then performs a\ntwo-stage aggregation: conventional weight averaging for homogeneous models in\nStage 1, and deep mutual learning with a diffusion model for aggregating\nheterogeneous models in Stage 2. Extensive experiments demonstrate that FedTSA\nnot only outperforms the baselines but also explores various factors\ninfluencing model performance, validating FedTSA as a promising approach for\nmodel-heterogeneous FL.\n","authors":["Boyu Fan","Chenrui Wu","Xiang Su","Pan Hui"],"pdf_url":"https://arxiv.org/pdf/2407.05098v2.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2312.15600v3","updated":"2024-07-15T08:02:25Z","published":"2023-12-25T03:33:08Z","title":"Context-aware Communication for Multi-agent Reinforcement Learning","summary":"  Effective communication protocols in multi-agent reinforcement learning\n(MARL) are critical to fostering cooperation and enhancing team performance. To\nleverage communication, many previous works have proposed to compress local\ninformation into a single message and broadcast it to all reachable agents.\nThis simplistic messaging mechanism, however, may fail to provide adequate,\ncritical, and relevant information to individual agents, especially in severely\nbandwidth-limited scenarios. This motivates us to develop context-aware\ncommunication schemes for MARL, aiming to deliver personalized messages to\ndifferent agents. Our communication protocol, named CACOM, consists of two\nstages. In the first stage, agents exchange coarse representations in a\nbroadcast fashion, providing context for the second stage. Following this,\nagents utilize attention mechanisms in the second stage to selectively generate\nmessages personalized for the receivers. Furthermore, we employ the learned\nstep size quantization (LSQ) technique for message quantization to reduce the\ncommunication overhead. To evaluate the effectiveness of CACOM, we integrate it\nwith both actor-critic and value-based MARL algorithms. Empirical results on\ncooperative benchmark tasks demonstrate that CACOM provides evident performance\ngains over baselines under communication-constrained scenarios. The code is\npublicly available at https://github.com/LXXXXR/CACOM.\n","authors":["Xinran Li","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.15600v3.pdf","comment":"Accepted by the 23nd International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS 2024)"},{"id":"http://arxiv.org/abs/2407.10504v1","updated":"2024-07-15T07:53:29Z","published":"2024-07-15T07:53:29Z","title":"A pragmatic policy learning approach to account for users' fatigue in\n  repeated auctions","summary":"  Online advertising banners are sold in real-time through auctions.Typically,\nthe more banners a user is shown, the smaller the marginalvalue of the next\nbanner for this user is. This fact can be detected bybasic ML models, that can\nbe used to predict how previously won auctionsdecrease the current opportunity\nvalue. However, learning is not enough toproduce a bid that correctly accounts\nfor how winning the current auctionimpacts the future values. Indeed, a policy\nthat uses this prediction tomaximize the expected payoff of the current auction\ncould be dubbedimpatient because such policy does not fully account for the\nrepeatednature of the auctions. Under this perspective, it seems that most\nbiddersin the literature are impatient. Unsurprisingly, impatience induces a\ncost.We provide two empirical arguments for the importance of this cost\nofimpatience. First, an offline counterfactual analysis and, second, a\nnotablebusiness metrics improvement by mitigating the cost of impatience\nwithpolicy learning\n","authors":["Benjamin Heymann","RÃ©mi Chan--Renous-Legoubin","Alexandre Gilotte"],"pdf_url":"https://arxiv.org/pdf/2407.10504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09636v3","updated":"2024-07-15T07:45:28Z","published":"2024-04-15T10:12:33Z","title":"All-in-one simulation-based inference","summary":"  Amortized Bayesian inference trains neural networks to solve stochastic\ninference problems using model simulations, thereby making it possible to\nrapidly perform Bayesian inference for any newly observed data. However,\ncurrent simulation-based amortized inference methods are simulation-hungry and\ninflexible: They require the specification of a fixed parametric prior,\nsimulator, and inference tasks ahead of time. Here, we present a new amortized\ninference method -- the Simformer -- which overcomes these limitations. By\ntraining a probabilistic diffusion model with transformer architectures, the\nSimformer outperforms current state-of-the-art amortized inference approaches\non benchmark tasks and is substantially more flexible: It can be applied to\nmodels with function-valued parameters, it can handle inference scenarios with\nmissing or unstructured data, and it can sample arbitrary conditionals of the\njoint distribution of parameters and data, including both posterior and\nlikelihood. We showcase the performance and flexibility of the Simformer on\nsimulators from ecology, epidemiology, and neuroscience, and demonstrate that\nit opens up new possibilities and application domains for amortized Bayesian\ninference on simulation-based models.\n","authors":["Manuel Gloeckler","Michael Deistler","Christian Weilbach","Frank Wood","Jakob H. Macke"],"pdf_url":"https://arxiv.org/pdf/2404.09636v3.pdf","comment":"To be published in the proceedings of the 41st International\n  Conference on Machine Learning (ICML 2024), Vienna, Austria. PMLR 235, 2024"},{"id":"http://arxiv.org/abs/2407.10495v1","updated":"2024-07-15T07:37:31Z","published":"2024-07-15T07:37:31Z","title":"Improving Hyperbolic Representations via Gromov-Wasserstein\n  Regularization","summary":"  Hyperbolic representations have shown remarkable efficacy in modeling\ninherent hierarchies and complexities within data structures. Hyperbolic neural\nnetworks have been commonly applied for learning such representations from\ndata, but they often fall short in preserving the geometric structures of the\noriginal feature spaces. In response to this challenge, our work applies the\nGromov-Wasserstein (GW) distance as a novel regularization mechanism within\nhyperbolic neural networks. The GW distance quantifies how well the original\ndata structure is maintained after embedding the data in a hyperbolic space.\nSpecifically, we explicitly treat the layers of the hyperbolic neural networks\nas a transport map and calculate the GW distance accordingly. We validate that\nthe GW distance computed based on a training set well approximates the GW\ndistance of the underlying data distribution. Our approach demonstrates\nconsistent enhancements over current state-of-the-art methods across various\ntasks, including few-shot image classification, as well as semi-supervised\ngraph link prediction and node classification.\n","authors":["Yifei Yang","Wonjun Lee","Dongmian Zou","Gilad Lerman"],"pdf_url":"https://arxiv.org/pdf/2407.10495v1.pdf","comment":"Accepted for ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10494v1","updated":"2024-07-15T07:36:00Z","published":"2024-07-15T07:36:00Z","title":"Learning to Unlearn for Robust Machine Unlearning","summary":"  Machine unlearning (MU) seeks to remove knowledge of specific data samples\nfrom trained models without the necessity for complete retraining, a task made\nchallenging by the dual objectives of effective erasure of data and maintaining\nthe overall performance of the model. Despite recent advances in this field,\nbalancing between the dual objectives of unlearning remains challenging. From a\nfresh perspective of generalization, we introduce a novel Learning-to-Unlearn\n(LTU) framework, which adopts a meta-learning approach to optimize the\nunlearning process to improve forgetting and remembering in a unified manner.\nLTU includes a meta-optimization scheme that facilitates models to effectively\npreserve generalizable knowledge with only a small subset of the remaining set,\nwhile thoroughly forgetting the specific data samples. We also introduce a\nGradient Harmonization strategy to align the optimization trajectories for\nremembering and forgetting via mitigating gradient conflicts, thus ensuring\nefficient and effective model updates. Our approach demonstrates improved\nefficiency and efficacy for MU, offering a promising solution to the challenges\nof data rights and model reusability.\n","authors":["Mark He Huang","Lin Geng Foo","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.10494v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10490v1","updated":"2024-07-15T07:30:28Z","published":"2024-07-15T07:30:28Z","title":"Learning Dynamics of LLM Finetuning","summary":"  Learning dynamics, which describes how the learning of specific training\nexamples influences the model's prediction of other examples, give us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during finetuning, by analyzing\nthe step-wise decomposition and accumulated influence among different\nresponses. Our framework allows a uniform interpretation of many interesting\nobservations about the training of popular algorithms for both instruction\ntuning and preference tuning. The analysis not only explains where the benefits\nof these methods come from but also inspires a simple, effective method to\nfurther improve the alignment performance. Code for experiments is available at\nhttps://github.com/Joshua-Ren/Learning_dynamics_LLM.\n","authors":["Yi Ren","Danica J. Sutherland"],"pdf_url":"https://arxiv.org/pdf/2407.10490v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2403.16497v2","updated":"2024-07-15T07:24:36Z","published":"2024-03-25T07:29:18Z","title":"PathoTune: Adapting Visual Foundation Model to Pathological Specialists","summary":"  As natural image understanding moves towards the pretrain-finetune era,\nresearch in pathology imaging is concurrently evolving. Despite the predominant\nfocus on pretraining pathological foundation models, how to adapt foundation\nmodels to downstream tasks is little explored. For downstream adaptation, we\npropose the existence of two domain gaps, i.e., the Foundation-Task Gap and the\nTask-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework\ndesigned to efficiently adapt pathological or even visual foundation models to\npathology-specific tasks via multi-modal prompt tuning. The proposed framework\nleverages Task-specific Visual Prompts and Task-specific Textual Prompts to\nidentify task-relevant features, along with Instance-specific Visual Prompts\nfor encoding single pathological image features. Results across multiple\ndatasets at both patch-level and WSI-level demonstrate its superior performance\nover single-modality prompt tuning approaches. Significantly, PathoTune\nfacilitates the direct adaptation of natural visual foundation models to\npathological tasks, drastically outperforming pathological foundation models\nwith simple linear probing. The code is available at\nhttps://github.com/openmedlab/PathoDuet.\n","authors":["Jiaxuan Lu","Fang Yan","Xiaofan Zhang","Yue Gao","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16497v2.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.10484v1","updated":"2024-07-15T07:11:44Z","published":"2024-07-15T07:11:44Z","title":"Understanding Matrix Function Normalizations in Covariance Pooling\n  through the Lens of Riemannian Geometry","summary":"  Global Covariance Pooling (GCP) has been demonstrated to improve the\nperformance of Deep Neural Networks (DNNs) by exploiting second-order\nstatistics of high-level representations. GCP typically performs classification\nof the covariance matrices by applying matrix function normalization, such as\nmatrix logarithm or power, followed by a Euclidean classifier. However,\ncovariance matrices inherently lie in a Riemannian manifold, known as the\nSymmetric Positive Definite (SPD) manifold. The current literature does not\nprovide a satisfactory explanation of why Euclidean classifiers can be applied\ndirectly to Riemannian features after the normalization of the matrix power. To\nmitigate this gap, this paper provides a comprehensive and unified\nunderstanding of the matrix logarithm and power from a Riemannian geometry\nperspective. The underlying mechanism of matrix functions in GCP is interpreted\nfrom two perspectives: one based on tangent classifiers (Euclidean classifiers\non the tangent space) and the other based on Riemannian classifiers. Via\ntheoretical analysis and empirical validation through extensive experiments on\nfine-grained and large-scale visual classification datasets, we conclude that\nthe working mechanism of the matrix functions should be attributed to the\nRiemannian classifiers they implicitly respect.\n","authors":["Ziheng Chen","Yue Song","Xiao-Jun Wu","Gaowen Liu","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2407.10484v1.pdf","comment":"24 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.10483v1","updated":"2024-07-15T07:11:00Z","published":"2024-07-15T07:11:00Z","title":"G-PCGRL: Procedural Graph Data Generation via Reinforcement Learning","summary":"  Graph data structures offer a versatile and powerful means to model\nrelationships and interconnections in various domains, promising substantial\nadvantages in data representation, analysis, and visualization. In games,\ngraph-based data structures are omnipresent and represent, for example, game\neconomies, skill trees or complex, branching quest lines. With this paper, we\npropose G-PCGRL, a novel and controllable method for the procedural generation\nof graph data using reinforcement learning. Therefore, we frame this problem as\nmanipulating a graph's adjacency matrix to fulfill a given set of constraints.\nOur method adapts and extends the Procedural Content Generation via\nReinforcement Learning (PCGRL) framework and introduces new representations to\nframe the problem of graph data generation as a Markov decision process. We\ncompare the performance of our method with the original PCGRL, the run time\nwith a random search and evolutionary algorithm, and evaluate G-PCGRL on two\ngraph data domains in games: game economies and skill trees. The results show\nthat our method is capable of generating graph-based content quickly and\nreliably to support and inspire designers in the game creation process. In\naddition, trained models are controllable in terms of the type and number of\nnodes to be generated.\n","authors":["Florian Rupp","Kai Eckert"],"pdf_url":"https://arxiv.org/pdf/2407.10483v1.pdf","comment":"8 pages, 7 figures, 2 tables. Accepted at the IEEE Conference on\n  Games (IEEE CoG) 2024"},{"id":"http://arxiv.org/abs/2407.10481v1","updated":"2024-07-15T07:07:11Z","published":"2024-07-15T07:07:11Z","title":"SuperPADL: Scaling Language-Directed Physics-Based Control with\n  Progressive Supervised Distillation","summary":"  Physically-simulated models for human motion can generate high-quality\nresponsive character animations, often in real-time. Natural language serves as\na flexible interface for controlling these models, allowing expert and\nnon-expert users to quickly create and edit their animations. Many recent\nphysics-based animation methods, including those that use text interfaces,\ntrain control policies using reinforcement learning (RL). However, scaling\nthese methods beyond several hundred motions has remained challenging.\nMeanwhile, kinematic animation models are able to successfully learn from\nthousands of diverse motions by leveraging supervised learning methods.\nInspired by these successes, in this work we introduce SuperPADL, a scalable\nframework for physics-based text-to-motion that leverages both RL and\nsupervised learning to train controllers on thousands of diverse motion clips.\nSuperPADL is trained in stages using progressive distillation, starting with a\nlarge number of specialized experts using RL. These experts are then\niteratively distilled into larger, more robust policies using a combination of\nreinforcement learning and supervised learning. Our final SuperPADL controller\nis trained on a dataset containing over 5000 skills and runs in real time on a\nconsumer GPU. Moreover, our policy can naturally transition between skills,\nallowing for users to interactively craft multi-stage animations. We\nexperimentally demonstrate that SuperPADL significantly outperforms RL-based\nbaselines at this large data scale.\n","authors":["Jordan Juravsky","Yunrong Guo","Sanja Fidler","Xue Bin Peng"],"pdf_url":"https://arxiv.org/pdf/2407.10481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10477v1","updated":"2024-07-15T07:05:34Z","published":"2024-07-15T07:05:34Z","title":"Deep Learning-Based Operators for Evolutionary Algorithms","summary":"  We present two novel domain-independent genetic operators that harness the\ncapabilities of deep learning: a crossover operator for genetic algorithms and\na mutation operator for genetic programming. Deep Neural Crossover leverages\nthe capabilities of deep reinforcement learning and an encoder-decoder\narchitecture to select offspring genes. BERT mutation masks multiple gp-tree\nnodes and then tries to replace these masks with nodes that will most likely\nimprove the individual's fitness. We show the efficacy of both operators\nthrough experimentation.\n","authors":["Eliad Shem-Tov","Moshe Sipper","Achiya Elyasaf"],"pdf_url":"https://arxiv.org/pdf/2407.10477v1.pdf","comment":"16 pages, 7 figures, 2 tables. Accepted to Genetic Programming Theory\n  & Practice XXI (GPTP 2024)"},{"id":"http://arxiv.org/abs/2407.07719v2","updated":"2024-07-15T06:54:53Z","published":"2024-06-17T13:09:25Z","title":"Model-based learning for multi-antenna multi-frequency\n  location-to-channel mapping","summary":"  Years of study of the propagation channel showed a close relation between a\nlocation and the associated communication channel response. The use of a neural\nnetwork to learn the location-to-channel mapping can therefore be envisioned.\nThe Implicit Neural Representation (INR) literature showed that classical\nneural architecture are biased towards learning low-frequency content, making\nthe location-to-channel mapping learning a non-trivial problem. Indeed, it is\nwell known that this mapping is a function rapidly varying with the location,\non the order of the wavelength. This paper leverages the model-based machine\nlearning paradigm to derive a problem-specific neural architecture from a\npropagation channel model. The resulting architecture efficiently overcomes the\nspectral-bias issue. It only learns low-frequency sparse correction terms\nactivating a dictionary of high-frequency components. The proposed architecture\nis evaluated against classical INR architectures on realistic synthetic data,\nshowing much better accuracy. Its mapping learning performance is explained\nbased on the approximated channel model, highlighting the explainability of the\nmodel-based machine learning paradigm.\n","authors":["Baptiste Chatelier","Vincent Corlay","Matthieu CrussiÃ¨re","Luc Le Magoarou"],"pdf_url":"https://arxiv.org/pdf/2407.07719v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17503v2","updated":"2024-07-15T06:41:13Z","published":"2024-06-25T12:43:33Z","title":"WAVE: Weight Template for Adaptive Initialization of Variable-sized\n  Models","summary":"  The expansion of model parameters underscores the significance of pre-trained\nmodels; however, the constraints encountered during model deployment\nnecessitate models of variable sizes. Consequently, the traditional\npre-training and fine-tuning paradigm fails to address the initialization\nproblem when target models are incompatible with pre-trained models. We tackle\nthis issue from a multitasking perspective and introduce \\textbf{WAVE}, which\nincorporates a set of shared \\textbf{W}eight templates for \\textbf{A}daptive\ninitialization of \\textbf{V}ariable-siz\\textbf{E}d Models. During\ninitialization, target models will initialize the corresponding weight scalers\ntailored to their model size, which are sufficient to learn the connection\nrules of weight templates based on the Kronecker product from a limited amount\nof data. For the construction of the weight templates, WAVE utilizes the\n\\textit{Learngene} framework, which structurally condenses common knowledge\nfrom ancestry models into weight templates as the learngenes through knowledge\ndistillation. This process allows the integration of pre-trained models'\nknowledge into structured knowledge according to the rules of weight templates.\nWe provide a comprehensive benchmark for the learngenes, and extensive\nexperiments demonstrate the efficacy of WAVE. The results show that WAVE\nachieves state-of-the-art performance when initializing models with various\ndepth and width, and even outperforms the direct pre-training of $n$ entire\nmodels, particularly for smaller models, saving approximately $n\\times$ and\n$5\\times$ in computational and storage resources, respectively. WAVE\nsimultaneously achieves the most efficient knowledge transfer across a series\nof datasets, specifically achieving an average improvement of 1.8\\% and 1.2\\%\non 7 downstream datasets.\n","authors":["Fu Feng","Yucheng Xie","Jing Wang","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2406.17503v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10494v3","updated":"2024-07-15T06:37:04Z","published":"2023-02-21T07:48:34Z","title":"The Role of Masking for Efficient Supervised Knowledge Distillation of\n  Vision Transformers","summary":"  Knowledge distillation is an effective method for training lightweight vision\nmodels. However, acquiring teacher supervision for training samples is often\ncostly, especially from large-scale models like vision transformers (ViTs). In\nthis paper, we develop a simple framework to reduce the supervision cost of ViT\ndistillation: masking out a fraction of input tokens given to the teacher. By\nmasking input tokens, one can skip the computations associated with the masked\ntokens without requiring any change to teacher parameters or architecture. We\nfind that masking patches with the lowest student attention scores is highly\neffective, saving up to 50% of teacher FLOPs without any drop in student\naccuracy, while other masking criterion leads to suboptimal efficiency gains.\nThrough in-depth analyses, we reveal that the student-guided masking provides a\ngood curriculum to the student, making teacher supervision easier to follow\nduring the early stage and challenging in the later stage.\n","authors":["Seungwoo Son","Jegwang Ryu","Namhoon Lee","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2302.10494v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.10454v1","updated":"2024-07-15T06:07:05Z","published":"2024-07-15T06:07:05Z","title":"Deflated Dynamics Value Iteration","summary":"  The Value Iteration (VI) algorithm is an iterative procedure to compute the\nvalue function of a Markov decision process, and is the basis of many\nreinforcement learning (RL) algorithms as well. As the error convergence rate\nof VI as a function of iteration $k$ is $O(\\gamma^k)$, it is slow when the\ndiscount factor $\\gamma$ is close to $1$. To accelerate the computation of the\nvalue function, we propose Deflated Dynamics Value Iteration (DDVI). DDVI uses\nmatrix splitting and matrix deflation techniques to effectively remove\n(deflate) the top $s$ dominant eigen-structure of the transition matrix\n$\\mathcal{P}^{\\pi}$. We prove that this leads to a $\\tilde{O}(\\gamma^k\n|\\lambda_{s+1}|^k)$ convergence rate, where $\\lambda_{s+1}$is $(s+1)$-th\nlargest eigenvalue of the dynamics matrix. We then extend DDVI to the RL\nsetting and present Deflated Dynamics Temporal Difference (DDTD) algorithm. We\nempirically show the effectiveness of the proposed algorithms.\n","authors":["Jongmin Lee","Amin Rakhsha","Ernest K. Ryu","Amir-massoud Farahmand"],"pdf_url":"https://arxiv.org/pdf/2407.10454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10452v1","updated":"2024-07-15T05:45:09Z","published":"2024-07-15T05:45:09Z","title":"GraphPrint: Extracting Features from 3D Protein Structure for Drug\n  Target Affinity Prediction","summary":"  Accurate drug target affinity prediction can improve drug candidate\nselection, accelerate the drug discovery process, and reduce drug production\ncosts. Previous work focused on traditional fingerprints or used features\nextracted based on the amino acid sequence in the protein, ignoring its 3D\nstructure which affects its binding affinity. In this work, we propose\nGraphPrint: a framework for incorporating 3D protein structure features for\ndrug target affinity prediction. We generate graph representations for protein\n3D structures using amino acid residue location coordinates and combine them\nwith drug graph representation and traditional features to jointly learn drug\ntarget affinity. Our model achieves a mean square error of 0.1378 and a\nconcordance index of 0.8929 on the KIBA dataset and improves over using\ntraditional protein features alone. Our ablation study shows that the 3D\nprotein structure-based features provide information complementary to\ntraditional features.\n","authors":["Amritpal Singh"],"pdf_url":"https://arxiv.org/pdf/2407.10452v1.pdf","comment":"Accepted: The NeurIPS 2023 Workshop on New Frontiers of AI for Drug\n  Discovery and Development (AI4D3 2023), New Orleans, LA, USA, 2023"},{"id":"http://arxiv.org/abs/2407.10449v1","updated":"2024-07-15T05:40:11Z","published":"2024-07-15T05:40:11Z","title":"A Fast, Robust Elliptical Slice Sampling Implementation for Linearly\n  Truncated Multivariate Normal Distributions","summary":"  Elliptical slice sampling, when adapted to linearly truncated multivariate\nnormal distributions, is a rejection-free Markov chain Monte Carlo method. At\nits core, it requires analytically constructing an ellipse-polytope\nintersection. The main novelty of this paper is an algorithm that computes this\nintersection in $\\mathcal{O}(m \\log m)$ time, where $m$ is the number of linear\ninequality constraints representing the polytope. We show that an\nimplementation based on this algorithm enhances numerical stability, speeds up\nrunning time, and is easy to parallelize for launching multiple Markov chains.\n","authors":["Kaiwen Wu","Jacob R. Gardner"],"pdf_url":"https://arxiv.org/pdf/2407.10449v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2407.10448v1","updated":"2024-07-15T05:39:56Z","published":"2024-07-15T05:39:56Z","title":"Spectral Representation for Causal Estimation with Hidden Confounders","summary":"  We address the problem of causal effect estimation where hidden confounders\nare present, with a focus on two settings: instrumental variable regression\nwith additional observed confounders, and proxy causal learning. Our approach\nuses a singular value decomposition of a conditional expectation operator,\nfollowed by a saddle-point optimization problem, which, in the context of IV\nregression, can be thought of as a neural net generalization of the seminal\napproach due to Darolles et al. [2011]. Saddle-point formulations have gathered\nconsiderable attention recently, as they can avoid double sampling bias and are\namenable to modern function approximation methods. We provide experimental\nvalidation in various settings, and show that our approach outperforms existing\nmethods on common benchmarks.\n","authors":["Tongzheng Ren","Haotian Sun","Antoine Moulin","Arthur Gretton","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2407.10448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03496v7","updated":"2024-07-15T05:23:41Z","published":"2024-02-05T20:15:19Z","title":"Can We Remove the Square-Root in Adaptive Gradient Methods? A\n  Second-Order Perspective","summary":"  Adaptive gradient optimizers like Adam(W) are the default training algorithms\nfor many deep learning architectures, such as transformers. Their diagonal\npreconditioner is based on the gradient outer product which is incorporated\ninto the parameter update via a square root. While these methods are often\nmotivated as approximate second-order methods, the square root represents a\nfundamental difference. In this work, we investigate how the behavior of\nadaptive methods changes when we remove the root, i.e., strengthen their\nsecond-order motivation. Surprisingly, we find that such square-root-free\nadaptive methods close the generalization gap to SGD on convolutional\narchitectures, while maintaining their root-based counterpart's performance on\ntransformers. The second-order perspective also has practical benefits for\ndeveloping non-diagonal methods that can incorporate arbitrary curvature\napproximations through the concept of preconditioner invariance. In contrast to\nroot-based methods like Shampoo, root-free counterparts work well and fast with\nhalf-precision since they do not require numerically unstable matrix root\ndecompositions and inversions. Overall, our findings provide new insights into\nthe development of adaptive methods and raise important questions regarding the\noverlooked role of adaptivity in their success. (experiment code:\nhttps://github.com/yorkerlin/remove-the-square-root optimizer code:\nhttps://github.com/f-dangel/sirfshampoo)\n","authors":["Wu Lin","Felix Dangel","Runa Eschenhagen","Juhan Bae","Richard E. Turner","Alireza Makhzani"],"pdf_url":"https://arxiv.org/pdf/2402.03496v7.pdf","comment":"A long version of the ICML 2024 paper. Fixed some typos and updated\n  the abstract and Sec. 4 to emphasize the concept of preconditioner invariance"},{"id":"http://arxiv.org/abs/2407.03234v2","updated":"2024-07-15T05:20:18Z","published":"2024-07-03T16:03:42Z","title":"Self-Evaluation as a Defense Against Adversarial Attacks on LLMs","summary":"  When LLMs are deployed in sensitive, human-facing settings, it is crucial\nthat they do not output unsafe, biased, or privacy-violating outputs. For this\nreason, models are both trained and instructed to refuse to answer unsafe\nprompts such as \"Tell me how to build a bomb.\" We find that, despite these\nsafeguards, it is possible to break model defenses simply by appending a space\nto the end of a model's input. In a study of eight open-source models, we\ndemonstrate that this acts as a strong enough attack to cause the majority of\nmodels to generate harmful outputs with very high success rates. We examine the\ncauses of this behavior, finding that the contexts in which single spaces occur\nin tokenized training data encourage models to generate lists when prompted,\noverriding training signals to refuse to answer unsafe requests. Our findings\nunderscore the fragile state of current model alignment and promote the\nimportance of developing more robust alignment methods. Code and data will be\nmade available at https://github.com/Linlt-leon/self-eval.\n","authors":["Hannah Brown","Leon Lin","Kenji Kawaguchi","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2407.03234v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.00218v2","updated":"2024-07-15T05:18:42Z","published":"2024-03-30T02:23:01Z","title":"Functional-Edged Network Modeling","summary":"  Contrasts with existing works which all consider nodes as functions and use\nedges to represent the relationships between different functions. We target at\nnetwork modeling whose edges are functional data and transform the adjacency\nmatrix into a functional adjacency tensor, introducing an additional dimension\ndedicated to function representation. Tucker functional decomposition is used\nfor the functional adjacency tensor, and to further consider the community\nbetween nodes, we regularize the basis matrices to be symmetrical. Furthermore,\nto deal with irregular observations of the functional edges, we conduct model\ninference to solve a tensor completion problem. It is optimized by a Riemann\nconjugate gradient descent method. Besides these, we also derive several\ntheorems to show the desirable properties of the functional edged network\nmodel. Finally, we evaluate the efficacy of our proposed model using simulation\ndata and real metro system data from Hong Kong and Singapore.\n","authors":["Haijie Xu","Chen Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.00218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08745v4","updated":"2024-07-15T05:17:24Z","published":"2023-11-15T07:27:40Z","title":"Using Stochastic Gradient Descent to Smooth Nonconvex Functions:\n  Analysis of Implicit Graduated Optimization with Optimal Noise Scheduling","summary":"  The graduated optimization approach is a heuristic method for finding\nglobally optimal solutions for nonconvex functions and has been theoretically\nanalyzed in several studies. This paper defines a new family of nonconvex\nfunctions for graduated optimization, discusses their sufficient conditions,\nand provides a convergence analysis of the graduated optimization algorithm for\nthem. It shows that stochastic gradient descent (SGD) with mini-batch\nstochastic gradients has the effect of smoothing the objective function, the\ndegree of which is determined by the learning rate, batch size, and variance of\nthe stochastic gradient. This finding provides theoretical insights on why\nlarge batch sizes fall into sharp local minima, why decaying learning rates and\nincreasing batch sizes are superior to fixed learning rates and batch sizes,\nand what the optimal learning rate scheduling is. To the best of our knowledge,\nthis is the first paper to provide a theoretical explanation for these aspects.\nIn addition, we show that the degree of smoothing introduced is strongly\ncorrelated with the generalization performance of the model. Moreover, a new\ngraduated optimization framework that uses a decaying learning rate and\nincreasing batch size is analyzed and experimental results of image\nclassification are reported that support our theoretical findings.\n","authors":["Naoki Sato","Hideaki Iiduka"],"pdf_url":"https://arxiv.org/pdf/2311.08745v4.pdf","comment":"The latest version was updated on Jul. 15"},{"id":"http://arxiv.org/abs/2407.10441v1","updated":"2024-07-15T05:08:38Z","published":"2024-07-15T05:08:38Z","title":"Enhancing Building Safety Design for Active Shooter Incidents:\n  Exploration of Building Exit Parameters using Reinforcement Learning-Based\n  Simulations","summary":"  With the alarming rise in active shooter incidents (ASIs) in the United\nStates, enhancing public safety through building design has become a pressing\nneed. This study proposes a reinforcement learning-based simulation approach\naddressing gaps in existing research that has neglected the dynamic behaviours\nof shooters. We developed an autonomous agent to simulate an active shooter\nwithin a realistic office environment, aiming to offer insights into the\ninteractions between building design parameters and ASI outcomes. A case study\nis conducted to quantitatively investigate the impact of building exit numbers\n(total count of accessible exits) and configuration (arrangement of which exits\nare available or not) on evacuation and harm rates. Findings demonstrate that\ngreater exit availability significantly improves evacuation outcomes and\nreduces harm. Exits nearer to the shooter's initial position hold greater\nimportance for accessibility than those farther away. By encompassing dynamic\nshooter behaviours, this study offers preliminary insights into effective\nbuilding safety design against evolving threats.\n","authors":["Ruying Liu","Wanjing Wu","Burcin Becerik-Gerber","Gale M. Lucas"],"pdf_url":"https://arxiv.org/pdf/2407.10441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09821v2","updated":"2024-07-15T05:03:01Z","published":"2024-05-16T05:37:50Z","title":"Evaluating Algorithmic Bias in Models for Predicting Academic\n  Performance of Filipino Students","summary":"  Algorithmic bias is a major issue in machine learning models in educational\ncontexts. However, it has not yet been studied thoroughly in Asian learning\ncontexts, and only limited work has considered algorithmic bias based on\nregional (sub-national) background. As a step towards addressing this gap, this\npaper examines the population of 5,986 students at a large university in the\nPhilippines, investigating algorithmic bias based on students' regional\nbackground. The university used the Canvas learning management system (LMS) in\nits online courses across a broad range of domains. Over the period of three\nsemesters, we collected 48.7 million log records of the students' activity in\nCanvas. We used these logs to train binary classification models that predict\nstudent grades from the LMS activity. The best-performing model reached AUC of\n0.75 and weighted F1-score of 0.79. Subsequently, we examined the data for bias\nbased on students' region. Evaluation using three metrics: AUC, weighted\nF1-score, and MADD showed consistent results across all demographic groups.\nThus, no unfairness was observed against a particular student group in the\ngrade predictions.\n","authors":["Valdemar Å vÃ¡benskÃ½","MÃ©lina Verger","Maria Mercedes T. Rodrigo","Clarence James G. Monterozo","Ryan S. Baker","Miguel Zenon Nicanor Lerias Saavedra","SÃ©bastien LallÃ©","Atsushi Shimada"],"pdf_url":"https://arxiv.org/pdf/2405.09821v2.pdf","comment":"Published in proceedings of the 17th Educational Data Mining\n  Conference (EDM 2024)"},{"id":"http://arxiv.org/abs/2406.16028v2","updated":"2024-07-15T04:36:30Z","published":"2024-06-23T06:32:27Z","title":"TimeAutoDiff: Combining Autoencoder and Diffusion model for time series\n  tabular data synthesizing","summary":"  In this paper, we leverage the power of latent diffusion models to generate\nsynthetic time series tabular data. Along with the temporal and feature\ncorrelations, the heterogeneous nature of the feature in the table has been one\nof the main obstacles in time series tabular data modeling. We tackle this\nproblem by combining the ideas of the variational auto-encoder (VAE) and the\ndenoising diffusion probabilistic model (DDPM). Our model named as\n\\texttt{TimeAutoDiff} has several key advantages including (1) Generality: the\nability to handle the broad spectrum of time series tabular data from single to\nmulti-sequence datasets; (2) Good fidelity and utility guarantees: numerical\nexperiments on six publicly available datasets demonstrating significant\nimprovements over state-of-the-art models in generating time series tabular\ndata, across four metrics measuring fidelity and utility; (3) Fast sampling\nspeed: entire time series data generation as opposed to the sequential data\nsampling schemes implemented in the existing diffusion-based models, eventually\nleading to significant improvements in sampling speed, (4) Entity conditional\ngeneration: the first implementation of conditional generation of\nmulti-sequence time series tabular data with heterogenous features in the\nliterature, enabling scenario exploration across multiple scientific and\nengineering domains. Codes are in preparation for release to the public, but\navailable upon request.\n","authors":["Namjoon Suh","Yuning Yang","Din-Yin Hsieh","Qitong Luan","Shirong Xu","Shixiang Zhu","Guang Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.16028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10342v2","updated":"2024-07-15T04:19:50Z","published":"2024-02-15T22:11:18Z","title":"Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on\n  Efficient Data Utilization","summary":"  Reinforcement Learning from Human Feedback (RLHF) has achieved impressive\nempirical successes while relying on a small amount of human feedback. However,\nthere is limited theoretical justification for this phenomenon. Additionally,\nmost recent studies focus on value-based algorithms despite the recent\nempirical successes of policy-based algorithms. In this work, we consider an\nRLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based\non the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes\nknowledge of the reward function. In PO-RLHF, knowledge of the reward function\nis not assumed, and the algorithm uses trajectory-based comparison feedback to\ninfer the reward function. We provide performance bounds for PO-RLHF with low\nquery complexity, which provides insight into why a small amount of human\nfeedback may be sufficient to achieve good performance with RLHF. A key novelty\nis a trajectory-level elliptical potential analysis, which bounds the reward\nestimation error when comparison feedback (rather than numerical reward\nobservation) is given. We provide and analyze algorithms PG-RLHF and NN-PG-RLHF\nfor two settings: linear and neural function approximation, respectively.\n","authors":["Yihan Du","Anna Winnicki","Gal Dalal","Shie Mannor","R. Srikant"],"pdf_url":"https://arxiv.org/pdf/2402.10342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13640v2","updated":"2024-07-15T04:17:11Z","published":"2024-06-19T15:39:27Z","title":"Transferable Tactile Transformers for Representation Learning Across\n  Diverse Sensors and Tasks","summary":"  This paper presents T3: Transferable Tactile Transformers, a framework for\ntactile representation learning that scales across multi-sensors and\nmulti-tasks. T3 is designed to overcome the contemporary issue that\ncamera-based tactile sensing is extremely heterogeneous, i.e. sensors are built\ninto different form factors, and existing datasets were collected for disparate\ntasks. T3 captures the shared latent information across different sensor-task\npairings by constructing a shared trunk transformer with sensor-specific\nencoders and task-specific decoders. The pre-training of T3 utilizes a novel\nFoundation Tactile (FoTa) dataset, which is aggregated from several\nopen-sourced datasets and it contains over 3 million data points gathered from\n13 sensors and 11 tasks. FoTa is the largest and most diverse dataset in\ntactile sensing to date and it is made publicly available in a unified format.\nAcross various sensors and tasks, experiments show that T3 pre-trained with\nFoTa achieved zero-shot transferability in certain sensor-task pairings, can be\nfurther fine-tuned with small amounts of domain-specific data, and its\nperformance scales with bigger network sizes. T3 is also effective as a tactile\nencoder for long horizon contact-rich manipulation. Results from sub-millimeter\nmulti-pin electronics insertion tasks show that T3 achieved a task success rate\n25% higher than that of policies trained with tactile encoders trained from\nscratch, or 53% higher than without tactile sensing. Data, code, and model\ncheckpoints are open-sourced at https://t3.alanz.info.\n","authors":["Jialiang Zhao","Yuxiang Ma","Lirui Wang","Edward H. Adelson"],"pdf_url":"https://arxiv.org/pdf/2406.13640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07886v2","updated":"2024-07-15T03:54:20Z","published":"2024-01-15T18:28:17Z","title":"Learned Best-Effort LLM Serving","summary":"  Many applications must provide low-latency LLM service to users or risk\nunacceptable user experience. However, over-provisioning resources to serve\nfluctuating request patterns is often prohibitively expensive. In this work, we\npresent a best-effort serving system that employs deep reinforcement learning\nto adjust service quality based on the task distribution and system load. Our\nbest-effort system can maintain availability with over 10x higher client\nrequest rates, serves above 96% of peak performance 4.1x more often, and serves\nabove 98% of peak performance 2.3x more often than static serving on\nunpredictable workloads. Our learned router is robust to shifts in both the\narrival and task distribution. Compared to static serving, learned best-effort\nserving allows for cost-efficient serving through increased hardware utility.\nAdditionally, we argue that learned best-effort LLM serving is applicable in\nwide variety of settings and provides application developers great flexibility\nto meet their specific needs.\n","authors":["Siddharth Jha","Coleman Hooper","Xiaoxuan Liu","Sehoon Kim","Kurt Keutzer"],"pdf_url":"https://arxiv.org/pdf/2401.07886v2.pdf","comment":"Es-FoMo @ ICML 2024"},{"id":"http://arxiv.org/abs/2305.03942v5","updated":"2024-07-15T03:49:48Z","published":"2023-05-06T05:55:27Z","title":"HACMan: Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile\n  Manipulation","summary":"  Manipulating objects without grasping them is an essential component of human\ndexterity, referred to as non-prehensile manipulation. Non-prehensile\nmanipulation may enable more complex interactions with the objects, but also\npresents challenges in reasoning about gripper-object interactions. In this\nwork, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a\nreinforcement learning approach for 6D non-prehensile manipulation of objects\nusing point cloud observations. HACMan proposes a temporally-abstracted and\nspatially-grounded object-centric action representation that consists of\nselecting a contact location from the object point cloud and a set of motion\nparameters describing how the robot will move after making contact. We modify\nan existing off-policy RL algorithm to learn in this hybrid discrete-continuous\naction representation. We evaluate HACMan on a 6D object pose alignment task in\nboth simulation and in the real world. On the hardest version of our task, with\nrandomized initial poses, randomized 6D goals, and diverse object categories,\nour policy demonstrates strong generalization to unseen object categories\nwithout a performance drop, achieving an 89% success rate on unseen objects in\nsimulation and 50% success rate with zero-shot transfer in the real world.\nCompared to alternative action representations, HACMan achieves a success rate\nmore than three times higher than the best baseline. With zero-shot sim2real\ntransfer, our policy can successfully manipulate unseen objects in the real\nworld for challenging non-planar goals, using dynamic and contact-rich\nnon-prehensile skills. Videos can be found on the project website:\nhttps://hacman-2023.github.io.\n","authors":["Wenxuan Zhou","Bowen Jiang","Fan Yang","Chris Paxton","David Held"],"pdf_url":"https://arxiv.org/pdf/2305.03942v5.pdf","comment":"7th Conference on Robot Learning (CoRL 2023)"},{"id":"http://arxiv.org/abs/2407.10419v1","updated":"2024-07-15T03:48:16Z","published":"2024-07-15T03:48:16Z","title":"Omni-Dimensional Frequency Learner for General Time Series Analysis","summary":"  Frequency domain representation of time series feature offers a concise\nrepresentation for handling real-world time series data with inherent\ncomplexity and dynamic nature. However, current frequency-based methods with\ncomplex operations still fall short of state-of-the-art time domain methods for\ngeneral time series analysis. In this work, we present Omni-Dimensional\nFrequency Learner (ODFL) model based on a in depth analysis among all the three\naspects of the spectrum feature: channel redundancy property among the\nfrequency dimension, the sparse and un-salient frequency energy distribution\namong the frequency dimension, and the semantic diversity among the variable\ndimension. Technically, our method is composed of a semantic-adaptive global\nfilter with attention to the un-salient frequency bands and partial operation\namong the channel dimension. Empirical results show that ODFL achieves\nconsistent state-of-the-art in five mainstream time series analysis tasks,\nincluding short- and long-term forecasting, imputation, classification, and\nanomaly detection, offering a promising foundation for time series analysis.\n","authors":["Xianing Chen. Hanting Chen","Hailin Hu"],"pdf_url":"https://arxiv.org/pdf/2407.10419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10418v1","updated":"2024-07-15T03:47:16Z","published":"2024-07-15T03:47:16Z","title":"An integrated perspective of robustness in regression through the lens\n  of the bias-variance trade-off","summary":"  This paper presents an integrated perspective on robustness in regression.\nSpecifically, we examine the relationship between traditional outlier-resistant\nrobust estimation and robust optimization, which focuses on parameter\nestimation resistant to imaginary dataset-perturbations. While both are\ncommonly regarded as robust methods, these concepts demonstrate a bias-variance\ntrade-off, indicating that they follow roughly converse strategies.\n","authors":["Akifumi Okuno"],"pdf_url":"https://arxiv.org/pdf/2407.10418v1.pdf","comment":"17pages, 16figures"},{"id":"http://arxiv.org/abs/2407.10417v1","updated":"2024-07-15T03:46:15Z","published":"2024-07-15T03:46:15Z","title":"Proper losses regret at least 1/2-order","summary":"  A fundamental challenge in machine learning is the choice of a loss as it\ncharacterizes our learning task, is minimized in the training phase, and serves\nas an evaluation criterion for estimators. Proper losses are commonly chosen,\nensuring minimizers of the full risk match the true probability vector.\nEstimators induced from a proper loss are widely used to construct forecasters\nfor downstream tasks such as classification and ranking. In this procedure, how\ndoes the forecaster based on the obtained estimator perform well under a given\ndownstream task? This question is substantially relevant to the behavior of the\n$p$-norm between the estimated and true probability vectors when the estimator\nis updated. In the proper loss framework, the suboptimality of the estimated\nprobability vector from the true probability vector is measured by a surrogate\nregret. First, we analyze a surrogate regret and show that the strict\nproperness of a loss is necessary and sufficient to establish a non-vacuous\nsurrogate regret bound. Second, we solve an important open question that the\norder of convergence in p-norm cannot be faster than the $1/2$-order of\nsurrogate regrets for a broad class of strictly proper losses. This implies\nthat strongly proper losses entail the optimal convergence rate.\n","authors":["Han Bao","Asuka Takatsu"],"pdf_url":"https://arxiv.org/pdf/2407.10417v1.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2407.10414v1","updated":"2024-07-15T03:31:42Z","published":"2024-07-15T03:31:42Z","title":"Teaching CORnet Human fMRI Representations for Enhanced Model-Brain\n  Alignment","summary":"  Deep convolutional neural networks (DCNNs) have demonstrated excellent\nperformance in object recognition and have been found to share some\nsimilarities with brain visual processing. However, the substantial gap between\nDCNNs and human visual perception still exists. Functional magnetic resonance\nimaging (fMRI) as a widely used technique in cognitive neuroscience can record\nneural activation in the human visual cortex during the process of visual\nperception. Can we teach DCNNs human fMRI signals to achieve a more brain-like\nmodel? To answer this question, this study proposed ReAlnet-fMRI, a model based\non the SOTA vision model CORnet but optimized using human fMRI data through a\nmulti-layer encoding-based alignment framework. This framework has been shown\nto effectively enable the model to learn human brain representations. The\nfMRI-optimized ReAlnet-fMRI exhibited higher similarity to the human brain than\nboth CORnet and the control model in within-and across-subject as well as\nwithin- and across-modality model-brain (fMRI and EEG) alignment evaluations.\nAdditionally, we conducted an in-depth analyses to investigate how the internal\nrepresentations of ReAlnet-fMRI differ from CORnet in encoding various object\ndimensions. These findings provide the possibility of enhancing the\nbrain-likeness of visual models by integrating human neural data, helping to\nbridge the gap between computer vision and visual neuroscience.\n","authors":["Zitong Lu","Yile Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17103v2","updated":"2024-07-15T03:22:05Z","published":"2024-06-24T19:42:22Z","title":"Maximum Likelihood Estimation of the Direction of Sound In A Reverberant\n  Noisy Environment","summary":"  We describe a new method for estimating the direction of sound in a\nreverberant environment from basic principles of sound propagation. The method\nutilizes SNR-adaptive features from time-delay and energy of the directional\ncomponents after acoustic wave decomposition of the observed sound field to\nestimate the line-of-sight direction under noisy and reverberant conditions.\nThe effectiveness of the approach is established with measured data of\ndifferent microphone array configurations under various usage scenarios.\n","authors":["Mohamed F. Mansour"],"pdf_url":"https://arxiv.org/pdf/2406.17103v2.pdf","comment":"5 pages, 2 figures, conference"},{"id":"http://arxiv.org/abs/2402.17012v4","updated":"2024-07-15T02:37:09Z","published":"2024-02-26T20:41:50Z","title":"Pandora's White-Box: Precise Training Data Detection and Extraction in\n  Large Language Models","summary":"  In this paper we develop state-of-the-art privacy attacks against Large\nLanguage Models (LLMs), where an adversary with some access to the model tries\nto learn something about the underlying training data. Our headline results are\nnew membership inference attacks (MIAs) against pretrained LLMs that perform\nhundreds of times better than baseline attacks, and a pipeline showing that\nover 50% (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM\nin natural settings. We consider varying degrees of access to the underlying\nmodel, pretraining and fine-tuning data, and both MIAs and training data\nextraction. For pretraining data, we propose two new MIAs: a supervised neural\nnetwork classifier that predicts training data membership on the basis of\n(dimensionality-reduced) model gradients, as well as a variant of this attack\nthat only requires logit access to the model by leveraging recent\nmodel-stealing work on LLMs. To our knowledge this is the first MIA that\nexplicitly incorporates model-stealing information. Both attacks outperform\nexisting black-box baselines, and our supervised attack closes the gap between\nMIA attack success against LLMs and the strongest known attacks for other\nmachine learning models. In fine-tuning, we find that a simple attack based on\nthe ratio of the loss between the base and fine-tuned models is able to achieve\nnear-perfect MIA performance; we then leverage our MIA to extract a large\nfraction of the fine-tuning dataset from fine-tuned Pythia and Llama models.\nOur code is available at github.com/safr-ai-lab/pandora-llm.\n","authors":["Jeffrey G. Wang","Jason Wang","Marvin Li","Seth Neel"],"pdf_url":"https://arxiv.org/pdf/2402.17012v4.pdf","comment":"Found software bug in experiments, withdrawing in order to address\n  and update results"},{"id":"http://arxiv.org/abs/2407.08047v2","updated":"2024-07-15T02:31:15Z","published":"2024-07-10T20:58:53Z","title":"Spatial-Temporal Attention Model for Traffic State Estimation with\n  Sparse Internet of Vehicles","summary":"  The growing number of connected vehicles offers an opportunity to leverage\ninternet of vehicles (IoV) data for traffic state estimation (TSE) which plays\na crucial role in intelligent transportation systems (ITS). By utilizing only a\nportion of IoV data instead of the entire dataset, the significant overheads\nassociated with collecting and processing large amounts of data can be avoided.\nIn this paper, we introduce a novel framework that utilizes sparse IoV data to\nachieve cost-effective TSE. Particularly, we propose a novel spatial-temporal\nattention model called the convolutional retentive network (CRNet) to improve\nthe TSE accuracy by mining spatial-temporal traffic state correlations. The\nmodel employs the convolutional neural network (CNN) for spatial correlation\naggregation and the retentive network (RetNet) based on the attention mechanism\nto extract temporal correlations. Extensive simulations on a real-world IoV\ndataset validate the advantage of the proposed TSE approach in achieving\naccurate TSE using sparse IoV data, demonstrating its cost effectiveness and\npracticality for real-world applications.\n","authors":["Jianzhe Xue","Dongcheng Yuan","Yu Sun","Tianqi Zhang","Wenchao Xu","Haibo Zhou"," Xuemin"," Shen"],"pdf_url":"https://arxiv.org/pdf/2407.08047v2.pdf","comment":"need further improvement"},{"id":"http://arxiv.org/abs/2407.10385v1","updated":"2024-07-15T01:33:54Z","published":"2024-07-15T01:33:54Z","title":"By My Eyes: Grounding Multimodal Large Language Models with Sensor Data\n  via Visual Prompting","summary":"  Large language models (LLMs) have demonstrated exceptional abilities across\nvarious domains. However, utilizing LLMs for ubiquitous sensing applications\nremains challenging as existing text-prompt methods show significant\nperformance degradation when handling long sensor data sequences. We propose a\nvisual prompting approach for sensor data using multimodal LLMs (MLLMs). We\ndesign a visual prompt that directs MLLMs to utilize visualized sensor data\nalongside the target sensory task descriptions. Additionally, we introduce a\nvisualization generator that automates the creation of optimal visualizations\ntailored to a given sensory task, eliminating the need for prior task-specific\nknowledge. We evaluated our approach on nine sensory tasks involving four\nsensing modalities, achieving an average of 10% higher accuracy than text-based\nprompts and reducing token costs by 15.8x. Our findings highlight the\neffectiveness and cost-efficiency of visual prompts with MLLMs for various\nsensory tasks.\n","authors":["Hyungjun Yoon","Biniyam Aschalew Tolera","Taesik Gong","Kimin Lee","Sung-Ju Lee"],"pdf_url":"https://arxiv.org/pdf/2407.10385v1.pdf","comment":"21 pages, 16 figures"},{"id":"http://arxiv.org/abs/2403.12326v2","updated":"2024-07-15T01:32:38Z","published":"2024-03-18T23:42:04Z","title":"Removing Undesirable Concepts in Text-to-Image Diffusion Models with\n  Learnable Prompts","summary":"  Diffusion models have shown remarkable capability in generating visually\nimpressive content from textual descriptions. However, these models are trained\non vast internet data, much of which contains undesirable elements such as\nsensitive content, copyrighted material, and unethical or harmful concepts.\nTherefore, beyond generating high-quality content, it is crucial to ensure\nthese models do not propagate these undesirable elements. To address this\nissue, we propose a novel method to remove undesirable concepts from\ntext-to-image diffusion models by incorporating a learnable prompt into the\ncross-attention module. This learnable prompt acts as additional memory,\ncapturing the knowledge of undesirable concepts and reducing their dependency\non the model parameters and corresponding textual inputs. By transferring this\nknowledge to the prompt, erasing undesirable concepts becomes more stable and\nhas minimal negative impact on other concepts. We demonstrate the effectiveness\nof our method on the Stable Diffusion model, showcasing its superiority over\nstate-of-the-art erasure methods in removing undesirable content while\npreserving unrelated elements.\n","authors":["Anh Bui","Khanh Doan","Trung Le","Paul Montague","Tamas Abraham","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2403.12326v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.10736v1","updated":"2024-07-15T14:01:35Z","published":"2024-07-15T14:01:35Z","title":"When Synthetic Traces Hide Real Content: Analysis of Stable Diffusion\n  Image Laundering","summary":"  In recent years, methods for producing highly realistic synthetic images have\nsignificantly advanced, allowing the creation of high-quality images from text\nprompts that describe the desired content. Even more impressively, Stable\nDiffusion (SD) models now provide users with the option of creating synthetic\nimages in an image-to-image translation fashion, modifying images in the latent\nspace of advanced autoencoders. This striking evolution, however, brings an\nalarming consequence: it is possible to pass an image through SD autoencoders\nto reproduce a synthetic copy of the image with high realism and almost no\nvisual artifacts. This process, known as SD image laundering, can transform\nreal images into lookalike synthetic ones and risks complicating forensic\nanalysis for content authenticity verification. Our paper investigates the\nforensic implications of image laundering, revealing a serious potential to\nobscure traces of real content, including sensitive and harmful materials that\ncould be mistakenly classified as synthetic, thereby undermining the protection\nof individuals depicted. To address this issue, we propose a two-stage\ndetection pipeline that effectively differentiates between pristine, laundered,\nand fully synthetic images (those generated from text prompts), showing\nrobustness across various conditions. Finally, we highlight another alarming\nproperty of image laundering, which appears to mask the unique artifacts\nexploited by forensic detectors to solve the camera model identification task,\nstrongly undermining their performance. Our experimental code is available at\nhttps://github.com/polimi-ispl/synthetic-image-detection.\n","authors":["Sara Mandelli","Paolo Bestagini","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2407.10736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08619v3","updated":"2024-07-15T08:53:55Z","published":"2024-05-14T13:59:24Z","title":"ALMol: Aligned Language-Molecule Translation LLMs through Offline\n  Preference Contrastive Optimisation","summary":"  The field of chemistry and Artificial Intelligence (AI) intersection is an\narea of active research that aims to accelerate scientific discovery. The\nintegration of large language models (LLMs) with scientific modalities has\nshown significant promise in this endeavour. However, challenges persist in\neffectively addressing training efficacy and the out-of-distribution problem,\nparticularly as existing approaches rely on larger models and datasets. In this\ncontext, we focus on machine language-molecule translation and deploy a novel\ntraining approach called contrastive preference optimisation, which avoids\ngenerating translations that are merely adequate but not perfect. To ensure\ngeneralisability and mitigate memorisation effects, we conduct experiments\nusing only 10% of the data. Our results demonstrate that our models achieve up\nto a 32% improvement compared to counterpart models. Finally, we introduce a\nfine-grained, domain-agnostic evaluation method to assess hallucination in LLMs\nand promote responsible use.\n","authors":["Dimitris Gkoumas"],"pdf_url":"https://arxiv.org/pdf/2405.08619v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10474v1","updated":"2024-07-15T07:01:05Z","published":"2024-07-15T07:01:05Z","title":"Multi-source Knowledge Enhanced Graph Attention Networks for Multimodal\n  Fact Verification","summary":"  Multimodal fact verification is an under-explored and emerging field that has\ngained increasing attention in recent years. The goal is to assess the veracity\nof claims that involve multiple modalities by analyzing the retrieved evidence.\nThe main challenge in this area is to effectively fuse features from different\nmodalities to learn meaningful multimodal representations. To this end, we\npropose a novel model named Multi-Source Knowledge-enhanced Graph Attention\nNetwork (MultiKE-GAT). MultiKE-GAT introduces external multimodal knowledge\nfrom different sources and constructs a heterogeneous graph to capture complex\ncross-modal and cross-source interactions. We exploit a Knowledge-aware Graph\nFusion (KGF) module to learn knowledge-enhanced representations for each claim\nand evidence and eliminate inconsistencies and noises introduced by redundant\nentities. Experiments on two public benchmark datasets demonstrate that our\nmodel outperforms other comparison methods, showing the effectiveness and\nsuperiority of the proposed model.\n","authors":["Han Cao","Lingwei Wei","Wei Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2407.10474v1.pdf","comment":"Accepted by ICME 2024"},{"id":"http://arxiv.org/abs/2407.10462v1","updated":"2024-07-15T06:33:25Z","published":"2024-07-15T06:33:25Z","title":"BandControlNet: Parallel Transformers-based Steerable Popular Music\n  Generation with Fine-Grained Spatiotemporal Features","summary":"  Controllable music generation promotes the interaction between humans and\ncomposition systems by projecting the users' intent on their desired music. The\nchallenge of introducing controllability is an increasingly important issue in\nthe symbolic music generation field. When building controllable generative\npopular multi-instrument music systems, two main challenges typically present\nthemselves, namely weak controllability and poor music quality. To address\nthese issues, we first propose spatiotemporal features as powerful and\nfine-grained controls to enhance the controllability of the generative model.\nIn addition, an efficient music representation called REMI_Track is designed to\nconvert multitrack music into multiple parallel music sequences and shorten the\nsequence length of each track with Byte Pair Encoding (BPE) techniques.\nSubsequently, we release BandControlNet, a conditional model based on parallel\nTransformers, to tackle the multiple music sequences and generate high-quality\nmusic samples that are conditioned to the given spatiotemporal control\nfeatures. More concretely, the two specially designed modules of\nBandControlNet, namely structure-enhanced self-attention (SE-SA) and\nCross-Track Transformer (CTT), are utilized to strengthen the resulting musical\nstructure and inter-track harmony modeling respectively. Experimental results\ntested on two popular music datasets of different lengths demonstrate that the\nproposed BandControlNet outperforms other conditional music generation models\non most objective metrics in terms of fidelity and inference speed and shows\ngreat robustness in generating long music samples. The subjective evaluations\nshow BandControlNet trained on short datasets can generate music with\ncomparable quality to state-of-the-art models, while outperforming them\nsignificantly using longer datasets.\n","authors":["Jing Luo","Xinyu Yang","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2407.10462v1.pdf","comment":"Demo page: https://chinglohsiu.github.io/files/bandcontrolnet.html"}]},"2024-07-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2404.13874v3","updated":"2024-07-14T23:11:05Z","published":"2024-04-22T04:49:22Z","title":"VALOR-EVAL: Holistic Coverage and Faithfulness Evaluation of Large\n  Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) suffer from hallucination issues,\nwherein the models generate plausible-sounding but factually incorrect outputs,\nundermining their reliability. A comprehensive quantitative evaluation is\nnecessary to identify and understand the extent of hallucinations in these\nmodels. However, existing benchmarks are often limited in scope, focusing\nmainly on object hallucinations. Furthermore, current evaluation methods\nstruggle to effectively address the subtle semantic distinctions between model\noutputs and reference data, as well as the balance between hallucination and\ninformativeness. To address these issues, we introduce a multi-dimensional\nbenchmark covering objects, attributes, and relations, with challenging images\nselected based on associative biases. Moreover, we propose a large language\nmodel (LLM)-based two-stage evaluation framework that generalizes the popular\nCHAIR metric and incorporates both faithfulness and coverage into the\nevaluation. Experiments on 10 established LVLMs demonstrate that our evaluation\nmetric is more comprehensive and better correlated with humans than existing\nwork when evaluating on our challenging human-annotated benchmark dataset. Our\nwork also highlights the critical balance between faithfulness and coverage of\nmodel outputs, and encourages future works to address hallucinations in LVLMs\nwhile keeping their outputs informative.\n","authors":["Haoyi Qiu","Wenbo Hu","Zi-Yi Dou","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2404.13874v3.pdf","comment":"ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2407.03651v2","updated":"2024-07-14T22:47:13Z","published":"2024-07-04T05:46:20Z","title":"Evaluating Language Model Context Windows: A \"Working Memory\" Test and\n  Inference-time Correction","summary":"  Large language models are prominently used in real-world applications, often\ntasked with reasoning over large volumes of documents. An exciting development\nin this space is models boasting extended context capabilities, with some\naccommodating over 2 million tokens. Such long context model capabilities\nremain uncertain in production systems, motivating the need to benchmark their\nperformance on real world use cases. We address this challenge by proposing\nSWiM, an evaluation framework that addresses the limitations of standard tests.\nTesting the framework on eight long context models, we find that even strong\nmodels such as GPT-4 and Claude 3 Opus degrade in performance when information\nis present in the middle of the context window (lost-in-the-middle effect).\nNext, in addition to our benchmark, we propose medoid voting, a simple, but\neffective training-free approach that helps alleviate this effect, by\ngenerating responses a few times, each time randomly permuting documents in the\ncontext, and selecting the medoid answer. We evaluate medoid voting on single\ndocument QA tasks, achieving up to a 24% lift in accuracy. Our code is\navailable at https://github.com/snorkel-ai/long-context-eval.\n","authors":["Amanda Dsouza","Christopher Glaze","Changho Shin","Frederic Sala"],"pdf_url":"https://arxiv.org/pdf/2407.03651v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00870v2","updated":"2024-07-14T22:35:34Z","published":"2024-07-01T00:43:02Z","title":"Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients\n  via Eliciting and Adhering to Principles","summary":"  Recent works leverage LLMs to roleplay realistic social scenarios, aiding\nnovices in practicing their social skills. However, simulating sensitive\ninteractions, such as in mental health, is challenging. Privacy concerns\nrestrict data access, and collecting expert feedback, although vital, is\nlaborious. To address this, we develop Roleplay-doh, a novel human-LLM\ncollaboration pipeline that elicits qualitative feedback from a domain-expert,\nwhich is transformed into a set of principles, or natural language rules, that\ngovern an LLM-prompted roleplay. We apply this pipeline to enable senior mental\nhealth supporters to create customized AI patients for simulated practice\npartners for novice counselors. After uncovering issues in GPT-4 simulations\nnot adhering to expert-defined principles, we also introduce a novel\nprinciple-adherence prompting pipeline which shows 30% improvements in response\nquality and principle following for the downstream task. Via a user study with\n25 counseling experts, we demonstrate that the pipeline makes it easy and\neffective to create AI patients that more faithfully resemble real patients, as\njudged by creators and third-party counselors. See our project website at\nhttps://roleplay-doh.github.io/ for code and data.\n","authors":["Ryan Louie","Ananjan Nandi","William Fang","Cheng Chang","Emma Brunskill","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2407.00870v2.pdf","comment":"34 pages, 24 figures, 11 Tables"},{"id":"http://arxiv.org/abs/2407.10351v1","updated":"2024-07-14T22:31:07Z","published":"2024-07-14T22:31:07Z","title":"Comparing Complex Concepts with Transformers: Matching Patent Claims\n  Against Natural Language Text","summary":"  A key capability in managing patent applications or a patent portfolio is\ncomparing claims to other text, e.g. a patent specification. Because the\nlanguage of claims is different from language used elsewhere in the patent\napplication or in non-patent text, this has been challenging for computer based\nnatural language processing. We test two new LLM-based approaches and find that\nboth provide substantially better performance than previously published values.\nThe ability to match dense information from one domain against much more\ndistributed information expressed in a different vocabulary may also be useful\nbeyond the intellectual property space.\n","authors":["Matthias Blume","Ghobad Heidari","Christoph Hewel"],"pdf_url":"https://arxiv.org/pdf/2407.10351v1.pdf","comment":"5th Workshop on Patent Text Mining and Semantic Technologies\n  (PatentSemTech 2024) at ACM SIGIR"},{"id":"http://arxiv.org/abs/2407.10347v1","updated":"2024-07-14T22:23:07Z","published":"2024-07-14T22:23:07Z","title":"MambaForGCN: Enhancing Long-Range Dependency with State Space Model and\n  Kolmogorov-Arnold Networks for Aspect-Based Sentiment Analysis","summary":"  Aspect-based sentiment Analysis (ABSA) identifies and evaluates sentiments\ntoward specific aspects of entities within text, providing detailed insights\nbeyond overall sentiment. However, Attention mechanisms and neural network\nmodels struggle with syntactic constraints, and the quadratic complexity of\nattention mechanisms hinders their adoption for capturing long-range\ndependencies between aspect and opinion words in ABSA. This complexity can lead\nto the misinterpretation of irrelevant con-textual words, restricting their\neffectiveness to short-range dependencies. Some studies have investigated\nmerging semantic and syntactic approaches but face challenges in effectively\nintegrating these methods. To address the above problems, we present\nMambaForGCN, a novel approach to enhance short and long-range dependencies\nbetween aspect and opinion words in ABSA. This innovative approach incorporates\nsyntax-based Graph Convolutional Network (SynGCN) and MambaFormer\n(Mamba-Transformer) modules to encode input with dependency relations and\nsemantic information. The Multihead Attention (MHA) and Mamba blocks in the\nMambaFormer module serve as channels to enhance the model with short and\nlong-range dependencies between aspect and opinion words. We also introduce the\nKolmogorov-Arnold Networks (KANs) gated fusion, an adaptively integrated\nfeature representation system combining SynGCN and MambaFormer representations.\nExperimental results on three benchmark datasets demonstrate MambaForGCN's\neffectiveness, outperforming state-of-the-art (SOTA) baseline models.\n","authors":["Adamu Lawan","Juhua Pu","Haruna Yunusa","Aliyu Umar","Muhammad Lawan"],"pdf_url":"https://arxiv.org/pdf/2407.10347v1.pdf","comment":"25 pages, 3 figures and 3 tables"},{"id":"http://arxiv.org/abs/2401.06310v3","updated":"2024-07-14T21:17:05Z","published":"2024-01-12T00:43:57Z","title":"ViSAGe: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image\n  Generation","summary":"  Recent studies have shown that Text-to-Image (T2I) model generations can\nreflect social stereotypes present in the real world. However, existing\napproaches for evaluating stereotypes have a noticeable lack of coverage of\nglobal identity groups and their associated stereotypes. To address this gap,\nwe introduce the ViSAGe (Visual Stereotypes Around the Globe) dataset to enable\nthe evaluation of known nationality-based stereotypes in T2I models, across 135\nnationalities. We enrich an existing textual stereotype resource by\ndistinguishing between stereotypical associations that are more likely to have\nvisual depictions, such as `sombrero', from those that are less visually\nconcrete, such as 'attractive'. We demonstrate ViSAGe's utility through a\nmulti-faceted evaluation of T2I generations. First, we show that stereotypical\nattributes in ViSAGe are thrice as likely to be present in generated images of\ncorresponding identities as compared to other attributes, and that the\noffensiveness of these depictions is especially higher for identities from\nAfrica, South America, and South East Asia. Second, we assess the stereotypical\npull of visual depictions of identity groups, which reveals how the 'default'\nrepresentations of all identity groups in ViSAGe have a pull towards\nstereotypical depictions, and that this pull is even more prominent for\nidentity groups from the Global South. CONTENT WARNING: Some examples contain\noffensive stereotypes.\n","authors":["Akshita Jha","Vinodkumar Prabhakaran","Remi Denton","Sarah Laszlo","Shachi Dave","Rida Qadri","Chandan K. Reddy","Sunipa Dev"],"pdf_url":"https://arxiv.org/pdf/2401.06310v3.pdf","comment":"Association for Computational Linguistics (ACL) 2024"},{"id":"http://arxiv.org/abs/2403.15498v2","updated":"2024-07-14T20:23:19Z","published":"2024-03-21T18:53:23Z","title":"Emergent World Models and Latent Variable Estimation in Chess-Playing\n  Language Models","summary":"  Language models have shown unprecedented capabilities, sparking debate over\nthe source of their performance. Is it merely the outcome of learning syntactic\npatterns and surface level statistics, or do they extract semantics and a world\nmodel from the text? Prior work by Li et al. investigated this by training a\nGPT model on synthetic, randomly generated Othello games and found that the\nmodel learned an internal representation of the board state. We extend this\nwork into the more complex domain of chess, training on real games and\ninvestigating our model's internal representations using linear probes and\ncontrastive activations. The model is given no a priori knowledge of the game\nand is solely trained on next character prediction, yet we find evidence of\ninternal representations of board state. We validate these internal\nrepresentations by using them to make interventions on the model's activations\nand edit its internal board state. Unlike Li et al's prior synthetic dataset\napproach, our analysis finds that the model also learns to estimate latent\nvariables like player skill to better predict the next character. We derive a\nplayer skill vector and add it to the model, improving the model's win rate by\nup to 2.6 times.\n","authors":["Adam Karvonen"],"pdf_url":"https://arxiv.org/pdf/2403.15498v2.pdf","comment":"Accepted to the 2024 Conference on Language Modeling"},{"id":"http://arxiv.org/abs/2407.06460v2","updated":"2024-07-14T20:14:02Z","published":"2024-07-08T23:47:29Z","title":"MUSE: Machine Unlearning Six-Way Evaluation for Language Models","summary":"  Language models (LMs) are trained on vast amounts of text data, which may\ninclude private and copyrighted content. Data owners may request the removal of\ntheir data from a trained model due to privacy or copyright concerns. However,\nexactly unlearning only these datapoints (i.e., retraining with the data\nremoved) is intractable in modern-day models. This has led to the development\nof many approximate unlearning algorithms. The evaluation of the efficacy of\nthese algorithms has traditionally been narrow in scope, failing to precisely\nquantify the success and practicality of the algorithm from the perspectives of\nboth the model deployers and the data owners. We address this issue by\nproposing MUSE, a comprehensive machine unlearning evaluation benchmark that\nenumerates six diverse desirable properties for unlearned models: (1) no\nverbatim memorization, (2) no knowledge memorization, (3) no privacy leakage,\n(4) utility preservation on data not intended for removal, (5) scalability with\nrespect to the size of removal requests, and (6) sustainability over sequential\nunlearning requests. Using these criteria, we benchmark how effectively eight\npopular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter\nbooks and news articles. Our results demonstrate that most algorithms can\nprevent verbatim memorization and knowledge memorization to varying degrees,\nbut only one algorithm does not lead to severe privacy leakage. Furthermore,\nexisting algorithms fail to meet deployer's expectations because they often\ndegrade general model utility and also cannot sustainably accommodate\nsuccessive unlearning requests or large-scale content removal. Our findings\nidentify key issues with the practicality of existing unlearning algorithms on\nlanguage models, and we release our benchmark to facilitate further\nevaluations: muse-bench.github.io\n","authors":["Weijia Shi","Jaechan Lee","Yangsibo Huang","Sadhika Malladi","Jieyu Zhao","Ari Holtzman","Daogao Liu","Luke Zettlemoyer","Noah A. Smith","Chiyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.06460v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10303v1","updated":"2024-07-14T19:32:33Z","published":"2024-07-14T19:32:33Z","title":"Improving Neural Biasing for Contextual Speech Recognition by Early\n  Context Injection and Text Perturbation","summary":"  Existing research suggests that automatic speech recognition (ASR) models can\nbenefit from additional contexts (e.g., contact lists, user specified\nvocabulary). Rare words and named entities can be better recognized with\ncontexts. In this work, we propose two simple yet effective techniques to\nimprove context-aware ASR models. First, we inject contexts into the encoders\nat an early stage instead of merely at their last layers. Second, to enforce\nthe model to leverage the contexts during training, we perturb the reference\ntranscription with alternative spellings so that the model learns to rely on\nthe contexts to make correct predictions. On LibriSpeech, our techniques\ntogether reduce the rare word error rate by 60% and 25% relatively compared to\nno biasing and shallow fusion, making the new state-of-the-art performance. On\nSPGISpeech and a real-world dataset ConEC, our techniques also yield good\nimprovements over the baselines.\n","authors":["Ruizhe Huang","Mahsa Yarmohammadi","Sanjeev Khudanpur","Daniel Povey"],"pdf_url":"https://arxiv.org/pdf/2407.10303v1.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.10301v1","updated":"2024-07-14T19:28:48Z","published":"2024-07-14T19:28:48Z","title":"Does Burrows' Delta really confirm that Rowling and Galbraith are the\n  same author?","summary":"  The stylo package includes a frequency table that can be used to calculate\ndistances between texts and thus independently solve the problem of attribution\nof The Cuckoo's Calling, a novel that J.K. Rowling said she wrote. However, the\nset of texts for this table is very vulnerable to criticism. The authors there\nare not modern, they wrote in a different genre. I set out to test the\nperformance of the method on texts that are more relevant to the research\nquestion.\n","authors":["Boris Orekhov"],"pdf_url":"https://arxiv.org/pdf/2407.10301v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.07188v2","updated":"2024-07-14T18:27:14Z","published":"2024-06-11T12:01:09Z","title":"Merging Improves Self-Critique Against Jailbreak Attacks","summary":"  The robustness of large language models (LLMs) against adversarial\nmanipulations, such as jailbreak attacks, remains a significant challenge. In\nthis work, we propose an approach that enhances the self-critique capability of\nthe LLM and further fine-tunes it over sanitized synthetic data. This is done\nwith the addition of an external critic model that can be merged with the\noriginal, thus bolstering self-critique capabilities and improving the\nrobustness of the LLMs response to adversarial prompts. Our results demonstrate\nthat the combination of merging and self-critique can reduce the attack success\nrate of adversaries significantly, thus offering a promising defense mechanism\nagainst jailbreak attacks. Code, data and models released at\nhttps://github.com/vicgalle/merging-self-critique-jailbreaks .\n","authors":["Victor Gallego"],"pdf_url":"https://arxiv.org/pdf/2406.07188v2.pdf","comment":"Published at ICML 2024 Workshop on Foundation Models in the Wild"},{"id":"http://arxiv.org/abs/2401.10712v4","updated":"2024-07-14T18:18:05Z","published":"2024-01-19T14:22:29Z","title":"Q&A Prompts: Discovering Rich Visual Clues through Mining\n  Question-Answer Prompts for VQA requiring Diverse World Knowledge","summary":"  With the breakthrough of multi-modal large language models, answering complex\nvisual questions that demand advanced reasoning abilities and world knowledge\nhas become a much more important testbed for developing AI models than ever.\nHowever, equipping AI models with robust cross-modality reasoning ability\nremains challenging since the cognition scheme of humans has not been\nunderstood systematically. In this paper, we believe that if we can collect\nvisual clues in the given image as much as possible, we will recognize the\nimage more accurately, understand the question better, recall relevant\nknowledge more easily, and finally reason out the answer. We discover these\nrich visual clues by mining question-answer pairs in images and sending them\ninto multi-modal large language models as prompts. We call the proposed method\nQ&A Prompts. Specifically, we first use the image-answer pairs and the\ncorresponding questions in the training set as inputs and outputs to train a\nvisual question generation model. Then, we use an image tagging model to\nidentify various instances and send packaged image-tag pairs into the visual\nquestion generation model to generate relevant questions with the extracted\nimage tags as answers. Finally, we encode these generated question-answer pairs\nas prompts with a visual-aware prompting module and send them into pre-trained\nmulti-modal large language models to reason out the final answers. Experimental\nresults show that, compared with state-of-the-art methods, our Q&A Prompts\nachieves substantial improvements on the challenging visual question answering\ndatasets requiring reasoning over diverse world knowledge, such as OK-VQA and\nA-OKVQA.\n","authors":["Haibi Wang","Weifeng Ge"],"pdf_url":"https://arxiv.org/pdf/2401.10712v4.pdf","comment":"Accepted by ECCV'24"},{"id":"http://arxiv.org/abs/2402.05119v5","updated":"2024-07-14T18:14:57Z","published":"2024-02-03T04:45:25Z","title":"A Closer Look at the Limitations of Instruction Tuning","summary":"  Instruction Tuning (IT), the process of training large language models (LLMs)\nusing instruction-response pairs, has emerged as the predominant method for\ntransforming base pre-trained LLMs into open-domain conversational agents.\nWhile IT has achieved notable success and widespread adoption, its limitations\nand shortcomings remain underexplored. In this paper, through rigorous\nexperiments and an in-depth analysis of the changes LLMs undergo through IT, we\nreveal various limitations of IT. In particular, we show that (1) IT fails to\nenhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning\nresponse initiation and style tokens, and full-parameter fine-tuning leads to\nknowledge degradation. (2) Copying response patterns from IT datasets derived\nfrom knowledgeable sources leads to a decline in response quality. (3)\nFull-parameter fine-tuning increases hallucination by inaccurately borrowing\ntokens from conceptually similar instances in the IT dataset for generating\nresponses. (4) Popular methods to improve IT do not lead to performance\nimprovements over a simple LoRA fine-tuned model. Our findings reveal that\nresponses generated solely from pre-trained knowledge consistently outperform\nresponses by models that learn any form of new knowledge from IT on open-source\ndatasets. We hope the insights and challenges revealed in this paper inspire\nfuture work in related directions.\n","authors":["Sreyan Ghosh","Chandra Kiran Reddy Evuru","Sonal Kumar","Ramaneswaran S","Deepali Aneja","Zeyu Jin","Ramani Duraiswami","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2402.05119v5.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2407.10275v1","updated":"2024-07-14T17:18:16Z","published":"2024-07-14T17:18:16Z","title":"Cross-Lingual Multi-Hop Knowledge Editing -- Benchmarks, Analysis and a\n  Simple Contrastive Learning based Approach","summary":"  Large language models are often expected to constantly adapt to new sources\nof knowledge and knowledge editing techniques aim to efficiently patch the\noutdated model knowledge, with minimal modification. Most prior works focus on\nmonolingual knowledge editing in English, even though new information can\nemerge in any language from any part of the world. We propose the Cross-Lingual\nMulti-Hop Knowledge Editing paradigm, for measuring and analyzing the\nperformance of various SoTA knowledge editing techniques in a cross-lingual\nsetup. Specifically, we create a parallel cross-lingual benchmark,\nCROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive\nanalysis over various knowledge editing techniques uncover significant gaps in\nperformance between the cross-lingual and English-centric setting. Following\nthis, we propose a significantly improved system for cross-lingual multi-hop\nknowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and\ngenerate knowledge editing framework, where a retriever is formulated to recall\nedited facts and support an LLM to adhere to knowledge edits. We develop\nlanguage-aware and hard-negative based contrastive objectives for improving the\ncross-lingual and fine-grained fact retrieval and verification process used in\nthis framework. Extensive experiments on three LLMs, eight languages, and two\ndatasets show CLEVER-CKE's significant gains of up to 30% over prior methods.\n","authors":["Aditi Khandelwal","Harman Singh","Hengrui Gu","Tianlong Chen","Kaixiong Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.10275v1.pdf","comment":"Paper on Cross-Lingual Multi-Hop Knowledge Editing"},{"id":"http://arxiv.org/abs/2407.10266v1","updated":"2024-07-14T16:20:42Z","published":"2024-07-14T16:20:42Z","title":"psifx -- Psychological and Social Interactions Feature Extraction\n  Package","summary":"  psifx is a plug-and-play multi-modal feature extraction toolkit, aiming to\nfacilitate and democratize the use of state-of-the-art machine learning\ntechniques for human sciences research. It is motivated by a need (a) to\nautomate and standardize data annotation processes, otherwise involving\nexpensive, lengthy, and inconsistent human labor, such as the transcription or\ncoding of behavior changes from audio and video sources; (b) to develop and\ndistribute open-source community-driven psychology research software; and (c)\nto enable large-scale access and ease of use to non-expert users. The framework\ncontains an array of tools for tasks, such as speaker diarization,\nclosed-caption transcription and translation from audio, as well as body, hand,\nand facial pose estimation and gaze tracking from video. The package has been\ndesigned with a modular and task-oriented approach, enabling the community to\nadd or update new tools easily. We strongly hope that this package will provide\npsychologists a simple and practical solution for efficiently a range of audio,\nlinguistic, and visual features from audio and video, thereby creating new\nopportunities for in-depth study of real-time behavioral phenomena.\n","authors":["Guillaume Rochette","Matthew J. Vowels"],"pdf_url":"https://arxiv.org/pdf/2407.10266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10864v3","updated":"2024-07-14T16:20:19Z","published":"2023-07-20T13:33:28Z","title":"Divide & Bind Your Attention for Improved Generative Semantic Nursing","summary":"  Emerging large-scale text-to-image generative models, e.g., Stable Diffusion\n(SD), have exhibited overwhelming results with high fidelity. Despite the\nmagnificent progress, current state-of-the-art models still struggle to\ngenerate images fully adhering to the input prompt. Prior work, Attend &\nExcite, has introduced the concept of Generative Semantic Nursing (GSN), aiming\nto optimize cross-attention during inference time to better incorporate the\nsemantics. It demonstrates promising results in generating simple prompts,\ne.g., \"a cat and a dog\". However, its efficacy declines when dealing with more\ncomplex prompts, and it does not explicitly address the problem of improper\nattribute binding. To address the challenges posed by complex prompts or\nscenarios involving multiple entities and to achieve improved attribute\nbinding, we propose Divide & Bind. We introduce two novel loss objectives for\nGSN: a novel attendance loss and a binding loss. Our approach stands out in its\nability to faithfully synthesize desired objects with improved attribute\nalignment from complex prompts and exhibits superior performance across\nmultiple evaluation benchmarks.\n","authors":["Yumeng Li","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2307.10864v3.pdf","comment":"Accepted at BMVC 2023 as Oral. Code:\n  https://github.com/boschresearch/Divide-and-Bind and project page:\n  https://sites.google.com/view/divide-and-bind"},{"id":"http://arxiv.org/abs/2407.10252v1","updated":"2024-07-14T15:37:28Z","published":"2024-07-14T15:37:28Z","title":"Nullpointer at CheckThat! 2024: Identifying Subjectivity from\n  Multilingual Text Sequence","summary":"  This study addresses a binary classification task to determine whether a text\nsequence, either a sentence or paragraph, is subjective or objective. The task\nspans five languages: Arabic, Bulgarian, English, German, and Italian, along\nwith a multilingual category. Our approach involved several key techniques.\nInitially, we preprocessed the data through parts of speech (POS) tagging,\nidentification of question marks, and application of attention masks. We\nfine-tuned the sentiment-based Transformer model\n'MarieAngeA13/Sentiment-Analysis-BERT' on our dataset. Given the imbalance with\nmore objective data, we implemented a custom classifier that assigned greater\nweight to objective data. Additionally, we translated non-English data into\nEnglish to maintain consistency across the dataset. Our model achieved notable\nresults, scoring top marks for the multilingual dataset (Macro F1=0.7121) and\nGerman (Macro F1=0.7908). It ranked second for Arabic (Macro F1=0.4908) and\nBulgarian (Macro F1=0.7169), third for Italian (Macro F1=0.7430), and ninth for\nEnglish (Macro F1=0.6893).\n","authors":["Md. Rafiul Biswas","Abrar Tasneem Abir","Wajdi Zaghouani"],"pdf_url":"https://arxiv.org/pdf/2407.10252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10245v1","updated":"2024-07-14T15:25:08Z","published":"2024-07-14T15:25:08Z","title":"GenSco: Can Question Decomposition based Passage Alignment improve\n  Question Answering?","summary":"  Retrieval augmented generation (RAG) with large language models (LLMs) for\nQuestion Answering (QA) entails furnishing relevant context within the prompt\nto facilitate the LLM in answer generation. During the generation, inaccuracies\nor hallucinations frequently occur due to two primary factors: inadequate or\ndistracting context in the prompts, and the inability of LLMs to effectively\nreason through the facts. In this paper, we investigate whether providing\naligned context via a carefully selected passage sequence leads to better\nanswer generation by the LLM for multi-hop QA. We introduce, \"GenSco\", a novel\napproach of selecting passages based on the predicted decomposition of the\nmulti-hop questions}. The framework consists of two distinct LLMs: (i)\nGenerator LLM, which is used for question decomposition and final answer\ngeneration; (ii) an auxiliary open-sourced LLM, used as the scorer, to\nsemantically guide the Generator for passage selection. The generator is\ninvoked only once for the answer generation, resulting in a cost-effective and\nefficient approach. We evaluate on three broadly established multi-hop question\nanswering datasets: 2WikiMultiHop, Adversarial HotPotQA and MuSiQue and achieve\nan absolute gain of $15.1$ and $5.9$ points in Exact Match score with respect\nto the best performing baselines over MuSiQue and 2WikiMultiHop respectively.\n","authors":["Barah Fazili","Koustava Goswami","Natwar Modani","Inderjeet Nair"],"pdf_url":"https://arxiv.org/pdf/2407.10245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09530v3","updated":"2024-07-14T15:17:44Z","published":"2023-09-18T07:17:52Z","title":"Adapting Large Language Models via Reading Comprehension","summary":"  We explore how continued pre-training on domain-specific corpora influences\nlarge language models, revealing that training on the raw corpora endows the\nmodel with domain knowledge, but drastically hurts its prompting ability for\nquestion answering. Taken inspiration from human learning via reading\ncomprehension--practice after reading improves the ability to answer questions\nbased on the learned knowledge--we propose a simple method for transforming raw\ncorpora into reading comprehension texts. Each raw text is enriched with a\nseries of tasks related to its content. Our method, highly scalable and\napplicable to any pre-training corpora, consistently enhances performance\nacross various tasks in three different domains: biomedicine, finance, and law.\nNotably, our 7B language model achieves competitive performance with\ndomain-specific models of much larger scales, such as BloombergGPT-50B.\nFurthermore, we demonstrate that domain-specific reading comprehension texts\ncan improve the model's performance even on general benchmarks, showing the\npotential to develop a general model across even more domains. Our model, code,\nand data are available at https://github.com/microsoft/LMOps.\n","authors":["Daixuan Cheng","Shaohan Huang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2309.09530v3.pdf","comment":"ICLR 2024 Conference"},{"id":"http://arxiv.org/abs/2407.10241v1","updated":"2024-07-14T15:17:02Z","published":"2024-07-14T15:17:02Z","title":"BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs","summary":"  Evaluating the bias in Large Language Models (LLMs) becomes increasingly\ncrucial with their rapid development. However, existing evaluation methods rely\non fixed-form outputs and cannot adapt to the flexible open-text generation\nscenarios of LLMs (e.g., sentence completion and question answering). To\naddress this, we introduce BiasAlert, a plug-and-play tool designed to detect\nsocial bias in open-text generations of LLMs. BiasAlert integrates external\nhuman knowledge with inherent reasoning capabilities to detect bias reliably.\nExtensive experiments demonstrate that BiasAlert significantly outperforms\nexisting state-of-the-art methods like GPT4-as-A-Judge in detecting bias.\nFurthermore, through application studies, we demonstrate the utility of\nBiasAlert in reliable LLM bias evaluation and bias mitigation across various\nscenarios. Model and code will be publicly released.\n","authors":["Zhiting Fan","Ruizhe Chen","Ruiling Xu","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2407.10241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14945v2","updated":"2024-07-14T14:01:01Z","published":"2023-12-06T15:24:01Z","title":"Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge\n  Base for Industrial Prognostics and Health Management","summary":"  Prognostics and health management (PHM) is essential for industrial operation\nand maintenance, focusing on predicting, diagnosing, and managing the health\nstatus of industrial systems. The emergence of the ChatGPT-Like large-scale\nlanguage model (LLM) has begun to lead a new round of innovation in the AI\nfield. It has extensively promoted the level of intelligence in various fields.\nTherefore, it is also expected further to change the application paradigm in\nindustrial PHM and promote PHM to become intelligent. Although ChatGPT-Like\nLLMs have rich knowledge reserves and powerful language understanding and\ngeneration capabilities, they lack domain-specific expertise, significantly\nlimiting their practicability in PHM applications. To this end, this study\nexplores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in\nindustrial PHM to solve the above limitations. In addition, we introduce the\nmethod and steps of combining the LKB with LLMs, including LKB preparation, LKB\nvectorization, prompt engineering, etc. Experimental analysis of real cases\nshows that combining the LKB with ChatGPT-Like LLM can significantly improve\nits performance and make ChatGPT-Like LLMs more accurate, relevant, and able to\nprovide more insightful information. This can promote the development of\nChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality.\n","authors":["Huan Wang","Yan-Fu Li","Min Xie"],"pdf_url":"https://arxiv.org/pdf/2312.14945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03629v7","updated":"2024-07-14T12:40:59Z","published":"2022-02-08T03:55:01Z","title":"Survey of Hallucination in Natural Language Generation","summary":"  Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the development of sequence-to-sequence deep learning technologies\nsuch as Transformer-based language models. This advancement has led to more\nfluent and coherent NLG, leading to improved development in downstream tasks\nsuch as abstractive summarization, dialogue generation and data-to-text\ngeneration. However, it is also apparent that deep learning based generation is\nprone to hallucinate unintended text, which degrades the system performance and\nfails to meet user expectations in many real-world scenarios. To address this\nissue, many studies have been presented in measuring and mitigating\nhallucinated texts, but these have never been reviewed in a comprehensive\nmanner before. In this survey, we thus provide a broad overview of the research\nprogress and challenges in the hallucination problem in NLG. The survey is\norganized into two parts: (1) a general overview of metrics, mitigation\nmethods, and future directions; (2) an overview of task-specific research\nprogress on hallucinations in the following downstream tasks, namely\nabstractive summarization, dialogue generation, generative question answering,\ndata-to-text generation, machine translation, and visual-language generation;\nand (3) hallucinations in large language models (LLMs). This survey serves to\nfacilitate collaborative efforts among researchers in tackling the challenge of\nhallucinated texts in NLG.\n","authors":["Ziwei Ji","Nayeon Lee","Rita Frieske","Tiezheng Yu","Dan Su","Yan Xu","Etsuko Ishii","Yejin Bang","Delong Chen","Wenliang Dai","Ho Shu Chan","Andrea Madotto","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2202.03629v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10167v1","updated":"2024-07-14T11:41:03Z","published":"2024-07-14T11:41:03Z","title":"Key-Point-Driven Mathematical Reasoning Distillation of Large Language\n  Model","summary":"  Large Language Models (LLMs) have demonstrated exceptional proficiency in\nmathematical reasoning tasks due to their extensive parameter counts and\ntraining on vast datasets. Despite these capabilities, deploying LLMs is\nhindered by their computational demands. Distilling LLM mathematical reasoning\ninto Smaller Language Models (SLMs) has emerged as a solution to this\nchallenge, although these smaller models often suffer from errors in\ncalculation and semantic understanding. Prior work has proposed\nProgram-of-Thought Distillation (PoTD) to avoid calculation error. To further\naddress semantic understanding errors, we propose Key-Point-Driven Mathematical\nReasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs\nby breaking down the problem-solving process into three stages: Core Question\nExtraction, Problem-Solving Information Extraction, and Step-by-Step Solution.\nThis method is further divided into KPDD-CoT, which generates Chain-of-Thought\nrationales, and KPDD-PoT, which creates Program-of-Thought rationales. The\nexperiment results show that KPDD-CoT significantly improves reasoning\nabilities, while KPDD-PoT achieves state-of-the-art performance in mathematical\nreasoning tasks. Our approach effectively mitigates misunderstanding errors,\nadvancing the deployment of efficient and capable SLMs.\n","authors":["Xunyu Zhu","Jian Li","Yong Liu","Can Ma","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10153v1","updated":"2024-07-14T10:47:44Z","published":"2024-07-14T10:47:44Z","title":"Look Within, Why LLMs Hallucinate: A Causal Perspective","summary":"  The emergence of large language models (LLMs) is a milestone in generative\nartificial intelligence, achieving significant success in text comprehension\nand generation tasks. Despite the tremendous success of LLMs in many downstream\ntasks, they suffer from severe hallucination problems, posing significant\nchallenges to the practical applications of LLMs. Most of the works about LLMs'\nhallucinations focus on data quality. Self-attention is a core module in\ntransformer-based LLMs, while its potential relationship with LLMs'\nhallucination has been hardly investigated. To fill this gap, we study this\nproblem from a causal perspective. We propose a method to intervene in LLMs'\nself-attention layers and maintain their structures and sizes intact.\nSpecifically, we disable different self-attention layers in several popular\nopen-source LLMs and then compare their degrees of hallucination with the\noriginal ones. We evaluate the intervened LLMs on hallucination assessment\nbenchmarks and conclude that disabling some specific self-attention layers in\nthe front or tail of the LLMs can alleviate hallucination issues. The study\npaves a new way for understanding and mitigating LLMs' hallucinations.\n","authors":["He Li","Haoang Chi","Mingyu Liu","Wenjing Yang"],"pdf_url":"https://arxiv.org/pdf/2407.10153v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.10152v1","updated":"2024-07-14T10:47:03Z","published":"2024-07-14T10:47:03Z","title":"Mitigating Translationese in Low-resource Languages: The Storyboard\n  Approach","summary":"  Low-resource languages often face challenges in acquiring high-quality\nlanguage data due to the reliance on translation-based methods, which can\nintroduce the translationese effect. This phenomenon results in translated\nsentences that lack fluency and naturalness in the target language. In this\npaper, we propose a novel approach for data collection by leveraging\nstoryboards to elicit more fluent and natural sentences. Our method involves\npresenting native speakers with visual stimuli in the form of storyboards and\ncollecting their descriptions without direct exposure to the source text. We\nconducted a comprehensive evaluation comparing our storyboard-based approach\nwith traditional text translation-based methods in terms of accuracy and\nfluency. Human annotators and quantitative metrics were used to assess\ntranslation quality. The results indicate a preference for text translation in\nterms of accuracy, while our method demonstrates worse accuracy but better\nfluency in the language focused.\n","authors":["Garry Kuwanto","Eno-Abasi E. Urua","Priscilla Amondi Amuok","Shamsuddeen Hassan Muhammad","Anuoluwapo Aremu","Verrah Otiende","Loice Emma Nanyanga","Teresiah W. Nyoike","Aniefon D. Akpan","Nsima Ab Udouboh","Idongesit Udeme Archibong","Idara Effiong Moses","Ifeoluwatayo A. Ige","Benjamin Ajibade","Olumide Benjamin Awokoya","Idris Abdulmumin","Saminu Mohammad Aliyu","Ruqayya Nasir Iro","Ibrahim Said Ahmad","Deontae Smith","Praise-EL Michaels","David Ifeoluwa Adelani","Derry Tanti Wijaya","Anietie Andy"],"pdf_url":"https://arxiv.org/pdf/2407.10152v1.pdf","comment":"published at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2407.03236v3","updated":"2024-07-14T10:01:40Z","published":"2024-07-03T16:05:20Z","title":"CATT: Character-based Arabic Tashkeel Transformer","summary":"  Tashkeel, or Arabic Text Diacritization (ATD), greatly enhances the\ncomprehension of Arabic text by removing ambiguity and minimizing the risk of\nmisinterpretations caused by its absence. It plays a crucial role in improving\nArabic text processing, particularly in applications such as text-to-speech and\nmachine translation. This paper introduces a new approach to training ATD\nmodels. First, we finetuned two transformers, encoder-only and encoder-decoder,\nthat were initialized from a pretrained character-based BERT. Then, we applied\nthe Noisy-Student approach to boost the performance of the best model. We\nevaluated our models alongside 11 commercial and open-source models using two\nmanually labeled benchmark datasets: WikiNews and our CATT dataset. Our\nfindings show that our top model surpasses all evaluated models by relative\nDiacritic Error Rates (DERs) of 30.83\\% and 35.21\\% on WikiNews and CATT,\nrespectively, achieving state-of-the-art in ATD. In addition, we show that our\nmodel outperforms GPT-4-turbo on CATT dataset by a relative DER of 9.36\\%. We\nopen-source our CATT models and benchmark dataset for the research\ncommunity\\footnote{https://github.com/abjadai/catt}.\n","authors":["Faris Alasmary","Orjuwan Zaafarani","Ahmad Ghannam"],"pdf_url":"https://arxiv.org/pdf/2407.03236v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10118v1","updated":"2024-07-14T08:38:14Z","published":"2024-07-14T08:38:14Z","title":"Textless Dependency Parsing by Labeled Sequence Prediction","summary":"  Traditional spoken language processing involves cascading an automatic speech\nrecognition (ASR) system into text processing models. In contrast, \"textless\"\nmethods process speech representations without ASR systems, enabling the direct\nuse of acoustic speech features. Although their effectiveness is shown in\ncapturing acoustic features, it is unclear in capturing lexical knowledge. This\npaper proposes a textless method for dependency parsing, examining its\neffectiveness and limitations. Our proposed method predicts a dependency tree\nfrom a speech signal without transcribing, representing the tree as a labeled\nsequence. scading method outperforms the textless method in overall parsing\naccuracy, the latter excels in instances with important acoustic features. Our\nfindings highlight the importance of fusing word-level representations and\nsentence-level prosody for enhanced parsing performance. The code and models\nare made publicly available: https://github.com/mynlp/SpeechParser.\n","authors":["Shunsuke Kando","Yusuke Miyao","Jason Naradowsky","Shinnosuke Takamichi"],"pdf_url":"https://arxiv.org/pdf/2407.10118v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2306.11400v2","updated":"2024-07-14T08:08:13Z","published":"2023-06-20T09:15:52Z","title":"MuDPT: Multi-modal Deep-symphysis Prompt Tuning for Large Pre-trained\n  Vision-Language Models","summary":"  Prompt tuning, like CoOp, has recently shown promising vision recognizing and\ntransfer learning ability on various downstream tasks with the emergence of\nlarge pre-trained vision-language models like CLIP. However, we identify that\nexisting uni-modal prompt tuning approaches may result in sub-optimal\nperformance since this uni-modal design breaks the original alignment of\ntextual and visual representations in the pre-trained model. Inspired by the\nnature of pre-trained vision-language models, we aim to achieve completeness in\nprompt tuning and propose a novel approach called Multi-modal Deep-symphysis\nPrompt Tuning, dubbed as MuDPT, which extends independent multi-modal prompt\ntuning by additionally learning a model-agnostic transformative network to\nallow deep hierarchical bi-directional prompt fusion. We evaluate the\neffectiveness of MuDPT on few-shot vision recognition and out-of-domain\ngeneralization tasks. Compared with the state-of-the-art methods, MuDPT\nachieves better recognition and generalization ability with an apparent margin\nthanks to synergistic alignment of textual and visual representations. Our code\nis available at: https://github.com/Mechrev0/MuDPT.\n","authors":["Yongzhu Miao","Shasha Li","Jintao Tang","Ting Wang"],"pdf_url":"https://arxiv.org/pdf/2306.11400v2.pdf","comment":"The paper has been accepted by ICME 2023"},{"id":"http://arxiv.org/abs/2407.10114v1","updated":"2024-07-14T08:07:50Z","published":"2024-07-14T08:07:50Z","title":"TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley\n  Value Estimation","summary":"  As large language models (LLMs) become increasingly prevalent in critical\napplications, the need for interpretable AI has grown. We introduce TokenSHAP,\na novel method for interpreting LLMs by attributing importance to individual\ntokens or substrings within input prompts. This approach adapts Shapley values\nfrom cooperative game theory to natural language processing, offering a\nrigorous framework for understanding how different parts of an input contribute\nto a model's response. TokenSHAP leverages Monte Carlo sampling for\ncomputational efficiency, providing interpretable, quantitative measures of\ntoken importance. We demonstrate its efficacy across diverse prompts and LLM\narchitectures, showing consistent improvements over existing baselines in\nalignment with human judgments, faithfulness to model behavior, and\nconsistency.\n  Our method's ability to capture nuanced interactions between tokens provides\nvaluable insights into LLM behavior, enhancing model transparency, improving\nprompt engineering, and aiding in the development of more reliable AI systems.\nTokenSHAP represents a significant step towards the necessary interpretability\nfor responsible AI deployment, contributing to the broader goal of creating\nmore transparent, accountable, and trustworthy AI systems.\n","authors":["Roni Goldshmidt","Miriam Horovicz"],"pdf_url":"https://arxiv.org/pdf/2407.10114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20628v2","updated":"2024-07-14T07:09:42Z","published":"2024-05-31T05:40:56Z","title":"ToxVidLM: A Multimodal Framework for Toxicity Detection in Code-Mixed\n  Videos","summary":"  In an era of rapidly evolving internet technology, the surge in multimodal\ncontent, including videos, has expanded the horizons of online communication.\nHowever, the detection of toxic content in this diverse landscape, particularly\nin low-resource code-mixed languages, remains a critical challenge. While\nsubstantial research has addressed toxic content detection in textual data, the\nrealm of video content, especially in non-English languages, has been\nrelatively underexplored. This paper addresses this research gap by introducing\na benchmark dataset, the first of its kind, consisting of 931 videos with 4021\ncode-mixed Hindi-English utterances collected from YouTube. Each utterance\nwithin this dataset has been meticulously annotated for toxicity, severity, and\nsentiment labels. We have developed an advanced Multimodal Multitask framework\nbuilt for Toxicity detection in Video Content by leveraging Language Models\n(LMs), crafted for the primary objective along with the additional tasks of\nconducting sentiment and severity analysis. ToxVidLM incorporates three key\nmodules - the Encoder module, Cross-Modal Synchronization module, and Multitask\nmodule - crafting a generic multimodal LM customized for intricate video\nclassification tasks. Our experiments reveal that incorporating multiple\nmodalities from the videos substantially enhances the performance of toxic\ncontent detection by achieving an Accuracy and Weighted F1 score of 94.29% and\n94.35%, respectively.\n","authors":["Krishanu Maity","A. S. Poornash","Sriparna Saha","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2405.20628v2.pdf","comment":"Accepted as a Long Paper in ACL Findings 2024. For acceptance\n  details, see https://2024.aclweb.org/program/finding_papers/"},{"id":"http://arxiv.org/abs/2210.12777v3","updated":"2024-07-14T07:02:20Z","published":"2022-10-23T16:34:39Z","title":"Retrieval-Augmented Generation and Knowledge-Grounded Reasoning for\n  Faithful Patient Discharge Instructions","summary":"  Language models (LMs), such as ChatGPT, have the potential to assist\nclinicians in generating various clinical notes. However, LMs are prone to\nproduce ``hallucinations'', i.e., generated content that is not aligned with\nfacts and knowledge. In this paper, we propose the Re$^3$Writer method with\nretrieval-augmented generation and knowledge-grounded reasoning to enable LMs\nto generate faithful clinical texts. We demonstrate the effectiveness of our\nmethod in generating patient discharge instructions. It requires the LMs to\nunderstand the patients' long clinical documents, i.e., the health records\nduring hospitalization, to generate critical instructional information provided\nboth to carers and to the patient at the time of discharge. The proposed\nRe$^3$Writer imitates the working patterns of physicians to first retrieve\nrelated working experience from historical instructions written by physicians,\nthen reason related medical knowledge. Finally, it refines the retrieved\nworking experience and reasoned medical knowledge to extract useful\ninformation, which is used to generate the discharge instructions for\npreviously-unseen patients. Our experiments show that, using our method, the\nperformance of five different LMs can be substantially boosted across all\nmetrics. Meanwhile, we show results from human evaluations to measure the\neffectiveness in terms of fluency, faithfulness, and comprehensiveness. The\ncode is available at https://github.com/AI-in-Hospitals/Patient-Instructions\n","authors":["Fenglin Liu","Bang Yang","Chenyu You","Xian Wu","Shen Ge","Zhangdaihong Liu","Xu Sun","Yang Yang","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2210.12777v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10091v1","updated":"2024-07-14T06:04:11Z","published":"2024-07-14T06:04:11Z","title":"Enhancing Emotion Prediction in News Headlines: Insights from ChatGPT\n  and Seq2Seq Models for Free-Text Generation","summary":"  Predicting emotions elicited by news headlines can be challenging as the task\nis largely influenced by the varying nature of people's interpretations and\nbackgrounds. Previous works have explored classifying discrete emotions\ndirectly from news headlines. We provide a different approach to tackling this\nproblem by utilizing people's explanations of their emotion, written in\nfree-text, on how they feel after reading a news headline. Using the dataset\nBU-NEmo+ (Gao et al., 2022), we found that for emotion classification, the\nfree-text explanations have a strong correlation with the dominant emotion\nelicited by the headlines. The free-text explanations also contain more\nsentimental context than the news headlines alone and can serve as a better\ninput to emotion classification models. Therefore, in this work we explored\ngenerating emotion explanations from headlines by training a\nsequence-to-sequence transformer model and by using pretrained large language\nmodel, ChatGPT (GPT-4). We then used the generated emotion explanations for\nemotion classification. In addition, we also experimented with training the\npretrained T5 model for the intermediate task of explanation generation before\nfine-tuning it for emotion classification. Using McNemar's significance test,\nmethods that incorporate GPT-generated free-text emotion explanations\ndemonstrated significant improvement (P-value < 0.05) in emotion classification\nfrom headlines, compared to methods that only use headlines. This underscores\nthe value of using intermediate free-text explanations for emotion prediction\ntasks with headlines.\n","authors":["Ge Gao","Jongin Kim","Sejin Paik","Ekaterina Novozhilova","Yi Liu","Sarah T. Bonna","Margrit Betke","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2407.10091v1.pdf","comment":"published at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2407.10086v1","updated":"2024-07-14T05:22:53Z","published":"2024-07-14T05:22:53Z","title":"Rapid Biomedical Research Classification: The Pandemic PACT Advanced\n  Categorisation Engine","summary":"  This paper introduces the Pandemic PACT Advanced Categorisation Engine\n(PPACE) along with its associated dataset. PPACE is a fine-tuned model\ndeveloped to automatically classify research abstracts from funded biomedical\nprojects according to WHO-aligned research priorities. This task is crucial for\nmonitoring research trends and identifying gaps in global health preparedness\nand response. Our approach builds on human-annotated projects, which are\nallocated one or more categories from a predefined list. A large language model\nis then used to generate `rationales' explaining the reasoning behind these\nannotations. This augmented data, comprising expert annotations and rationales,\nis subsequently used to fine-tune a smaller, more efficient model. Developed as\npart of the Pandemic PACT project, which aims to track and analyse research\nfunding and clinical evidence for a wide range of diseases with outbreak\npotential, PPACE supports informed decision-making by research funders,\npolicymakers, and independent researchers. We introduce and release both the\ntrained model and the instruction-based dataset used for its training. Our\nevaluation shows that PPACE significantly outperforms its baselines. The\nrelease of PPACE and its associated dataset offers valuable resources for\nresearchers in multilabel biomedical document classification and supports\nadvancements in aligning biomedical research with key global health priorities.\n","authors":["Omid Rohanian","Mohammadmahdi Nouriborji","Olena Seminog","Rodrigo Furst","Thomas Mendy","Shanthi Levanita","Zaharat Kadri-Alab","Nusrat Jabin","Daniela Toale","Georgina Humphreys","Emilia Antonio","Adrian Bucher","Alice Norton","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2407.10086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10068v1","updated":"2024-07-14T03:51:49Z","published":"2024-07-14T03:51:49Z","title":"Multi-Granularity Semantic Revision for Large Language Model\n  Distillation","summary":"  Knowledge distillation plays a key role in compressing the Large Language\nModels (LLMs), which boosts a small-size student model under large teacher\nmodels' guidance. However, existing LLM distillation methods overly rely on\nstudent-generated outputs, which may introduce generation errors and misguide\nthe distillation process. Moreover, the distillation loss functions introduced\nin previous art struggle to align the most informative part due to the complex\ndistribution of LLMs' outputs. To address these problems, we propose a\nmulti-granularity semantic revision method for LLM distillation. At the\nsequence level, we propose a sequence correction and re-generation (SCRG)\nstrategy. SCRG first calculates the semantic cognitive difference between the\nteacher and student to detect the error token, then corrects it with the\nteacher-generated one, and re-generates the sequence to reduce generation\nerrors and enhance generation diversity. At the token level, we design a\ndistribution adaptive clipping Kullback-Leibler (DAC-KL) loss as the\ndistillation objective function. DAC-KL loss exploits a learnable sub-network\nto adaptively extract semantically dense areas from the teacher's output,\navoiding the interference of redundant information in the distillation process.\nFinally, at the span level, we leverage the span priors of a sequence to\ncompute the probability correlations within spans, and constrain the teacher\nand student's probability correlations to be consistent, further enhancing the\ntransfer of semantic information. Extensive experiments across different model\nfamilies with parameters ranging from 0.1B to 13B demonstrate the superiority\nof our method compared to existing methods.\n","authors":["Xiaoyu Liu","Yun Zhang","Wei Li","Simiao Li","Xudong Huang","Hanting Chen","Yehui Tang","Jie Hu","Zhiwei Xiong","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10058v1","updated":"2024-07-14T03:05:53Z","published":"2024-07-14T03:05:53Z","title":"Learning to Refuse: Towards Mitigating Privacy Risks in LLMs","summary":"  Large language models (LLMs) exhibit remarkable capabilities in understanding\nand generating natural language. However, these models can inadvertently\nmemorize private information, posing significant privacy risks. This study\naddresses the challenge of enabling LLMs to protect specific individuals'\nprivate data without the need for complete retraining. We propose \\return, a\nReal-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from\nWikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods\nfor protecting personal data in a realistic scenario. Additionally, we\nintroduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,\nwhich enables the model to learn which individuals' information should be\nprotected without affecting its ability to answer questions related to other\nunrelated individuals. Our extensive experiments demonstrate that NAUF achieves\na state-of-the-art average unlearning score, surpassing the best baseline\nmethod by 5.65 points, effectively protecting target individuals' personal data\nwhile maintaining the model's general capabilities.\n","authors":["Zhenhua Liu","Tong Zhu","Chuanyuan Tan","Wenliang Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10049v1","updated":"2024-07-14T02:25:45Z","published":"2024-07-14T02:25:45Z","title":"AutoGRAMS: Autonomous Graphical Agent Modeling Software","summary":"  We introduce the AutoGRAMS framework for programming multi-step interactions\nwith language models. AutoGRAMS represents AI agents as a graph, where each\nnode can execute either a language modeling instruction or traditional code.\nLikewise, transitions in the graph can be governed by either language modeling\ndecisions or traditional branch logic. AutoGRAMS supports using variables as\nmemory and allows nodes to call other AutoGRAMS graphs as functions. We show\nhow AutoGRAMS can be used to design highly sophisticated agents, including\nself-referential agents that can modify their own graph. AutoGRAMS's\ngraph-centric approach aids interpretability, controllability, and safety\nduring the design, development, and deployment of AI agents. We provide our\nframework as open source at https://github.com/autograms/autograms .\n","authors":["Ben Krause","Lucia Chen","Emmanuel Kahembwe"],"pdf_url":"https://arxiv.org/pdf/2407.10049v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.10283v1","updated":"2024-07-14T17:56:11Z","published":"2024-07-14T17:56:11Z","title":"Numbers Matter! Bringing Quantity-awareness to Retrieval Systems","summary":"  Quantitative information plays a crucial role in understanding and\ninterpreting the content of documents. Many user queries contain quantities and\ncannot be resolved without understanding their semantics, e.g., ``car that\ncosts less than $10k''. Yet, modern search engines apply the same ranking\nmechanisms for both words and quantities, overlooking magnitude and unit\ninformation. In this paper, we introduce two quantity-aware ranking techniques\ndesigned to rank both the quantity and textual content either jointly or\nindependently. These techniques incorporate quantity information in available\nretrieval systems and can address queries with numerical conditions equal,\ngreater than, and less than. To evaluate the effectiveness of our proposed\nmodels, we introduce two novel quantity-aware benchmark datasets in the domains\nof finance and medicine and compare our method against various lexical and\nneural models. The code and data are available under\nhttps://github.com/satya77/QuantityAwareRankers.\n","authors":["Satya Almasian","Milena Bruseva","Michael Gertz"],"pdf_url":"https://arxiv.org/pdf/2407.10283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10245v1","updated":"2024-07-14T15:25:08Z","published":"2024-07-14T15:25:08Z","title":"GenSco: Can Question Decomposition based Passage Alignment improve\n  Question Answering?","summary":"  Retrieval augmented generation (RAG) with large language models (LLMs) for\nQuestion Answering (QA) entails furnishing relevant context within the prompt\nto facilitate the LLM in answer generation. During the generation, inaccuracies\nor hallucinations frequently occur due to two primary factors: inadequate or\ndistracting context in the prompts, and the inability of LLMs to effectively\nreason through the facts. In this paper, we investigate whether providing\naligned context via a carefully selected passage sequence leads to better\nanswer generation by the LLM for multi-hop QA. We introduce, \"GenSco\", a novel\napproach of selecting passages based on the predicted decomposition of the\nmulti-hop questions}. The framework consists of two distinct LLMs: (i)\nGenerator LLM, which is used for question decomposition and final answer\ngeneration; (ii) an auxiliary open-sourced LLM, used as the scorer, to\nsemantically guide the Generator for passage selection. The generator is\ninvoked only once for the answer generation, resulting in a cost-effective and\nefficient approach. We evaluate on three broadly established multi-hop question\nanswering datasets: 2WikiMultiHop, Adversarial HotPotQA and MuSiQue and achieve\nan absolute gain of $15.1$ and $5.9$ points in Exact Match score with respect\nto the best performing baselines over MuSiQue and 2WikiMultiHop respectively.\n","authors":["Barah Fazili","Koustava Goswami","Natwar Modani","Inderjeet Nair"],"pdf_url":"https://arxiv.org/pdf/2407.10245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14945v2","updated":"2024-07-14T14:01:01Z","published":"2023-12-06T15:24:01Z","title":"Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge\n  Base for Industrial Prognostics and Health Management","summary":"  Prognostics and health management (PHM) is essential for industrial operation\nand maintenance, focusing on predicting, diagnosing, and managing the health\nstatus of industrial systems. The emergence of the ChatGPT-Like large-scale\nlanguage model (LLM) has begun to lead a new round of innovation in the AI\nfield. It has extensively promoted the level of intelligence in various fields.\nTherefore, it is also expected further to change the application paradigm in\nindustrial PHM and promote PHM to become intelligent. Although ChatGPT-Like\nLLMs have rich knowledge reserves and powerful language understanding and\ngeneration capabilities, they lack domain-specific expertise, significantly\nlimiting their practicability in PHM applications. To this end, this study\nexplores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in\nindustrial PHM to solve the above limitations. In addition, we introduce the\nmethod and steps of combining the LKB with LLMs, including LKB preparation, LKB\nvectorization, prompt engineering, etc. Experimental analysis of real cases\nshows that combining the LKB with ChatGPT-Like LLM can significantly improve\nits performance and make ChatGPT-Like LLMs more accurate, relevant, and able to\nprovide more insightful information. This can promote the development of\nChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality.\n","authors":["Huan Wang","Yan-Fu Li","Min Xie"],"pdf_url":"https://arxiv.org/pdf/2312.14945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10184v1","updated":"2024-07-14T13:03:35Z","published":"2024-07-14T13:03:35Z","title":"Towards Robust Recommendation via Decision Boundary-aware Graph\n  Contrastive Learning","summary":"  In recent years, graph contrastive learning (GCL) has received increasing\nattention in recommender systems due to its effectiveness in reducing bias\ncaused by data sparsity. However, most existing GCL models rely on heuristic\napproaches and usually assume entity independence when constructing contrastive\nviews. We argue that these methods struggle to strike a balance between\nsemantic invariance and view hardness across the dynamic training process, both\nof which are critical factors in graph contrastive learning.\n  To address the above issues, we propose a novel GCL-based recommendation\nframework RGCL, which effectively maintains the semantic invariance of\ncontrastive pairs and dynamically adapts as the model capability evolves\nthrough the training process. Specifically, RGCL first introduces decision\nboundary-aware adversarial perturbations to constrain the exploration space of\ncontrastive augmented views, avoiding the decrease of task-specific\ninformation. Furthermore, to incorporate global user-user and item-item\ncollaboration relationships for guiding on the generation of hard contrastive\nviews, we propose an adversarial-contrastive learning objective to construct a\nrelation-aware view-generator. Besides, considering that unsupervised GCL could\npotentially narrower margins between data points and the decision boundary,\nresulting in decreased model robustness, we introduce the adversarial examples\nbased on maximum perturbations to achieve margin maximization. We also provide\ntheoretical analyses on the effectiveness of our designs. Through extensive\nexperiments on five public datasets, we demonstrate the superiority of RGCL\ncompared against twelve baseline models.\n","authors":["Jiakai Tang","Sunhao Dai","Zexu Sun","Xu Chen","Jun Xu","Wenhui Yu","Lantao Hu","Peng Jiang","Han Li"],"pdf_url":"https://arxiv.org/pdf/2407.10184v1.pdf","comment":"KDD 2024"},{"id":"http://arxiv.org/abs/2402.16664v2","updated":"2024-07-14T10:35:23Z","published":"2024-02-26T15:35:24Z","title":"LLM-Assisted Multi-Teacher Continual Learning for Visual Question\n  Answering in Robotic Surgery","summary":"  Visual question answering (VQA) is crucial for promoting surgical education.\nIn practice, the needs of trainees are constantly evolving, such as learning\nmore surgical types, adapting to different robots, and learning new surgical\ninstruments and techniques for various surgeries. However, patient data privacy\noften restricts the availability of old data when updating the model,\nnecessitating an exemplar-free continual learning (CL) setup. Prior CL studies\noverlooked two vital problems in the surgical domain: 1) large domain shifts\nfrom diverse surgical operations collected from multiple sources, and 2) severe\ndata imbalance arising from the uneven presence of surgical instruments or\nactivities. This paper proposes addressing these problems with a multimodal\nlarge language model (LLM) and an adaptive weight assignment methodology. We\nfirst develop a new multi-teacher CL framework that leverages a multimodal LLM\nas the additional teacher. The strong generalization ability of the LLM can\nbridge the knowledge gap when domain shifts and data imbalances occur. We then\nput forth a novel data processing method that transforms complex LLM embeddings\ninto logits compatible with our CL framework. We further design an adaptive\nweight assignment approach that balances the generalization ability of the LLM\nand the domain expertise of the old CL model. Finally, to comprehensively test\nthe effectiveness of our proposed method, we have also constructed two new\nsurgical VQA datasets that are largely different from existing ones and could\nbe valuable resources for future research. Extensive experimental results on\nthe tested datasets demonstrate the superiority of our method to other advanced\nCL schemes.\n","authors":["Yuyang Du","Kexin Chen","Yue Zhan","Chang Han Low","Tao You","Mobarakol Islam","Ziyu Guo","Yueming Jin","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2402.16664v2.pdf","comment":"This paper has been accapted by 2024 IEEE International Conference on\n  Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2407.10112v1","updated":"2024-07-14T07:58:13Z","published":"2024-07-14T07:58:13Z","title":"Warming Up Cold-Start CTR Prediction by Learning Item-Specific Feature\n  Interactions","summary":"  In recommendation systems, new items are continuously introduced, initially\nlacking interaction records but gradually accumulating them over time.\nAccurately predicting the click-through rate (CTR) for these items is crucial\nfor enhancing both revenue and user experience. While existing methods focus on\nenhancing item ID embeddings for new items within general CTR models, they tend\nto adopt a global feature interaction approach, often overshadowing new items\nwith sparse data by those with abundant interactions. Addressing this, our work\nintroduces EmerG, a novel approach that warms up cold-start CTR prediction by\nlearning item-specific feature interaction patterns. EmerG utilizes\nhypernetworks to generate an item-specific feature graph based on item\ncharacteristics, which is then processed by a Graph Neural Network (GNN). This\nGNN is specially tailored to provably capture feature interactions at any order\nthrough a customized message passing mechanism. We further design a meta\nlearning strategy that optimizes parameters of hypernetworks and GNN across\nvarious item CTR prediction tasks, while only adjusting a minimal set of\nitem-specific parameters within each task. This strategy effectively reduces\nthe risk of overfitting when dealing with limited data. Extensive experiments\non benchmark datasets validate that EmerG consistently performs the best given\nno, a few and sufficient instances of new items.\n","authors":["Yaqing Wang","Hongming Piao","Daxiang Dong","Quanming Yao","Jingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.10112v1.pdf","comment":"KDD 2024"},{"id":"http://arxiv.org/abs/2407.10081v1","updated":"2024-07-14T05:02:21Z","published":"2024-07-14T05:02:21Z","title":"All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems\n  Across the LLM Era","summary":"  Recommender systems (RS) are vital for managing information overload and\ndelivering personalized content, responding to users' diverse information\nneeds. The emergence of large language models (LLMs) offers a new horizon for\nredefining recommender systems with vast general knowledge and reasoning\ncapabilities. Standing across this LLM era, we aim to integrate recommender\nsystems into a broader picture, and pave the way for more comprehensive\nsolutions for future research. Therefore, we first offer a comprehensive\noverview of the technical progression of recommender systems, particularly\nfocusing on language foundation models and their applications in\nrecommendation. We identify two evolution paths of modern recommender systems\n-- via list-wise recommendation and conversational recommendation. These two\npaths finally converge at LLM agents with superior capabilities of long-term\nmemory, reflection, and tool intelligence. Along these two paths, we point out\nthat the information effectiveness of the recommendation is increased, while\nthe user's acquisition cost is decreased. Technical features, research\nmethodologies, and inherent challenges for each milestone along the path are\ncarefully investigated -- from traditional list-wise recommendation to\nLLM-enhanced recommendation to recommendation with LLM agents. Finally, we\nhighlight several unresolved challenges crucial for the development of future\npersonalization technologies and interfaces and discuss the future prospects.\n","authors":["Bo Chen","Xinyi Dai","Huifeng Guo","Wei Guo","Weiwen Liu","Yong Liu","Jiarui Qin","Ruiming Tang","Yichao Wang","Chuhan Wu","Yaxiong Wu","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.10081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10078v1","updated":"2024-07-14T04:53:36Z","published":"2024-07-14T04:53:36Z","title":"Semantic Understanding and Data Imputation using Large Language Model to\n  Accelerate Recommendation System","summary":"  This paper aims to address the challenge of sparse and missing data in\nrecommendation systems, a significant hurdle in the age of big data.\nTraditional imputation methods struggle to capture complex relationships within\nthe data. We propose a novel approach that fine-tune Large Language Model (LLM)\nand use it impute missing data for recommendation systems. LLM which is trained\non vast amounts of text, is able to understand complex relationship among data\nand intelligently fill in missing information. This enriched data is then used\nby the recommendation system to generate more accurate and personalized\nsuggestions, ultimately enhancing the user experience. We evaluate our\nLLM-based imputation method across various tasks within the recommendation\nsystem domain, including single classification, multi-classification, and\nregression compared to traditional data imputation methods. By demonstrating\nthe superiority of LLM imputation over traditional methods, we establish its\npotential for improving recommendation system performance.\n","authors":["Zhicheng Ding","Jiahao Tian","Zhenkai Wang","Jinman Zhao","Siyang Li"],"pdf_url":"https://arxiv.org/pdf/2407.10078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10042v1","updated":"2024-07-14T01:52:10Z","published":"2024-07-14T01:52:10Z","title":"Harnessing Feature Clustering For Enhanced Anomaly Detection With\n  Variational Autoencoder And Dynamic Threshold","summary":"  We introduce an anomaly detection method for multivariate time series data\nwith the aim of identifying critical periods and features influencing extreme\nclimate events like snowmelt in the Arctic. This method leverages the\nVariational Autoencoder (VAE) integrated with dynamic thresholding and\ncorrelation-based feature clustering. This framework enhances the VAE's ability\nto identify localized dependencies and learn the temporal relationships in\nclimate data, thereby improving the detection of anomalies as demonstrated by\nits higher F1-score on benchmark datasets. The study's main contributions\ninclude the development of a robust anomaly detection method, improving feature\nrepresentation within VAEs through clustering, and creating a dynamic threshold\nalgorithm for localized anomaly detection. This method offers explainability of\nclimate anomalies across different regions.\n","authors":["Tolulope Ale","Nicole-Jeanne Schlegel","Vandana P. Janeja"],"pdf_url":"https://arxiv.org/pdf/2407.10042v1.pdf","comment":"This work was presented at the 2024 IEEE International Geoscience and\n  Remote Sensing Symposium, IGARSS 2024, 07-12 July 2024, Athens, Greece"}],"Multimedia":[{"id":"http://arxiv.org/abs/2211.15597v3","updated":"2024-07-14T11:34:21Z","published":"2022-11-28T17:50:19Z","title":"Lightning Fast Video Anomaly Detection via Adversarial Knowledge\n  Distillation","summary":"  We propose a very fast frame-level model for anomaly detection in video,\nwhich learns to detect anomalies by distilling knowledge from multiple highly\naccurate object-level teacher models. To improve the fidelity of our student,\nwe distill the low-resolution anomaly maps of the teachers by jointly applying\nstandard and adversarial distillation, introducing an adversarial discriminator\nfor each teacher to distinguish between target and generated anomaly maps. We\nconduct experiments on three benchmarks (Avenue, ShanghaiTech, UCSD Ped2),\nshowing that our method is over 7 times faster than the fastest competing\nmethod, and between 28 and 62 times faster than object-centric models, while\nobtaining comparable results to recent methods. Our evaluation also indicates\nthat our model achieves the best trade-off between speed and accuracy, due to\nits previously unheard-of speed of 1480 FPS. In addition, we carry out a\ncomprehensive ablation study to justify our architectural design choices. Our\ncode is freely available at: https://github.com/ristea/fast-aed.\n","authors":["Florinel-Alin Croitoru","Nicolae-Catalin Ristea","Dana Dascalescu","Radu Tudor Ionescu","Fahad Shahbaz Khan","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2211.15597v3.pdf","comment":"Accepted in Computer Vision and Image Understanding"}]},"2024-07-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.06245v2","updated":"2024-07-13T22:48:44Z","published":"2024-07-08T13:07:50Z","title":"ORAN-Bench-13K: An Open Source Benchmark for Assessing LLMs in Open\n  Radio Access Networks","summary":"  Large Language Models (LLMs) can revolutionize how we deploy and operate Open\nRadio Access Networks (O-RAN) by enhancing network analytics, anomaly\ndetection, and code generation and significantly increasing the efficiency and\nreliability of a plethora of O-RAN tasks. In this paper, we present\nORAN-Bench-13K, the first comprehensive benchmark designed to evaluate the\nperformance of Large Language Models (LLMs) within the context of O-RAN. Our\nbenchmark consists of 13,952 meticulously curated multiple-choice questions\ngenerated from 116 O-RAN specification documents. We leverage a novel\nthree-stage LLM framework, and the questions are categorized into three\ndistinct difficulties to cover a wide spectrum of ORAN-related knowledge. We\nthoroughly evaluate the performance of several state-of-the-art LLMs, including\nGemini, Chat-GPT, and Mistral. Additionally, we propose ORANSight, a\nRetrieval-Augmented Generation (RAG)-based pipeline that demonstrates superior\nperformance on ORAN-Bench-13K compared to other tested closed-source models.\nOur findings indicate that current popular LLM models are not proficient in\nO-RAN, highlighting the need for specialized models. We observed a noticeable\nperformance improvement when incorporating the RAG-based ORANSight pipeline,\nwith a Macro Accuracy of 0.784 and a Weighted Accuracy of 0.776, which was on\naverage 21.55% and 22.59% better than the other tested LLMs.\n","authors":["Pranshav Gajjar","Vijay K. Shah"],"pdf_url":"https://arxiv.org/pdf/2407.06245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10021v1","updated":"2024-07-13T22:45:46Z","published":"2024-07-13T22:45:46Z","title":"Document-level Clinical Entity and Relation Extraction via Knowledge\n  Base-Guided Generation","summary":"  Generative pre-trained transformer (GPT) models have shown promise in\nclinical entity and relation extraction tasks because of their precise\nextraction and contextual understanding capability. In this work, we further\nleverage the Unified Medical Language System (UMLS) knowledge base to\naccurately identify medical concepts and improve clinical entity and relation\nextraction at the document level. Our framework selects UMLS concepts relevant\nto the text and combines them with prompts to guide language models in\nextracting entities. Our experiments demonstrate that this initial concept\nmapping and the inclusion of these mapped concepts in the prompts improves\nextraction results compared to few-shot extraction tasks on generic language\nmodels that do not leverage UMLS. Further, our results show that this approach\nis more effective than the standard Retrieval Augmented Generation (RAG)\ntechnique, where retrieved data is compared with prompt embeddings to generate\nresults. Overall, we find that integrating UMLS concepts with GPT models\nsignificantly improves entity and relation identification, outperforming the\nbaseline and RAG models. By combining the precise concept mapping capability of\nknowledge-based approaches like UMLS with the contextual understanding\ncapability of GPT, our method highlights the potential of these approaches in\nspecialized domains like healthcare.\n","authors":["Kriti Bhattarai","Inez Y. Oh","Zachary B. Abrams","Albert M. Lai"],"pdf_url":"https://arxiv.org/pdf/2407.10021v1.pdf","comment":"Accepted at Association for Computational Linguistics BioNLP 2024"},{"id":"http://arxiv.org/abs/2407.10020v1","updated":"2024-07-13T22:33:29Z","published":"2024-07-13T22:33:29Z","title":"Causality extraction from medical text using Large Language Models\n  (LLMs)","summary":"  This study explores the potential of natural language models, including large\nlanguage models, to extract causal relations from medical texts, specifically\nfrom Clinical Practice Guidelines (CPGs). The outcomes causality extraction\nfrom Clinical Practice Guidelines for gestational diabetes are presented,\nmarking a first in the field. We report on a set of experiments using variants\nof BERT (BioBERT, DistilBERT, and BERT) and using Large Language Models (LLMs),\nnamely GPT-4 and LLAMA2. Our experiments show that BioBERT performed better\nthan other models, including the Large Language Models, with an average\nF1-score of 0.72. GPT-4 and LLAMA2 results show similar performance but less\nconsistency. We also release the code and an annotated a corpus of causal\nstatements within the Clinical Practice Guidelines for gestational diabetes.\n","authors":["Seethalakshmi Gopalakrishnan","Luciana Garbayo","Wlodek Zadrozny"],"pdf_url":"https://arxiv.org/pdf/2407.10020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10005v1","updated":"2024-07-13T21:13:55Z","published":"2024-07-13T21:13:55Z","title":"Fine-grained Analysis of In-context Linear Estimation: Data,\n  Architecture, and Beyond","summary":"  Recent research has shown that Transformers with linear attention are capable\nof in-context learning (ICL) by implementing a linear estimator through\ngradient descent steps. However, the existing results on the optimization\nlandscape apply under stylized settings where task and feature vectors are\nassumed to be IID and the attention weights are fully parameterized. In this\nwork, we develop a stronger characterization of the optimization and\ngeneralization landscape of ICL through contributions on architectures,\nlow-rank parameterization, and correlated designs: (1) We study the landscape\nof 1-layer linear attention and 1-layer H3, a state-space model. Under a\nsuitable correlated design assumption, we prove that both implement 1-step\npreconditioned gradient descent. We show that thanks to its native convolution\nfilters, H3 also has the advantage of implementing sample weighting and\noutperforming linear attention in suitable settings. (2) By studying correlated\ndesigns, we provide new risk bounds for retrieval augmented generation (RAG)\nand task-feature alignment which reveal how ICL sample complexity benefits from\ndistributional alignment. (3) We derive the optimal risk for low-rank\nparameterized attention weights in terms of covariance spectrum. Through this,\nwe also shed light on how LoRA can adapt to a new distribution by capturing the\nshift between task covariances. Experimental results corroborate our\ntheoretical findings. Overall, this work explores the optimization and risk\nlandscape of ICL in practically meaningful settings and contributes to a more\nthorough understanding of its mechanics.\n","authors":["Yingcong Li","Ankit Singh Rawat","Samet Oymak"],"pdf_url":"https://arxiv.org/pdf/2407.10005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02061v4","updated":"2024-07-13T21:02:21Z","published":"2024-06-04T07:43:33Z","title":"Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown\n  in State-Of-the-Art Large Language Models","summary":"  Large Language Models (LLMs) are often described as being instances of\nfoundation models - that is, models that transfer strongly across various tasks\nand conditions in few-show or zero-shot manner, while exhibiting scaling laws\nthat predict function improvement when increasing the pre-training scale. These\nclaims of excelling in different functions and tasks rely on measurements taken\nacross various sets of standardized benchmarks showing high scores for such\nmodels. We demonstrate here a dramatic breakdown of function and reasoning\ncapabilities of state-of-the-art models trained at the largest available scales\nwhich claim strong function, using a simple, short, conventional common sense\nproblem (AIW problem) formulated in concise natural language, easily solvable\nby humans. The breakdown is dramatic, as models show strong fluctuations across\neven slight problem variations that should not affect problem solving, also\nexpressing strong overconfidence in the wrong solutions, often backed up by\nplausible sounding explanation-like confabulations. Various standard\ninterventions in an attempt to get the right solution, like various type of\nenhanced prompting, or urging the models to reconsider the wrong solutions\nagain by multi step re-evaluation, fail. We take these initial observations to\nthe scientific and technological community to stimulate urgent re-assessment of\nthe claimed capabilities of current generation of LLMs. Such re-assessment also\nrequires common action to create standardized benchmarks that would allow\nproper detection of such basic reasoning deficits that obviously manage to\nremain undiscovered by current state-of-the-art evaluation procedures and\nbenchmarks. Code for reproducing experiments in the paper and raw experiments\ndata can be found at https://github.com/LAION-AI/AIW\n","authors":["Marianna Nezhurina","Lucia Cipolina-Kun","Mehdi Cherti","Jenia Jitsev"],"pdf_url":"https://arxiv.org/pdf/2406.02061v4.pdf","comment":"v2.01. Minor edits. Further experiments on various AIW problem\n  variations. AIW \"Alice Female Power Boost\", AIW Extension (AIW Ext).\n  Including recent Claude 3.5 Sonnet and Qwen 2 72B Instruct results"},{"id":"http://arxiv.org/abs/2402.12332v2","updated":"2024-07-13T17:58:25Z","published":"2024-02-19T18:06:02Z","title":"Triple-Encoders: Representations That Fire Together, Wire Together","summary":"  Search-based dialog models typically re-encode the dialog history at every\nturn, incurring high cost. Curved Contrastive Learning, a representation\nlearning method that encodes relative distances between utterances into the\nembedding space via a bi-encoder, has recently shown promising results for\ndialog modeling at far superior efficiency. While high efficiency is achieved\nthrough independently encoding utterances, this ignores the importance of\ncontextualization. To overcome this issue, this study introduces\ntriple-encoders, which efficiently compute distributed utterance mixtures from\nthese independently encoded utterances through a novel hebbian inspired\nco-occurrence learning objective in a self-organizing manner, without using any\nweights, i.e., merely through local interactions. Empirically, we find that\ntriple-encoders lead to a substantial improvement over bi-encoders, and even to\nbetter zero-shot generalization than single-vector representation models\nwithout requiring re-encoding. Our code\n(https://github.com/UKPLab/acl2024-triple-encoders) and model\n(https://huggingface.co/UKPLab/triple-encoders-dailydialog) are publicly\navailable.\n","authors":["Justus-Jonas Erker","Florian Mai","Nils Reimers","Gerasimos Spanakis","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2402.12332v2.pdf","comment":"accepted at ACL 2024 (main conference)"},{"id":"http://arxiv.org/abs/2310.06684v2","updated":"2024-07-13T17:43:09Z","published":"2023-10-10T14:59:22Z","title":"Learning Multiplex Representations on Text-Attributed Graphs with One\n  Language Model Encoder","summary":"  In real-world scenarios, texts in a graph are often linked by multiple\nsemantic relations (e.g., papers in an academic graph are referenced by other\npublications, written by the same author, or published in the same venue),\nwhere text documents and their relations form a multiplex text-attributed\ngraph. Mainstream text representation learning methods use pretrained language\nmodels (PLMs) to generate one embedding for each text unit, expecting that all\ntypes of relations between texts can be captured by these single-view\nembeddings. However, this presumption does not hold particularly in multiplex\ntext-attributed graphs. Along another line of work, multiplex graph neural\nnetworks (GNNs) directly initialize node attributes as a feature vector for\nnode representation learning, but they cannot fully capture the semantics of\nthe nodes' associated texts. To bridge these gaps, we propose METAG, a new\nframework for learning Multiplex rEpresentations on Text-Attributed Graphs. In\ncontrast to existing methods, METAG uses one text encoder to model the shared\nknowledge across relations and leverages a small number of parameters per\nrelation to derive relation-specific representations. This allows the encoder\nto effectively capture the multiplex structures in the graph while also\npreserving parameter efficiency. We conduct experiments on nine downstream\ntasks in five graphs from both academic and e-commerce domains, where METAG\noutperforms baselines significantly and consistently. The code is available at\nhttps://github.com/PeterGriffinJin/METAG.\n","authors":["Bowen Jin","Wentao Zhang","Yu Zhang","Yu Meng","Han Zhao","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2310.06684v2.pdf","comment":"9 pages, 11 appendix pages"},{"id":"http://arxiv.org/abs/2407.09943v1","updated":"2024-07-13T16:47:20Z","published":"2024-07-13T16:47:20Z","title":"Minimizing PLM-Based Few-Shot Intent Detectors","summary":"  Recent research has demonstrated the feasibility of training efficient intent\ndetectors based on pre-trained language model~(PLM) with limited labeled data.\nHowever, deploying these detectors in resource-constrained environments such as\nmobile devices poses challenges due to their large sizes. In this work, we aim\nto address this issue by exploring techniques to minimize the size of PLM-based\nintent detectors trained with few-shot data. Specifically, we utilize large\nlanguage models (LLMs) for data augmentation, employ a cutting-edge model\ncompression method for knowledge distillation, and devise a vocabulary pruning\nmechanism called V-Prune. Through these approaches, we successfully achieve a\ncompression ratio of 21 in model memory usage, including both Transformer and\nthe vocabulary, while maintaining almost identical performance levels on four\nreal-world benchmarks.\n","authors":["Haode Zhang","Xiao-Ming Wu","Albert Y. S. Lam"],"pdf_url":"https://arxiv.org/pdf/2407.09943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09936v1","updated":"2024-07-13T16:17:08Z","published":"2024-07-13T16:17:08Z","title":"WojoodNER 2024: The Second Arabic Named Entity Recognition Shared Task","summary":"  We present WojoodNER-2024, the second Arabic Named Entity Recognition (NER)\nShared Task. In WojoodNER-2024, we focus on fine-grained Arabic NER. We\nprovided participants with a new Arabic fine-grained NER dataset called\nwojoodfine, annotated with subtypes of entities. WojoodNER-2024 encompassed\nthree subtasks: (i) Closed-Track Flat Fine-Grained NER, (ii) Closed-Track\nNested Fine-Grained NER, and (iii) an Open-Track NER for the Israeli War on\nGaza. A total of 43 unique teams registered for this shared task. Five teams\nparticipated in the Flat Fine-Grained Subtask, among which two teams tackled\nthe Nested Fine-Grained Subtask and one team participated in the Open-Track NER\nSubtask. The winning teams achieved F-1 scores of 91% and 92% in the Flat\nFine-Grained and Nested Fine-Grained Subtasks, respectively. The sole team in\nthe Open-Track Subtask achieved an F-1 score of 73.7%.\n","authors":["Mustafa Jarrar","Nagham Hamad","Mohammed Khalilia","Bashar Talafha","AbdelRahim Elmadany","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2407.09936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09897v1","updated":"2024-07-13T14:24:45Z","published":"2024-07-13T14:24:45Z","title":"Cohesive Conversations: Enhancing Authenticity in Multi-Agent Simulated\n  Dialogues","summary":"  This paper investigates the quality of multi-agent dialogues in simulations\npowered by Large Language Models (LLMs), focusing on a case study from Park et\nal. (2023), where 25 agents engage in day-long simulations of life, showcasing\ncomplex behaviors and interactions. Analyzing dialogues and memory over\nmultiple sessions revealed significant issues such as repetition,\ninconsistency, and hallucination, exacerbated by the propagation of erroneous\ninformation. To combat these challenges, we propose a novel Screening,\nDiagnosis, and Regeneration (SDR) framework that detects and corrects utterance\nerrors through a comprehensive process involving immediate issue\nidentification, evidence gathering from past dialogues, and LLM analysis for\nutterance revision. The effectiveness of the SDR framework is validated through\nGPT-4 assessments and human evaluations, demonstrating marked improvements in\ndialogue consistency, diversity, and the reduction of false information. This\nwork presents a pioneering approach to enhancing dialogue quality in\nmulti-agent simulations, establishing a new standard for future research in the\nfield.\n","authors":["KuanChao Chu","Yi-Pei Chen","Hideki Nakayama"],"pdf_url":"https://arxiv.org/pdf/2407.09897v1.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2407.09894v1","updated":"2024-07-13T14:04:55Z","published":"2024-07-13T14:04:55Z","title":"Transferring Structure Knowledge: A New Task to Fake news Detection\n  Towards Cold-Start Propagation","summary":"  Many fake news detection studies have achieved promising performance by\nextracting effective semantic and structure features from both content and\npropagation trees. However, it is challenging to apply them to practical\nsituations, especially when using the trained propagation-based models to\ndetect news with no propagation data. Towards this scenario, we study a new\ntask named cold-start fake news detection, which aims to detect content-only\nsamples with missing propagation. To achieve the task, we design a simple but\neffective Structure Adversarial Net (SAN) framework to learn transferable\nfeatures from available propagation to boost the detection of content-only\nsamples. SAN introduces a structure discriminator to estimate dissimilarities\namong learned features with and without propagation, and further learns\nstructure-invariant features to enhance the generalization of existing\npropagation-based methods for content-only samples. We conduct qualitative and\nquantitative experiments on three datasets. Results show the challenge of the\nnew task and the effectiveness of our SAN framework.\n","authors":["Lingwei Wei","Dou Hu","Wei Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2407.09894v1.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2407.09893v1","updated":"2024-07-13T13:58:24Z","published":"2024-07-13T13:58:24Z","title":"Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks","summary":"  Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long- and Short-Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on 5 tasks demonstrate SMART's\nsuperior performance compared to previous widely adopted methods.\n","authors":["Shengbin Yue","Siyuan Wang","Wei Chen","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.09893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09888v1","updated":"2024-07-13T13:30:20Z","published":"2024-07-13T13:30:20Z","title":"FarFetched: Entity-centric Reasoning and Claim Validation for the Greek\n  Language based on Textually Represented Environments","summary":"  Our collective attention span is shortened by the flood of online\ninformation. With \\textit{FarFetched}, we address the need for automated claim\nvalidation based on the aggregated evidence derived from multiple online news\nsources. We introduce an entity-centric reasoning framework in which latent\nconnections between events, actions, or statements are revealed via entity\nmentions and represented in a graph database. Using entity linking and semantic\nsimilarity, we offer a way for collecting and combining information from\ndiverse sources in order to generate evidence relevant to the user's claim.\nThen, we leverage textual entailment recognition to quantitatively determine\nwhether this assertion is credible, based on the created evidence. Our approach\ntries to fill the gap in automated claim validation for less-resourced\nlanguages and is showcased on the Greek language, complemented by the training\nof relevant semantic textual similarity (STS) and natural language inference\n(NLI) models that are evaluated on translated versions of common benchmarks.\n","authors":["Dimitris Papadopoulos","Katerina Metropoulou","Nikolaos Matsatsinis","Nikolaos Papadakis"],"pdf_url":"https://arxiv.org/pdf/2407.09888v1.pdf","comment":"DeepLo NAACL 2022"},{"id":"http://arxiv.org/abs/2407.09886v1","updated":"2024-07-13T13:26:43Z","published":"2024-07-13T13:26:43Z","title":"Speech-Copilot: Leveraging Large Language Models for Speech Processing\n  via Task Decomposition, Modularization, and Program Generation","summary":"  In this work, we introduce Speech-Copilot, a modular framework for\ninstruction-oriented speech-processing tasks that minimizes human effort in\ntoolset construction. Unlike end-to-end methods using large audio-language\nmodels, Speech-Copilot builds speech processing-specific toolsets by analyzing\npre-collected task instructions and breaking tasks into manageable sub-tasks.\nIt features a flexible agent based on large language models that performs tasks\nthrough program generation. Our approach achieves state-of-the-art performance\non the Dynamic-SUPERB benchmark, demonstrating its effectiveness across diverse\nspeech-processing tasks. Key contributions include: 1) developing an innovative\nframework for speech processing-specific toolset construction, 2) establishing\na high-performing agent based on large language models, and 3) offering a new\nperspective on addressing challenging instruction-oriented speech-processing\ntasks. Without additional training processes required by end-to-end approaches,\nour method provides a flexible and extendable solution for a wide range of\nspeech-processing applications.\n","authors":["Chun-Yi Kuan","Chih-Kai Yang","Wei-Ping Huang","Ke-Han Lu","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2407.09886v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2401.05544v2","updated":"2024-07-13T13:12:53Z","published":"2024-01-10T20:49:59Z","title":"Enhancing Source Code Classification Effectiveness via Prompt Learning\n  Incorporating Knowledge Features","summary":"  Researchers have investigated the potential of leveraging pre-trained\nlanguage models, such as CodeBERT, to enhance source code-related tasks.\nPrevious methodologies have relied on CodeBERT's '[CLS]' token as the embedding\nrepresentation of input sequences for task performance, necessitating\nadditional neural network layers to enhance feature representation, which in\nturn increases computational expenses. These approaches have also failed to\nfully leverage the comprehensive knowledge inherent within the source code and\nits associated text, potentially limiting classification efficacy. We propose\nCodeClassPrompt, a text classification technique that harnesses prompt learning\nto extract rich knowledge associated with input sequences from pre-trained\nmodels, thereby eliminating the need for additional layers and lowering\ncomputational costs. By applying an attention mechanism, we synthesize\nmulti-layered knowledge into task-specific features, enhancing classification\naccuracy. Our comprehensive experimentation across four distinct source\ncode-related tasks reveals that CodeClassPrompt achieves competitive\nperformance while significantly reducing computational overhead.\n","authors":["Yong Ma","Senlin Luo","Yu-Ming Shang","Yifei Zhang","Zhengjun Li"],"pdf_url":"https://arxiv.org/pdf/2401.05544v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09879v1","updated":"2024-07-13T13:03:45Z","published":"2024-07-13T13:03:45Z","title":"sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through\n  N-shot Guided Prompting","summary":"  Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small and\nMistral-7B and then evaluating them across a comprehensive suite of\nmultilingual benchmarks that test reasoning, question answering, and reading\ncomprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned with\nsPhinX perform better on an average by 4.2%pt and 5%pt respectively as compared\nto the baselines. We also devise a strategy to incorporate N-shot examples in\neach fine-tuning sample which further boosts the performance of these models by\n3%pt and 10%pt respectively. Additionally, sPhinX also outperforms other\nmultilingual instruction tuning datasets on the same benchmarks along with\nbeing sample efficient and diverse, thereby reducing dataset creation costs.\nAdditionally, instruction tuning with sPhinX does not lead to regression on\nmost English benchmarks.\n","authors":["Sanchit Ahuja","Kumar Tanmay","Hardik Hansrajbhai Chauhan","Barun Patra","Kriti Aggarwal","Tejas Indulal Dhamecha","Monojit Choudhary","Vishrav Chaudhary","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2407.09879v1.pdf","comment":"20 pages, 12 tables, 5 figures"},{"id":"http://arxiv.org/abs/2407.09861v1","updated":"2024-07-13T12:01:52Z","published":"2024-07-13T12:01:52Z","title":"Towards Systematic Monolingual NLP Surveys: GenA of Greek NLP","summary":"  Natural Language Processing (NLP) research has traditionally been\npredominantly focused on English, driven by the availability of resources, the\nsize of the research community, and market demands. Recently, there has been a\nnoticeable shift towards multilingualism in NLP, recognizing the need for\ninclusivity and effectiveness across diverse languages and cultures.\nMonolingual surveys have the potential to complement the broader trend towards\nmultilingualism in NLP by providing foundational insights and resources\nnecessary for effectively addressing the linguistic diversity of global\ncommunication. However, monolingual NLP surveys are extremely rare in\nliterature. This study fills the gap by introducing a method for creating\nsystematic and comprehensive monolingual NLP surveys. Characterized by a\nstructured search protocol, it can be used to select publications and organize\nthem through a taxonomy of NLP tasks. We include a classification of Language\nResources (LRs), according to their availability, and datasets, according to\ntheir annotation, to highlight publicly-available and machine-actionable LRs.\nBy applying our method, we conducted a systematic literature review of Greek\nNLP from 2012 to 2022, providing a comprehensive overview of the current state\nand challenges of Greek NLP research. We discuss the progress of Greek NLP and\noutline encountered Greek LRs, classified by availability and usability. As we\nshow, our proposed method helps avoid common pitfalls, such as data leakage and\ncontamination, and to assess language support per NLP task. We consider this\nsystematic literature review of Greek NLP an application of our method that\nshowcases the benefits of a monolingual NLP survey. Similar applications could\nbe regard the myriads of languages whose progress in NLP lags behind that of\nwell-supported languages.\n","authors":["Juli Bakagianni","Kanella Pouli","Maria Gavriilidou","John Pavlopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.09861v1.pdf","comment":"68 pages"},{"id":"http://arxiv.org/abs/2407.09855v1","updated":"2024-07-13T11:29:20Z","published":"2024-07-13T11:29:20Z","title":"Building pre-train LLM Dataset for the INDIC Languages: a case study on\n  Hindi","summary":"  Large language models (LLMs) demonstrated transformative capabilities in many\napplications that require automatically generating responses based on human\ninstruction. However, the major challenge for building LLMs, particularly in\nIndic languages, is the availability of high-quality data for building\nfoundation LLMs. In this paper, we are proposing a large pre-train dataset in\nHindi useful for the Indic language Hindi. We have collected the data span\nacross several domains including major dialects in Hindi. The dataset contains\n1.28 billion Hindi tokens. We have explained our pipeline including data\ncollection, pre-processing, and availability for LLM pre-training. The proposed\napproach can be easily extended to other Indic and low-resource languages and\nwill be available freely for LLM pre-training and LLM research purposes.\n","authors":["Shantipriya Parida","Shakshi Panwar","Kusum Lata","Sanskruti Mishra","Sambit Sekhar"],"pdf_url":"https://arxiv.org/pdf/2407.09855v1.pdf","comment":"Accepted as a book chapter in the book Title \"APPLIED SPEECH AND TEXT\n  PROCESSING FOR LOW RESOURCE LANGUAGES\""},{"id":"http://arxiv.org/abs/2407.09849v1","updated":"2024-07-13T11:11:41Z","published":"2024-07-13T11:11:41Z","title":"Text-Based Detection of On-Hold Scripts in Contact Center Calls","summary":"  Average hold time is a concern for call centers because it affects customer\nsatisfaction. Contact centers should instruct their agents to use special\non-hold scripts to maintain positive interactions with clients. This study\npresents a natural language processing model that detects on-hold phrases in\ncustomer service calls transcribed by automatic speech recognition technology.\nThe task of finding hold scripts in dialogue was formulated as a multiclass\ntext classification problem with three mutually exclusive classes: scripts for\nputting a client on hold, scripts for returning to a client, and phrases\nirrelevant to on-hold scripts. We collected an in-house dataset of calls and\nlabeled each dialogue turn in each call. We fine-tuned RuBERT on the dataset by\nexploring various hyperparameter sets and achieved high model performance. The\ndeveloped model can help agent monitoring by providing a way to check whether\nan agent follows predefined on-hold scripts.\n","authors":["Dmitrii Galimzianov","Viacheslav Vyshegorodtsev"],"pdf_url":"https://arxiv.org/pdf/2407.09849v1.pdf","comment":"9 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.08370v3","updated":"2024-07-13T11:01:14Z","published":"2024-03-13T09:31:50Z","title":"SMART: Submodular Data Mixture Strategy for Instruction Tuning","summary":"  Instruction Tuning involves finetuning a language model on a collection of\ninstruction-formatted datasets in order to enhance the generalizability of the\nmodel to unseen tasks. Studies have shown the importance of balancing different\ntask proportions during finetuning, but finding the right balance remains\nchallenging. Unfortunately, there's currently no systematic method beyond\nmanual tuning or relying on practitioners' intuition. In this paper, we\nintroduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a\nnovel data mixture strategy which makes use of a submodular function to assign\nimportance scores to tasks which are then used to determine the mixture\nweights. Given a fine-tuning budget, SMART redistributes the budget among tasks\nand selects non-redundant samples from each task. Experimental results\ndemonstrate that SMART significantly outperforms traditional methods such as\nexamples proportional mixing and equal mixing. Furthermore, SMART facilitates\nthe creation of data mixtures based on a few representative subsets of tasks\nalone and through task pruning analysis, we reveal that in a limited budget\nsetting, allocating budget among a subset of representative tasks yields\nsuperior performance compared to distributing the budget among all tasks. The\ncode for reproducing our results is open-sourced at\nhttps://github.com/kowndinya-renduchintala/SMART.\n","authors":["H S V N S Kowndinya Renduchintala","Sumit Bhatia","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2403.08370v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09835v1","updated":"2024-07-13T10:08:55Z","published":"2024-07-13T10:08:55Z","title":"Investigating Low-Rank Training in Transformer Language Models:\n  Efficiency and Scaling Analysis","summary":"  State-of-the-art LLMs often rely on scale with high computational costs,\nwhich has sparked a research agenda to reduce parameter counts and costs\nwithout significantly impacting performance. Our study focuses on\nTransformer-based LLMs, specifically applying low-rank parametrization to the\ncomputationally intensive feedforward networks (FFNs), which are less studied\nthan attention blocks. In contrast to previous works, (i) we explore low-rank\nparametrization at scale, up to 1.3B parameters; (ii) within Transformer\nlanguage models rather than convolutional architectures; and (iii) starting\nfrom training from scratch. Experiments on the large RefinedWeb dataset show\nthat low-rank parametrization is both efficient (e.g., 2.6$\\times$ FFN speed-up\nwith 32\\% parameters) and effective during training. Interestingly, these\nstructured FFNs exhibit steeper scaling curves than the original models.\nMotivated by this finding, we develop the wide and structured networks\nsurpassing the current medium-sized and large-sized Transformer in perplexity\nand throughput performance.\n","authors":["Xiuying Wei","Skander Moalla","Razvan Pascanu","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2407.09835v1.pdf","comment":"Accepted by ICML 2024 Next Generation of Sequence Modeling\n  Architectures Workshop. arXiv admin note: substantial text overlap with\n  arXiv:2406.16450"},{"id":"http://arxiv.org/abs/2407.09823v1","updated":"2024-07-13T09:34:00Z","published":"2024-07-13T09:34:00Z","title":"NativQA: Multilingual Culturally-Aligned Natural Query for LLMs","summary":"  Natural Question Answering (QA) datasets play a crucial role in developing\nand evaluating the capabilities of large language models (LLMs), ensuring their\neffective usage in real-world applications. Despite the numerous QA datasets\nthat have been developed, there is a notable lack of region-specific datasets\ngenerated by native users in their own languages. This gap hinders the\neffective benchmarking of LLMs for regional and cultural specificities. In this\nstudy, we propose a scalable framework, NativQA, to seamlessly construct\nculturally and regionally aligned QA datasets in native languages, for LLM\nevaluation and tuning. Moreover, to demonstrate the efficacy of the proposed\nframework, we designed a multilingual natural QA dataset, MultiNativQA,\nconsisting of ~72K QA pairs in seven languages, ranging from high to extremely\nlow resource, based on queries from native speakers covering 18 topics. We\nbenchmark the MultiNativQA dataset with open- and closed-source LLMs. We made\nboth the framework NativQA and MultiNativQA dataset publicly available for the\ncommunity. (https://nativqa.gitlab.io)\n","authors":["Md. Arid Hasan","Maram Hasanain","Fatema Ahmad","Sahinur Rahman Laskar","Sunaya Upadhyay","Vrunda N Sukhadia","Mucahid Kutlu","Shammur Absar Chowdhury","Firoj Alam"],"pdf_url":"https://arxiv.org/pdf/2407.09823v1.pdf","comment":"LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models"},{"id":"http://arxiv.org/abs/2407.09818v1","updated":"2024-07-13T09:28:44Z","published":"2024-07-13T09:28:44Z","title":"AraFinNLP 2024: The First Arabic Financial NLP Shared Task","summary":"  The expanding financial markets of the Arab world require sophisticated\nArabic NLP tools. To address this need within the banking domain, the Arabic\nFinancial NLP (AraFinNLP) shared task proposes two subtasks: (i) Multi-dialect\nIntent Detection and (ii) Cross-dialect Translation and Intent Preservation.\nThis shared task uses the updated ArBanking77 dataset, which includes about 39k\nparallel queries in MSA and four dialects. Each query is labeled with one or\nmore of a common 77 intents in the banking domain. These resources aim to\nfoster the development of robust financial Arabic NLP, particularly in the\nareas of machine translation and banking chat-bots. A total of 45 unique teams\nregistered for this shared task, with 11 of them actively participated in the\ntest phase. Specifically, 11 teams participated in Subtask 1, while only 1 team\nparticipated in Subtask 2. The winning team of Subtask 1 achieved F1 score of\n0.8773, and the only team submitted in Subtask 2 achieved a 1.667 BLEU score.\n","authors":["Sanad Malaysha","Mo El-Haj","Saad Ezzini","Mohammed Khalilia","Mustafa Jarrar","Sultan Almujaiwel","Ismail Berrada","Houda Bouamor"],"pdf_url":"https://arxiv.org/pdf/2407.09818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09817v1","updated":"2024-07-13T09:28:24Z","published":"2024-07-13T09:28:24Z","title":"Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech\n  Recognition System","summary":"  Multi-talker speech recognition and target-talker speech recognition, both\ninvolve transcription in multi-talker contexts, remain significant challenges.\nHowever, existing methods rarely attempt to simultaneously address both tasks.\nIn this study, we propose a pioneering approach to empower Whisper, which is a\nspeech foundation model, to tackle joint multi-talker and target-talker speech\nrecognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar\nseparator into its encoder to separate mixed embedding for multiple talkers;\n(ii) a Target Talker Identifier is introduced to identify the embedding flow of\nthe target talker on the fly, requiring only three-second enrollment speech as\na cue; (iii) soft prompt tuning for decoder is explored for better task\nadaptation. Our method outperforms previous methods on two- and three-talker\nLibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable\nzero-shot performance on multi-talker ASR on AishellMix Mandarin dataset.\n","authors":["Lingwei Meng","Jiawen Kang","Yuejiao Wang","Zengrui Jin","Xixin Wu","Xunying Liu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2407.09817v1.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.09816v1","updated":"2024-07-13T09:22:33Z","published":"2024-07-13T09:22:33Z","title":"MaskMoE: Boosting Token-Level Learning via Routing Mask in\n  Mixture-of-Experts","summary":"  Scaling model capacity enhances its capabilities but significantly increases\ncomputation. Mixture-of-Experts models (MoEs) address this by allowing model\ncapacity to scale without substantially increasing training or inference costs.\nDespite their promising results, MoE models encounter several challenges.\nPrimarily, the dispersion of training tokens across multiple experts can lead\nto underfitting, particularly for infrequent tokens. Additionally, while fixed\nrouting mechanisms can mitigate this issue, they compromise on the diversity of\nrepresentations. In this paper, we propose MaskMoE, a method designed to\nenhance token-level learning by employing a routing masking technique within\nthe Mixture-of-Experts model. MaskMoE is capable of maintaining representation\ndiversity while achieving more comprehensive training. Experimental results\ndemonstrate that our method outperforms previous dominant Mixture-of-Experts\nmodels in both perplexity (PPL) and downstream tasks.\n","authors":["Zhenpeng Su","Zijia Lin","Xue Bai","Xing Wu","Yizhe Xiong","Haoran Lian","Guangyuan Ma","Hui Chen","Guiguang Ding","Wei Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2407.09816v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.02707v2","updated":"2024-07-13T08:52:50Z","published":"2023-03-05T16:17:56Z","title":"Industry Risk Assessment via Hierarchical Financial Data Using Stock\n  Market Sentiment Indicators","summary":"  Risk assessment across industries is paramount for ensuring a robust and\nsustainable economy. While previous studies have relied heavily on official\nstatistics for their accuracy, they often lag behind real-time developments.\nAddressing this gap, our research endeavors to integrate market microstructure\ntheory with AI technologies to refine industry risk predictions. This paper\npresents an approach to analyzing industry trends leveraging real-time stock\nmarket data and generative small language models (SLMs). By enhancing the\ntimeliness of risk assessments and delving into the influence of\nnon-traditional factors such as market sentiment and investor behavior, we\nstrive to develop a more holistic and dynamic risk assessment model. One of the\nkey challenges lies in the inherent noise in raw data, which can compromise the\nprecision of statistical analyses. Moreover, textual data about industry\nanalysis necessitates a deeper understanding facilitated by pre-trained\nlanguage models. To tackle these issues, we propose a dual-pronged approach to\nindustry trend analysis: explicit and implicit analysis. For explicit analysis,\nwe employ a hierarchical data analysis methodology that spans the industry and\nindividual listed company levels. This strategic breakdown helps mitigate the\nimpact of data noise, ensuring a more accurate portrayal of industry dynamics.\nIn parallel, we introduce implicit analysis, where we pre-train an SML to\ninterpret industry trends within the context of current news events. This\napproach leverages the extensive knowledge embedded in the pre-training corpus,\nenabling a nuanced understanding of industry trends and their underlying\ndrivers. Experimental results based on our proposed methodology demonstrate its\neffectiveness in delivering robust industry trend analyses, underscoring its\npotential to revolutionize risk assessment practices across industries.\n","authors":["Hongyin Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.02707v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.10020v1","updated":"2024-07-13T22:33:29Z","published":"2024-07-13T22:33:29Z","title":"Causality extraction from medical text using Large Language Models\n  (LLMs)","summary":"  This study explores the potential of natural language models, including large\nlanguage models, to extract causal relations from medical texts, specifically\nfrom Clinical Practice Guidelines (CPGs). The outcomes causality extraction\nfrom Clinical Practice Guidelines for gestational diabetes are presented,\nmarking a first in the field. We report on a set of experiments using variants\nof BERT (BioBERT, DistilBERT, and BERT) and using Large Language Models (LLMs),\nnamely GPT-4 and LLAMA2. Our experiments show that BioBERT performed better\nthan other models, including the Large Language Models, with an average\nF1-score of 0.72. GPT-4 and LLAMA2 results show similar performance but less\nconsistency. We also release the code and an annotated a corpus of causal\nstatements within the Clinical Practice Guidelines for gestational diabetes.\n","authors":["Seethalakshmi Gopalakrishnan","Luciana Garbayo","Wlodek Zadrozny"],"pdf_url":"https://arxiv.org/pdf/2407.10020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09962v1","updated":"2024-07-13T18:05:07Z","published":"2024-07-13T18:05:07Z","title":"Correlating Power Outage Spread with Infrastructure Interdependencies\n  During Hurricanes","summary":"  Power outages caused by extreme weather events, such as hurricanes, can\nsignificantly disrupt essential services and delay recovery efforts,\nunderscoring the importance of enhancing our infrastructure's resilience. This\nstudy investigates the spread of power outages during hurricanes by analyzing\nthe correlation between the network of critical infrastructure and outage\npropagation. We leveraged datasets from Hurricanemapping.com, the North\nAmerican Energy Resilience Model Interdependency Analysis (NAERM-IA), and\nhistorical power outage data from the Oak Ridge National Laboratory (ORNL)'s\nEAGLE-I system. Our analysis reveals a consistent positive correlation between\nthe extent of critical infrastructure components accessible within a certain\nnumber of steps (k-hop distance) from initial impact areas and the occurrence\nof power outages in broader regions. This insight suggests that understanding\nthe interconnectedness among critical infrastructure elements is key to\nidentifying areas indirectly affected by extreme weather events.\n","authors":["Avishek Bose","Sangkeun Lee","Narayan Bhusal","Supriya Chinthavali"],"pdf_url":"https://arxiv.org/pdf/2407.09962v1.pdf","comment":"IEEE 25th International Conference on Information Reuse and\n  Integration for Data Science (IEEE IRI-2024)"},{"id":"http://arxiv.org/abs/2407.09939v1","updated":"2024-07-13T16:32:16Z","published":"2024-07-13T16:32:16Z","title":"Popular News Always Compete for the User's Attention! POPK: Mitigating\n  Popularity Bias via a Temporal-Counterfactual","summary":"  In news recommendation systems, reducing popularity bias is essential for\ndelivering accurate and diverse recommendations. This paper presents POPK, a\nnew method that uses temporal-counterfactual analysis to mitigate the influence\nof popular news articles. By asking, \"What if, at a given time $t$, a set of\npopular news articles were competing for the user's attention to be clicked?\",\nPOPK aims to improve recommendation accuracy and diversity. We tested POPK on\nthree different language datasets (Japanese, English, and Norwegian) and found\nthat it successfully enhances traditional methods. POPK offers flexibility for\ncustomization to enhance either accuracy or diversity, alongside providing\ndistinct ways of measuring popularity. We argue that popular news articles\nalways compete for attention, even if they are not explicitly present in the\nuser's impression list. POPK systematically eliminates the implicit influence\nof popular news articles during each training step. We combine counterfactual\nreasoning with a temporal approach to adjust the negative sample space,\nrefining understanding of user interests. Our findings underscore how POPK\neffectively enhances the accuracy and diversity of recommended articles while\nalso tailoring the approach to specific needs.\n","authors":["Igor L. R. Azevedo","Toyotaro Suzumura","Yuichiro Yasui"],"pdf_url":"https://arxiv.org/pdf/2407.09939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09747v1","updated":"2024-07-13T02:46:37Z","published":"2024-07-13T02:46:37Z","title":"SocialRec: User Activity Based Post Weighted Dynamic Personalized Post\n  Recommendation System in Social Media","summary":"  User activities can influence their subsequent interactions with a post,\ngenerating interest in the user. Typically, users interact with posts from\nfriends by commenting and using reaction emojis, reflecting their level of\ninterest on social media such as Facebook, Twitter, and Reddit. Our objective\nis to analyze user history over time, including their posts and engagement on\nvarious topics. Additionally, we take into account the user's profile, seeking\nconnections between their activities and social media platforms. By integrating\nuser history, engagement, and persona, we aim to assess recommendation scores\nbased on relevant item sharing by Hit Rate (HR) and the quality of the ranking\nsystem by Normalized Discounted Cumulative Gain (NDCG), where we achieve the\nhighest for NeuMF 0.80 and 0.6 respectively. Our hybrid approach solves the\ncold-start problem when there is a new user, for new items cold-start problem\nwill never occur, as we consider the post category values. To improve the\nperformance of the model during cold-start we introduce collaborative filtering\nby looking for similar users and ranking the users based on the highest\nsimilarity scores.\n","authors":["Ismail Hossain","Sai Puppala","Md Jahangir Alam","Sajedul Talukder"],"pdf_url":"https://arxiv.org/pdf/2407.09747v1.pdf","comment":"This research paper has been accepted in the Social Media Sway:\n  Unraveling the Impact of Social Media on Human Behavior - SMS workshop, to be\n  held in conjunction with the International Conference on Social Networks\n  Analysis and Mining (ASONAM 2024) and will be published in Springer"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.09992v1","updated":"2024-07-13T19:56:31Z","published":"2024-07-13T19:56:31Z","title":"TOP:A New Target-Audience Oriented Content Paraphrase Task","summary":"  Recommendation systems usually recommend the existing contents to different\nusers. However, in comparison to static recommendation methods, a\nrecommendation logic that dynamically adjusts based on user interest\npreferences may potentially attract a larger user base. Thus, we consider\nparaphrasing existing content based on the interests of the users to modify the\ncontent to better align with the preferences of users. In this paper, we\npropose a new task named Target-Audience Oriented Content Paraphrase aims to\ngenerate more customized contents for the target audience. We introduce the\ntask definition and the corresponding framework for the proposed task and the\ncreation of the corresponding datasets. We utilize the Large Language Models\n(LLMs) and Large Vision Models (LVMs) to accomplish the base implementation of\nthe TOP framework and provide the referential baseline results for the proposed\ntask.\n","authors":["Boda Lin","Jiaxin Shi","Haolong Yan","Binghao Tang","Xiaocheng Gong","Si Li"],"pdf_url":"https://arxiv.org/pdf/2407.09992v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2407.09935v1","updated":"2024-07-13T16:09:45Z","published":"2024-07-13T16:09:45Z","title":"LeRF: Learning Resampling Function for Adaptive and Efficient Image\n  Interpolation","summary":"  Image resampling is a basic technique that is widely employed in daily\napplications, such as camera photo editing. Recent deep neural networks (DNNs)\nhave made impressive progress in performance by introducing learned data\npriors. Still, these methods are not the perfect substitute for interpolation,\ndue to the drawbacks in efficiency and versatility. In this work, we propose a\nnovel method of Learning Resampling Function (termed LeRF), which takes\nadvantage of both the structural priors learned by DNNs and the locally\ncontinuous assumption of interpolation. Specifically, LeRF assigns spatially\nvarying resampling functions to input image pixels and learns to predict the\nhyper-parameters that determine the shapes of these resampling functions with a\nneural network. Based on the formulation of LeRF, we develop a family of\nmodels, including both efficiency-orientated and performance-orientated ones.\nTo achieve interpolation-level efficiency, we adopt look-up tables (LUTs) to\naccelerate the inference of the learned neural network. Furthermore, we design\na directional ensemble strategy and edge-sensitive indexing patterns to better\ncapture local structures. On the other hand, to obtain DNN-level performance,\nwe propose an extension of LeRF to enable it in cooperation with pre-trained\nupsampling models for cascaded resampling. Extensive experiments show that the\nefficiency-orientated version of LeRF runs as fast as interpolation,\ngeneralizes well to arbitrary transformations, and outperforms interpolation\nsignificantly, e.g., up to 3dB PSNR gain over Bicubic for x2 upsampling on\nManga109. Besides, the performance-orientated version of LeRF reaches\ncomparable performance with existing DNNs at much higher efficiency, e.g., less\nthan 25% running time on a desktop GPU.\n","authors":["Jiacheng Li","Chang Chen","Fenglong Song","Youliang Yan","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2407.09935v1.pdf","comment":"Code: https://github.com/ddlee-cn/LeRF-PyTorch"},{"id":"http://arxiv.org/abs/2407.09801v1","updated":"2024-07-13T08:20:37Z","published":"2024-07-13T08:20:37Z","title":"IoT-LM: Large Multisensory Language Models for the Internet of Things","summary":"  The Internet of Things (IoT) network integrating billions of smart physical\ndevices embedded with sensors, software, and communication technologies is a\ncritical and rapidly expanding component of our modern world. The IoT ecosystem\nprovides a rich source of real-world modalities such as motion, thermal,\ngeolocation, imaging, depth, sensors, and audio to recognize the states of\nhumans and physical objects. Machine learning presents a rich opportunity to\nautomatically process IoT data at scale, enabling efficient inference for\nunderstanding human wellbeing, controlling physical devices, and\ninterconnecting smart cities. To realize this potential, we introduce IoT-LM,\nan open-source large multisensory language model tailored for the IoT\necosystem. IoT-LM is enabled by two technical contributions: the first is\nMultiIoT, the most expansive unified IoT dataset to date, encompassing over\n1.15 million samples from 12 modalities and 8 tasks prepared for multisensory\npre-training and instruction-tuning. The second is a new multisensory multitask\nadapter layer to condition pre-trained large language models on multisensory\nIoT data. Not only does IoT-LM yield substantial improvements on 8 supervised\nIoT classification tasks, but it also demonstrates new interactive\nquestion-answering, reasoning, and dialog capabilities conditioned on IoT\nsensors. We release IoT-LM's data sources and new multisensory language\nmodeling framework.\n","authors":["Shentong Mo","Russ Salakhutdinov","Louis-Philippe Morency","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2407.09801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09774v1","updated":"2024-07-13T05:02:42Z","published":"2024-07-13T05:02:42Z","title":"TemporalStory: Enhancing Consistency in Story Visualization using\n  Spatial-Temporal Attention","summary":"  Story visualization presents a challenging task in text-to-image generation,\nrequiring not only the rendering of visual details from text prompt but also\nensuring consistency across images. Recently, most approaches address\ninconsistency problem using an auto-regressive manner conditioned on previous\nimage-sentence pairs. However, they overlook the fact that story context is\ndispersed across all sentences. The auto-regressive approach fails to encode\ninformation from susequent image-sentence pairs, thus unable to capture the\nentirety of the story context. To address this, we introduce TemporalStory,\nleveraging Spatial-Temporal attention to model complex spatial and temporal\ndependencies in images, enabling the generation of coherent images based on a\ngiven storyline. In order to better understand the storyline context, we\nintroduce a text adapter capable of integrating information from other\nsentences into the embedding of the current sentence. Additionally, to utilize\nscene changes between story images as guidance for the model, we propose the\nStoryFlow Adapter to measure the degree of change between images. Through\nextensive experiments on two popular benchmarks, PororoSV and FlintstonesSV,\nour TemporalStory outperforms the previous state-of-the-art in both story\nvisualization and story continuation tasks.\n","authors":["Sixiao Zheng","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2407.09774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06524v3","updated":"2024-07-13T04:52:32Z","published":"2024-07-09T03:32:00Z","title":"Improving Speech Enhancement by Integrating Inter-Channel and Band\n  Features with Dual-branch Conformer","summary":"  Recent speech enhancement methods based on convolutional neural networks\n(CNNs) and transformer have been demonstrated to efficaciously capture\ntime-frequency (T-F) information on spectrogram. However, the correlation of\neach channels of speech features is failed to explore. Theoretically, each\nchannel map of speech features obtained by different convolution kernels\ncontains information with different scales demonstrating strong correlations.\nTo fill this gap, we propose a novel dual-branch architecture named\nchannel-aware dual-branch conformer (CADB-Conformer), which effectively\nexplores the long range time and frequency correlations among different\nchannels, respectively, to extract channel relation aware time-frequency\ninformation. Ablation studies conducted on DNS-Challenge 2020 dataset\ndemonstrate the importance of channel feature leveraging while showing the\nsignificance of channel relation aware T-F information for speech enhancement.\nExtensive experiments also show that the proposed model achieves superior\nperformance than recent methods with an attractive computational costs.\n","authors":["Jizhen Li","Xinmeng Xu","Weiping Tu","Yuhong Yang","Rong Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.06524v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09766v1","updated":"2024-07-13T04:15:02Z","published":"2024-07-13T04:15:02Z","title":"User Digital Twin-Driven Video Streaming for Customized Preferences and\n  Adaptive Transcoding","summary":"  In the rapidly evolving field of multimedia services, video streaming has\nbecome increasingly prevalent, demanding innovative solutions to enhance user\nexperience and system efficiency. This paper introduces a novel approach that\nintegrates user digital twins-a dynamic digital representation of a user's\npreferences and behaviors-with traditional video streaming systems. We explore\nthe potential of this integration to dynamically adjust video preferences and\noptimize transcoding processes according to real-time data. The methodology\nleverages advanced machine learning algorithms to continuously update the\nuser's digital twin, which in turn informs the transcoding service to adapt\nvideo parameters for optimal quality and minimal buffering. Experimental\nresults show that our approach not only improves the personalization of content\ndelivery but also significantly enhances the overall efficiency of video\nstreaming services by reducing bandwidth usage and improving video playback\nquality. The implications of such advancements suggest a shift towards more\nadaptive, user-centric multimedia services, potentially transforming how video\ncontent is consumed and delivered.\n","authors":["Stephen Jimmy","Kalkidan Berhane","Kevin Muhammad"],"pdf_url":"https://arxiv.org/pdf/2407.09766v1.pdf","comment":null}]}}